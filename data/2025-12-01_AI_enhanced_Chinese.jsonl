{"id": "2511.21910", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21910", "abs": "https://arxiv.org/abs/2511.21910", "authors": ["Haoxuan Shan", "Cong Guo", "Chiyue Wei", "Feng Cheng", "Junyao Zhang", "Hai", "Li", "Yiran Chen"], "title": "Platinum: Path-Adaptable LUT-Based Accelerator Tailored for Low-Bit Weight Matrix Multiplication", "comment": null, "summary": "The rapid scaling of large language models demands more efficient hardware. Quantization offers a promising trade-off between efficiency and performance. With ultra-low-bit quantization, there are abundant opportunities for results reuse, and thus it can be boosted with lookup tables (LUTs) based acceleration. However, existing LUT-based methods suffer from computation and hardware overheads for LUT construction, and rely solely on bit-serial computation, which is suboptimal for ternary-weight networks. We propose Platinum, a lightweight ASIC accelerator for integer weight mixed-precision matrix multiplication (mpGEMM) using LUTs. Platinum reduces LUT construction overhead via offline-generated construction paths and supports both general bit-serial and optimized ternary-weight execution through adaptive path switching. On BitNet b1.58-3B, Platinum achieves up to 73.6x, 4.09x, and 2.15x speedups over SpikingEyeriss, Prosperity, and 16-thread T-MAC (CPU), respectively, along with energy reductions of 32.4x, 3.23x, and 20.9x, all within a 0.96mm2 chip area. This demonstrates the potential of LUT-based ASICs as efficient, scalable solutions for ultra-low-bit neural networks on edge platforms.", "AI": {"tldr": "Platinum\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7ASIC\u52a0\u901f\u5668\uff0c\u4f7f\u7528\u67e5\u627e\u8868\u52a0\u901f\u6574\u6570\u6743\u91cd\u6df7\u5408\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u751f\u6210\u6784\u9020\u8def\u5f84\u964d\u4f4e\u5f00\u9500\uff0c\u652f\u6301\u6bd4\u7279\u4e32\u884c\u548c\u4e09\u503c\u6743\u91cd\u81ea\u9002\u5e94\u5207\u6362\uff0c\u5728BitNet b1.58-3B\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u548c\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u6269\u5c55\u9700\u8981\u66f4\u9ad8\u6548\u786c\u4ef6\uff0c\u8d85\u4f4e\u4f4d\u91cf\u5316\u63d0\u4f9b\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u67e5\u627e\u8868\u7684\u65b9\u6cd5\u5b58\u5728\u6784\u9020\u5f00\u9500\u5927\u3001\u4ec5\u4f9d\u8d56\u6bd4\u7279\u4e32\u884c\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u5bf9\u4e09\u503c\u6743\u91cd\u7f51\u7edc\u4e0d\u6700\u4f18\u3002", "method": "\u63d0\u51faPlatinum\u8f7b\u91cf\u7ea7ASIC\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u79bb\u7ebf\u751f\u6210\u7684\u6784\u9020\u8def\u5f84\u51cf\u5c11\u67e5\u627e\u8868\u6784\u5efa\u5f00\u9500\uff0c\u652f\u6301\u901a\u7528\u6bd4\u7279\u4e32\u884c\u548c\u4f18\u5316\u7684\u4e09\u503c\u6743\u91cd\u6267\u884c\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u8def\u5f84\u5207\u6362\u673a\u5236\u3002", "result": "\u5728BitNet b1.58-3B\u4e0a\uff0c\u76f8\u6bd4SpikingEyeriss\u3001Prosperity\u548c16\u7ebf\u7a0bT-MAC(CPU)\u5206\u522b\u5b9e\u73b073.6\u500d\u30014.09\u500d\u548c2.15\u500d\u52a0\u901f\uff0c\u80fd\u6548\u63d0\u534732.4\u500d\u30013.23\u500d\u548c20.9\u500d\uff0c\u82af\u7247\u9762\u79ef\u4ec50.96mm\u00b2\u3002", "conclusion": "\u57fa\u4e8e\u67e5\u627e\u8868\u7684ASIC\u662f\u8fb9\u7f18\u5e73\u53f0\u4e0a\u8d85\u4f4e\u4f4d\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0cPlatinum\u5c55\u793a\u4e86\u5176\u5728\u52a0\u901f\u6df7\u5408\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.22166", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22166", "abs": "https://arxiv.org/abs/2511.22166", "authors": ["Shuai Dong", "Junyi Yang", "Ye Ke", "Hongyang Shang", "Arindam Basu"], "title": "CADC: Crossbar-Aware Dendritic Convolution for Efficient In-memory Computing", "comment": null, "summary": "Convolutional neural networks (CNNs) are computationally intensive and often accelerated using crossbar-based in-memory computing (IMC) architectures. However, large convolutional layers must be partitioned across multiple crossbars, generating numerous partial sums (psums) that require additional buffer, transfer, and accumulation, thus introducing significant system-level overhead. Inspired by dendritic computing principles from neuroscience, we propose crossbar-aware dendritic convolution (CADC), a novel approach that dramatically increases sparsity in psums by embedding a nonlinear dendritic function (zeroing negative values) directly within crossbar computations. Experimental results demonstrate that CADC significantly reduces psums, eliminating 80% in LeNet-5 on MNIST, 54% in ResNet-18 on CIFAR-10, 66% in VGG-16 on CIFAR-100, and up to 88% in spiking neural networks (SNN) on the DVS Gesture dataset. The induced sparsity from CADC provides two key benefits: (1) enabling zero-compression and zero-skipping, thus reducing buffer and transfer overhead by 29.3% and accumulation overhead by 47.9%; (2) minimizing ADC quantization noise accumulation, resulting in small accuracy degradation - only 0.01% for LeNet-5, 0.1% for ResNet-18, 0.5% for VGG-16, and 0.9% for SNN. Compared to vanilla convolution (vConv), CADC exhibits accuracy changes ranging from +0.11% to +0.19% for LeNet-5, -0.04% to -0.27% for ResNet-18, +0.99% to +1.60% for VGG-16, and -0.57% to +1.32% for SNN, across crossbar sizes from 64x64 to 256x256. Ultimately, a SRAM-based IMC implementation of CADC achieves 2.15 TOPS and 40.8 TOPS/W for ResNet-18 (4/2/4b), realizing an 11x-18x speedup and 1.9x-22.9x improvement in energy efficiency compared to existing IMC accelerators.", "AI": {"tldr": "\u63d0\u51faCADC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u6811\u7a81\u51fd\u6570\u5d4c\u5165\u5b58\u5185\u8ba1\u7b97\u4ea4\u53c9\u9635\u5217\uff0c\u5927\u5e45\u51cf\u5c11\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u90e8\u5206\u548c\u6570\u91cf\uff0c\u964d\u4f4e\u7cfb\u7edf\u5f00\u9500\u5e76\u63d0\u5347\u80fd\u6548", "motivation": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u4e2d\u9700\u8981\u8de8\u591a\u4e2a\u4ea4\u53c9\u9635\u5217\u5206\u533a\uff0c\u4ea7\u751f\u5927\u91cf\u90e8\u5206\u548c\uff0c\u5bfc\u81f4\u7f13\u51b2\u3001\u4f20\u8f93\u548c\u7d2f\u52a0\u7684\u7cfb\u7edf\u7ea7\u5f00\u9500\u663e\u8457", "method": "\u63d0\u51fa\u4ea4\u53c9\u9635\u5217\u611f\u77e5\u7684\u6811\u7a81\u5377\u79ef\uff08CADC\uff09\uff0c\u5728\u4ea4\u53c9\u9635\u5217\u8ba1\u7b97\u4e2d\u76f4\u63a5\u5d4c\u5165\u975e\u7ebf\u6027\u6811\u7a81\u51fd\u6570\uff08\u5c06\u8d1f\u503c\u5f52\u96f6\uff09\uff0c\u5927\u5e45\u589e\u52a0\u90e8\u5206\u548c\u7684\u7a00\u758f\u6027", "result": "CADC\u663e\u8457\u51cf\u5c11\u90e8\u5206\u548c\u6570\u91cf\uff1aLeNet-5\u51cf\u5c1180%\uff0cResNet-18\u51cf\u5c1154%\uff0cVGG-16\u51cf\u5c1166%\uff0cSNN\u51cf\u5c1188%\uff1bSRAM\u5b58\u5185\u8ba1\u7b97\u5b9e\u73b0\u8fbe\u52302.15 TOPS\u548c40.8 TOPS/W\uff0c\u76f8\u6bd4\u73b0\u6709\u52a0\u901f\u5668\u5b9e\u73b011-18\u500d\u52a0\u901f\u548c1.9-22.9\u500d\u80fd\u6548\u63d0\u5347", "conclusion": "CADC\u901a\u8fc7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u6811\u7a81\u8ba1\u7b97\u539f\u7406\uff0c\u6709\u6548\u89e3\u51b3\u5b58\u5185\u8ba1\u7b97\u4e2d\u7684\u90e8\u5206\u548c\u5f00\u9500\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u6548"}}
{"id": "2511.22267", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22267", "abs": "https://arxiv.org/abs/2511.22267", "authors": ["Yuyang Zou", "Youwei Xiao", "Yansong Xu", "Chenyun Yin", "Yuhao Luo", "Yitian Sun", "Ruifan Xu", "Renze Chen", "Yun Liang"], "title": "Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR", "comment": null, "summary": "Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.", "AI": {"tldr": "Aquas\u662f\u4e00\u4e2a\u57fa\u4e8eMLIR\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u7a81\u53d1DMA\u5f15\u64ce\u548c\u9ad8\u7ea7HLS\u4f18\u5316\u6765\u63d0\u5347RISC-V ASIP\u6027\u80fd\uff0c\u540c\u65f6\u91c7\u7528\u57fa\u4e8ee-graph\u7684\u53ef\u91cd\u5b9a\u5411\u7f16\u8bd1\u65b9\u6cd5\uff0c\u5728\u70b9\u4e91\u5904\u7406\u548cLLM\u63a8\u7406\u7b49\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad89.27\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90RISC-V\u751f\u6001\u7cfb\u7edf\u4e2d\u7684ASIP\u6846\u67b6\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u786c\u4ef6\u5408\u6210\u80fd\u529b\u53d7\u9650\u548c\u7f16\u8bd1\u5668\u652f\u6301\u50f5\u5316\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325ASIP\u5728\u7279\u5b9a\u5e94\u7528\u4e2d\u7684\u4e13\u4e1a\u5316\u6f5c\u529b\u3002", "method": "\u63d0\u51faAquas\u6846\u67b6\uff1a1\uff09\u786c\u4ef6\u65b9\u9762\uff1a\u96c6\u6210\u7a81\u53d1DMA\u5f15\u64ce\u5b9e\u73b0\u5feb\u901f\u5185\u5b58\u8bbf\u95ee\uff0c\u91c7\u7528\u9ad8\u7ea7HLS\u4f18\u5316\uff1b2\uff09\u7f16\u8bd1\u5668\u65b9\u9762\uff1a\u63d0\u51fa\u57fa\u4e8ee-graph\u7684\u53ef\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u5305\u542b\u65b0\u9896\u7684\u6307\u4ee4\u5339\u914d\u5f15\u64ce\u3002", "result": "\u5728\u70b9\u4e91\u5904\u7406\u548cLLM\u63a8\u7406\u7b49\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad89.27\u500d\u7684\u6027\u80fd\u52a0\u901f\uff0c\u663e\u8457\u63d0\u5347\u4e86ASIP\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "Aquas\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RISC-V ASIP\u6846\u67b6\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u7279\u5b9a\u5e94\u7528\u5904\u7406\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22348", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22348", "abs": "https://arxiv.org/abs/2511.22348", "authors": ["Shuao Jia", "Zichao Ling", "Chen Bai", "Kang Zhao", "Jianwang Zhai"], "title": "FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators", "comment": "7 pages, 4 figures", "summary": "Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.", "AI": {"tldr": "FADiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5bfb\u627eDNN\u5728\u5f20\u91cf\u52a0\u901f\u5668\u4e0a\u7684\u6700\u4f18\u5c42\u5185\u6620\u5c04\u548c\u5c42\u95f4\u878d\u5408\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5728\u5f20\u91cf\u52a0\u901f\u5668\u4e0a\u9ad8\u6548\u90e8\u7f72DNN\uff08\u5982LLM\uff09\u5bf9\u6700\u5927\u5316\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5c42\u5185\u6620\u5c04\u548c\u5c42\u95f4\u878d\u5408\u7684\u590d\u6742\u4ea4\u4e92\u5f62\u6210\u4e86\u5de8\u5927\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u9996\u5148\u6784\u5efa\u7edf\u4e00\u4e14\u53ef\u5fae\u7684\u5206\u6790\u6210\u672c\u6a21\u578b\uff0c\u51c6\u786e\u9884\u6d4b\u5355\u5c42\u6620\u5c04\u548c\u5404\u79cd\u5c42\u878d\u5408\u7b56\u7565\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\uff1b\u7136\u540e\u901a\u8fc7\u5c06\u79bb\u6563\u7ea6\u675f\u7f16\u7801\u5230\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u9ad8\u6548\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u786e\u5b9a\u6620\u5c04\u548c\u878d\u5408\u7684\u6700\u4f18\u8054\u5408\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eFADiff\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u80fd\u8017\u548c\u5ef6\u8fdf\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4f18\u5316\u6548\u679c\u3002", "conclusion": "FADiff\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u9ad8\u8d28\u91cf\u7684DNN\u90e8\u7f72\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f20\u91cf\u52a0\u901f\u5668\u4e0aDNN\u63a8\u7406\u7684\u4f18\u5316\u6311\u6218\u3002"}}
{"id": "2511.22551", "categories": ["cs.AR", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.22551", "abs": "https://arxiv.org/abs/2511.22551", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asad"], "title": "3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison", "comment": null, "summary": "Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3RSeT\u7684\u4f4e\u6210\u672c\u65b9\u6848\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6807\u7b7e\u6bd4\u8f83\u6765\u51cf\u5c11STT-MRAM\u7f13\u5b58\u4e2d\u7684\u8bfb\u53d6\u5e72\u6270\u9519\u8bef\u7387\uff0c\u5c06\u6807\u7b7e\u9635\u5217\u7684\u8bfb\u53d6\u5e72\u6270\u7387\u964d\u4f4e71.8%\uff0cMTTF\u63d0\u9ad83.6\u500d\uff0c\u80fd\u8017\u964d\u4f4e62.1%\uff0c\u6027\u80fd\u4e0d\u53d7\u5f71\u54cd\u4e14\u9762\u79ef\u5f00\u9500\u5c0f\u4e8e0.4%\u3002", "motivation": "STT-MRAM\u4f5c\u4e3a\u7247\u4e0a\u7f13\u5b58\u5b58\u50a8\u5668\u6700\u6709\u5e0c\u671b\u7684SRAM\u66ff\u4ee3\u54c1\uff0c\u867d\u7136\u5177\u6709\u4f4e\u6cc4\u6f0f\u529f\u8017\u3001\u9ad8\u5bc6\u5ea6\u3001\u6297\u8f90\u5c04\u548c\u975e\u6613\u5931\u6027\u7b49\u4f18\u70b9\uff0c\u4f46\u5728\u8bfb\u53d6\u64cd\u4f5c\u4e2d\u4f1a\u53d1\u751f\u65e0\u610f\u7684\u4f4d\u7ffb\u8f6c\uff08\u8bfb\u53d6\u5e72\u6270\u9519\u8bef\uff09\uff0c\u8fd9\u662f\u4e25\u91cd\u7684\u53ef\u9760\u6027\u6311\u6218\u3002\u7279\u522b\u662f\u5728\u7f13\u5b58\u96c6\u5408\u4e2d\u540c\u65f6\u8bbf\u95ee\u6240\u6709\u6807\u7b7e\u8fdb\u884c\u5e76\u884c\u6bd4\u8f83\u64cd\u4f5c\uff0c\u662f\u8bfb\u53d6\u5e72\u6270\u9519\u8bef\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u8fd9\u4e00\u95ee\u9898\u5728\u5148\u524d\u5de5\u4f5c\u4e2d\u672a\u5f97\u5230\u89e3\u51b3\u3002", "method": "\u63d0\u51fa3RSeT\u65b9\u6848\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6807\u7b7e\u6bd4\u8f83\u6765\u51cf\u5c11\u8bfb\u53d6\u5e72\u6270\u7387\u3002\u8be5\u65b9\u6cd5\u4e3b\u52a8\u7981\u7528\u90a3\u4e9b\u6ca1\u6709\u547d\u4e2d\u673a\u4f1a\u7684\u6807\u7b7e\uff0c\u5229\u7528\u6bcf\u4e2a\u8bbf\u95ee\u8bf7\u6c42\u4e2d\u6807\u7b7e\u7684\u4f4e\u6709\u6548\u4f4d\u6765\u9884\u6d4b\u54ea\u4e9b\u6807\u7b7e\u4e0d\u9700\u8981\u8bfb\u53d6\uff0c\u4ece\u800c\u6d88\u9664\u5927\u90e8\u5206\u6807\u7b7e\u8bfb\u53d6\u64cd\u4f5c\u3002", "result": "\u4f7f\u7528gem5\u5168\u7cfb\u7edf\u5468\u671f\u7cbe\u786e\u6a21\u62df\u5668\u8bc4\u4f30\u663e\u793a\uff1a3RSeT\u5c06\u6807\u7b7e\u9635\u5217\u7684\u8bfb\u53d6\u5e72\u6270\u7387\u964d\u4f4e71.8%\uff0c\u5e73\u5747\u6545\u969c\u65f6\u95f4\uff08MTTF\uff09\u63d0\u9ad83.6\u500d\uff0c\u80fd\u8017\u964d\u4f4e62.1%\uff0c\u6027\u80fd\u4e0d\u53d7\u5f71\u54cd\uff0c\u9762\u79ef\u5f00\u9500\u5c0f\u4e8e0.4%\u3002", "conclusion": "3RSeT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4eSTT-MRAM\u7f13\u5b58\u4e2d\u7684\u8bfb\u53d6\u5e72\u6270\u9519\u8bef\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u80fd\u8017\uff0c\u4e14\u5bf9\u6027\u80fd\u548c\u9762\u79ef\u5f71\u54cd\u6781\u5c0f\uff0c\u4e3aSTT-MRAM\u7f13\u5b58\u7684\u53ef\u9760\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u9014\u5f84\u3002"}}
{"id": "2511.22889", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22889", "abs": "https://arxiv.org/abs/2511.22889", "authors": ["Fang Li"], "title": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference", "comment": "Code and data can be found here: https://github.com/fanglioc/ita-fpga-prototype", "summary": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.", "AI": {"tldr": "\u63d0\u51faITA\u67b6\u6784\uff0c\u5c06LLM\u6743\u91cd\u7f16\u7801\u5230ASIC\u7269\u7406\u7535\u8def\u4e2d\uff0c\u6d88\u9664\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\uff0c\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u4e2d\u7684\"\u5185\u5b58\u5899\"\u95ee\u9898", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d88\u8d39\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u53d7\u5230\"\u5185\u5b58\u5899\"\u9650\u5236\uff0c\u5373\u4eceDRAM\u83b7\u53d6\u6a21\u578b\u6743\u91cd\u9700\u8981\u5de8\u5927\u7684\u5e26\u5bbd\u548c\u80fd\u8017\u6210\u672c\u3002\u5f53\u524d\u67b6\u6784\u5c06\u6a21\u578b\u6743\u91cd\u89c6\u4e3a\u53ef\u53d8\u8f6f\u4ef6\u6570\u636e\uff0c\u4e3a\u4fdd\u6301\u901a\u7528\u53ef\u7f16\u7a0b\u6027\u4ed8\u51fa\u4e86\u5de8\u5927\u80fd\u8017\u4ee3\u4ef7\u3002", "method": "\u63d0\u51fa\u4e0d\u53ef\u53d8\u5f20\u91cf\u67b6\u6784\uff08ITA\uff09\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u89c6\u4e3a\u7269\u7406\u7535\u8def\u62d3\u6251\u800c\u975e\u6570\u636e\uff0c\u901a\u8fc7\u5c06\u53c2\u6570\u76f4\u63a5\u7f16\u7801\u5230\u6210\u719f\u8282\u70b9ASIC\uff0828nm/40nm\uff09\u7684\u91d1\u5c5e\u4e92\u8fde\u548c\u903b\u8f91\u4e2d\uff0c\u5b8c\u5168\u6d88\u9664\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u3002\u91c7\u7528\"\u5206\u88c2\u5927\u8111\"\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4e3b\u673aCPU\u7ba1\u7406\u52a8\u6001KV\u7f13\u5b58\u64cd\u4f5c\uff0cITA ASIC\u4f5c\u4e3a\u65e0\u72b6\u6001\u7684ROM\u5d4c\u5165\u5f0f\u6570\u636e\u6d41\u5f15\u64ce\u3002", "result": "ITA\u67b6\u6784\u901a\u8fc7\u5c06\u6743\u91cd\u7269\u7406\u7f16\u7801\u5230\u7535\u8def\u4e2d\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u5185\u5b58\u8bbf\u95ee\u74f6\u9888\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "conclusion": "ITA\u4ee3\u8868\u4e86\u4e00\u79cd\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u4ece\u8f6f\u4ef6\u6570\u636e\u91cd\u65b0\u5b9a\u4e49\u4e3a\u786c\u4ef6\u7535\u8def\u62d3\u6251\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u67b6\u6784\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.23011", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.23011", "abs": "https://arxiv.org/abs/2511.23011", "authors": ["Yanjing Wang", "Lizhou Wu", "Sunfeng Gao", "Yibo Tang", "Junhui Luo", "Zicong Wang", "Yang Ou", "Dezun Dong", "Nong Xiao", "Mingche Lai"], "title": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation", "comment": "Accepted by HPCA 2026", "summary": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.", "AI": {"tldr": "Cohet\uff1a\u9996\u4e2a\u57fa\u4e8eCXL\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u5f02\u6784\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8ba1\u7b97\u4e0e\u5185\u5b58\u8d44\u6e90\uff0c\u6784\u5efaCPU\u548cXPU\u8ba1\u7b97\u6c60\u5171\u4eab\u7edf\u4e00\u5185\u5b58\u6c60\uff0c\u663e\u8457\u63d0\u5347\u5f02\u6784\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8ePCIe\u7684\u5f02\u6784\u8ba1\u7b97\u7cfb\u7edf\u5b58\u5728\u7ec6\u7c92\u5ea6\u4e3b\u673a-\u8bbe\u5907\u4ea4\u4e92\u6548\u7387\u4f4e\u4e0b\u548c\u7f16\u7a0b\u6a21\u578b\u590d\u6742\u7684\u95ee\u9898\u3002CXL\u4f5c\u4e3a\u65b0\u5174\u7f13\u5b58\u4e00\u81f4\u6027\u4e92\u8fde\u6807\u51c6\uff0c\u867d\u7136\u6709\u671b\u4ece\u6839\u672c\u4e0a\u6539\u53d8CPU\u4e0eXPU\u7684\u534f\u4f5c\u8ba1\u7b97\u6a21\u5f0f\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u53d7\u9650\u4e8e\u5e73\u53f0\u7a00\u7f3a\u3001\u751f\u6001\u7cfb\u7edf\u4e0d\u6210\u719f\u548c\u5e94\u7528\u524d\u666f\u4e0d\u660e\u6717\u3002", "method": "\u63d0\u51faCohet\u6846\u67b6\uff0c\u5c06\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u89e3\u8026\uff0c\u5f62\u6210\u72ec\u7acb\u7684CPU\u548cXPU\u8ba1\u7b97\u6c60\uff0c\u5171\u4eab\u7edf\u4e00\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u5185\u5b58\u6c60\u3002\u901a\u8fc7\u6807\u51c6malloc/mmap\u63a5\u53e3\u5411CPU\u548cXPU\u8ba1\u7b97\u7ebf\u7a0b\u66b4\u9732\u5185\u5b58\uff0c\u7531\u64cd\u4f5c\u7cfb\u7edf\u667a\u80fd\u7ba1\u7406\u5f02\u6784\u8d44\u6e90\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u5168\u7cfb\u7edf\u5468\u671f\u7ea7\u6a21\u62df\u5668SimCXL\uff0c\u652f\u6301\u6240\u6709CXL\u5b50\u534f\u8bae\u548c\u8bbe\u5907\u7c7b\u578b\u5efa\u6a21\u3002", "result": "CXL.cache\u76f8\u6bd4DMA\u4f20\u8f93\u5728\u7f13\u5b58\u884c\u7c92\u5ea6\u4e0a\u5ef6\u8fdf\u964d\u4f4e68%\uff0c\u5e26\u5bbd\u63d0\u534714.4\u500d\u3002\u57fa\u4e8eCohet\u7684\u4e24\u4e2a\u6740\u624b\u7ea7\u5e94\u7528\uff1a\u8fdc\u7a0b\u539f\u5b50\u64cd\u4f5c\uff08RAO\uff09\u548c\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\uff08RPC\uff09\uff0c\u76f8\u6bd4PCIe-NIC\u8bbe\u8ba1\uff0cCXL-NIC\u5728RAO\u5378\u8f7d\u4e0a\u5b9e\u73b05.5-40.2\u500d\u52a0\u901f\uff0cRPC\uff08\u53cd\uff09\u5e8f\u5217\u5316\u5378\u8f7d\u5e73\u5747\u52a0\u901f1.86\u500d\u3002", "conclusion": "Cohet\u4f5c\u4e3a\u9996\u4e2aCXL\u9a71\u52a8\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u5f02\u6784\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u8d44\u6e90\u89e3\u8026\u548c\u7edf\u4e00\u5185\u5b58\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u6784\u8ba1\u7b97\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6613\u7528\u6027\uff0c\u4e3aCXL\u5728\u5f02\u6784\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.23203", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23203", "abs": "https://arxiv.org/abs/2511.23203", "authors": ["Jordi Fornt", "Pau Fontova-Must\u00e9", "Adrian Gras", "Omar Lahyani", "Mart\u00ed Caro", "Jaume Abella", "Francesc Moll", "Josep Altet"], "title": "GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration", "comment": "Presented in the 2025 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED). Conference proceedings pending to be published", "summary": "Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.", "AI": {"tldr": "\u63d0\u51faGAV\u6280\u672f\uff0c\u7ed3\u5408\u6b20\u538b\u548c\u4f4d\u4e32\u884c\u8ba1\u7b97\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u5e76\u8bbe\u8ba1GAVINA\u52a0\u901f\u5668\uff0c\u652f\u6301\u4efb\u610f\u6df7\u5408\u7cbe\u5ea6\u548c\u7075\u6d3b\u6b20\u538b\uff0c\u5728ResNet-18\u4e0a\u5b9e\u73b020%\u80fd\u6548\u63d0\u5347\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "\u7535\u538b\u8fc7\u7f29\u653e\uff08\u6b20\u538b\uff09\u662f\u4e00\u79cd\u6709\u5438\u5f15\u529b\u7684\u8fd1\u4f3c\u6280\u672f\uff0c\u4f46\u7531\u4e8e\u5176\u9ad8\u9519\u8bef\u7387\u963b\u788d\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u6b20\u538b\u52a0\u901f\u5668\u4f9d\u8d568\u4f4d\u7b97\u672f\uff0c\u65e0\u6cd5\u4e0e\u5148\u8fdb\u4f4e\u7cbe\u5ea6\uff08<8\u4f4d\uff09\u67b6\u6784\u7ade\u4e89\u3002", "method": "\u63d0\u51faGAV\u6280\u672f\uff0c\u7ed3\u5408\u6b20\u538b\u548c\u4f4d\u4e32\u884c\u8ba1\u7b97\uff0c\u9009\u62e9\u6027\u5730\u5bf9\u6700\u4f4e\u6709\u6548\u4f4d\u7ec4\u5408\u8fdb\u884c\u6fc0\u8fdb\u964d\u538b\u3002\u57fa\u4e8e\u6b64\u5b9e\u73b0GAVINA\u67b6\u6784\uff0c\u652f\u6301\u4efb\u610f\u6df7\u5408\u7cbe\u5ea6\u548c\u7075\u6d3b\u6b20\u538b\u3002", "result": "GAVINA\u5728\u6700\u6fc0\u8fdb\u914d\u7f6e\u4e0b\u80fd\u6548\u8fbe89 TOP/sW\u3002\u901a\u8fc7\u5efa\u7acbGAVINA\u8bef\u5dee\u6a21\u578b\uff0c\u5728ResNet-18\u4e0a\u5b9e\u73b020%\u80fd\u6548\u63d0\u5347\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "GAV\u6280\u672f\u6210\u529f\u89e3\u51b3\u4e86\u6b20\u538b\u6280\u672f\u7684\u9ad8\u9519\u8bef\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u4f4d\u4e32\u884c\u8ba1\u7b97\u5b9e\u73b0\u7075\u6d3b\u8fd1\u4f3c\uff0cGAVINA\u67b6\u6784\u5728\u80fd\u6548\u548c\u7cbe\u5ea6\u65b9\u9762\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
