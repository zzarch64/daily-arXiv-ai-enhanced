{"id": "2509.02873", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.02873", "abs": "https://arxiv.org/abs/2509.02873", "authors": ["Zhantong Qiu", "Mahyar Samani", "Jason Lowe-Power"], "title": "Portable Targeted Sampling Framework Using LLVM", "comment": null, "summary": "Comprehensive architectural evaluation of full workloads is throttled by slow\nsimulation and per-binary sampling pipelines. We present Nugget, a flexible\nframework for portable sampling across simulators and real hardware, ISAs, and\nlibraries. Nugget operates at the LLVM IR level to perform binary-agnostic\ninterval analysis, then emits lightweight, cross-platform\nexecutables--nuggets--that can be validated on real machines before driving\nsimulation. Across SPEC CPU2017, NPB, and LSMS, Nugget cuts interval-analysis\ncost by orders of magnitude relative to functional simulation (up to ~578X on\nmultithreaded NPB), keeps single-thread overhead low, and enables native-speed\nvalidation of selected samples. Case studies with gem5 show that nuggets\nsupport evaluation of system performance and model accuracy. Nugget makes\nsampling methodology research faster and more portable."}
{"id": "2509.03103", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.03103", "abs": "https://arxiv.org/abs/2509.03103", "authors": ["Abdul Rahoof", "Vivek Chaturvedi", "Muhammad Shafique"], "title": "FastCaps: A Design Methodology for Accelerating Capsule Network on Field Programmable Gate Arrays", "comment": "2023 International Joint Conference on Neural Networks (IJCNN)", "summary": "Capsule Network (CapsNet) has shown significant improvement in understanding\nthe variation in images along with better generalization ability compared to\ntraditional Convolutional Neural Network (CNN). CapsNet preserves spatial\nrelationship among extracted features and apply dynamic routing to efficiently\nlearn the internal connections between capsules. However, due to the capsule\nstructure and the complexity of the routing mechanism, it is non-trivial to\naccelerate CapsNet performance in its original form on Field Programmable Gate\nArray (FPGA). Most of the existing works on CapsNet have achieved limited\nacceleration as they implement only the dynamic routing algorithm on FPGA,\nwhile considering all the processing steps synergistically is important for\nreal-world applications of Capsule Networks. Towards this, we propose a novel\ntwo-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune\nthe network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that\nuses the sum of look-ahead scores of the model parameters. Next, we simplify\nthe nonlinear operations, reorder loops, and parallelize operations of the\nrouting algorithm to reduce CapsNet hardware complexity. To the best of our\nknowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.\nExperimental results on the MNIST and F-MNIST datasets (typical in Capsule\nNetwork community) show that the proposed LAKP approach achieves an effective\ncompression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and\n48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware\ncomplexity of the routing algorithm increases the throughput to 1351 FPS and\n934 FPS respectively. As corroborated by our results, this work enables highly\nperformance-efficient deployment of CapsNets on low-cost FPGA that are popular\nin modern edge devices."}
{"id": "2509.03201", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.03201", "abs": "https://arxiv.org/abs/2509.03201", "authors": ["Abdul Rahoof", "Vivek Chaturvedi", "Mahesh Raveendranatha Panicker", "Muhammad Shafique"], "title": "CapsBeam: Accelerating Capsule Network based Beamformer for Ultrasound Non-Steered Plane Wave Imaging on Field Programmable Gate Array", "comment": null, "summary": "In recent years, there has been a growing trend in accelerating\ncomputationally complex non-real-time beamforming algorithms in ultrasound\nimaging using deep learning models. However, due to the large size and\ncomplexity these state-of-the-art deep learning techniques poses significant\nchallenges when deploying on resource-constrained edge devices. In this work,\nwe propose a novel capsule network based beamformer called CapsBeam, designed\nto operate on raw radio-frequency data and provide an envelope of beamformed\ndata through non-steered plane wave insonification. Experiments on in-vivo\ndata, CapsBeam reduced artifacts compared to the standard Delay-and-Sum (DAS)\nbeamforming. For in-vitro data, CapsBeam demonstrated a 32.31% increase in\ncontrast, along with gains of 16.54% and 6.7% in axial and lateral resolution\ncompared to the DAS. Similarly, in-silico data showed a 26% enhancement in\ncontrast, along with improvements of 13.6% and 21.5% in axial and lateral\nresolution, respectively, compared to the DAS. To reduce the parameter\nredundancy and enhance the computational efficiency, we pruned the model using\nour multi-layer LookAhead Kernel Pruning (LAKP-ML) methodology, achieving a\ncompression ratio of 85% without affecting the image quality. Additionally, the\nhardware complexity of the proposed model is reduced by applying quantization,\nsimplification of non-linear operations, and parallelizing operations. Finally,\nwe proposed a specialized accelerator architecture for the pruned and optimized\nCapsBeam model, implemented on a Xilinx ZU7EV FPGA. The proposed accelerator\nachieved a throughput of 30 GOPS for the convolution operation and 17.4 GOPS\nfor the dynamic routing operation."}
{"id": "2509.03377", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.03377", "abs": "https://arxiv.org/abs/2509.03377", "authors": ["Rui Xie", "Asad Ul Haq", "Linsen Ma", "Yunhua Fang", "Zirak Burzin Engineer", "Liu Liu", "Tong Zhang"], "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via Transparent Near-Data Processing", "comment": null, "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."}
