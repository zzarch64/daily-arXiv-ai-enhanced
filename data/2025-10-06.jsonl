{"id": "2510.02675", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02675", "abs": "https://arxiv.org/abs/2510.02675", "authors": ["Shubham Negi", "Kaushik Roy"], "title": "HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference", "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) has driven a growing\ndemand for efficient inference, particularly in latency-sensitive applications\nsuch as chatbots and personalized assistants. Unlike traditional deep neural\nnetworks, LLM inference proceeds in two distinct phases: the prefill phase,\nwhich processes the full input sequence in parallel, and the decode phase,\nwhich generates tokens sequentially. These phases exhibit highly diverse\ncompute and memory requirements, which makes accelerator design particularly\nchallenging. Prior works have primarily been optimized for high-batch inference\nor evaluated only short input context lengths, leaving the low-batch and long\ncontext regime, which is critical for interactive applications, largely\nunderexplored.\n  We propose HALO, a heterogeneous memory centric accelerator designed for\nthese unique challenges of prefill and decode phases in low-batch LLM\ninference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip\nanalog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further\nimprove the hardware utilization, we introduce a phase-aware mapping strategy\nthat adapts to the distinct demands of the prefill and decode phases. Compute\nbound operations in the prefill phase are mapped to CiM to exploit its high\nthroughput matrix multiplication capability, while memory-bound operations in\nthe decode phase are executed on CiD to benefit from reduced data movement\nwithin DRAM. Additionally, we present an analysis of the performance tradeoffs\nof LLMs under two architectural extremes: a fully CiD and a fully on-chip\nanalog CiM design to highlight the need for a heterogeneous design. We evaluate\nHALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs\nmapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an\nattention-optimized mapping and 2.5x over CENT, a fully CiD based mapping."}
{"id": "2510.02863", "categories": ["cs.AR", "cs.DS", "cs.NA", "math.NA", "quant-ph", "G.1.3; J.2; B.6.1"], "pdf": "https://arxiv.org/pdf/2510.02863", "abs": "https://arxiv.org/abs/2510.02863", "authors": ["D. A. Herrera-Martí", "E. Guthmuller", "J. Fereyre"], "title": "A Hardware Accelerator for the Goemans-Williamson Algorithm", "comment": "Impact of Extended Precision Arithmetic in Interior Point Methods\n  using Conjugate Gradient. 10 pages. Hardware estimates", "summary": "The combinatorial problem Max-Cut has become a benchmark in the evaluation of\nlocal search heuristics for both quantum and classical optimisers. In contrast\nto local search, which only provides average-case performance guarantees, the\nconvex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides\nworst-case guarantees and is therefore suited to both the construction of\nbenchmarks and in applications to performance-critic scenarios.\n  We show how extended floating point precision can be incorporated in\nalgebraic subroutines in convex optimisation, namely in indirect matrix\ninversion methods like Conjugate Gradient, which are used in Interior Point\nMethods in the case of very large problem sizes. Also, an estimate is provided\nof the expected acceleration of the time to solution for a hardware\narchitecture that runs natively on extended precision. Specifically, when using\nindirect matrix inversion methods like Conjugate Gradient, which have lower\ncomplexity than direct methods and are therefore used in very large problems,\nwe see that increasing the internal working precision reduces the time to\nsolution by a factor that increases with the system size."}
{"id": "2510.02990", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.02990", "abs": "https://arxiv.org/abs/2510.02990", "authors": ["Philippe Magalhães", "Virginie Fresse", "Benoît Suffran", "Olivier Alata"], "title": "A Resource-Driven Approach for Implementing CNNs on FPGAs Using Adaptive IPs", "comment": "HiPEAC Workshop on Reconfigurable Computing (WRC), Jan 2025,\n  Barcelona, Spain", "summary": "The increasing demand for real-time, low-latency artificial intelligence\napplications has propelled the use of Field-Programmable Gate Arrays (FPGAs)\nfor Convolutional Neural Network (CNN) implementations. FPGAs offer\nreconfigurability, energy efficiency, and performance advantages over GPUs,\nmaking them suitable for edge devices and embedded systems. This work presents\na novel library of resource-efficient convolution IPs designed to automatically\nadapt to the available FPGA resources. Developed in VHDL, these IPs are\nparameterizable and utilize fixed-point arithmetic for optimal performance.\nFour IPs are introduced, each tailored to specific resource constraints,\noffering flexibility in DSP usage, logic consumption, and precision.\nExperimental results on a Zynq UltraScale+ FPGA highlight the trade-offs\nbetween performance and resource usage. The comparison with recent FPGA-based\nCNN acceleration techniques emphasizes the versatility and independence of this\napproach from specific FPGA architectures or technological advancements. Future\nwork will expand the library to include pooling and activation functions,\nenabling broader applicability and integration into CNN frameworks."}
