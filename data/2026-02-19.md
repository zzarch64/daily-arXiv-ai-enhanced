<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Bit-Width-Aware Design Environment for Few-Shot Learning on Edge AI Hardware](https://arxiv.org/abs/2602.16024)
*R. Kanda,H. L. Blevec,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: 在PYNQ-Z1等小型FPGA SoC上实现实时少样本学习的任意位宽定点数方法，通过FINN框架和节点优化，在保持精度同时将吞吐量提升约2倍


<details>
  <summary>Details</summary>
Motivation: 现有Tensil设计环境限制硬件实现只能使用16或32位定点数，无法灵活调整位宽，限制了在资源受限的小型FPGA SoC上的优化空间

Method: 采用FINN框架支持任意位宽定点数实现，进行两项关键优化：1) 优化Transpose节点解决数据格式不匹配；2) 添加最终reduce mean操作转换为全局平均池化(GAP)的处理

Result: 在CIFAR-10数据集评估中，能够在减少位宽的同时保持与传统实现相同的精度，并实现约2倍的吞吐量提升

Conclusion: 提出的方法成功在资源受限的小型FPGA SoC上实现了实时少样本学习，通过位宽优化显著提升了处理效率，为边缘设备上的AI应用提供了有效解决方案

Abstract: In this study, we propose an implementation methodology of real-time few-shot learning on tiny FPGA SoCs such as the PYNQ-Z1 board with arbitrary fixed-point bit-widths. Tensil-based conventional design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits. To address this, we adopt the FINN framework, enabling implementations with arbitrary bit-widths. Several customizations and minor adjustments are made, including: 1.Optimization of Transpose nodes to resolve data format mismatches, 2.Addition of handling for converting the final reduce mean operation to Global Average Pooling (GAP). These adjustments allow us to reduce the bit-width while maintaining the same accuracy as the conventional realization, and achieve approximately twice the throughput in evaluations using CIFAR-10 dataset.

</details>


### [2] [DARTH-PUM: A Hybrid Processing-Using-Memory Architecture](https://arxiv.org/abs/2602.16075)
*Ryan Wong,Ben Feinberg,Saugata Ghose*

Main category: cs.AR

TL;DR: DARTH-PUM提出了一种通用混合内存计算架构，整合模拟和数字内存计算，通过优化外围电路和编程接口，实现多种应用的高效内存内计算。


<details>
  <summary>Details</summary>
Motivation: 传统模拟内存计算只能执行矩阵向量乘法操作，无法处理非MVM运算。虽然可以通过添加专用CMOS硬件来扩展功能，但这种集成困难限制了其应用范围。需要一种能够整合模拟和数字内存计算的通用架构。

Method: 提出DARTH-PUM混合架构，包含：优化的外围电路、协调管理两种PUM的硬件接口、易用的编程接口、灵活数据宽度的低成本支持。这些设计使得架构能够完全在内存中执行内核，并易于扩展到不同应用领域。

Result: 在三种流行应用上展示性能提升：AES加密（59.4倍加速）、卷积神经网络（14.8倍加速）、大语言模型（40.8倍加速），相比模拟+CPU基线有明显优势。

Conclusion: DARTH-PUM成功整合了模拟和数字内存计算，解决了硬件和软件集成挑战，为从嵌入式应用到大规模数据驱动计算提供了实用的通用内存计算架构。

Abstract: Analog processing-using-memory (PUM; a.k.a. in-memory computing) makes use of electrical interactions inside memory arrays to perform bulk matrix-vector multiplication (MVM) operations. However, many popular matrix-based kernels need to execute non-MVM operations, which analog PUM cannot directly perform. To retain its energy efficiency, analog PUM architectures augment memory arrays with CMOS-based domain-specific fixed-function hardware to provide complete kernel functionality, but the difficulty of integrating such specialized CMOS logic with memory arrays has largely limited analog PUM to being an accelerator for machine learning inference, or for closely related kernels. An opportunity exists to harness analog PUM for general-purpose computation: recent works have shown that memory arrays can also perform Boolean PUM operations, albeit with very different supporting hardware and electrical signals than analog PUM.
  We propose DARTH-PUM, a general-purpose hybrid PUM architecture that tackles key hardware and software challenges to integrating analog PUM and digital PUM. We propose optimized peripheral circuitry, coordinating hardware to manage and interface between both types of PUM, an easy-to-use programming interface, and low-cost support for flexible data widths. These design elements allow us to build a practical PUM architecture that can execute kernels fully in memory, and can scale easily to cater to domains ranging from embedded applications to large-scale data-driven computing. We show how three popular applications (AES encryption, convolutional neural networks, large-language models) can map to and benefit from DARTH-PUM, with speedups of 59.4x, 14.8x, and 40.8x over an analog+CPU baseline.

</details>


### [3] [Energy-Efficient p-Bit-Based Fully-Connected Quantum-Inspired Simulated Annealer with Dual BRAM Architecture](https://arxiv.org/abs/2602.16143)
*Naoya Onizawa,Taiga Kubuta,Duckgyu Shin,Takahiro Hanyu*

Main category: cs.AR

TL;DR: 提出基于FPGA的随机模拟量子退火架构，通过自旋串行-副本并行更新和双BRAM延迟线设计，解决传统p-bit退火器在完全连接图上的可扩展性问题，显著降低能耗和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有基于概率比特(p-bit)的模拟退火加速器存在可扩展性差、对完全连接图支持有限的问题，主要受限于扇出和内存开销。需要设计更高效的硬件架构来解决这些挑战。

Method: 提出结合自旋串行和副本并行更新调度的FPGA架构，采用双BRAM延迟线设计，支持完全连接的Ising模型，消除逻辑资源中的扇出增长。利用随机模拟量子退火(SSQA)仅使用最终副本状态实现快速收敛。

Result: 在Xilinx ZC706 FPGA上实现，成功求解800节点的MAX-CUT基准问题。相比之前的FPGA p-bit退火架构，能耗降低50%，逻辑资源减少90%以上。

Conclusion: 该量子启发的p-bit退火硬件架构在严格能耗和资源约束下，为大规模组合优化问题提供了实用的解决方案，展示了其在实际应用中的可行性。

Abstract: Probabilistic bits (p-bits) offer an energy-efficient hardware abstraction for stochastic optimization; however, existing p-bit-based simulated annealing accelerators suffer from poor scalability and limited support for fully connected graphs due to fan-out and memory overhead. This paper presents an energy-efficient FPGA architecture for stochastic simulated quantum annealing (SSQA) that addresses these challenges. The proposed design combines a spin-serial and replica-parallel update schedule with a dual-BRAM delay-line architecture, enabling scalable support for fully connected Ising models while eliminating fan-out growth in logic resources. By exploiting SSQA, the architecture achieves fast convergence using only final replica states, significantly reducing memory requirements compared to conventional p-bit-based annealers. Implemented on a Xilinx ZC706 FPGA, the proposed system solves an 800-node MAX-CUT benchmark and achieves up to 50% reduction in energy consumption and over 90\% reduction in logic resources compared with prior FPGA-based p-bit annealing architectures. These results demonstrate the practicality of quantum-inspired, p-bit-based annealing hardware for large-scale combinatorial optimization under strict energy and resource constraints.

</details>
