<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Implementation and Analysis of Thermometer Encoding in DWN FPGA Accelerators](https://arxiv.org/abs/2512.15251)
*Michael Mecik,Martin Kumm*

Main category: cs.AR

TL;DR: DWN硬件生成器包含温度计编码，实验显示编码会增加LUT使用达3.20倍，在小网络中占主导成本，强调DWN加速器需要编码感知的硬件设计。


<details>
  <summary>Details</summary>
Motivation: FPGA上的全并行神经网络加速器在延迟关键应用中提供高吞吐量，但面临硬件资源限制。DWN通过基于梯度的训练优化资源使用，但其依赖温度计编码，相关的硬件成本尚未充分评估。

Method: 提出了一个包含温度计编码的DWN硬件生成器，并在Jet Substructure Classification (JSC)任务上进行实验评估。

Result: 实验显示编码可以增加LUT使用达3.20倍，在小网络中占主导成本，突显了编码对硬件资源的重要影响。

Conclusion: DWN加速器需要编码感知的硬件设计，温度计编码的硬件成本在小型网络中可能主导总体资源使用，需要在设计时充分考虑。

Abstract: Fully parallel neural network accelerators on field-programmable gate arrays (FPGAs) offer high throughput for latency-critical applications but face hardware resource constraints. Weightless neural networks (WNNs) efficiently replace arithmetic with logic-based inference. Differential weightless neural networks (DWN) further optimize resource usage by learning connections between encoders and LUT layers via gradient-based training. However, DWNs rely on thermometer encoding, and the associated hardware cost has not been fully evaluated. We present a DWN hardware generator that includes thermometer encoding explicitly. Experiments on the Jet Substructure Classification (JSC) task show that encoding can increase LUT usage by up to 3.20$\times$, dominating costs in small networks and highlighting the need for encoding-aware hardware design in DWN accelerators.

</details>


### [2] [FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption](https://arxiv.org/abs/2512.15515)
*Zhihan Xu,Rajgopal Kannan,Viktor K. Prasanna*

Main category: cs.AR

TL;DR: 提出FAME：首个专为同态加密矩阵乘法优化的FPGA加速器，通过创新的数据通路设计减少内存访问，实现221倍加速。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）虽然能保护云计算的隐私，但其高计算成本（特别是矩阵乘法）限制了实际应用。加速HE矩阵乘法对隐私保护机器学习等应用至关重要。

Method: 1. 开发成本模型评估HE参数和输入矩阵大小的片上内存需求；2. 设计新型同态线性变换（HLT）数据通路，通过细粒度数据重用减少片外内存流量和片上内存需求；3. 基于此通路构建FAME加速器，支持任意矩阵形状和多种HE参数配置。

Result: 在Alveo U280 FPGA上实现FAME，实验显示相比最先进的CPU实现平均加速221倍，证明其在大规模连续HE矩阵乘法和实际工作负载中的可扩展性和实用性。

Conclusion: FAME通过优化内存使用和数据重用，显著提升了HE矩阵乘法的效率，为隐私保护计算的实际部署提供了可行的硬件加速解决方案。

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.
  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.

</details>
