<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 19]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Flexible Bit-Truncation Memory for Approximate Applications on the Edge](https://arxiv.org/abs/2601.19900)
*William Oswald,Mario Renteria-Pinon,Md. Sajjad Hossain,Kyle Mooney,Md. Bipul Hossain,Destinie Diggs,Yiwen Xu,Mohamed Shaban,Jinhui Wang,Na Gong*

Main category: cs.AR

TL;DR: 提出一种具有完全自适应灵活性的位截断存储器，可运行时截断任意数据位以满足不同近似应用的质量-功耗权衡需求，应用于视频处理和深度学习，显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 现有位截断存储器需要为特定应用定制设计，缺乏灵活性。需要一种能够适应多种近似应用、在运行时灵活调整质量-功耗权衡的通用位截断存储器解决方案。

Method: 开发了一种具有完全自适应灵活性的新型位截断存储器架构，能够在运行时截断任意数量的数据位。该存储器应用于两种数据密集型近似应用：视频处理和深度学习，支持三种不同的视频应用模式。

Result: 相比现有技术，在视频应用中实现高达47.02%的功耗节省；在深度学习模型中实现高达51.69%的功耗节省，同时仅带来2.89%的硅面积开销。

Conclusion: 提出的位截断存储器具有完全自适应灵活性，能够有效支持多种近似应用的质量-功耗权衡优化，显著提升能效，且实现成本低，适合边缘环境部署。

Abstract: Bit truncation has demonstrated great potential to enable run-time quality-power adaptive data storage, thereby optimizing the power/energy efficiency of approximate applications and supporting their deployment in edge environments. However, existing bit-truncation memories require custom designs for a specific application. In this paper, we present a novel bit-truncation memory with full adaptation flexibility, which can truncate any number of data bits at run time to meet different quality and power trade-off requirements for various approximate applications. The developed bit-truncation memory has been applied to two representative data-intensive approximate applications: video processing and deep learning. Our experiments show that the proposed memory can support three different video applications (including luminance-aware, content-aware, and region-of-interest-aware) with enhanced power efficiency (up to 47.02% power savings) as compared to state-of-the-art. In addition, the proposed memory achieves significant (up to 51.69%) power savings for both baseline and pruned lightweight deep learning models, respectively, with a low implementation cost (2.89% silicon area overhead).

</details>


### [2] [A Flower-Inspired Solution for Computer Memory Wear-Leveling](https://arxiv.org/abs/2601.19902)
*Elizabeth Shen,Huiyang Zhou*

Main category: cs.AR

TL;DR: 提出双环磨损均衡方法，利用黄金比例原理，通过两个环形内存模型结合垃圾回收机制，无需硬件修改即可延长内存寿命


<details>
  <summary>Details</summary>
Motivation: 延长计算机内存寿命对电子垃圾和可持续发展至关重要。内存磨损不均衡是主要障碍，而相变存储器等新兴内存寿命更短，问题更加紧迫。现有解决方案要么需要复杂硬件扩展，要么仅适用于特定程序结构如循环。

Method: 提出双环磨损均衡方法，灵感来源于黄金比例的自然法则及其帮助花瓣均匀接收阳光的原理。将内存建模为两个环形结构，结合现有的内存管理和垃圾回收机制，实现有效的磨损均衡。

Result: 该方法具有确定性，能自动适应内存大小，无需硬件修改，且不会给程序执行带来额外延迟，有效减少内存磨损从而延长内存寿命。

Conclusion: 双环磨损均衡方法为解决内存磨损不均衡问题提供了一种有效解决方案，通过借鉴自然界的黄金比例原理，结合现有内存管理技术，实现了无需硬件修改、无性能开销的内存寿命延长方案。

Abstract: Lengthening a computer memory's lifespan is important for e-waste and sustainability. Uneven wear of memory is a major barrier. The problem is becoming even more urgent as emerging memory such as phase-change memory is subject to even shorter lifespan. Various solutions have been proposed, but they either require complicated hardware extensions or apply only to certain program constructs such as loops. This research proposes a new method, dual-ring wear leveling. It takes inspiration from the natural law known as the ``golden ratio" and how it helps flower petals evenly receive sun lights. By modeling memory as two rings and combines the idea with existing memory management, garbage collection, the new solution offers an effective way to reduce memory wear and hence lengthen memory lifespan. It is deterministic, able to automatically adapt to memory size, requiring no hardware changes, and adding no slowdown to program executions.

</details>


### [3] [STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification](https://arxiv.org/abs/2601.19903)
*Saeid Rajabi,Chengmo Yang,Satwik Patnaik*

Main category: cs.AR

TL;DR: STELLAR：首个基于结构相似性引导LLM生成SystemVerilog断言的框架，通过RTL块的AST结构指纹检索相关知识，显著提升断言生成的语法正确性、风格对齐和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 手动编写SystemVerilog断言(SVA)过程缓慢且容易出错，现有LLM方法要么从头生成断言，要么忽略硬件设计中的结构模式和专家编写的断言模式，需要更智能的生成方法。

Method: STELLAR将RTL块表示为AST结构指纹，从知识库中检索结构相关的(RTL, SVA)对，并将这些结构信息整合到引导提示中，实现结构感知的LLM断言生成。

Result: 实验表明STELLAR在语法正确性、风格对齐和功能正确性方面表现优异，证明了结构感知检索在工业形式验证中的潜力。

Conclusion: 结构感知检索是提升LLM生成SystemVerilog断言质量的有效方向，STELLAR框架为工业形式验证提供了有前景的解决方案。

Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.

</details>


### [4] [DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs](https://arxiv.org/abs/2601.19904)
*Ziyu Hu,Zhiqing Zhong,Weijian Zheng,Zhijing Ye,Xuwei Tan,Xueru Zhang,Zheng Xie,Rajkumar Kettimuthu,Xiaodong Yu*

Main category: cs.AR

TL;DR: DABench-LLM是首个针对数据流AI加速器的LLM训练基准测试框架，通过芯片内性能分析和芯片间可扩展性分析，全面评估资源分配、负载平衡和资源效率等关键指标。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的指数级增长，传统CPU/GPU架构受摩尔定律放缓限制，数据流AI加速器成为有前景的替代方案，但缺乏深入的性能分析和标准化基准测试方法。

Method: 开发DABench-LLM基准测试框架，结合芯片内性能分析和芯片间可扩展性分析，支持对Cerebras WSE-2、SambaNova RDU和Graphcore IPU等数据流加速器的评估。

Result: 在三种商用数据流加速器上验证了DABench-LLM的有效性，揭示了性能瓶颈并提供了具体的优化策略，展示了框架在不同数据流AI硬件平台上的通用性和有效性。

Conclusion: DABench-LLM为数据流加速器的LLM训练提供了首个标准化基准测试框架，帮助研究人员快速理解底层硬件和系统行为，并为性能优化提供指导。

Abstract: The exponential growth of large language models has outpaced the capabilities of traditional CPU and GPU architectures due to the slowdown of Moore's Law. Dataflow AI accelerators present a promising alternative; however, there remains a lack of in-depth performance analysis and standardized benchmarking methodologies for LLM training. We introduce DABench-LLM, the first benchmarking framework designed for evaluating LLM workloads on dataflow-based accelerators. By combining intra-chip performance profiling and inter-chip scalability analysis, DABench-LLM enables comprehensive evaluation across key metrics such as resource allocation, load balance, and resource efficiency. The framework helps researchers rapidly gain insights into underlying hardware and system behaviors, and provides guidance for performance optimizations. We validate DABench-LLM on three commodity dataflow accelerators, Cerebras WSE-2, SambaNova RDU, and Graphcore IPU. Our framework reveals performance bottlenecks and provides specific optimization strategies, demonstrating its generality and effectiveness across a diverse range of dataflow-based AI hardware platforms.

</details>


### [5] [Hardware-Aware Model Design and Training of Silicon-based Analog Neural Networks](https://arxiv.org/abs/2601.19905)
*Giulio Filippeschi,Mirko Brazzini,Cristhopher Mosquera,Marco Lanuzza,Alessandro Catania,Sebastiano Strangio,Giuseppe Iannaccone*

Main category: cs.AR

TL;DR: 通过物理感知的硬件感知模型重新训练神经网络，可以在存在显著非理想性的硅基模拟神经网络中完全恢复理想网络模型的推理精度，避免了传统方法中通过校准和保守设计带来的能耗和面积开销。


<details>
  <summary>Details</summary>
Motivation: 硅基模拟神经网络在物理实现中存在非理想性（如电容串扰和位线电压降），传统方法通过提高模拟保真度来应对这些非理想性，但这会带来显著的能耗、面积和设计开销。本文旨在开发一种更有效的方法来恢复推理精度。

Method: 提出了一个物理感知的硬件感知模型，专门针对使用单晶体管浮栅存储单元实现的时域向量矩阵乘法器。该模型明确考虑了两种主要的物理非理想性：电容串扰和位线电压降。模型将每个操作离散化为自适应时隙，并行处理激活模式，并累积它们的贡献来预测有效的乘法器输出。

Result: 使用16x16硅阵列的测量数据校准模型，发现串扰是布局依赖的且通常是主导因素。改进的权重提取程序将信噪比相对于理想向量矩阵乘法器模型提高了一倍。通过在前向传播中使用硬件感知模型训练硅基模拟神经网络，在三种架构上恢复了理想软件网络的精度：低分辨率MNIST上的自定义MLP、MNIST上的LeNet-5以及CIFAR-10上的VGG风格CNN。

Conclusion: 本文建立了一个完整的时域模拟神经形态芯片从设计到部署的工作流程，证明了通过硬件感知模型重新训练可以有效地补偿物理非理想性，为模拟神经网络的扩展性和集成密度提供了更有前景的途径。

Abstract: Silicon-based analog neural networks physically embody the ideal neural network model in an approximate way. We show that by retraining the neural network using a physics-informed hardware-aware model one can fully recover the inference accuracy of the ideal network model even in the presence of significant non-idealities. This is way more promising for scalability and integration density than the default option of improving the fidelity of the analog neural network at the cost of significant energy, area, and design overhead, through extensive calibration and conservative analog design.
  We first present a physics-informed hardware-aware model for a time-domain vector-matrix multiplier implemented with single-transistor floating-gate memory cells that explicitly accounts for two dominant non-idealities of the physical implementation - capacitive crosstalk and bit-line voltage drop - and integrates seamlessly with modern deep-learning workflows. The model discretizes each operation into adaptive time slots, processes activation patterns in parallel, and accumulates their contributions to predict effective multiplier outputs. Using measurements from a 16x16 silicon array, we calibrate the model, show that crosstalk is layout-dependent and often dominant, and introduce an improved weight-extraction procedure that doubles signal-to-error ratio versus an ideal vector-matrix multiplier model. Finally, we show that by training silicon-based analog neural networks using an hardware-aware model in the forward pass we can recover the accuracy of the ideal software networks across three architectures -- custom MLP on low-resolution MNIST, LeNet-5 on MNIST, and a VGG-style CNN on CIFAR-10 - establishing a complete design-to-deployment workflow for time-domain analog neuromorphic chips.

</details>


### [6] [GTAC: A Generative Transformer for Approximate Circuits](https://arxiv.org/abs/2601.19906)
*Jingxin Wang,Shitong Guo,Ruicheng Dai,Wenhui Liang,Ruogu Ding,Xin Ning,Weikang Qian*

Main category: cs.AR

TL;DR: GTAC是一种基于生成式Transformer的近似电路设计模型，通过集成误差阈值约束，在误差容忍应用中显著提升电路性能、功耗和面积，相比现有方法面积减少6.4%，速度提升4.3倍。


<details>
  <summary>Details</summary>
Motivation: 针对误差容忍应用，近似电路通过引入可控误差来显著改善电路的性能、功耗和面积。传统方法在平衡误差约束与PPA优化方面存在局限性，需要更智能的设计自动化方法。

Method: 提出GTAC模型，这是一种基于生成式Transformer的近似电路设计方法。创新性地将误差阈值集成到设计过程中，结合近似计算和AI驱动的EDA技术，实现智能化的电路近似优化。

Result: 与最先进方法相比，GTAC在满足误差率约束的条件下，进一步减少了6.4%的面积，同时设计速度提升了4.3倍。

Conclusion: GTAC证明了生成式Transformer模型在近似电路设计中的有效性，为AI驱动的EDA提供了新的解决方案，能够高效地在误差约束下优化电路PPA指标。

Abstract: Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce GTAC, a novel generative Transformer-based model for producing approximate circuits. By leveraging principles of approximate computing and AI-driven EDA, our model innovatively integrates error thresholds into the design process. Experimental results show that compared with a state-of-the-art method, GTAC further reduces 6.4% area under the error rate constraint, while being 4.3x faster.

</details>


### [7] [RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs](https://arxiv.org/abs/2601.19907)
*Yanru Chen,Zheyu Li,Keming Fan,Runyang Tian,John Hsu,Weihong Xu,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: RAPID-Graph：一种针对全对最短路径（APSP）问题的存内计算（PIM）系统，通过算法、架构和器件级协同设计，在大型图分析中实现显著性能提升和能效改进。


<details>
  <summary>Details</summary>
Motivation: 全对最短路径（APSP）在大规模图分析中面临数据移动的立方复杂度瓶颈，传统内存层次结构的带宽无法满足需求，需要新的计算范式来解决这一挑战。

Method: 1. 算法层面：引入递归感知分区器，将图分解为顶点瓦片以减少数据依赖性，使Floyd-Warshall和Min-Plus内核完全在数字PIM阵列中原地执行
2. 架构层面：设计2.5D PIM堆栈，集成两个相变内存计算芯片、一个逻辑芯片和高带宽暂存内存
3. 器件层面：外部非易失性存储堆栈持久存储大型APSP结果，支持瓦片级和单元级并行处理

Result: 在2.45M节点的OGBN-Products数据集上：比最先进的GPU集群快5.8倍、能效高1,186倍；比现有PIM加速器快8.3倍、能效高104倍；比NVIDIA H100 GPU快42.8倍、节能392倍

Conclusion: RAPID-Graph通过算法、架构和器件级的协同优化，成功解决了APSP在大规模图分析中的性能瓶颈，为存内计算系统设计提供了有效的解决方案，显著提升了计算速度和能效。

Abstract: All-pairs shortest paths (APSP) remains a major bottleneck for large-scale graph analytics, as data movement with cubic complexity overwhelms the bandwidth of conventional memory hierarchies. In this work, we propose RAPID-Graph to address this challenge through a co-designed processing-in-memory (PIM) system that integrates algorithm, architecture, and device-level optimizations. At the algorithm level, we introduce a recursion-aware partitioner that enables an exact APSP computation by decomposing graphs into vertex tiles to reduce data dependency, such that both Floyd-Warshall and Min-Plus kernels execute fully in-place within digital PIM arrays. At the architecture and device levels, we design a 2.5D PIM stack integrating two phase-change memory compute dies, a logic die, and high-bandwidth scratchpad memory within a unified advanced package. An external non-volatile storage stack stores large APSP results persistently. The design achieves both tile-level and unit-level parallel processing to sustain high throughput. On the 2.45M-node OGBN-Products dataset, RAPID-Graph is 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters, while exceeding prior PIM accelerators by 8.3x in speed and 104x in efficiency. It further delivers up to 42.8x speedup and 392x energy savings over an NVIDIA H100 GPU.

</details>


### [8] [CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference](https://arxiv.org/abs/2601.19908)
*Yanru Chen,Runyang Tian,Yue Pan,Zheyu Li,Weihong Xu,Tajana Rosing*

Main category: cs.AR

TL;DR: CHIME：基于Chiplet的异构近内存加速器，用于边缘多模态大语言模型推理，通过结合M3D DRAM和RRAM芯片，实现高达54倍加速和246倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的多模态大语言模型推理面临延迟、能耗和连接不稳定的挑战，特别是视觉输入转化为大量token序列会显著增加KV缓存和数据移动开销。

Method: 采用基于Chiplet的异构近内存加速架构，结合M3D DRAM（提供低延迟带宽用于注意力计算）和RRAM（提供高密度非易失性权重存储），并设计协同映射框架执行近数据融合内核。

Result: 在FastVLM和MobileVLM模型上，相比NVIDIA Jetson Orin NX实现高达54倍加速和246倍能效提升，相比最先进的PIM加速器FACIL实现69.2倍吞吐量提升。

Conclusion: CHIME通过异构内存架构和近数据计算有效解决了边缘MLLM推理的瓶颈，显著提升了性能和能效，为边缘AI部署提供了高效解决方案。

Abstract: The proliferation of large language models (LLMs) is accelerating the integration of multimodal assistants into edge devices, where inference is executed under stringent latency and energy constraints, often exacerbated by intermittent connectivity. These challenges become particularly acute in the context of multimodal LLMs (MLLMs), as high-dimensional visual inputs are transformed into extensive token sequences, thereby inflating the key-value (KV) cache and imposing substantial data movement overheads to the LLM backbone. To address these issues, we present CHIME, a chiplet-based heterogeneous near-memory acceleration for edge MLLMs inference. CHIME leverages the complementary strengths of integrated monolithic 3D (M3D) DRAM and RRAM chiplets: DRAM supplies low-latency bandwidth for attention, while RRAM offers dense, non-volatile storage for weights. This heterogeneous hardware is orchestrated by a co-designed mapping framework that executes fused kernels near data, minimizing cross-chiplet traffic to maximize effective bandwidth. On FastVLM (0.6B/1.7B) and MobileVLM (1.7B/3B), CHIME achieves up to 54x speedup and up to 246x better energy efficiency per inference as compared to the edge GPU NVIDIA Jetson Orin NX. It sustains 116.5-266.5 token/J compared to Jetson's 0.7-1.1 token/J. Furthermore, it delivers up to 69.2x higher throughput than the state-of-the-art PIM accelerator FACIL. Compared to the M3D DRAM-only design, CHIME's heterogeneous memory further improves energy efficiency by 7% and performance by 2.4x.

</details>


### [9] [Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading](https://arxiv.org/abs/2601.19910)
*William Meng,Benjamin Lee,Hong Wang*

Main category: cs.AR

TL;DR: KV缓存卸载通过将缓存存储在CPU DRAM中实现长上下文LLM推理，但PCIe带宽限制造成严重瓶颈。本文开发分析框架推导临界缓存-预填充令牌比κ_crit，发现典型工作负载远超此阈值，99%延迟用于数据传输，GPU仅使用28%额定TDP，提出硬件互连、模型架构和调度算法优化。


<details>
  <summary>Details</summary>
Motivation: KV缓存卸载技术虽然能支持长上下文LLM推理，但受限于PCIe带宽瓶颈，导致严重的性能问题。当前系统在数据传输上花费大量时间，GPU利用率低下，需要系统性优化。

Method: 开发分析框架推导临界缓存-预填充令牌比κ_crit，通过经验性表征分析系统性能瓶颈，测量数据传输延迟和GPU功耗利用率。

Result: 发现典型工作负载远超临界阈值，99%的延迟用于PCIe数据传输，GPU仅消耗28%额定TDP，表明系统严重受限于内存带宽而非计算能力。

Conclusion: 需要从硬件互连（如更高带宽连接）、模型架构（减少缓存需求）和调度算法（优化数据传输）三个层面进行系统性优化，以解决KV缓存卸载的性能瓶颈问题。

Abstract: KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks. In this paper, we develops an analytical framework that derives $κ_{\text{crit}}$, the critical cached-to-prefill token ratio where execution becomes memory-bound and show typical workloads exceed this threshold by orders of magnitude. Empirical characterization reveals 99\% of latency spent on transfers and serving offloaded requests results in GPU's consuming only 28\% of their rated TDP, motivating our proposed optimizations for hardware interconnects, model architectures, and scheduling algorithms.

</details>


### [10] [GPU-Augmented OLAP Execution Engine: GPU Offloading](https://arxiv.org/abs/2601.19911)
*Ilsun Chang*

Main category: cs.AR

TL;DR: 提出混合架构，在向量化执行基础上选择性将高影响原语卸载到GPU，通过键值传输和风险感知门控机制优化OLAP系统性能


<details>
  <summary>Details</summary>
Motivation: 现代OLAP系统通过存储计算分离和列式布局缓解了I/O瓶颈，但执行层的CPU成本（特别是Top-K选择和连接探测）在大规模场景下成为新的瓶颈

Method: 1) 混合架构：在现有向量化执行基础上选择性卸载高影响原语到GPU；2) 键值传输：仅传输键和指针，延迟物化减少数据移动；3) 风险感知门控：基于输入大小、传输成本、内核成本、后处理成本和候选集复杂度(K,M)动态触发卸载

Result: 使用PostgreSQL微基准测试和GPU代理测量，相比始终开启的GPU卸载，门控卸载在尾部延迟(P95/P99)方面表现更优

Conclusion: 将风险感知门控原则从优化器阶段的GPU辅助测量扩展到执行层的OLAP原语，为大规模OLAP系统提供有效的CPU-GPU混合执行方案

Abstract: Modern OLAP systems have mitigated I/O bottlenecks via storage-compute separation and columnar layouts, but CPU costs in the execution layer (especially Top-K selection and join probe) are emerging as new bottlenecks at scale. This paper proposes a hybrid architecture that augments existing vectorized execution by selectively offloading only high-impact primitives to the GPU. To reduce data movement, we use key-only transfer (keys and pointers) with late materialization. We further introduce a Risky Gate (risk-aware gating) that triggers offloading only in gain/risk intervals based on input size, transfer, kernel and post-processing costs, and candidate-set complexity (K, M). Using PostgreSQL microbenchmarks and GPU proxy measurements, we observe improved tail latency (P95/P99) under gated offloading compared to always-on GPU offloading. This work extends the risk-aware gating principle used for optimizer-stage GPU-assisted measurement (arXiv:2512.19750) to execution-layer OLAP primitives.

</details>


### [11] [Analysis of LLM Vulnerability to GPU Soft Errors: An Instruction-Level Fault Injection Study](https://arxiv.org/abs/2601.19912)
*Duo Chai,Zizhen Liu,Shuhuai Wang,Songwei Pei,Cheng Liu,Huawei Li,Shangguang Wang*

Main category: cs.AR

TL;DR: 首次对大型语言模型推理进行指令级故障注入研究，分析模型架构、参数规模和任务复杂度对可靠性的影响


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对GPU计算和内存需求高，而GPU技术发展使其更易受软错误影响。现有研究主要关注通用应用或传统视觉神经网络，缺乏对现代大规模LLM的系统性可靠性分析

Method: 采用指令级故障注入研究方法，从多个角度分析LLM推理的可靠性特征

Result: 揭示了模型架构、参数规模和任务复杂度对LLM可靠性的影响，为理解LLM可靠性提供了新见解

Conclusion: LLM对软错误的恢复能力可能与早期模型有显著差异，研究结果为设计更有效的容错机制提供了依据

Abstract: Large language models (LLMs) are highly compute- and memory-intensive, posing significant demands on high-performance GPUs. At the same time, advances in GPU technology driven by shrinking transistor sizes and lower operating voltages have made these devices increasingly susceptible to soft errors. While prior work has examined GPU reliability, most studies have focused on general-purpose applications or conventional neural networks mostly used for vision tasks such as classification and detection. In contrast, systematic analysis of modern large-scale LLMs remains limited, despite their rapid adoption in diverse application scenarios. Given the unique characteristics of LLMs, their resilience to soft errors may differ substantially from earlier models. To bridge this gap, we conduct the first instruction-level fault injection study of LLM inference. Our approach reveals reliability characteristics from multiple perspectives, highlighting the effects of model architecture, parameter scale, and task complexity. These findings provide new insights into LLM reliability and inform the design of more effective fault tolerance mechanisms.

</details>


### [12] [PiC-BNN: A 128-kbit 65 nm Processing-in-CAM-Based End-to-End Binary Neural Network Accelerator](https://arxiv.org/abs/2601.19920)
*Yuval Harary,Almog Sharoni,Esteban Garzón,Marco Lanuzza,Adam Teman,Leonid Yavits*

Main category: cs.AR

TL;DR: PiC-BNN是一个真正的端到端二进制神经网络加速器，使用汉明距离容错的内容寻址存储器，无需全精度运算即可实现准确分类。


<details>
  <summary>Details</summary>
Motivation: 传统BNN虽然对线性层进行二值化，但仍需在批归一化、softmax等层使用全精度运算，限制了面积和能效优势。需要设计真正的端到端二进制加速器。

Method: 提出PiC-BNN加速器，基于汉明距离容错的内容寻址存储器，利用大数定律实现准确分类，无需全精度运算。采用65nm工艺制造。

Result: 在MNIST数据集上达到95.2%准确率，在手势数据集上达到93.5%准确率，吞吐量560K推理/秒，能效703M推理/秒/瓦。

Conclusion: PiC-BNN证明了真正的端到端二进制神经网络加速器的可行性，通过汉明距离容错机制实现了高能效和高吞吐量，无需全精度运算支持。

Abstract: Binary Neural Networks (BNNs), where weights and activations are constrained to binary values (+1, -1), are a highly efficient alternative to traditional neural networks. Unfortunately, typical BNNs, while binarizing linear layers (matrix-vector multiplication), still implement other network layers (batch normalization, softmax, output layer, and sometimes the input layer of a convolutional neural network) in full precision. This limits the area and energy benefits and requires architectural support for full precision operations. We propose PiC-BNN, a true end-to-end binary in-approximate search (Hamming distance tolerant) Content Addressable Memory based BNN accelerator. PiC-BNN is designed and manufactured in a commercial 65nm process. PiC-BNN uses Hamming distance tolerance to apply the law of large numbers to enable accurate classification without implementing full precision operations. PiC-BNN achieves baseline software accuracy (95.2%) on the MNIST dataset and 93.5% on the Hand Gesture (HG) dataset, a throughput of 560K inferences/s, and presents a power efficiency of 703M inferences/s/W when implementing a binary MLP model for MNIST/HG dataset classification.

</details>


### [13] [Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation](https://arxiv.org/abs/2601.19941)
*M Zafir Sadik Khan,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: Bench4HLS：首个针对LLM生成HLS设计的综合评估框架，包含170个测试案例，支持自动化编译、功能验证和PPA分析。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在RTL设计中的成功应用，HLS领域的LLM研究正在快速增长（HLS:RTL研究比例从1:10提升到2:10），但缺乏专门的评估框架来系统评估LLM生成的HLS设计质量。

Method: 构建包含170个手动编写和验证的测试案例集，涵盖从简单内核到复杂加速器的各种设计。提供自动化评估流程，包括编译成功率、功能正确性验证和综合可行性分析。关键创新是集成了可插拔API，支持跨不同HLS工具链和架构的PPA分析。

Result: 开发了Bench4HLS框架，已在Xilinx Vitis HLS和Catapult HLS上验证。该框架提供了结构化、可扩展的即插即用测试平台，为LLM在HLS工作流中的基准测试建立了基础方法论。

Conclusion: Bench4HLS填补了LLM在HLS领域评估的空白，通过提供全面的基准测试框架，将促进LLM在高级综合中的研究和应用发展，支持跨工具链的性能、功耗和面积分析。

Abstract: In last two years, large language models (LLMs) have shown strong capabilities in code generation, including hardware design at register-transfer level (RTL). While their use in high-level synthesis (HLS) remains comparatively less mature, the ratio of HLS- to RTL-focused studies has shifted from 1:10 to 2:10 in the past six months, indicating growing interest in leveraging LLMs for high-level design entry while relying on downstream synthesis for optimization. This growing trend highlights the need for a comprehensive benchmarking and evaluation framework dedicated to LLM-based HLS. To address this, We present Bench4HLS for evaluating LLM-generated HLS designs. Bench4HLS comprises 170 manually drafted and validated case studies, spanning small kernels to complex accelerators, curated from widely used public repositories. The framework supports fully automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility/optimization. Crucially, Bench4HLS integrates a pluggable API for power, performance, and area (PPA) analysis across various HLS toolchains and architectures, demonstrated here with Xilinx Vitis HLS and validated on Catapult HLS. By providing a structured, extensible, and plug-and-play testbed, Bench4HLS establishes a foundational methodology for benchmarking LLMs in HLS workflows.

</details>


### [14] [Primitive-Driven Acceleration of Hyperdimensional Computing for Real-Time Image Classification](https://arxiv.org/abs/2601.20061)
*Dhruv Parikh,Jebacyril Arockiaraj,Viktor Prasanna*

Main category: cs.AR

TL;DR: 本文提出了一种用于超维计算（HDC）的新型图像编码算法和FPGA加速器，在MNIST和Fashion-MNIST数据集上取得了优越的准确率，并实现了显著的推理加速。


<details>
  <summary>Details</summary>
Motivation: 超维计算使用高维低精度向量表示数据，具有轻量级和噪声容忍性，但在传统CPU/GPU上执行核心操作时存在计算效率低、内存瓶颈和实时性能限制的问题。

Method: 1. 开发类似卷积神经网络的图像编码算法，将局部图像块映射到富含空间信息的超向量，然后通过HDC基本操作合并为全局表示；2. 设计端到端FPGA加速器，通过流水线架构在超向量维度和图像块集合上并行执行计算操作。

Result: 编码器在MNIST上达到95.67%准确率，在Fashion-MNIST上达到85.14%准确率，优于现有HDC图像编码器。Alveo U280 FPGA实现0.09ms推理延迟，比最优CPU和GPU基线分别加速1300倍和60倍。

Conclusion: 提出的HDC图像编码算法和FPGA加速器有效解决了传统处理器上的性能瓶颈，实现了高精度和实时推理性能，为超维计算在图像处理领域的应用提供了高效解决方案。

Abstract: Hyperdimensional Computing (HDC) represents data using extremely high-dimensional, low-precision vectors, termed hypervectors (HVs), and performs learning and inference through lightweight, noise-tolerant operations. However, the high dimensionality, sparsity, and repeated data movement involved in HDC make these computations difficult to accelerate efficiently on conventional processors. As a result, executing core HDC operations: binding, permutation, bundling, and similarity search: on CPUs or GPUs often leads to suboptimal utilization, memory bottlenecks, and limits on real-time performance. In this paper, our contributions are two-fold. First, we develop an image-encoding algorithm that, similar in spirit to convolutional neural networks, maps local image patches to hypervectors enriched with spatial information. These patch-level hypervectors are then merged into a global representation using the fundamental HDC operations, enabling spatially sensitive and robust image encoding. This encoder achieves 95.67% accuracy on MNIST and 85.14% on Fashion-MNIST, outperforming prior HDC-based image encoders. Second, we design an end-to-end accelerator that implements these compute operations on an FPGA through a pipelined architecture that exploits parallelism both across the hypervector dimensionality and across the set of image patches. Our Alveo U280 implementation delivers 0.09ms inference latency, achieving up to 1300x and 60x speedup over state-of-the-art CPU and GPU baselines, respectively.

</details>


### [15] [A Paradigm for Generalized Multi-Level Priority Encoders](https://arxiv.org/abs/2601.20067)
*Maxwell Phillips,Firas Hassan,Ahmed Ammar*

Main category: cs.AR

TL;DR: 本文提出了一种新的多级优先编码器设计范式，通过级联和组合技术将两级结构扩展到三、四级，分析了各种架构在复杂度和延迟上的权衡，为硬件设计者提供了选择指南。


<details>
  <summary>Details</summary>
Motivation: 传统优先编码器在高比特精度（如512位以上）时硬件复杂度高，限制了其在关键应用（如高精度整数运算和内容可寻址存储器）中的使用。需要降低复杂度以加速这些应用。

Method: 提出多级优先编码器设计范式，将已有的两级结构通过级联和组合技术扩展到三、四级，并与传统单级、树形、递归等设计进行比较分析。

Result: 两级架构在复杂度和延迟间提供最佳平衡（复杂度减半但延迟相应增加），更多层级收益递减。树形和递归设计更快但更复杂。提供了基于输入长度和实现技术的设计选择建议。

Conclusion: 通过分析各种优先编码器架构的权衡关系，为硬件设计者提供了优先编码器工具包，帮助根据具体需求（复杂度或延迟优先）选择最优设计。

Abstract: Priority encoders are typically considered expensive hardware components in terms of complexity, especially at high bit precisions or input lengths (e.g., above 512 bits). However, if the complexity can be reduced, priority encoders can feasibly accelerate a variety of key applications, such as high-precision integer arithmetic and content-addressable memory. We propose a new paradigm for constructing priority encoders by generalizing the previously proposed two-level priority encoder structure. We extend this concept to three and four levels using two techniques -- cascading and composition -- and discuss further generalization. We then analyze the complexity and delay of new and existing priority encoder designs as a function of input length, for both FPGA and ASIC implementation technologies. In particular, we compare the multi-level structure to the traditional single-level priority encoder structure, a tree-based design, a recursive design, and the two-level structure. We find that the two-level architecture provides balanced performance -- reducing complexity by around half, but at the cost of a corresponding increase in delay. Additional levels have diminishing returns, highlighting a tradeoff between complexity and delay. Meanwhile, the tree and recursive designs are generally faster, but are more complex than the two-level and multi-level structures. We explore several characteristics and patterns of the designs across a wide range of input lengths. We then provide recommendations on which architecture to use for a given input length and implementation technology, based on which design factors -- such as complexity or delay -- are most important to the hardware designer. With this overview and analysis of various priority encoder architectures, we provide a priority encoder toolkit to assist hardware designers in creating the most optimal design.

</details>


### [16] [How Much Progress Has There Been in NVIDIA Datacenter GPUs?](https://arxiv.org/abs/2601.20115)
*Emanuele Del Sozzo,Martin Fleming,Kenneth Flamm,Neil Thompson*

Main category: cs.AR

TL;DR: 该论文分析了2000年代中期至今NVIDIA数据中心GPU的技术进步趋势，量化了各项性能指标的倍增时间，并评估了美国出口管制对潜在性能差距的影响。


<details>
  <summary>Details</summary>
Motivation: 随着GPU在AI等关键任务中的重要性日益增加，了解其技术进步趋势对于预测未来科学研究的约束条件至关重要。特别是在美国实施AI芯片出口管制的背景下，分析GPU技术发展轨迹具有重要现实意义。

Method: 收集了从2000年代中期至今的NVIDIA数据中心GPU全面数据集，包括计算性能、价格等多项特征。分析主要GPU特征趋势，并估算每内存带宽、每美元、每瓦特的进步指标。

Result: FP16和FP32操作的倍增时间为1.44-1.69年，FP64为2.06-3.79年。内存大小和带宽增长较慢（3.32-3.53年）。GPU价格每5.1年翻倍，功耗每16年翻倍。出口管制若完全实施，潜在性能差距将从23.6倍缩小到3.54倍。

Conclusion: GPU计算性能增长远快于内存带宽和价格增长，出口管制措施可能显著缩小国际间的技术性能差距，这对全球AI竞争格局具有重要影响。

Abstract: Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performance demands, analyzing past and current progress becomes paramount in determining future constraints on scientific research. This is particularly compelling in the AI domain, where rapid technological advancements and fierce global competition have led the United States to recently implement export control regulations limiting international access to advanced AI chips. For this reason, this paper studies technical progress in NVIDIA datacenter GPUs released from the mid-2000s until today. Specifically, we compile a comprehensive dataset of datacenter NVIDIA GPUs comprising several features, ranging from computational performance to release price. Then, we examine trends in main GPU features and estimate progress indicators for per-memory bandwidth, per-dollar, and per-watt increase rates. Our main results identify doubling times of 1.44 and 1.69 years for FP16 and FP32 operations (without accounting for sparsity benefits), while FP64 doubling times range from 2.06 to 3.79 years. Off-chip memory size and bandwidth grew at slower rates than computing performance, doubling every 3.32 to 3.53 years. The release prices of datacenter GPUs have roughly doubled every 5.1 years, while their power consumption has approximately doubled every 16 years. Finally, we quantify the potential implications of current U.S. export control regulations in terms of the potential performance gaps that would result if implementation were assumed to be complete and successful. We find that recently proposed changes to export controls would shrink the potential performance gap from 23.6x to 3.54x.

</details>


### [17] [SATA: Sparsity-Aware Scheduling for Selective Token Attention](https://arxiv.org/abs/2601.20267)
*Zhenkun Fan,Zishen Wan,Che-Kai Liu,Ashwin Sanjay Lele,Win-San Khwa,Bo Zhang,Meng-Fan Chang,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: SATA：一种面向稀疏查询-键操作的动态调度方案，通过重新排序操作数流和利用数据局部性，提高选择性注意力模型的系统吞吐量和能效


<details>
  <summary>Details</summary>
Motivation: Transformer注意力机制的二次方复杂度给硬件实现带来挑战，选择性令牌注意力通过聚焦相关令牌来减少计算，但会产生稀疏访问模式，需要高效调度

Method: 提出SATA（局部性中心的动态调度方案），主动管理稀疏分布的查询-键操作访问模式，通过重新排序操作数流、利用数据局部性，实现中间查询/键向量的提前获取和释放

Result: 实验结果显示，该方法将系统吞吐量提升最高1.76倍，能效提升2.94倍，同时调度开销最小

Conclusion: SATA通过高效的动态调度策略有效解决了选择性注意力模型的稀疏访问模式问题，显著提升了硬件实现效率和能效

Abstract: Transformers have become the foundation of numerous state-of-the-art AI models across diverse domains, thanks to their powerful attention mechanism for modeling long-range dependencies. However, the quadratic scaling complexity of attention poses significant challenges for efficient hardware implementation. While techniques such as quantization and pruning help mitigate this issue, selective token attention offers a promising alternative by narrowing the attention scope to only the most relevant tokens, reducing computation and filtering out noise.
  In this work, we propose SATA, a locality-centric dynamic scheduling scheme that proactively manages sparsely distributed access patterns from selective Query-Key operations. By reordering operand flow and exploiting data locality, our approach enables early fetch and retirement of intermediate Query/Key vectors, improving system utilization. We implement and evaluate our token management strategy in a control and compute system, using runtime traces from selective-attention-based models. Experimental results show that our method improves system throughput by up to 1.76x and boosts energy efficiency by 2.94x, while incurring minimal scheduling overhead.

</details>


### [18] [VersaQ-3D: A Reconfigurable Accelerator Enabling Feed-Forward and Generalizable 3D Reconstruction via Versatile Quantization](https://arxiv.org/abs/2601.20317)
*Yipu Zhang,Jintao Cheng,Xingyu Liu,Zeyu Li,Carol Jingyi Li,Jin Wu,Lin Jiang,Yuan Xie,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: VersaQ-3D是一个算法-架构协同设计框架，通过无校准的4位量化技术解决VGGT模型的内存和计算瓶颈，实现高效即时3D重建。


<details>
  <summary>Details</summary>
Motivation: VGGT模型虽然能实现无需逐场景优化的前馈式3D重建，但其十亿参数规模导致高内存和计算需求，阻碍了在设备上的部署。现有LLM量化方法因饱和激活通道和多样3D语义而失效，且硬件上存在精度敏感非线性算子和内存密集型全局注意力问题。

Method: 提出VersaQ-3D框架：算法上采用首个无校准、场景无关的4位量化方法，利用正交变换解相关特征并抑制异常值；架构上设计可重构加速器，支持BF16、INT8和INT4精度，统一脉动数据通路处理线性和非线性算子，两阶段重计算分块缓解长序列注意力内存压力。

Result: 在W4A8精度下保持98-99%准确率；在W4A4精度下，相比先前方法在多样场景中提升1.61x-2.39x；加速器相比边缘GPU实现5.2x-10.8x加速，功耗低，支持高效即时3D重建。

Conclusion: VersaQ-3D成功解决了VGGT模型的部署瓶颈，通过算法-架构协同设计实现了高效低精度的3D重建，为设备端即时3D重建提供了可行解决方案。

Abstract: The Visual Geometry Grounded Transformer (VGGT) enables strong feed-forward 3D reconstruction without per-scene optimization. However, its billion-parameter scale creates high memory and compute demands, hindering on-device deployment. Existing LLM quantization methods fail on VGGT due to saturated activation channels and diverse 3D semantics, which cause unreliable calibration. Furthermore, VGGT presents hardware challenges regarding precision-sensitive nonlinear operators and memory-intensive global attention. To address this, we propose VersaQ-3D, an algorithm-architecture co-design framework. Algorithmically, we introduce the first calibration-free, scene-agnostic quantization for VGGT down to 4-bit, leveraging orthogonal transforms to decorrelate features and suppress outliers. Architecturally, we design a reconfigurable accelerator supporting BF16, INT8, and INT4. A unified systolic datapath handles both linear and nonlinear operators, reducing latency by 60%, while two-stage recomputation-based tiling alleviates memory pressure for long-sequence attention. Evaluations show VersaQ-3D preserves 98-99% accuracy at W4A8. At W4A4, it outperforms prior methods by 1.61x-2.39x across diverse scenes. The accelerator delivers 5.2x-10.8x speedup over edge GPUs with low power, enabling efficient instant 3D reconstruction.

</details>


### [19] [Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling](https://arxiv.org/abs/2601.20706)
*Binglei Lou,Haoran Wu,Yao Lai,Jiayi Nie,Can Xiao,Xuan Guo,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: 本文提出针对扩散大语言模型采样阶段的NPU架构优化，通过轻量级向量原语、内存重用策略和混合精度内存层次结构，相比GPU实现2.53倍加速。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的采样阶段占推理延迟的70%，主要由于词汇级logits的大量内存读写、基于规约的token选择和迭代掩码更新，这些操作需要大容量片上SRAM且涉及不规则内存访问，传统NPU难以高效处理。

Method: 识别NPU架构必须优化的关键指令集，采用轻量级非GEMM向量原语、原地内存重用策略和解耦的混合精度内存层次结构设计。

Result: 在等效纳米技术节点下，相比NVIDIA RTX A6000 GPU实现最高2.53倍加速，并开源了周期精确模拟和后综合RTL验证代码。

Conclusion: 针对dLLM采样阶段的特定架构优化能显著提升推理效率，提出的NPU设计为扩散大语言模型的高效部署提供了有效解决方案。

Abstract: Diffusion Large Language Models (dLLMs) introduce iterative denoising to enable parallel token generation, but their sampling phase displays fundamentally different characteristics compared to GEMM-centric transformer layers. Profiling on modern GPUs reveals that sampling can account for up to 70% of total model inference latency-primarily due to substantial memory loads and writes from vocabulary-wide logits, reduction-based token selection, and iterative masked updates. These processes demand large on-chip SRAM and involve irregular memory accesses that conventional NPUs struggle to handle efficiently. To address this, we identify a set of critical instructions that an NPU architecture must specifically optimize for dLLM sampling. Our design employs lightweight non-GEMM vector primitives, in-place memory reuse strategies, and a decoupled mixed-precision memory hierarchy. Together, these optimizations deliver up to a 2.53x speedup over the NVIDIA RTX A6000 GPU under an equivalent nm technology node. We also open-source our cycle-accurate simulation and post-synthesis RTL verification code, confirming functional equivalence with current dLLM PyTorch implementations.

</details>
