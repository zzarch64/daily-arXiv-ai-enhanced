<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: FlexLLM是一个可组合的HLS库，用于快速开发领域特定的LLM加速器，支持阶段定制化推理和量化部署，在FPGA上实现了超越GPU的性能和能效


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理需要高性能加速器，但开发过程复杂耗时。FlexLLM旨在通过可组合的HLS库简化领域特定LLM加速器的开发，桥接算法创新与高性能硬件实现

Method: 提供可组合的High-Level Synthesis库，暴露架构自由度支持阶段定制化推理（prefill和decode采用不同设计），包含全面的量化套件支持低比特部署，并开发了Hierarchical Memory Transformer插件用于长上下文处理

Result: 在AMD U280 FPGA上，相比NVIDIA A100 GPU：端到端加速1.29倍，解码吞吐量提高1.64倍，能效提升3.14倍；在V80 FPGA上分别达到4.71倍、6.55倍和4.13倍。HMT插件在长上下文场景中减少prefill延迟23.23倍，扩展上下文窗口64倍

Conclusion: FlexLLM能够以最小的人工工作量桥接LLM推理算法创新与高性能加速器开发，显著提升FPGA上LLM推理的性能和能效，特别是在长上下文处理方面

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>


### [2] [A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware](https://arxiv.org/abs/2601.16118)
*Marco Ronzani,Cristina Silvano*

Main category: cs.AR

TL;DR: 该论文提出将脉冲神经网络(SNN)从图抽象提升到超图抽象，以改进在神经形态硬件上的映射技术，通过超边共成员关系更好地捕捉核心内脉冲复制，从而减少通信流量和硬件资源使用。


<details>
  <summary>Details</summary>
Motivation: 在神经形态硬件上执行SNN面临神经元到核心的映射问题。随着SNN和硬件规模扩展到数十亿神经元，现有的图分区和放置方法难以有效处理，需要更高层次的抽象来捕捉核心内脉冲复制的特性。

Method: 将SNN从图模型提升到超图模型，利用超边共成员关系捕捉神经元间的通信模式。基于超边的重叠性和局部性设计映射算法，包括新设计的算法和从文献改编的算法，通过超边共享来分组神经元。

Result: 超图基技术能够在多个执行时间范围内实现比现有技术更好的映射效果。超边的重叠性和局部性与高质量映射强相关，利用这些特性可以显著减少通信流量和硬件资源使用。

Conclusion: 超图抽象为SNN在神经形态硬件上的映射提供了更有效的模型，能够处理大规模SNN的映射问题。基于研究结果，识别出了一组有前景的算法选择，可在任何规模下实现有效映射。

Abstract: Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.

</details>
