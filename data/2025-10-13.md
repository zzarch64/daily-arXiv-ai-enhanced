<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Mozart: A Chiplet Ecosystem-Accelerator Codesign Framework for Composable Bespoke Application Specific Integrated Circuits](https://arxiv.org/abs/2510.08873)
*Haoran Jin,Jirong Yang,Yunpeng Liu,Barry Lyu,Kangqi Zhang,Nathaniel Bleier*

Main category: cs.AR

TL;DR: Mozart是一个芯片生态系统和加速器协同设计框架，通过算子级解构、芯片异构性和系统级优化，构建低成本定制ASIC，显著提升能效和性能。


<details>
  <summary>Details</summary>
Motivation: 传统AI加速器假设存在内存需求、批处理效果和延迟吞吐量权衡的系统级泛化问题，忽视了神经网络算子的异构计算模式。芯片级定制和算子级异构性会带来高昂的非重复性工程成本。

Method: 采用算子级解构探索芯片和内存异构性、张量融合和张量并行，通过布局布线验证确保物理可实现性，支持从数据中心到边缘计算的多场景约束感知系统级优化。

Result: 仅使用8个战略选择的芯片，Mozart生成的复合BASIC相比传统同构加速器在能量、能量成本乘积、能量延迟乘积和能量延迟成本乘积上分别减少43.5%、25.4%、67.7%和78.8%。在数据中心LLM服务中实现15-19%能量减少和35-39%能量成本改进。

Conclusion: Mozart框架通过系统化的芯片生态系统设计，有效解决了AI加速中的异构计算挑战，在多种部署场景下显著提升了能效和性能表现。

Abstract: Modern AI acceleration faces a fundamental challenge: conventional
assumptions about memory requirements, batching effectiveness, and
latency-throughput tradeoffs are systemwide generalizations that ignore the
heterogeneous computational patterns of individual neural network operators.
However, going towards network-level customization and operator-level
heterogeneity incur substantial Non-Recurring Engineering (NRE) costs. While
chiplet-based approaches have been proposed to amortize NRE costs, reuse
opportunities remain limited without carefully identifying which chiplets are
truly necessary. This paper introduces Mozart, a chiplet ecosystem and
accelerator codesign framework that systematically constructs low cost bespoke
application-specific integrated circuits (BASICs). BASICs leverage
operator-level disaggregation to explore chiplet and memory heterogeneity,
tensor fusion, and tensor parallelism, with place-and-route validation ensuring
physical implementability. The framework also enables constraint-aware
system-level optimization across deployment contexts ranging from datacenter
inference serving to edge computing in autonomous vehicles. The evaluation
confirms that with just 8 strategically selected chiplets, Mozart-generated
composite BASICs achieve 43.5%, 25.4%, 67.7%, and 78.8% reductions in energy,
energy-cost product, energy-delay product (EDP), and energy-delay-cost product
compared to traditional homogeneous accelerators. For datacenter LLM serving,
Mozart achieves 15-19% energy reduction and 35-39% energy-cost improvement. In
speculative decoding, Mozart delivers throughput improvements of 24.6-58.6%
while reducing energy consumption by 38.6-45.6%. For autonomous vehicle
perception, Mozart reduces energy-cost by 25.54% and energy by 10.53% under
real-time constraints.

</details>


### [2] [A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing](https://arxiv.org/abs/2510.08940)
*Abel Beyene,Zhongpan Wu,Yunus Dawji,Karim Hammad,Ebrahim Ghafar-Zadeh,Sebastian Magierowski*

Main category: cs.AR

TL;DR: 本文提出了一种基于RISC-V的SoC设计，用于手持DNA测序机，通过专用加速器实现了13倍性能提升和近3000倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 当前手持DNA测序机缺乏足够的嵌入式计算能力，依赖外部设备处理大量测量数据，导致通信负担重且无法实现真正的移动实时测序。

Method: 设计了一个22nm CMOS工艺的SoC，基于通用RISC-V核心，并集成了DNA检测专用加速器。

Result: 相比商用嵌入式多核处理器，系统性能提升13倍，能效提升近3000倍。

Conclusion: 该SoC设计为下一代手持DNA测序机提供了高性能、高能效的嵌入式处理解决方案，可实现真正的移动实时测序。

Abstract: Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing
importance in several life sciences fields as their small footprints enable a
broader range of use cases than their larger, stationary counterparts. However,
as currently designed, they lack sufficient embedded computing to process the
large volume of measurements generated by their internal sensory system. As a
consequence, they rely on external devices for additional processing
capability. This dependence on external processing places a significant
communication burden on the sequencer's embedded electronics. Moreover, it also
prevents a truly mobile solution for sequencing in real-time. Anticipating
next-generation machines that include suitably advanced processing, we present
a System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide
semiconductor (CMOS). Our design, based on a general-purpose reduced
instruction set computing (RISC-V) core, also includes accelerators for DNA
detection that allow our system to demonstrate a 13X performance improvement
over commercial embedded multicore processors combined with a near 3000X boost
in energy efficiency.

</details>


### [3] [HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization](https://arxiv.org/abs/2510.09010)
*Yipu Zhang,Chaofang Ma,Jinming Ge,Lin Jiang,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: HERO是一个基于强化学习的硬件感知量化框架，专门针对NeRF（神经辐射场）的3D重建应用，通过集成NeRF加速器模拟器实现自动化的硬件约束适应。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF量化方法未考虑硬件架构，导致在精度、延迟和模型大小的设计空间中难以找到最优解；同时现有加速器依赖人工专家探索设计空间，过程耗时低效。

Method: 使用强化学习框架，集成NeRF加速器模拟器生成实时硬件反馈，实现完全自动化的硬件约束适应。

Result: 相比之前最先进的CAQ框架，HERO实现了1.31-1.33倍的延迟改善、1.29-1.33倍的成本效率提升，以及更紧凑的模型大小。

Conclusion: HERO能够有效导航硬件与算法需求之间的复杂设计空间，为NeRF实现发现更优的量化策略。

Abstract: Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction
method, delivering high-quality results for AR/VR applications. While
quantization methods and hardware accelerators have been proposed to enhance
NeRF's computational efficiency, existing approaches face crucial limitations.
Current quantization methods operate without considering hardware architecture,
resulting in sub-optimal solutions within the vast design space encompassing
accuracy, latency, and model size. Additionally, existing NeRF accelerators
heavily rely on human experts to explore this design space, making the
optimization process time-consuming, inefficient, and unlikely to discover
optimal solutions. To address these challenges, we introduce HERO, a
reinforcement learning framework performing hardware-aware quantization for
NeRF. Our framework integrates a NeRF accelerator simulator to generate
real-time hardware feedback, enabling fully automated adaptation to hardware
constraints. Experimental results demonstrate that HERO achieves 1.31-1.33
$\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a
more compact model size compared to CAQ, a previous state-of-the-art NeRF
quantization framework. These results validate our framework's capability to
effectively navigate the complex design space between hardware and algorithm
requirements, discovering superior quantization policies for NeRF
implementation. Code is available at https://github.com/ypzhng/HERO.

</details>


### [4] [Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge](https://arxiv.org/abs/2510.09339)
*Sebastian Magierowski,Zhongpan Wu,Abel Beyene,Karim Hammad*

Main category: cs.AR

TL;DR: 开发了一个用于移动基因分析的CMOS片上系统，结合多核RISC-V处理器和深度学习/生物信息学加速器，实现实时设备端基因组分析


<details>
  <summary>Details</summary>
Motivation: 随着微型DNA测序硬件在移动场景中的成功应用，需要高效的边缘机器学习来处理比音频数据率高100倍以上的纳米孔测序原始数据

Method: 采用硬件/软件协同设计策略，在多核RISC-V处理器上紧密集成深度学习加速器和生物信息学加速器，构建异构计算架构

Result: 实现了能量高效的操作，能够在移动设备上进行实时基因组分析

Conclusion: 这项工作展示了深度学习、边缘计算和领域专用硬件的集成，推动了下一代移动基因组学的发展

Abstract: Miniature DNA sequencing hardware has begun to succeed in mobile contexts,
driving demand for efficient machine learning at the edge. This domain
leverages deep learning techniques familiar from speech and time-series
analysis for both low-level signal processing and high-level genomic
interpretation. Unlike audio, however, nanopore sequencing presents raw data
rates over 100X higher, requiring more aggressive compute and memory handling.
In this paper, we present a CMOS system-on-chip (SoC) designed for mobile
genetic analysis. Our approach combines a multi-core RISC-V processor with
tightly coupled accelerators for deep learning and bioinformatics. A
hardware/software co-design strategy enables energy-efficient operation across
a heterogeneous compute fabric, targeting real-time, on-device genome analysis.
This work exemplifies the integration of deep learning, edge computing, and
domain-specific hardware to advance next-generation mobile genomics.

</details>
