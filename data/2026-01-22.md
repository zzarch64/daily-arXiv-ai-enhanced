<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [End-to-End Transformer Acceleration Through Processing-in-Memory Architectures](https://arxiv.org/abs/2601.14260)
*Xiaoxuan Yang,Peilin Chen,Tergel Molom-Ochir,Yiran Chen*

Main category: cs.AR

TL;DR: 本文提出基于存内计算的方法来解决Transformer部署中的三大挑战：注意力机制的数据移动开销、KV缓存的内存瓶颈以及注意力二次复杂度问题，显著提升了能效和延迟性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在大规模部署时面临三个主要挑战：注意力机制需要大量矩阵乘法和频繁的数据移动导致高延迟和高能耗；长上下文推理中KV缓存会超过模型权重大小造成内存和带宽瓶颈；注意力的二次复杂度放大了数据移动和计算开销。

Method: 采用存内计算解决方案：1）重构注意力和前馈计算以最小化片外数据传输；2）动态压缩和剪枝KV缓存以管理内存增长；3）将注意力重新解释为关联内存操作以降低复杂度和硬件占用。

Result: 与最先进的加速器和通用GPU相比，存内计算设计在能效和延迟方面表现出显著改进，有效解决了计算开销、内存可扩展性和注意力复杂度问题。

Conclusion: 通过存内计算方法，本文解决了Transformer部署中的关键瓶颈，实现了Transformer模型的高效端到端加速，为大规模推理提供了可行的解决方案。

Abstract: Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models.

</details>


### [2] [Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability](https://arxiv.org/abs/2601.14347)
*George Rafael Gourdoumanis,Fotoini Oikonomou,Maria Pantazi-Kypraiou,Pavlos Stoikos,Olympia Axelou,Athanasios Tziouvaras,Georgios Karakonstantis,Tahani Aladwani,Christos Anagnostopoulos,Yixian Shen,Anuj Pathania,Alberto Garcia-Ortiz,George Floros*

Main category: cs.AR

TL;DR: 本文介绍了COIN-3D项目，这是一个欧洲合作项目，旨在开发用于3D芯片系统可靠性评估的开源EDA工具，以应对先进半导体制造中的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造从3纳米工艺向亚纳米领域发展，从FinFET转向GAAFET，制造复杂性和挑战不断增加。3D芯片方法成为应对这些限制的关键，但需要解决可靠性问题。

Method: 通过Horizon Europe Twinning项目COIN-3D，在欧洲领先机构间建立合作，开发用于2.5D/3D VLSI系统可靠性评估的新型开源EDA工具，集成物理层和系统层可靠性分析的先进算法。

Result: 项目旨在提供开源EDA工具用于3D系统可靠性评估，但目前处于项目介绍阶段，具体成果尚未公布。

Conclusion: COIN-3D项目通过跨机构合作开发可靠性评估工具，为应对先进3D芯片系统的制造挑战提供重要支持，促进异构系统设计的可靠性和成本效益。

Abstract: As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis.

</details>


### [3] [Pipeline Automation Framework for Reusable High-throughput Network Applications on FPGA](https://arxiv.org/abs/2601.15151)
*Jean Bruant,Pierre-Henri Horrein,Olivier Muller,Frédéric Pétrot*

Main category: cs.AR

TL;DR: PAF是一个基于Chisel的开源流水线自动化框架，用于参数化FPGA网络功能设计，实现跨不同FPGA平台的高效复用和优化。


<details>
  <summary>Details</summary>
Motivation: 在云服务提供商部署可扩展基础设施的背景下，FPGA因其低延迟和高吞吐量特性适合网络处理，但硬件设计过程缓慢，难以适应快速演进的敏捷基础设施需求。跨多种FPGA部署和维护网络功能需要对硬件设计进行针对性优化。

Method: 提出PAF（Pipeline Automation Framework），这是一个基于流水线导向设计方法的开源架构参数化框架。实现基于Chisel（Scala嵌入式硬件构造语言），利用其电路细化接口能力。应用于工业网络数据包分类系统。

Result: PAF展示了高效的参数化能力，能够在多种FPGA上复用和优化相同的流水线设计。框架将流水线描述聚焦于架构意图，显著减少了表达复杂功能所需的代码行数。自动化并未导致架构控制力的损失，实现了与详尽描述实现相当的性能和资源使用效率。

Conclusion: PAF框架成功解决了FPGA硬件设计过程缓慢的问题，通过参数化和自动化实现了跨平台设计复用，同时保持了对架构的精细控制，为敏捷网络基础设施部署提供了有效解决方案。

Abstract: In a context of ever-growing worldwide communication traffic, cloud service providers aim at deploying scalable infrastructures to address heterogeneous needs. Part of the network infrastructure, FPGAs are tailored to guarantee low-latency and high-throughput packet processing. However, slowness of the hardware design process impairs FPGA ability to be part of an agile infrastructure under constant evolution, from incident response to long-term transformation. Deploying and maintaining network functionalities across a wide variety of FPGAs raises the need to fine-tune hardware designs for several FPGA targets. To address this issue, we introduce PAF, an open-source architectural parameterization framework based on a pipeline-oriented design methodology. PAF (Pipeline Automation Framework) implementation is based on Chisel, a Scala-embedded Hardware Construction Language (HCL), that we leverage to interface with circuit elaboration. Applied to industrial network packet classification systems, PAF demonstrates efficient parameterization abilities, enabling to reuse and optimize the same pipelined design on several FPGAs. In addition, PAF focuses the pipeline description on the architectural intent, incidentally reducing the number of lines of code to express complex functionalities. Finally, PAF confirms that automation does not imply any loss of tight control on the architecture by achieving on par performance and resource usage with equivalent exhaustively described implementations.

</details>
