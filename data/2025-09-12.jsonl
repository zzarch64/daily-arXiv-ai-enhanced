{"id": "2509.09178", "categories": ["cs.AR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.09178", "abs": "https://arxiv.org/abs/2509.09178", "authors": ["Ayan Biswas", "Jimmy Jin"], "title": "Implementation of a 8-bit Wallace Tree Multiplier", "comment": null, "summary": "Wallace tree multipliers are a parallel digital multiplier architecture\ndesigned to minimize the worst-case time complexity of the circuit depth\nrelative to the input size [1]. In particular, it seeks to perform long\nmultiplication in the binary sense, reducing as many partial products per stage\nas possible through full and half adders circuits, achieving O(log(n)) where n\n= bit length of input. This paper provides an overview of the design, progress\nand methodology in the final project of ECE 55900, consisting of the schematic\nand layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in\nCadence Virtuoso, as well as any design attempts prior to the final product.\nThis also includes our endeavors in designing the final MAC (Multiply\nAccumulate) unit with undefined targets, which we chose to implement as a 16\nbit combinational multiply-add."}
{"id": "2509.09505", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.09505", "abs": "https://arxiv.org/abs/2509.09505", "authors": ["Haoran Wu", "Can Xiao", "Jiayi Nie", "Xuan Guo", "Binglei Lou", "Jeffrey T. H. Wong", "Zhiwen Mo", "Cheng Zhang", "Przemyslaw Forys", "Wayne Luk", "Hongxiang Fan", "Jianyi Cheng", "Timothy M. Jones", "Rika Antonova", "Robert Mullins", "Aaron Zhao"], "title": "Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference", "comment": null, "summary": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced."}
