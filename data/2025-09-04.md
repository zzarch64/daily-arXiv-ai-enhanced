<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Portable Targeted Sampling Framework Using LLVM](https://arxiv.org/abs/2509.02873)
*Zhantong Qiu,Mahyar Samani,Jason Lowe-Power*

Main category: cs.AR

TL;DR: Nugget是一个灵活的采样框架，通过LLVM IR级别的二进制无关区间分析，大幅降低架构评估成本，支持跨平台验证和仿真。


<details>
  <summary>Details</summary>
Motivation: 全面的架构评估受到仿真速度慢和逐二进制采样流程的限制，需要一种更高效、可移植的采样方法。

Method: 在LLVM IR级别进行二进制无关的区间分析，生成轻量级、跨平台的可执行文件(nuggets)，支持在真实硬件上验证后驱动仿真。

Result: 在SPEC CPU2017、NPB和LSMS基准测试中，相比功能仿真将区间分析成本降低了数个数量级(多线程NPB最高达578倍)，单线程开销低，支持原生速度验证。

Conclusion: Nugget使采样方法研究更快、更可移植，支持系统性能和模型准确性的评估。

Abstract: Comprehensive architectural evaluation of full workloads is throttled by slow
simulation and per-binary sampling pipelines. We present Nugget, a flexible
framework for portable sampling across simulators and real hardware, ISAs, and
libraries. Nugget operates at the LLVM IR level to perform binary-agnostic
interval analysis, then emits lightweight, cross-platform
executables--nuggets--that can be validated on real machines before driving
simulation. Across SPEC CPU2017, NPB, and LSMS, Nugget cuts interval-analysis
cost by orders of magnitude relative to functional simulation (up to ~578X on
multithreaded NPB), keeps single-thread overhead low, and enables native-speed
validation of selected samples. Case studies with gem5 show that nuggets
support evaluation of system performance and model accuracy. Nugget makes
sampling methodology research faster and more portable.

</details>


### [2] [FastCaps: A Design Methodology for Accelerating Capsule Network on Field Programmable Gate Arrays](https://arxiv.org/abs/2509.03103)
*Abdul Rahoof,Vivek Chaturvedi,Muhammad Shafique*

Main category: cs.AR

TL;DR: 该论文提出了一种在FPGA上加速完整胶囊网络的两步方法：首先使用前瞻核剪枝(LAKP)压缩网络，然后简化路由算法的非线性操作和并行化处理，实现了99%以上的压缩率和显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 胶囊网络(CapsNet)相比传统CNN在理解图像变化和泛化能力方面有显著改进，但由于胶囊结构和路由机制的复杂性，在FPGA上加速完整CapsNet具有挑战性。现有工作仅实现动态路由算法，无法满足实际应用需求。

Method: 提出两步方法：1) 使用新颖的前瞻核剪枝(LAKP)方法剪枝网络，基于模型参数的前瞻评分总和；2) 简化路由算法的非线性操作、重排序循环和并行化操作，降低硬件复杂度。

Result: 在MNIST和F-MNIST数据集上，LAKP方法分别实现99.26%和98.84%的有效压缩率，在PYNQ-Z1 FPGA上分别达到82FPS和48FPS的吞吐量。简化路由算法后吞吐量分别提升至1351FPS和934FPS。

Conclusion: 这项工作首次在低成本FPGA上实现了完整胶囊网络的高性能高效部署，为现代边缘设备中的CapsNet应用提供了可行解决方案。

Abstract: Capsule Network (CapsNet) has shown significant improvement in understanding
the variation in images along with better generalization ability compared to
traditional Convolutional Neural Network (CNN). CapsNet preserves spatial
relationship among extracted features and apply dynamic routing to efficiently
learn the internal connections between capsules. However, due to the capsule
structure and the complexity of the routing mechanism, it is non-trivial to
accelerate CapsNet performance in its original form on Field Programmable Gate
Array (FPGA). Most of the existing works on CapsNet have achieved limited
acceleration as they implement only the dynamic routing algorithm on FPGA,
while considering all the processing steps synergistically is important for
real-world applications of Capsule Networks. Towards this, we propose a novel
two-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune
the network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that
uses the sum of look-ahead scores of the model parameters. Next, we simplify
the nonlinear operations, reorder loops, and parallelize operations of the
routing algorithm to reduce CapsNet hardware complexity. To the best of our
knowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.
Experimental results on the MNIST and F-MNIST datasets (typical in Capsule
Network community) show that the proposed LAKP approach achieves an effective
compression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and
48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware
complexity of the routing algorithm increases the throughput to 1351 FPS and
934 FPS respectively. As corroborated by our results, this work enables highly
performance-efficient deployment of CapsNets on low-cost FPGA that are popular
in modern edge devices.

</details>


### [3] [CapsBeam: Accelerating Capsule Network based Beamformer for Ultrasound Non-Steered Plane Wave Imaging on Field Programmable Gate Array](https://arxiv.org/abs/2509.03201)
*Abdul Rahoof,Vivek Chaturvedi,Mahesh Raveendranatha Panicker,Muhammad Shafique*

Main category: cs.AR

TL;DR: 基于胶囊网络的CapsBeam材材形成算法，通过剪枝和量化优化，在FPGA上实现高效加速，显著提升了超声成像质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习材材形成算法模型过大过复杂，在边缘设备上部署面临重大挑战，需要轻量化高效的解决方案。

Method: 设计CapsBeam胶囊网络材材形成器，使用LAKP-ML多层预览内核剪枝技术减少参数冗余，通过量化、非线性操作简化和并行化降低硬件复杂度，并在FPGA上实现专门加速器。

Result: 与标准DAS算法相比，CapsBeam在离体实验中提升对比32.31%，轴向和侧向分辨率分别提升16.54%和6.7%；计算机模拟数据中对比度提升26%，分辨率提升13.6%和21.5%。剪枝后模型压缩比85%，FPGA加速器完成卷积操作30 GOPS和动态路由操作17.4 GOPS。

Conclusion: CapsBeam结合剪枝优化技术能够在保持图像质量的同时显著减少模型规模和计算复杂度，为边缘设备上部署高性能超声材材形成算法提供了可行解决方案。

Abstract: In recent years, there has been a growing trend in accelerating
computationally complex non-real-time beamforming algorithms in ultrasound
imaging using deep learning models. However, due to the large size and
complexity these state-of-the-art deep learning techniques poses significant
challenges when deploying on resource-constrained edge devices. In this work,
we propose a novel capsule network based beamformer called CapsBeam, designed
to operate on raw radio-frequency data and provide an envelope of beamformed
data through non-steered plane wave insonification. Experiments on in-vivo
data, CapsBeam reduced artifacts compared to the standard Delay-and-Sum (DAS)
beamforming. For in-vitro data, CapsBeam demonstrated a 32.31% increase in
contrast, along with gains of 16.54% and 6.7% in axial and lateral resolution
compared to the DAS. Similarly, in-silico data showed a 26% enhancement in
contrast, along with improvements of 13.6% and 21.5% in axial and lateral
resolution, respectively, compared to the DAS. To reduce the parameter
redundancy and enhance the computational efficiency, we pruned the model using
our multi-layer LookAhead Kernel Pruning (LAKP-ML) methodology, achieving a
compression ratio of 85% without affecting the image quality. Additionally, the
hardware complexity of the proposed model is reduced by applying quantization,
simplification of non-linear operations, and parallelizing operations. Finally,
we proposed a specialized accelerator architecture for the pruned and optimized
CapsBeam model, implemented on a Xilinx ZU7EV FPGA. The proposed accelerator
achieved a throughput of 30 GOPS for the convolution operation and 17.4 GOPS
for the dynamic routing operation.

</details>


### [4] [Amplifying Effective CXL Memory Bandwidth for LLM Inference via Transparent Near-Data Processing](https://arxiv.org/abs/2509.03377)
*Rui Xie,Asad Ul Haq,Linsen Ma,Yunhua Fang,Zirak Burzin Engineer,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: CXL-NDP是一种透明近数据处理架构，通过动态量化和无损压缩技术，在不改变CXL接口和AI模型的情况下，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理受限于CXL内存扩展的带宽瓶颈，需要一种透明且高效的解决方案来提升CXL带宽利用率。

Method: 在CXL设备内部集成精度可扩展的位平面布局进行动态量化，并对权重和KV缓存进行透明无损压缩。

Result: 端到端服务中，吞吐量提升43%，最大上下文长度扩展87%，KV缓存占用减少46.9%，且无精度损失。硬件合成验证了其实际可行性。

Conclusion: CXL-NDP为生成式AI基础设施提供了高效、可扩展的CXL内存解决方案，降低了采用门槛。

Abstract: Large language model (LLM) inference is bottlenecked by the limited bandwidth
of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a
transparent near-data processing architecture that amplifies effective CXL
bandwidth without requiring changes to the CXL.mem interface or AI models.
CXL-NDP integrates a precision-scalable bit-plane layout for dynamic
quantization with transparent lossless compression of weights and KV caches
directly within the CXL device. In end-to-end serving, CXL-NDP improves
throughput by 43%, extends the maximum context length by 87%, and reduces the
KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms
its practicality with a modest silicon footprint, lowering the barrier for
adopting efficient, scalable CXL-based memory in generative AI infrastructure.

</details>
