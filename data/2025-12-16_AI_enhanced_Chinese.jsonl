{"id": "2512.11826", "categories": ["cs.AR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11826", "abs": "https://arxiv.org/abs/2512.11826", "authors": ["Weihong Xu", "Chang Eun Song", "Haichao Yang", "Leo Liu", "Meng-Fan Chang", "Carlos H. Diaz", "Tajana Rosing", "Mingu Kang"], "title": "FSL-HDnn: A 40 nm Few-shot On-Device Learning Accelerator with Integrated Feature Extraction and Hyperdimensional Computing", "comment": null, "summary": "This paper introduces FSL-HDnn, an energy-efficient accelerator that implements the end-to-end pipeline of feature extraction and on-device few-shot learning (FSL). The accelerator addresses fundamental challenges of on-device learning (ODL) for resource-constrained edge applications through two synergistic modules: a parameter-efficient feature extractor employing weight clustering and an FSL classifier based on hyperdimensional computing (HDC). The feature extractor exploits the weight clustering mechanism to reduce computational complexity, while the HDC-based FSL classifier eliminates gradient-based back propagation operations, enabling single-pass training with substantially reduced latency. Additionally, FSL-HDnn enables low-latency ODL and inference via two proposed optimization strategies, including an early-exit mechanism with branch feature extraction and batched single-pass training that improves hardware utilization. Measurement results demonstrate that our chip fabricated in a 40 nm CMOS process delivers superior training energy efficiency of 6 mJ/image and end-to-end training throughput of 28 images/s on a 10-way 5-shot FSL task. The end-to-end training latency is also reduced by 2x to 20.9x compared to state-of-the-art ODL chips.", "AI": {"tldr": "FSL-HDnn\u662f\u4e00\u4e2a\u80fd\u6548\u4f18\u5316\u7684\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u6743\u91cd\u805a\u7c7b\u7279\u5f81\u63d0\u53d6\u5668\u548c\u8d85\u7ef4\u8ba1\u7b97\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u8bbe\u5907\u4e0a\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8bbe\u5907\u5b66\u4e60\uff08ODL\uff09\u7684\u57fa\u672c\u6311\u6218\uff0c\u5305\u62ec\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u8bad\u7ec3\u5ef6\u8fdf\u5927\u548c\u80fd\u8017\u9ad8\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u534f\u540c\u6a21\u5757\uff1a1\uff09\u57fa\u4e8e\u6743\u91cd\u805a\u7c7b\u7684\u53c2\u6570\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u5668\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b2\uff09\u57fa\u4e8e\u8d85\u7ef4\u8ba1\u7b97\uff08HDC\uff09\u7684\u5c11\u6837\u672c\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u6d88\u9664\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\uff0c\u5b9e\u73b0\u5355\u6b21\u8bad\u7ec3\u3002\u8fd8\u63d0\u51fa\u4e24\u79cd\u4f18\u5316\u7b56\u7565\uff1a\u5206\u652f\u7279\u5f81\u63d0\u53d6\u7684\u65e9\u9000\u673a\u5236\u548c\u6279\u91cf\u5355\u6b21\u8bad\u7ec3\u3002", "result": "40nm CMOS\u5de5\u827a\u82af\u7247\u572810-way 5-shot\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e0a\uff0c\u8fbe\u52306 mJ/\u56fe\u50cf\u7684\u8bad\u7ec3\u80fd\u6548\u548c28\u56fe\u50cf/\u79d2\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u3002\u7aef\u5230\u7aef\u8bad\u7ec3\u5ef6\u8fdf\u6bd4\u6700\u5148\u8fdb\u7684ODL\u82af\u7247\u964d\u4f4e2-20.9\u500d\u3002", "conclusion": "FSL-HDnn\u901a\u8fc7\u521b\u65b0\u7684\u6743\u91cd\u805a\u7c7b\u7279\u5f81\u63d0\u53d6\u548c\u8d85\u7ef4\u8ba1\u7b97\u5206\u7c7b\u5668\u67b6\u6784\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5c11\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u80fd\u6548\u548c\u541e\u5410\u91cf\u3002"}}
{"id": "2512.12106", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.12106", "abs": "https://arxiv.org/abs/2512.12106", "authors": ["Victor Cai", "Jennifer Zhou", "Haebin Do", "David Brooks", "Gu-Yeon Wei"], "title": "DreamRAM: A Fine-Grained Configurable Design Space Modeling Tool for Custom 3D Die-Stacked DRAM", "comment": "Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "3D die-stacked DRAM has emerged as a key technology for delivering high bandwidth and high density for applications such as high-performance computing, graphics, and machine learning. However, different applications place diverse and sometimes diverging demands on power, performance, and area that cannot be universally satisfied with fixed commodity DRAM designs. Die stacking creates the opportunity for a large DRAM design space through 3D integration and expanded total die area. To open and navigate this expansive design space of customized memory architectures that cater to application-specific needs, we introduce DreamRAM, a configurable bandwidth, capacity, energy, latency, and area modeling tool for custom 3D die-stacked DRAM designs. DreamRAM exposes fine-grained design customization parameters at the MAT, subarray, bank, and inter-bank levels, including extensions of partial page and subarray parallelism proposals found in the literature, to open a large previously-unexplored design space. DreamRAM analytically models wire pitch, width, length, capacitance, and scaling parameters to capture the performance tradeoffs of physical layout and routing design choices. Routing awareness enables DreamRAM to model a custom MAT-level routing scheme, Dataline-Over-MAT (DLOMAT), to facilitate better bandwidth tradeoffs. DreamRAM is calibrated and validated against published industry HBM3 and HBM2E designs. Within DreamRAM's rich design space, we identify designs that achieve each of 66% higher bandwidth, 100% higher capacity, and 45% lower power and energy per bit compared to the baseline design, each on an iso-bandwidth, iso-capacity, and iso-power basis.", "AI": {"tldr": "DreamRAM\u662f\u4e00\u4e2a\u53ef\u914d\u7f6e\u76843D\u5806\u53e0DRAM\u5efa\u6a21\u5de5\u5177\uff0c\u7528\u4e8e\u63a2\u7d22\u5b9a\u5236\u5316\u5185\u5b58\u67b6\u6784\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u901a\u8fc7\u7cbe\u7ec6\u53c2\u6570\u8c03\u8282\u5b9e\u73b0\u5e26\u5bbd\u3001\u5bb9\u91cf\u3001\u80fd\u8017\u548c\u5ef6\u8fdf\u7684\u4f18\u5316\u3002", "motivation": "\u4e0d\u540c\u5e94\u7528\u5bf9DRAM\u7684\u529f\u8017\u3001\u6027\u80fd\u548c\u9762\u79ef\u9700\u6c42\u5404\u5f02\uff0c\u800c\u56fa\u5b9a\u5546\u54c1\u5316DRAM\u8bbe\u8ba1\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u591a\u6837\u5316\u9700\u6c42\u30023D\u5806\u53e0\u6280\u672f\u521b\u9020\u4e86\u66f4\u5927\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u9700\u8981\u5de5\u5177\u6765\u63a2\u7d22\u5b9a\u5236\u5316\u5185\u5b58\u67b6\u6784\u3002", "method": "DreamRAM\u63d0\u4f9b\u53ef\u914d\u7f6e\u7684\u5e26\u5bbd\u3001\u5bb9\u91cf\u3001\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u9762\u79ef\u5efa\u6a21\u5de5\u5177\uff0c\u66b4\u9732MAT\u3001\u5b50\u9635\u5217\u3001bank\u548cbank\u95f4\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u8bbe\u8ba1\u53c2\u6570\uff0c\u5305\u62ec\u6269\u5c55\u90e8\u5206\u9875\u9762\u548c\u5b50\u9635\u5217\u5e76\u884c\u6027\u65b9\u6848\u3002\u5de5\u5177\u901a\u8fc7\u5206\u6790\u7ebf\u5bbd\u3001\u95f4\u8ddd\u3001\u957f\u5ea6\u3001\u7535\u5bb9\u548c\u7f29\u653e\u53c2\u6570\u6765\u5efa\u6a21\u7269\u7406\u5e03\u5c40\u548c\u5e03\u7ebf\u8bbe\u8ba1\u9009\u62e9\u7684\u6027\u80fd\u6743\u8861\u3002", "result": "DreamRAM\u5df2\u9488\u5bf9HBM3\u548cHBM2E\u884c\u4e1a\u8bbe\u8ba1\u8fdb\u884c\u6821\u51c6\u548c\u9a8c\u8bc1\u3002\u5728\u5176\u4e30\u5bcc\u7684\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\uff0c\u8bc6\u522b\u51fa\u76f8\u6bd4\u57fa\u7ebf\u8bbe\u8ba1\u5206\u522b\u5b9e\u73b066%\u66f4\u9ad8\u5e26\u5bbd\u3001100%\u66f4\u9ad8\u5bb9\u91cf\u4ee5\u53ca45%\u66f4\u4f4e\u6bcf\u6bd4\u7279\u529f\u8017\u548c\u80fd\u8017\u7684\u8bbe\u8ba1\u65b9\u6848\u3002", "conclusion": "DreamRAM\u4e3a\u5b9a\u5236\u53163D\u5806\u53e0DRAM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5efa\u6a21\u5de5\u5177\uff0c\u80fd\u591f\u63a2\u7d22\u4f20\u7edf\u56fa\u5b9aDRAM\u8bbe\u8ba1\u65e0\u6cd5\u6ee1\u8db3\u7684\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42\uff0c\u5728\u5e26\u5bbd\u3001\u5bb9\u91cf\u548c\u529f\u8017\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2512.12847", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12847", "abs": "https://arxiv.org/abs/2512.12847", "authors": ["Jonathan Herbst", "Michael Pellauer", "Sherief Reda"], "title": "HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility", "comment": "12 pages, 6 figures, 5 tables", "summary": "We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u541e\u5410\u91cf\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u786c\u4ef6\u5d4c\u5165\u5927\u90e8\u5206\u7f51\u7edc\u5c42\u3001Po2\u91cf\u5316\u6743\u91cd\u5b9e\u73b0\u4e58\u6cd5\u66ff\u6362\u4e3a\u52a0\u6cd5\uff0c\u4fdd\u7559\u53ef\u7f16\u7a0b\u6700\u7ec8\u5c42\u4ee5\u517c\u987e\u7075\u6d3b\u6027\u548c\u541e\u5410\u91cf\uff0c\u5728MobileNetV2\u4e0a\u5b9e\u73b020-67\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8ba1\u7b97\u548c\u6570\u636e\u4e2d\u5fc3\u90e8\u7f72\u4e2d\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u9ad8\u541e\u5410\u91cf\u3001\u4f4e\u80fd\u8017\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e00\u5b9a\u7684\u6a21\u578b\u7075\u6d3b\u6027\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u53c2\u6570\u7a33\u5b9a\u7684\u8fde\u7eed\u611f\u77e5\u4efb\u52a1\u4e2d\u3002", "method": "1) \u786c\u4ef6\u5d4c\u5165\u5927\u90e8\u5206\u7f51\u7edc\u5c42\u4ee5\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u548c\u5185\u5b58\u4f7f\u7528\uff1b2) \u91c7\u75282\u7684\u5e42\u6b21(Po2)\u91cf\u5316\u6743\u91cd\uff0c\u5c06\u4e58\u6cd5\u8f6c\u6362\u4e3a\u7b80\u5355\u91cd\u8fde\u548c\u52a0\u6cd5\uff1b3) \u4fdd\u7559\u5c0f\u578b\u795e\u7ecf\u5904\u7406\u5355\u5143\u4f5c\u4e3a\u53ef\u7f16\u7a0b\u6700\u7ec8\u5206\u7c7b\u5c42\uff1b4) \u57287nm ASIC\u6d41\u7a0b\u4e2d\u5b9e\u73b0\uff0c\u4ee5MobileNetV2\u4e3a\u57fa\u51c6\u3002", "result": "MobileNetV2\u63a8\u7406\u541e\u5410\u91cf\u76f8\u6bd4\u5168\u53ef\u7f16\u7a0bGPU\u63d0\u534720\u500d\uff08\u9700\u5fae\u8c03\u65f6\uff09\u81f367\u500d\uff08\u65e0\u9700\u5fae\u8c03\u65f6\uff09\uff0c\u5206\u522b\u8fbe\u5230121\u4e07\u548c400\u4e07\u56fe\u50cf/\u79d2\uff0c\u540c\u65f6\u4fdd\u6301\u5fae\u8c03\u7075\u6d3b\u6027\uff0c\u5c55\u793a\u4e86\u67b6\u6784\u5728\u541e\u5410\u91cf\u3001\u9762\u79ef\u3001\u7cbe\u5ea6\u548c\u91cf\u5316\u654f\u611f\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u786c\u4ef6\u5d4c\u5165\u52a0Po2\u91cf\u5316\u7684\u52a0\u901f\u5668\u67b6\u6784\u5728\u4fdd\u6301\u6700\u7ec8\u5c42\u53ef\u7f16\u7a0b\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u541e\u5410\u91cf\u548c\u80fd\u6548\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6a21\u578b\u53c2\u6570\u7a33\u5b9a\u7684\u8fb9\u7f18\u8ba1\u7b97\u548c\u6570\u636e\u4e2d\u5fc3\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2512.12850", "categories": ["cs.AR", "cs.LG", "eess.SY", "hep-ex"], "pdf": "https://arxiv.org/pdf/2512.12850", "abs": "https://arxiv.org/abs/2512.12850", "authors": ["Duc Hoang", "Aarush Gupta", "Philip Harris"], "title": "KANEL\u00c9: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation", "comment": "International Symposium on Field-Programmable Gate Arrays 2026 (ISFPGA'2026)", "summary": "Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANEL\u00c9, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANEL\u00c9 matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.", "AI": {"tldr": "KANEL\u00c9\u6846\u67b6\u9996\u6b21\u7cfb\u7edf\u5316\u5730\u5c06KAN\u7f51\u7edc\u90e8\u7f72\u5230FPGA\u4e0a\uff0c\u901a\u8fc7\u91cf\u5316\u526a\u679d\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u8fbe2700\u500d\u52a0\u901f\u548c\u663e\u8457\u8d44\u6e90\u8282\u7701\uff0c\u5728\u7b26\u53f7/\u7269\u7406\u516c\u5f0f\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "FPGA\u4e0a\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u73b0\u6709LUT-based\u65b9\u6848\u867d\u6709\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9KAN\u7f51\u7edc\u7684\u7cfb\u7edf\u5316\u90e8\u7f72\u6846\u67b6\u3002KAN\u7f51\u7edc\u72ec\u7279\u7684\u53ef\u5b66\u4e60\u4e00\u7ef4\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u5929\u7136\u9002\u5408\u79bb\u6563\u5316\u548cLUT\u6620\u5c04\uff0c\u4f46\u5c1a\u672a\u88ab\u5145\u5206\u5229\u7528\u4e8eFPGA\u90e8\u7f72\u3002", "method": "\u63d0\u51faKANEL\u00c9\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u5316\u8bbe\u8ba1KAN\u5728FPGA\u4e0a\u7684\u5b9e\u73b0\u6d41\u7a0b\u3002\u901a\u8fc7\u534f\u540c\u4f18\u5316\u8bad\u7ec3\u3001\u91cf\u5316\u548c\u526a\u679d\uff0c\u5b9e\u73b0\u7d27\u51d1\u3001\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u7684KAN\u67b6\u6784\u3002\u5229\u7528KAN\u7684\u53ef\u5b66\u4e60\u4e00\u7ef4\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u56fa\u5b9a\u57df\u7279\u6027\uff0c\u81ea\u7136\u9002\u5408\u79bb\u6563\u5316\u548c\u9ad8\u6548LUT\u6620\u5c04\u3002", "result": "\u76f8\u6bd4\u73b0\u6709KAN-on-FPGA\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u8fbe2700\u500d\u52a0\u901f\u548c\u6570\u91cf\u7ea7\u8d44\u6e90\u8282\u7701\u3002\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKANEL\u00c9\u5339\u914d\u6216\u8d85\u8d8a\u5176\u4ed6LUT-based\u67b6\u6784\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u7b26\u53f7\u6216\u7269\u7406\u516c\u5f0f\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u540c\u65f6\u5e73\u8861FPGA\u786c\u4ef6\u8d44\u6e90\u4f7f\u7528\u3002", "conclusion": "KANEL\u00c9\u6846\u67b6\u6210\u529f\u5c06KAN\u7f51\u7edc\u4f18\u52bf\u4e0eFPGA\u786c\u4ef6\u7279\u6027\u7ed3\u5408\uff0c\u4e3a\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u63a7\u5236\u7cfb\u7edf\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.12990", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.12990", "abs": "https://arxiv.org/abs/2512.12990", "authors": ["Yuseon Choi", "Sangjin Kim", "Jungjun Oh", "Gwangtae Park", "Byeongcheol Kim", "Hoi-Jun Yoo"], "title": "SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference", "comment": null, "summary": "MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.", "AI": {"tldr": "SliceMoE\uff1a\u4e00\u79cd\u9762\u5411\u9519\u5931\u7387\u7ea6\u675f\u90e8\u7f72\u7684\u8282\u80fdMoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f4d\u5207\u7247\u7f13\u5b58\u548c\u9884\u6d4b\u6027\u7f13\u5b58\u9884\u70ed\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u89e3\u7801\u9636\u6bb5\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "MoE\u6a21\u578b\u901a\u8fc7\u6761\u4ef6\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u53c2\u6570\u91cf\u548c\u6602\u8d35\u7684\u4e13\u5bb6\u5378\u8f7d\u4f7f\u5f97\u5728\u8bbe\u5907\u7aef\u90e8\u7f72\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u52a0\u901f\u6280\u672f\uff08\u5982\u9884\u53d6\u6216\u4e13\u5bb6\u805a\u7c7b\uff09\u5f80\u5f80\u589e\u52a0\u80fd\u8017\u6216\u964d\u4f4e\u4e13\u5bb6\u591a\u6837\u6027\u3002", "method": "1. \u52a8\u6001\u4f4d\u5207\u7247\u7f13\u5b58\uff08DBSC\uff09\uff1a\u4ee5\u5207\u7247\u7ea7\u7c92\u5ea6\u7f13\u5b58\u4e13\u5bb6\uff0c\u6309\u9700\u5206\u914d\u7cbe\u5ea6\u4ee5\u6269\u5c55\u6709\u6548\u4e13\u5bb6\u5bb9\u91cf\uff1b2. \u65e0\u9700\u6821\u51c6\u7684\u975e\u5bf9\u79f0\u5957\u5a03\u91cf\u5316\uff08AMAT\uff09\uff1a\u57fa\u4e8e\u622a\u65ad\u7684\u65b9\u6848\uff0c\u5728\u4f4e\u6bd4\u7279\u548c\u9ad8\u6bd4\u7279\u5207\u7247\u4e4b\u95f4\u4fdd\u6301\u517c\u5bb9\u6027\uff0c\u65e0\u9700\u5185\u5b58\u590d\u5236\uff1b3. \u9884\u6d4b\u6027\u7f13\u5b58\u9884\u70ed\uff08PCW\uff09\uff1a\u5728\u9884\u586b\u5145\u9636\u6bb5\u91cd\u5851\u7f13\u5b58\u5185\u5bb9\u4ee5\u51cf\u5c11\u65e9\u671f\u89e3\u7801\u7684\u51b7\u7f3a\u5931\u3002", "result": "\u5728DeepSeek-V2-Lite\u548cQwen1.5-MoE-A2.7B\u4e0a\u8bc4\u4f30\uff0cSliceMoE\u5206\u522b\u5c06\u89e3\u7801\u9636\u6bb5\u80fd\u8017\u964d\u4f4e2.37\u500d\u548c2.85\u500d\uff0c\u89e3\u7801\u5ef6\u8fdf\u63d0\u53471.81\u500d\u548c1.64\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u9ad8\u6bd4\u7279\u7cbe\u5ea6\u3002", "conclusion": "\u5207\u7247\u7ea7\u7f13\u5b58\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bbe\u5907\u7aefMoE\u90e8\u7f72\uff0c\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u548c\u5ef6\u8fdf\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.13133", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13133", "abs": "https://arxiv.org/abs/2512.13133", "authors": ["Shuo Liu"], "title": "An Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework for High-Performance Ultra-Large Scale Layout Pattern Clustering", "comment": "First Place Winner of the 2025 China Postgraduate EDA Elite Challenge (Problem 7)", "summary": "With the aggressive scaling of VLSI technology, the explosion of layout patterns creates a critical bottleneck for DFM applications like OPC. Pattern clustering is essential to reduce data complexity, yet existing methods struggle with computational prohibitiveness ($O(N^2)$ comparisons), sub-optimal discrete sampling for center alignment, and difficult speed-quality trade-offs. To address these, we propose an Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework. First, to resolve alignment ambiguity, we introduce a hybrid suite of high-performance algorithms: an FFT-based Phase Correlation method for cosine similarity constraints, and a Robust Geometric Min-Max strategy for edge displacement constraints that analytically solves for the global optimum. Second, we model clustering as a Set Cover Problem (SCP) using a Surprisal-Based Lazy Greedy heuristic within a coarse-to-fine iterative refinement loop to ensure convergence. Additionally, a multi-stage pruning mechanism filters over 99% of redundant computations. Experimental results on the 2025 China Postgraduate EDA Elite Challenge benchmark demonstrate a 93.4% compression ratio relative to raw inputs and an over 100x speedup compared to the official baseline, effectively handling tens of thousands of patterns in seconds. Securing First Place among 77 teams, this approach proves its superiority in solving the NP-Hard layout clustering problem with an optimal balance of scalability and precision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u5bf9\u9f50\u7684\u8fed\u4ee3\u95ed\u73af\u6536\u655b\u6846\u67b6\uff0c\u89e3\u51b3VLSI\u5e03\u5c40\u6a21\u5f0f\u805a\u7c7b\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3001\u5bf9\u9f50\u6a21\u7cca\u548c\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u5728EDA\u6311\u6218\u8d5b\u4e2d\u5b9e\u73b093.4%\u538b\u7f29\u6bd4\u548c100\u500d\u52a0\u901f\u3002", "motivation": "\u968f\u7740VLSI\u6280\u672f\u7f29\u653e\uff0c\u5e03\u5c40\u6a21\u5f0f\u7206\u70b8\u5f0f\u589e\u957f\u6210\u4e3aDFM\u5e94\u7528\uff08\u5982OPC\uff09\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff08O(N\u00b2)\uff09\u3001\u4e2d\u5fc3\u5bf9\u9f50\u79bb\u6563\u91c7\u6837\u6b21\u4f18\u3001\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u56f0\u96be\u7b49\u95ee\u9898\u3002", "method": "1) \u63d0\u51fa\u6df7\u5408\u9ad8\u6027\u80fd\u7b97\u6cd5\u89e3\u51b3\u5bf9\u9f50\u6a21\u7cca\uff1aFFT\u76f8\u4f4d\u76f8\u5173\u6cd5\u7528\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7ea6\u675f\uff0c\u9c81\u68d2\u51e0\u4f55\u6700\u5c0f-\u6700\u5927\u7b56\u7565\u7528\u4e8e\u8fb9\u7f18\u4f4d\u79fb\u7ea6\u675f\uff1b2) \u5c06\u805a\u7c7b\u5efa\u6a21\u4e3a\u96c6\u5408\u8986\u76d6\u95ee\u9898\uff0c\u5728\u7c97\u5230\u7ec6\u8fed\u4ee3\u7ec6\u5316\u5faa\u73af\u4e2d\u4f7f\u7528\u57fa\u4e8e\u4fe1\u606f\u91cf\u7684\u60f0\u6027\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\u786e\u4fdd\u6536\u655b\uff1b3) \u591a\u9636\u6bb5\u526a\u679d\u673a\u5236\u8fc7\u6ee499%\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u57282025\u5e74\u4e2d\u56fd\u7814\u7a76\u751fEDA\u7cbe\u82f1\u6311\u6218\u8d5b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u76f8\u5bf9\u4e8e\u539f\u59cb\u8f93\u516593.4%\u7684\u538b\u7f29\u6bd4\uff0c\u76f8\u6bd4\u5b98\u65b9\u57fa\u7ebf\u8d85\u8fc7100\u500d\u52a0\u901f\uff0c\u80fd\u5728\u6570\u79d2\u5185\u5904\u7406\u6570\u4e07\u4e2a\u6a21\u5f0f\u3002\u572877\u652f\u961f\u4f0d\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u89e3\u51b3NP-Hard\u5e03\u5c40\u805a\u7c7b\u95ee\u9898\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u6027\u548c\u7cbe\u5ea6\u7684\u6700\u4f18\u5e73\u8861\uff0c\u4e3aDFM\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13282", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13282", "abs": "https://arxiv.org/abs/2512.13282", "authors": ["Endri Taka", "Andre Roesti", "Joseph Melber", "Pranathi Vasireddy", "Kristof Denolf", "Diana Marculescu"], "title": "Striking the Balance: GEMM Performance Optimization Across Generations of Ryzen AI NPUs", "comment": null, "summary": "The high computational and memory demands of modern deep learning (DL) workloads have led to the development of specialized hardware devices from cloud to edge, such as AMD's Ryzen AI XDNA NPUs. Optimizing general matrix multiplication (GEMM) algorithms for these architectures is critical for improving DL workload performance. To this end, this paper presents a common systematic methodology to optimize GEMM workloads across the two current NPU generations, namely XDNA and XDNA2. Our implementations exploit the unique architectural features of AMD's NPUs and address key performance bottlenecks at the system level. End-to-end performance evaluation across various GEMM sizes demonstrates state-of-the-art throughput of up to 6.76 TOPS (XDNA) and 38.05 TOPS (XDNA2) for 8-bit integer (int8) precision. Similarly, for brain floating-point (bf16) precision, our GEMM implementations attain up to 3.14 TOPS (XDNA) and 14.71 TOPS (XDNA2). This work provides significant insights into key performance aspects of optimizing GEMM workloads on Ryzen AI NPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316AMD Ryzen AI NPU\uff08XDNA\u548cXDNA2\uff09\u4e0a\u7684\u901a\u7528\u77e9\u9635\u4e58\u6cd5\uff08GEMM\uff09\u6027\u80fd\uff0c\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u4e86int8\u7cbe\u5ea6\u4e0b\u6700\u9ad86.76 TOPS\uff08XDNA\uff09\u548c38.05 TOPS\uff08XDNA2\uff09\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u6781\u9ad8\uff0c\u9700\u8981\u9488\u5bf9\u4ece\u4e91\u5230\u8fb9\u7f18\u7684\u4e13\u7528\u786c\u4ef6\uff08\u5982AMD Ryzen AI XDNA NPU\uff09\u8fdb\u884c\u4f18\u5316\u3002\u4f18\u5316\u8fd9\u4e9b\u67b6\u6784\u4e0a\u7684GEMM\u7b97\u6cd5\u5bf9\u4e8e\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u7cfb\u7edf\u5316\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u4f18\u5316\u4e24\u4e2aNPU\u4e16\u4ee3\uff08XDNA\u548cXDNA2\uff09\u4e0a\u7684GEMM\u5de5\u4f5c\u8d1f\u8f7d\u3002\u5b9e\u73b0\u5229\u7528\u4e86AMD NPU\u7684\u72ec\u7279\u67b6\u6784\u7279\u6027\uff0c\u5e76\u5728\u7cfb\u7edf\u5c42\u9762\u89e3\u51b3\u4e86\u5173\u952e\u6027\u80fd\u74f6\u9888\u3002", "result": "\u7aef\u5230\u7aef\u6027\u80fd\u8bc4\u4f30\u663e\u793a\uff0c\u57288\u4f4d\u6574\u6570\uff08int8\uff09\u7cbe\u5ea6\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad86.76 TOPS\uff08XDNA\uff09\u548c38.05 TOPS\uff08XDNA2\uff09\u7684\u541e\u5410\u91cf\uff1b\u5728\u8111\u6d6e\u70b9\u6570\uff08bf16\uff09\u7cbe\u5ea6\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad83.14 TOPS\uff08XDNA\uff09\u548c14.71 TOPS\uff08XDNA2\uff09\u7684\u541e\u5410\u91cf\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4f18\u5316Ryzen AI NPU\u4e0a\u7684GEMM\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6027\u80fd\u6d1e\u5bdf\uff0c\u5c55\u793a\u4e86\u9488\u5bf9\u7279\u5b9a\u786c\u4ef6\u67b6\u6784\u8fdb\u884c\u7cfb\u7edf\u5316\u4f18\u5316\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.13479", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13479", "abs": "https://arxiv.org/abs/2512.13479", "authors": ["Kunal Pai", "Harshil Patel", "Erin Le", "Noah Krim", "Mahyar Samani", "Bobby R. Bruce", "Jason Lowe-Power"], "title": "Reproducibility and Standardization in gem5 Resources v25.0", "comment": null, "summary": "Reproducibility in simulation-based computer architecture research requires coordinating artifacts like disk images, kernels, and benchmarks, but existing workflows are inconsistent. We improve gem5, an open-source simulator with over 1600 forks, and gem5 Resources, a centralized repository of over 2000 pre-packaged artifacts, to address these issues. While gem5 Resources enables artifact sharing, researchers still face challenges. Creating custom disk images is complex and time-consuming, with no standardized process across ISAs, making it difficult to extend and share images. gem5 provides limited guest-host communication features through a set of predefined exit events that restrict researchers' ability to dynamically control and monitor simulations. Lastly, running simulations with multiple workloads requires researchers to write custom external scripts to coordinate multiple gem5 simulations which creates error-prone and hard-to-reproduce workflows. To overcome this, we introduce several features in gem5 and gem5 Resources. We standardize disk-image creation across x86, ARM, and RISC-V using Packer, and provide validated base images with pre-annotated benchmark suites (NPB, GAPBS). We provide 12 new disk images, 6 new kernels, and over 200 workloads across three ISAs. We refactor the exit event system to a class-based model and introduce hypercalls for enhanced guest-host communication that allows researchers to define custom behavior for their exit events. We also provide a utility to remotely monitor simulations and the gem5-bridge driver for user-space m5 operations. Additionally, we implemented Suites and MultiSim to enable parallel full-system simulations from gem5 configuration scripts, eliminating the need for external scripting. These features reduce setup complexity and provide extensible, validated resources that improve reproducibility and standardization.", "AI": {"tldr": "\u8bba\u6587\u6539\u8fdb\u4e86gem5\u6a21\u62df\u5668\u548cgem5 Resources\u8d44\u6e90\u5e93\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u78c1\u76d8\u955c\u50cf\u521b\u5efa\u3001\u91cd\u6784\u9000\u51fa\u4e8b\u4ef6\u7cfb\u7edf\u3001\u5f15\u5165\u5e76\u884c\u6a21\u62df\u529f\u80fd\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u673a\u67b6\u6784\u7814\u7a76\u4e2d\u4eff\u771f\u53ef\u91cd\u590d\u6027\u7684\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u4eff\u771f\u7684\u8ba1\u7b97\u673a\u67b6\u6784\u7814\u7a76\u9700\u8981\u534f\u8c03\u78c1\u76d8\u955c\u50cf\u3001\u5185\u6838\u548c\u57fa\u51c6\u6d4b\u8bd5\u7b49\u5de5\u4ef6\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u4e00\u81f4\u3002gem5 Resources\u867d\u7136\u652f\u6301\u5de5\u4ef6\u5171\u4eab\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u4ecd\u9762\u4e34\u521b\u5efa\u81ea\u5b9a\u4e49\u78c1\u76d8\u955c\u50cf\u590d\u6742\u3001\u4e3b\u673a-\u5ba2\u673a\u901a\u4fe1\u529f\u80fd\u6709\u9650\u3001\u591a\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u9700\u8981\u5916\u90e8\u811a\u672c\u534f\u8c03\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u53ef\u91cd\u590d\u6027\u5dee\u3002", "method": "1. \u4f7f\u7528Packer\u6807\u51c6\u5316x86\u3001ARM\u548cRISC-V\u7684\u78c1\u76d8\u955c\u50cf\u521b\u5efa\uff0c\u63d0\u4f9b\u9884\u6807\u6ce8\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u7684\u9a8c\u8bc1\u57fa\u7840\u955c\u50cf\uff1b2. \u91cd\u6784\u9000\u51fa\u4e8b\u4ef6\u7cfb\u7edf\u4e3a\u57fa\u4e8e\u7c7b\u7684\u6a21\u578b\uff0c\u5f15\u5165hypercalls\u589e\u5f3a\u4e3b\u673a-\u5ba2\u673a\u901a\u4fe1\uff1b3. \u5b9e\u73b0Suites\u548cMultiSim\u529f\u80fd\uff0c\u652f\u6301\u4ecegem5\u914d\u7f6e\u811a\u672c\u8fdb\u884c\u5e76\u884c\u5168\u7cfb\u7edf\u6a21\u62df\u3002", "result": "\u63d0\u4f9b\u4e8612\u4e2a\u65b0\u78c1\u76d8\u955c\u50cf\u30016\u4e2a\u65b0\u5185\u6838\u548c\u8de8\u4e09\u79cdISA\u7684200\u591a\u4e2a\u5de5\u4f5c\u8d1f\u8f7d\u3002\u589e\u5f3a\u4e86gem5\u7684\u901a\u4fe1\u548c\u76d1\u63a7\u80fd\u529b\uff0c\u5305\u62ec\u8fdc\u7a0b\u76d1\u63a7\u5de5\u5177\u548cgem5-bridge\u9a71\u52a8\u7a0b\u5e8f\u3002\u6d88\u9664\u4e86\u591a\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u5bf9\u5916\u90e8\u811a\u672c\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u5e76\u884c\u6a21\u62df\u3002", "conclusion": "\u8fd9\u4e9b\u6539\u8fdb\u964d\u4f4e\u4e86\u8bbe\u7f6e\u590d\u6742\u6027\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9a8c\u8bc1\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8e\u4eff\u771f\u7684\u8ba1\u7b97\u673a\u67b6\u6784\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u6807\u51c6\u5316\u6c34\u5e73\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u521b\u5efa\u3001\u5171\u4eab\u548c\u590d\u73b0\u5b9e\u9a8c\u3002"}}
{"id": "2512.13686", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13686", "abs": "https://arxiv.org/abs/2512.13686", "authors": ["Juncheng Huo", "Yunfan Gao", "Xinxin Liu", "Sa Wang", "Yungang Bao", "Xitong Gao", "Kan Shi"], "title": "Lyra: A Hardware-Accelerated RISC-V Verification Framework with Generative Model-Based Processor Fuzzing", "comment": null, "summary": "As processor designs grow more complex, verification remains bottlenecked by slow software simulation and low-quality random test stimuli. Recent research has applied software fuzzers to hardware verification, but these rely on semantically blind random mutations that may generate shallow, low-quality stimuli unable to explore complex behaviors. These limitations result in slow coverage convergence and prohibitively high verification costs. In this paper, we present Lyra, a heterogeneous RISC-V verification framework that addresses both challenges by pairing hardware-accelerated verification with an ISA-aware generative model. Lyra executes the DUT and reference model concurrently on an FPGA SoC, enabling high-throughput differential checking and hardware-level coverage collection. Instead of creating verification stimuli randomly or through simple mutations, we train a domain-specialized generative model, LyraGen, with inherent semantic awareness to generate high-quality, semantically rich instruction sequences. Empirical results show Lyra achieves up to $1.27\\times$ higher coverage and accelerates end-to-end verification by up to $107\\times$ to $3343\\times$ compared to state-of-the-art software fuzzers, while consistently demonstrating lower convergence difficulty.", "AI": {"tldr": "Lyra\u662f\u4e00\u4e2a\u5f02\u6784RISC-V\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u786c\u4ef6\u52a0\u901f\u548cISA\u611f\u77e5\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u9a8c\u8bc1\u8986\u76d6\u7387\u548c\u901f\u5ea6", "motivation": "\u5904\u7406\u5668\u8bbe\u8ba1\u65e5\u76ca\u590d\u6742\uff0c\u4f46\u9a8c\u8bc1\u4ecd\u53d7\u9650\u4e8e\u8f6f\u4ef6\u6a21\u62df\u901f\u5ea6\u6162\u548c\u968f\u673a\u6d4b\u8bd5\u523a\u6fc0\u8d28\u91cf\u4f4e\u3002\u73b0\u6709\u786c\u4ef6\u9a8c\u8bc1\u4e2d\u7684\u8f6f\u4ef6\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u8bed\u4e49\u76f2\u968f\u673a\u7a81\u53d8\uff0c\u751f\u6210\u6d45\u5c42\u3001\u4f4e\u8d28\u91cf\u523a\u6fc0\uff0c\u96be\u4ee5\u63a2\u7d22\u590d\u6742\u884c\u4e3a\uff0c\u5bfc\u81f4\u8986\u76d6\u6536\u655b\u6162\u3001\u9a8c\u8bc1\u6210\u672c\u9ad8", "method": "Lyra\u91c7\u7528\u5f02\u6784\u9a8c\u8bc1\u6846\u67b6\uff1a1) \u5728FPGA SoC\u4e0a\u5e76\u884c\u6267\u884c\u88ab\u6d4b\u8bbe\u8ba1\u548c\u53c2\u8003\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u541e\u5410\u5dee\u5206\u68c0\u67e5\u548c\u786c\u4ef6\u7ea7\u8986\u76d6\u6536\u96c6\uff1b2) \u8bad\u7ec3\u9886\u57df\u4e13\u7528\u751f\u6210\u6a21\u578bLyraGen\uff0c\u5177\u6709\u5185\u5728\u8bed\u4e49\u611f\u77e5\u80fd\u529b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u6307\u4ee4\u5e8f\u5217", "result": "Lyra\u5b9e\u73b0\u9ad8\u8fbe1.27\u500d\u7684\u8986\u76d6\u7387\u63d0\u5347\uff0c\u7aef\u5230\u7aef\u9a8c\u8bc1\u52a0\u901f\u8fbe107\u500d\u52303343\u500d\uff08\u76f8\u6bd4\u6700\u5148\u8fdb\u8f6f\u4ef6\u6a21\u7cca\u6d4b\u8bd5\uff09\uff0c\u4e14\u59cb\u7ec8\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u6536\u655b\u96be\u5ea6", "conclusion": "Lyra\u901a\u8fc7\u786c\u4ef6\u52a0\u901f\u9a8c\u8bc1\u548c\u8bed\u4e49\u611f\u77e5\u751f\u6210\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u786c\u4ef6\u9a8c\u8bc1\u4e2d\u7684\u8986\u76d6\u6536\u655b\u6162\u548c\u9a8c\u8bc1\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u5904\u7406\u5668\u8bbe\u8ba1\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
