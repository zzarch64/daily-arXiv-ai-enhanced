<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories](https://arxiv.org/abs/2511.00075)
*Qianhui Li,Weiya Wang,Qianqi Zhao,Tong Qu,Jing He,Xuhong Qiang,Jingwen Hou,Ke Chen,Bao Zhang,Qi Wang*

Main category: cs.AR

TL;DR: 提出PDA-LSTM模型，通过LSTM神经网络优化QLC 3D NAND闪存中的数据排列，抑制横向电荷迁移，降低比特错误率。


<details>
  <summary>Details</summary>
Motivation: QLC 3D NAND闪存由于每单元存储4比特导致读取裕度变窄，易受横向电荷迁移影响。现有算法如WBVM、DVDS仅关注页内数据映射，而页间数据排列也能有效抑制电荷迁移。

Method: 设计PDA-LSTM模型，使用LSTM神经网络从输入页面数据模式计算数据排列概率矩阵。通过最小化字线间LCM的全局影响来优化数据排列，并将LSTM输出矩阵转换为非重复序列生成概率矩阵以辅助训练。

Result: 实验结果显示，PDA-LSTM相比无数据排列策略平均BER降低80.4%，相比WBVM和DVDS（码长64）分别降低18.4%和15.2%，且无需额外标志位。

Conclusion: PDA-LSTM通过智能数据排列有效抑制QLC 3D NAND闪存中的横向电荷迁移，显著降低比特错误率，优于现有方法。

Abstract: Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant
storage solution in the era of artificial intelligence. QLC 3D NAND flash
stores 4 bit per cell to expand the storage density, resulting in narrower read
margins. Constrained to read margins, QLC always suffers from lateral charge
migration (LCM), which caused by non-uniform charge density across adjacent
memory cells. To suppress charge density gap between cells, there are some
algorithm in form of intra-page data mapping such as WBVM, DVDS. However, we
observe inter-page data arrangements also approach the suppression. Thus, we
proposed an intelligent model PDA-LSTM to arrange intra-page data for LCM
suppression, which is a physics-knowledge-driven neural network model. PDA-LSTM
applies a long-short term memory (LSTM) neural network to compute a data
arrangement probability matrix from input page data pattern. The arrangement is
to minimize the global impacts derived from the LCM among wordlines. Since each
page data can be arranged only once, we design a transformation from output
matrix of LSTM network to non-repetitive sequence generation probability matrix
to assist training process. The arranged data pattern can decrease the bit
error rate (BER) during data retention. In addition, PDA-LSTM do not need extra
flag bits to record data transport of 3D NAND flash compared with WBVM, DVDS.
The experiment results show that the PDA-LSTM reduces the average BER by 80.4%
compared with strategy without data arrangement, and by 18.4%, 15.2% compared
respectively with WBVM and DVDS with code-length 64.

</details>


### [2] [H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention](https://arxiv.org/abs/2511.00295)
*Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: H-FA是一种改进的FlashAttention硬件实现，通过混合使用浮点数和定点数对数域表示，在保持性能的同时显著降低了面积和功耗。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制在处理长序列时存在计算瓶颈，FlashAttention通过分块计算解决了这个问题，但其硬件实现仍有优化空间，特别是在减少面积和功耗方面。

Method: 采用浮点数计算查询和键矩阵的注意力分数，在对数域中使用定点数算术简化softmax归一化与值矩阵乘法的融合计算，用加减法替代乘除法操作。

Result: 在28nm工艺下，H-FA相比纯浮点数数据路径的FlashAttention并行硬件架构，平均面积减少26.5%，功耗降低23.4%，且不影响性能。

Conclusion: H-FA通过混合浮点-定点对数域计算策略，成功实现了FlashAttention硬件加速器的高效优化，为长序列处理提供了更节能的解决方案。

Abstract: Transformers have significantly advanced AI and machine learning through
their powerful attention mechanism. However, computing attention on long
sequences can become a computational bottleneck. FlashAttention mitigates this
by fusing the softmax and matrix operations into a tiled computation pattern
that decouples performance from sequence length. Though designed for GPUs, its
simplicity also makes it well suited for direct hardware acceleration. To
improve hardware implementation, we compute FlashAttention using a mixture of
floating-point and fixed-point logarithm domain representations. Floating-point
is used to compute attention scores from query and key matrices, while
logarithmic computation simplifies the fused computation of softmax
normalization and the multiplication with the value matrix. This
transformation, called H-FA, replaces vector-wide floating-point multiplication
and division operations by additions and subtractions implemented efficiently
with fixed-point arithmetic in the logarithm domain. Exponential function
evaluations are effectively omitted and fused with the rest operations, and the
final result is directly returned to floating-point arithmetic without any
additional hardware overhead. Hardware implementation results at 28nm
demonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction
in power, on average, compared to FlashAttention parallel hardware
architectures built solely with floating-point datapaths, without hindering
performance.

</details>


### [3] [Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits](https://arxiv.org/abs/2511.00321)
*Dowon Kim,MinJae Lee,Janghyeon Kim,HyuckSung Kwon,Hyeonggyu Jeong,Sang-Soo Park,Minyong Yoon,Si-Dong Roh,Yongsuk Kwon,Jinin So,Jungwook Choi*

Main category: cs.AR

TL;DR: 提出了一种基于CXL的PNM KV缓存管理系统，通过内存近处理技术解决大语言模型长上下文推理中的内存和计算瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型上下文窗口扩展到百万token级别时，KV缓存管理面临严重的内存和计算瓶颈，现有CXL非驱逐框架在召回非驻留KV token时仍存在昂贵的数据传输成本

Method: 设计CXL启用的KV缓存管理系统，将token页面选择卸载到CXL内存中的PNM加速器，引入混合并行化策略和稳态token选择机制

Result: 在405B参数和1M token上下文的LLM上实现一致性能提升，PNM-KV和PnG-KV方案相比基线实现21.9倍吞吐量提升、60倍每token能耗降低和7.3倍总成本效率提升

Conclusion: CXL启用的多PNM架构可以作为未来长上下文LLM推理的可扩展骨干

Abstract: The expansion of context windows in large language models (LLMs) to
multi-million tokens introduces severe memory and compute bottlenecks,
particularly in managing the growing Key-Value (KV) cache. While Compute
Express Link (CXL) enables non-eviction frameworks that offload the full
KV-cache to scalable external memory, these frameworks still suffer from costly
data transfers when recalling non-resident KV tokens to limited GPU memory as
context lengths increase. This work proposes scalable Processing-Near-Memory
(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that
coordinates memory and computation beyond GPU limits. Our design offloads token
page selection to a PNM accelerator within CXL memory, eliminating costly
recalls and enabling larger GPU batch sizes. We further introduce a hybrid
parallelization strategy and a steady-token selection mechanism to enhance
compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM
system, our solution delivers consistent performance gains for LLMs with up to
405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)
and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x
throughput improvement, up to 60x lower energy per token, and up to 7.3x better
total cost efficiency than the baseline, demonstrating that CXL-enabled
multi-PNM architectures can serve as a scalable backbone for future
long-context LLM inference.

</details>


### [4] [Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim](https://arxiv.org/abs/2511.01244)
*Wajid Ali,Ayaz Akram,Deepak Shankar*

Main category: cs.AR

TL;DR: 使用VisualSim仿真多芯片系统架构，重点研究chiplet技术的性能分析和建模，评估通信延迟、内存访问效率等关键指标。


<details>
  <summary>Details</summary>
Motivation: 传统单片芯片面临制造成本、功耗效率和性能扩展的挑战，chiplet技术通过集成多个模块化硅单元提供更灵活、可扩展且成本更低的解决方案。

Method: 开发了chiplet系统的详细仿真模型，包含多核ARM处理器集群，通过ARM CMN600片上网络互联，使用VisualSim框架评估系统性能指标。

Result: 仿真分析揭示了影响chiplet系统性能的关键因素，包括通信延迟、内存访问效率和功耗性能权衡。

Conclusion: 研究为优化未来chiplet基半导体设计提供了基础，展示了仿真驱动方法在评估多芯片系统性能方面的价值。

Abstract: This paper focuses on the simulation of multi-die System-on-Chip (SoC)
architectures using VisualSim, emphasiz- ing chiplet-based system modeling and
performance analysis. Chiplet technology presents a promising alternative to
traditional monolithic chips, which face increasing challenges in manufactur-
ing costs, power efficiency, and performance scaling. By integrat- ing multiple
small modular silicon units into a single package, chiplet-based architectures
offer greater flexibility and scalability at a lower overall cost. In this
study, we developed a detailed sim- ulation model of a chiplet-based system,
incorporating multicore ARM processor clusters interconnected through a ARM
CMN600 network-on-chip (NoC) for efficient communication [4], [7]. The
simulation framework in VisualSim enables the evaluation of critical system
metrics, including inter-chiplet communication latency, memory access
efficiency, workload distribution, and the power-performance tradeoff under
various workloads. Through simulation-driven insights, this research highlights
key factors influencing chiplet system performance and provides a foundation
for optimizing future chiplet-based semiconductor designs.

</details>
