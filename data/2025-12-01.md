<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 8]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Platinum: Path-Adaptable LUT-Based Accelerator Tailored for Low-Bit Weight Matrix Multiplication](https://arxiv.org/abs/2511.21910)
*Haoxuan Shan,Cong Guo,Chiyue Wei,Feng Cheng,Junyao Zhang,Hai,Li,Yiran Chen*

Main category: cs.AR

TL;DR: Platinum是一个轻量级ASIC加速器，使用查找表加速整数权重混合精度矩阵乘法，通过离线生成构造路径降低开销，支持比特串行和三值权重自适应切换，在BitNet b1.58-3B上实现显著加速和能效提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速扩展需要更高效硬件，超低位量化提供了效率与性能的平衡，但现有基于查找表的方法存在构造开销大、仅依赖比特串行计算的问题，对三值权重网络不最优。

Method: 提出Platinum轻量级ASIC加速器，通过离线生成的构造路径减少查找表构建开销，支持通用比特串行和优化的三值权重执行，采用自适应路径切换机制。

Result: 在BitNet b1.58-3B上，相比SpikingEyeriss、Prosperity和16线程T-MAC(CPU)分别实现73.6倍、4.09倍和2.15倍加速，能效提升32.4倍、3.23倍和20.9倍，芯片面积仅0.96mm²。

Conclusion: 基于查找表的ASIC是边缘平台上超低位神经网络的高效、可扩展解决方案，Platinum展示了其在加速混合精度矩阵乘法方面的潜力。

Abstract: The rapid scaling of large language models demands more efficient hardware. Quantization offers a promising trade-off between efficiency and performance. With ultra-low-bit quantization, there are abundant opportunities for results reuse, and thus it can be boosted with lookup tables (LUTs) based acceleration. However, existing LUT-based methods suffer from computation and hardware overheads for LUT construction, and rely solely on bit-serial computation, which is suboptimal for ternary-weight networks. We propose Platinum, a lightweight ASIC accelerator for integer weight mixed-precision matrix multiplication (mpGEMM) using LUTs. Platinum reduces LUT construction overhead via offline-generated construction paths and supports both general bit-serial and optimized ternary-weight execution through adaptive path switching. On BitNet b1.58-3B, Platinum achieves up to 73.6x, 4.09x, and 2.15x speedups over SpikingEyeriss, Prosperity, and 16-thread T-MAC (CPU), respectively, along with energy reductions of 32.4x, 3.23x, and 20.9x, all within a 0.96mm2 chip area. This demonstrates the potential of LUT-based ASICs as efficient, scalable solutions for ultra-low-bit neural networks on edge platforms.

</details>


### [2] [CADC: Crossbar-Aware Dendritic Convolution for Efficient In-memory Computing](https://arxiv.org/abs/2511.22166)
*Shuai Dong,Junyi Yang,Ye Ke,Hongyang Shang,Arindam Basu*

Main category: cs.AR

TL;DR: 提出CADC方法，通过将非线性树突函数嵌入存内计算交叉阵列，大幅减少卷积神经网络中的部分和数量，降低系统开销并提升能效


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络在存内计算架构中需要跨多个交叉阵列分区，产生大量部分和，导致缓冲、传输和累加的系统级开销显著

Method: 提出交叉阵列感知的树突卷积（CADC），在交叉阵列计算中直接嵌入非线性树突函数（将负值归零），大幅增加部分和的稀疏性

Result: CADC显著减少部分和数量：LeNet-5减少80%，ResNet-18减少54%，VGG-16减少66%，SNN减少88%；SRAM存内计算实现达到2.15 TOPS和40.8 TOPS/W，相比现有加速器实现11-18倍加速和1.9-22.9倍能效提升

Conclusion: CADC通过神经科学启发的树突计算原理，有效解决存内计算中的部分和开销问题，在保持精度的同时显著提升计算效率和能效

Abstract: Convolutional neural networks (CNNs) are computationally intensive and often accelerated using crossbar-based in-memory computing (IMC) architectures. However, large convolutional layers must be partitioned across multiple crossbars, generating numerous partial sums (psums) that require additional buffer, transfer, and accumulation, thus introducing significant system-level overhead. Inspired by dendritic computing principles from neuroscience, we propose crossbar-aware dendritic convolution (CADC), a novel approach that dramatically increases sparsity in psums by embedding a nonlinear dendritic function (zeroing negative values) directly within crossbar computations. Experimental results demonstrate that CADC significantly reduces psums, eliminating 80% in LeNet-5 on MNIST, 54% in ResNet-18 on CIFAR-10, 66% in VGG-16 on CIFAR-100, and up to 88% in spiking neural networks (SNN) on the DVS Gesture dataset. The induced sparsity from CADC provides two key benefits: (1) enabling zero-compression and zero-skipping, thus reducing buffer and transfer overhead by 29.3% and accumulation overhead by 47.9%; (2) minimizing ADC quantization noise accumulation, resulting in small accuracy degradation - only 0.01% for LeNet-5, 0.1% for ResNet-18, 0.5% for VGG-16, and 0.9% for SNN. Compared to vanilla convolution (vConv), CADC exhibits accuracy changes ranging from +0.11% to +0.19% for LeNet-5, -0.04% to -0.27% for ResNet-18, +0.99% to +1.60% for VGG-16, and -0.57% to +1.32% for SNN, across crossbar sizes from 64x64 to 256x256. Ultimately, a SRAM-based IMC implementation of CADC achieves 2.15 TOPS and 40.8 TOPS/W for ResNet-18 (4/2/4b), realizing an 11x-18x speedup and 1.9x-22.9x improvement in energy efficiency compared to existing IMC accelerators.

</details>


### [3] [Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR](https://arxiv.org/abs/2511.22267)
*Yuyang Zou,Youwei Xiao,Yansong Xu,Chenyun Yin,Yuhao Luo,Yitian Sun,Ruifan Xu,Renze Chen,Yun Liang*

Main category: cs.AR

TL;DR: Aquas是一个基于MLIR的硬件-软件协同设计框架，通过集成突发DMA引擎和高级HLS优化来提升RISC-V ASIP性能，同时采用基于e-graph的可重定向编译方法，在点云处理和LLM推理等实际工作负载上实现了最高9.27倍的加速。


<details>
  <summary>Details</summary>
Motivation: 当前开源RISC-V生态系统中的ASIP框架存在性能限制，主要原因是硬件合成能力受限和编译器支持僵化，无法充分发挥ASIP在特定应用中的专业化潜力。

Method: 提出Aquas框架：1）硬件方面：集成突发DMA引擎实现快速内存访问，采用高级HLS优化；2）编译器方面：提出基于e-graph的可重定向方法，包含新颖的指令匹配引擎。

Result: 在点云处理和LLM推理等实际工作负载上实现了最高9.27倍的性能加速，显著提升了ASIP的性能表现。

Conclusion: Aquas通过硬件-软件协同设计方法有效解决了现有RISC-V ASIP框架的性能瓶颈，为特定应用处理器设计提供了高效的端到端解决方案。

Abstract: Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.

</details>


### [4] [FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators](https://arxiv.org/abs/2511.22348)
*Shuao Jia,Zichao Ling,Chen Bai,Kang Zhao,Jianwang Zhai*

Main category: cs.AR

TL;DR: FADiff是一个基于梯度的优化框架，用于自动寻找DNN在张量加速器上的最优层内映射和层间融合策略，以提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 在张量加速器上高效部署DNN（如LLM）对最大化计算效率至关重要，但由于层内映射和层间融合的复杂交互形成了巨大的设计空间，实现这一目标具有挑战性。

Method: 首先构建统一且可微的分析成本模型，准确预测单层映射和各种层融合策略的能耗和延迟；然后通过将离散约束编码到损失函数中，采用基于梯度的方法高效探索设计空间，确定映射和融合的最优联合策略。

Result: 实验结果表明FADiff优于现有方法，在能耗和延迟方面实现了更好的优化效果。

Conclusion: FADiff框架能够自动识别高质量的DNN部署策略，有效解决了张量加速器上DNN推理的优化挑战。

Abstract: Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.

</details>


### [5] [3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison](https://arxiv.org/abs/2511.22551)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asad*

Main category: cs.AR

TL;DR: 本文提出了一种名为3RSeT的低成本方案，通过选择性标签比较来减少STT-MRAM缓存中的读取干扰错误率，将标签阵列的读取干扰率降低71.8%，MTTF提高3.6倍，能耗降低62.1%，性能不受影响且面积开销小于0.4%。


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为片上缓存存储器最有希望的SRAM替代品，虽然具有低泄漏功耗、高密度、抗辐射和非易失性等优点，但在读取操作中会发生无意的位翻转（读取干扰错误），这是严重的可靠性挑战。特别是在缓存集合中同时访问所有标签进行并行比较操作，是读取干扰错误的主要来源，这一问题在先前工作中未得到解决。

Method: 提出3RSeT方案，通过选择性标签比较来减少读取干扰率。该方法主动禁用那些没有命中机会的标签，利用每个访问请求中标签的低有效位来预测哪些标签不需要读取，从而消除大部分标签读取操作。

Result: 使用gem5全系统周期精确模拟器评估显示：3RSeT将标签阵列的读取干扰率降低71.8%，平均故障时间（MTTF）提高3.6倍，能耗降低62.1%，性能不受影响，面积开销小于0.4%。

Conclusion: 3RSeT是一种高效的低成本解决方案，能够显著降低STT-MRAM缓存中的读取干扰错误率，同时减少能耗，且对性能和面积影响极小，为STT-MRAM缓存的可靠性问题提供了有效的解决途径。

Abstract: Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.

</details>


### [6] [The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference](https://arxiv.org/abs/2511.22889)
*Fang Li*

Main category: cs.AR

TL;DR: 提出ITA架构，将LLM权重编码到ASIC物理电路中，消除内存层次结构，解决边缘设备部署中的"内存墙"问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在消费边缘设备部署受到"内存墙"限制，即从DRAM获取模型权重需要巨大的带宽和能耗成本。当前架构将模型权重视为可变软件数据，为保持通用可编程性付出了巨大能耗代价。

Method: 提出不可变张量架构（ITA），将模型权重视为物理电路拓扑而非数据，通过将参数直接编码到成熟节点ASIC（28nm/40nm）的金属互连和逻辑中，完全消除内存层次结构。采用"分裂大脑"系统设计，主机CPU管理动态KV缓存操作，ITA ASIC作为无状态的ROM嵌入式数据流引擎。

Result: ITA架构通过将权重物理编码到电路中，从根本上解决了内存访问瓶颈，显著降低了边缘设备上LLM推理的能耗和延迟。

Conclusion: ITA代表了一种范式转变，将模型权重从软件数据重新定义为硬件电路拓扑，为边缘设备上的高效LLM部署提供了新的架构解决方案。

Abstract: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

</details>


### [7] [Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation](https://arxiv.org/abs/2511.23011)
*Yanjing Wang,Lizhou Wu,Sunfeng Gao,Yibo Tang,Junhui Luo,Zicong Wang,Yang Ou,Dezun Dong,Nong Xiao,Mingche Lai*

Main category: cs.AR

TL;DR: Cohet：首个基于CXL的缓存一致性异构计算框架，通过解耦计算与内存资源，构建CPU和XPU计算池共享统一内存池，显著提升异构系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于PCIe的异构计算系统存在细粒度主机-设备交互效率低下和编程模型复杂的问题。CXL作为新兴缓存一致性互连标准，虽然有望从根本上改变CPU与XPU的协作计算模式，但相关研究受限于平台稀缺、生态系统不成熟和应用前景不明朗。

Method: 提出Cohet框架，将计算和内存资源解耦，形成独立的CPU和XPU计算池，共享统一的缓存一致性内存池。通过标准malloc/mmap接口向CPU和XPU计算线程暴露内存，由操作系统智能管理异构资源。同时开发了全系统周期级模拟器SimCXL，支持所有CXL子协议和设备类型建模。

Result: CXL.cache相比DMA传输在缓存行粒度上延迟降低68%，带宽提升14.4倍。基于Cohet的两个杀手级应用：远程原子操作（RAO）和远程过程调用（RPC），相比PCIe-NIC设计，CXL-NIC在RAO卸载上实现5.5-40.2倍加速，RPC（反）序列化卸载平均加速1.86倍。

Conclusion: Cohet作为首个CXL驱动的缓存一致性异构计算框架，通过资源解耦和统一内存管理，显著提升了异构计算系统的性能和易用性，为CXL在异构计算领域的应用提供了重要参考。

Abstract: Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.

</details>


### [8] [GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration](https://arxiv.org/abs/2511.23203)
*Jordi Fornt,Pau Fontova-Musté,Adrian Gras,Omar Lahyani,Martí Caro,Jaume Abella,Francesc Moll,Josep Altet*

Main category: cs.AR

TL;DR: 提出GAV技术，结合欠压和位串行计算，实现灵活的近似计算，并设计GAVINA加速器，支持任意混合精度和灵活欠压，在ResNet-18上实现20%能效提升且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 电压过缩放（欠压）是一种有吸引力的近似技术，但由于其高错误率阻碍了广泛应用。现有欠压加速器依赖8位算术，无法与先进低精度（<8位）架构竞争。

Method: 提出GAV技术，结合欠压和位串行计算，选择性地对最低有效位组合进行激进降压。基于此实现GAVINA架构，支持任意混合精度和灵活欠压。

Result: GAVINA在最激进配置下能效达89 TOP/sW。通过建立GAVINA误差模型，在ResNet-18上实现20%能效提升，精度损失可忽略。

Conclusion: GAV技术成功解决了欠压技术的高错误率问题，通过位串行计算实现灵活近似，GAVINA架构在能效和精度方面取得良好平衡。

Abstract: Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.

</details>
