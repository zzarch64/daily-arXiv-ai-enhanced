<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 9]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [FSL-HDnn: A 40 nm Few-shot On-Device Learning Accelerator with Integrated Feature Extraction and Hyperdimensional Computing](https://arxiv.org/abs/2512.11826)
*Weihong Xu,Chang Eun Song,Haichao Yang,Leo Liu,Meng-Fan Chang,Carlos H. Diaz,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: FSL-HDnn是一个能效优化的加速器，通过权重聚类特征提取器和超维计算分类器，实现端到端的设备上少样本学习，显著降低训练延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备上设备学习（ODL）的基本挑战，包括计算复杂度高、训练延迟大和能耗高等问题。

Method: 采用两个协同模块：1）基于权重聚类的参数高效特征提取器降低计算复杂度；2）基于超维计算（HDC）的少样本学习分类器，消除梯度反向传播，实现单次训练。还提出两种优化策略：分支特征提取的早退机制和批量单次训练。

Result: 40nm CMOS工艺芯片在10-way 5-shot少样本学习任务上，达到6 mJ/图像的训练能效和28图像/秒的端到端训练吞吐量。端到端训练延迟比最先进的ODL芯片降低2-20.9倍。

Conclusion: FSL-HDnn通过创新的权重聚类特征提取和超维计算分类器架构，为边缘设备上的少样本学习提供了高效、低延迟的解决方案，显著提升了训练能效和吞吐量。

Abstract: This paper introduces FSL-HDnn, an energy-efficient accelerator that implements the end-to-end pipeline of feature extraction and on-device few-shot learning (FSL). The accelerator addresses fundamental challenges of on-device learning (ODL) for resource-constrained edge applications through two synergistic modules: a parameter-efficient feature extractor employing weight clustering and an FSL classifier based on hyperdimensional computing (HDC). The feature extractor exploits the weight clustering mechanism to reduce computational complexity, while the HDC-based FSL classifier eliminates gradient-based back propagation operations, enabling single-pass training with substantially reduced latency. Additionally, FSL-HDnn enables low-latency ODL and inference via two proposed optimization strategies, including an early-exit mechanism with branch feature extraction and batched single-pass training that improves hardware utilization. Measurement results demonstrate that our chip fabricated in a 40 nm CMOS process delivers superior training energy efficiency of 6 mJ/image and end-to-end training throughput of 28 images/s on a 10-way 5-shot FSL task. The end-to-end training latency is also reduced by 2x to 20.9x compared to state-of-the-art ODL chips.

</details>


### [2] [DreamRAM: A Fine-Grained Configurable Design Space Modeling Tool for Custom 3D Die-Stacked DRAM](https://arxiv.org/abs/2512.12106)
*Victor Cai,Jennifer Zhou,Haebin Do,David Brooks,Gu-Yeon Wei*

Main category: cs.AR

TL;DR: DreamRAM是一个可配置的3D堆叠DRAM建模工具，用于探索定制化内存架构设计空间，通过精细参数调节实现带宽、容量、能耗和延迟的优化。


<details>
  <summary>Details</summary>
Motivation: 不同应用对DRAM的功耗、性能和面积需求各异，而固定商品化DRAM设计无法满足这些多样化需求。3D堆叠技术创造了更大的设计空间，需要工具来探索定制化内存架构。

Method: DreamRAM提供可配置的带宽、容量、能耗、延迟和面积建模工具，暴露MAT、子阵列、bank和bank间级别的细粒度设计参数，包括扩展部分页面和子阵列并行性方案。工具通过分析线宽、间距、长度、电容和缩放参数来建模物理布局和布线设计选择的性能权衡。

Result: DreamRAM已针对HBM3和HBM2E行业设计进行校准和验证。在其丰富的设计空间中，识别出相比基线设计分别实现66%更高带宽、100%更高容量以及45%更低每比特功耗和能耗的设计方案。

Conclusion: DreamRAM为定制化3D堆叠DRAM设计提供了强大的建模工具，能够探索传统固定DRAM设计无法满足的多样化应用需求，在带宽、容量和功耗方面展现出显著优化潜力。

Abstract: 3D die-stacked DRAM has emerged as a key technology for delivering high bandwidth and high density for applications such as high-performance computing, graphics, and machine learning. However, different applications place diverse and sometimes diverging demands on power, performance, and area that cannot be universally satisfied with fixed commodity DRAM designs. Die stacking creates the opportunity for a large DRAM design space through 3D integration and expanded total die area. To open and navigate this expansive design space of customized memory architectures that cater to application-specific needs, we introduce DreamRAM, a configurable bandwidth, capacity, energy, latency, and area modeling tool for custom 3D die-stacked DRAM designs. DreamRAM exposes fine-grained design customization parameters at the MAT, subarray, bank, and inter-bank levels, including extensions of partial page and subarray parallelism proposals found in the literature, to open a large previously-unexplored design space. DreamRAM analytically models wire pitch, width, length, capacitance, and scaling parameters to capture the performance tradeoffs of physical layout and routing design choices. Routing awareness enables DreamRAM to model a custom MAT-level routing scheme, Dataline-Over-MAT (DLOMAT), to facilitate better bandwidth tradeoffs. DreamRAM is calibrated and validated against published industry HBM3 and HBM2E designs. Within DreamRAM's rich design space, we identify designs that achieve each of 66% higher bandwidth, 100% higher capacity, and 45% lower power and energy per bit compared to the baseline design, each on an iso-bandwidth, iso-capacity, and iso-power basis.

</details>


### [3] [HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility](https://arxiv.org/abs/2512.12847)
*Jonathan Herbst,Michael Pellauer,Sherief Reda*

Main category: cs.AR

TL;DR: 提出一种高吞吐量神经网络加速器，通过硬件嵌入大部分网络层、Po2量化权重实现乘法替换为加法，保留可编程最终层以兼顾灵活性和吞吐量，在MobileNetV2上实现20-67倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算和数据中心部署中神经网络推理的高吞吐量、低能耗需求，同时保持一定的模型灵活性，特别是在模型参数稳定的连续感知任务中。

Method: 1) 硬件嵌入大部分网络层以减少数据传输和内存使用；2) 采用2的幂次(Po2)量化权重，将乘法转换为简单重连和加法；3) 保留小型神经处理单元作为可编程最终分类层；4) 在7nm ASIC流程中实现，以MobileNetV2为基准。

Result: MobileNetV2推理吞吐量相比全可编程GPU提升20倍（需微调时）至67倍（无需微调时），分别达到121万和400万图像/秒，同时保持微调灵活性，展示了架构在吞吐量、面积、精度和量化敏感性方面的优势。

Conclusion: 该硬件嵌入加Po2量化的加速器架构在保持最终层可编程灵活性的同时，显著提升了神经网络推理吞吐量和能效，特别适用于模型参数稳定的边缘计算和数据中心部署场景。

Abstract: We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.

</details>


### [4] [KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation](https://arxiv.org/abs/2512.12850)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

TL;DR: KANELÉ框架首次系统化地将KAN网络部署到FPGA上，通过量化剪枝协同优化，实现高达2700倍加速和显著资源节省，在符号/物理公式任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: FPGA上需要低延迟、资源高效的神经网络推理，现有LUT-based方案虽有优势，但缺乏针对KAN网络的系统化部署框架。KAN网络独特的可学习一维样条激活函数天然适合离散化和LUT映射，但尚未被充分利用于FPGA部署。

Method: 提出KANELÉ框架，首次系统化设计KAN在FPGA上的实现流程。通过协同优化训练、量化和剪枝，实现紧凑、高吞吐、低延迟的KAN架构。利用KAN的可学习一维样条激活函数固定域特性，自然适合离散化和高效LUT映射。

Result: 相比现有KAN-on-FPGA方法，实现高达2700倍加速和数量级资源节省。在广泛使用的基准测试中，KANELÉ匹配或超越其他LUT-based架构，尤其在涉及符号或物理公式的任务上表现突出，同时平衡FPGA硬件资源使用。

Conclusion: KANELÉ框架成功将KAN网络优势与FPGA硬件特性结合，为实时、低功耗控制系统等应用提供了高效解决方案，展示了框架的通用性和实用性。

Abstract: Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.

</details>


### [5] [SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference](https://arxiv.org/abs/2512.12990)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Gwangtae Park,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.AR

TL;DR: SliceMoE：一种面向错失率约束部署的节能MoE推理框架，通过动态位切片缓存和预测性缓存预热，在保持精度的同时显著降低解码阶段能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过条件计算实现高效扩展，但其庞大的参数量和昂贵的专家卸载使得在设备端部署具有挑战性。现有的加速技术（如预取或专家聚类）往往增加能耗或降低专家多样性。

Method: 1. 动态位切片缓存（DBSC）：以切片级粒度缓存专家，按需分配精度以扩展有效专家容量；2. 无需校准的非对称套娃量化（AMAT）：基于截断的方案，在低比特和高比特切片之间保持兼容性，无需内存复制；3. 预测性缓存预热（PCW）：在预填充阶段重塑缓存内容以减少早期解码的冷缺失。

Result: 在DeepSeek-V2-Lite和Qwen1.5-MoE-A2.7B上评估，SliceMoE分别将解码阶段能耗降低2.37倍和2.85倍，解码延迟提升1.81倍和1.64倍，同时保持接近高比特精度。

Conclusion: 切片级缓存实现了高效的设备端MoE部署，证明了在保持模型准确性的同时显著降低能耗和延迟的可行性。

Abstract: MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.

</details>


### [6] [An Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework for High-Performance Ultra-Large Scale Layout Pattern Clustering](https://arxiv.org/abs/2512.13133)
*Shuo Liu*

Main category: cs.AR

TL;DR: 提出一种基于最优对齐的迭代闭环收敛框架，解决VLSI布局模式聚类中的计算复杂度、对齐模糊和速度-质量权衡问题，在EDA挑战赛中实现93.4%压缩比和100倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着VLSI技术缩放，布局模式爆炸式增长成为DFM应用（如OPC）的关键瓶颈。现有聚类方法面临计算复杂度高（O(N²)）、中心对齐离散采样次优、速度-质量权衡困难等问题。

Method: 1) 提出混合高性能算法解决对齐模糊：FFT相位相关法用于余弦相似度约束，鲁棒几何最小-最大策略用于边缘位移约束；2) 将聚类建模为集合覆盖问题，在粗到细迭代细化循环中使用基于信息量的惰性贪婪启发式算法确保收敛；3) 多阶段剪枝机制过滤99%冗余计算。

Result: 在2025年中国研究生EDA精英挑战赛基准测试中，实现相对于原始输入93.4%的压缩比，相比官方基线超过100倍加速，能在数秒内处理数万个模式。在77支队伍中获得第一名。

Conclusion: 该方法证明了在解决NP-Hard布局聚类问题上的优越性，实现了可扩展性和精度的最优平衡，为DFM应用提供了高效的解决方案。

Abstract: With the aggressive scaling of VLSI technology, the explosion of layout patterns creates a critical bottleneck for DFM applications like OPC. Pattern clustering is essential to reduce data complexity, yet existing methods struggle with computational prohibitiveness ($O(N^2)$ comparisons), sub-optimal discrete sampling for center alignment, and difficult speed-quality trade-offs. To address these, we propose an Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework. First, to resolve alignment ambiguity, we introduce a hybrid suite of high-performance algorithms: an FFT-based Phase Correlation method for cosine similarity constraints, and a Robust Geometric Min-Max strategy for edge displacement constraints that analytically solves for the global optimum. Second, we model clustering as a Set Cover Problem (SCP) using a Surprisal-Based Lazy Greedy heuristic within a coarse-to-fine iterative refinement loop to ensure convergence. Additionally, a multi-stage pruning mechanism filters over 99% of redundant computations. Experimental results on the 2025 China Postgraduate EDA Elite Challenge benchmark demonstrate a 93.4% compression ratio relative to raw inputs and an over 100x speedup compared to the official baseline, effectively handling tens of thousands of patterns in seconds. Securing First Place among 77 teams, this approach proves its superiority in solving the NP-Hard layout clustering problem with an optimal balance of scalability and precision.

</details>


### [7] [Striking the Balance: GEMM Performance Optimization Across Generations of Ryzen AI NPUs](https://arxiv.org/abs/2512.13282)
*Endri Taka,Andre Roesti,Joseph Melber,Pranathi Vasireddy,Kristof Denolf,Diana Marculescu*

Main category: cs.AR

TL;DR: 本文提出了一种系统化方法，用于优化AMD Ryzen AI NPU（XDNA和XDNA2）上的通用矩阵乘法（GEMM）性能，针对深度学习工作负载，实现了int8精度下最高6.76 TOPS（XDNA）和38.05 TOPS（XDNA2）的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习工作负载对计算和内存需求极高，需要针对从云到边缘的专用硬件（如AMD Ryzen AI XDNA NPU）进行优化。优化这些架构上的GEMM算法对于提升深度学习性能至关重要。

Method: 提出了一种通用的系统化方法论，用于优化两个NPU世代（XDNA和XDNA2）上的GEMM工作负载。实现利用了AMD NPU的独特架构特性，并在系统层面解决了关键性能瓶颈。

Result: 端到端性能评估显示，在8位整数（int8）精度下，实现了最高6.76 TOPS（XDNA）和38.05 TOPS（XDNA2）的吞吐量；在脑浮点数（bf16）精度下，实现了最高3.14 TOPS（XDNA）和14.71 TOPS（XDNA2）的吞吐量，达到了最先进的性能水平。

Conclusion: 这项工作为优化Ryzen AI NPU上的GEMM工作负载提供了重要的性能洞察，展示了针对特定硬件架构进行系统化优化的有效性，能够显著提升深度学习工作负载的性能。

Abstract: The high computational and memory demands of modern deep learning (DL) workloads have led to the development of specialized hardware devices from cloud to edge, such as AMD's Ryzen AI XDNA NPUs. Optimizing general matrix multiplication (GEMM) algorithms for these architectures is critical for improving DL workload performance. To this end, this paper presents a common systematic methodology to optimize GEMM workloads across the two current NPU generations, namely XDNA and XDNA2. Our implementations exploit the unique architectural features of AMD's NPUs and address key performance bottlenecks at the system level. End-to-end performance evaluation across various GEMM sizes demonstrates state-of-the-art throughput of up to 6.76 TOPS (XDNA) and 38.05 TOPS (XDNA2) for 8-bit integer (int8) precision. Similarly, for brain floating-point (bf16) precision, our GEMM implementations attain up to 3.14 TOPS (XDNA) and 14.71 TOPS (XDNA2). This work provides significant insights into key performance aspects of optimizing GEMM workloads on Ryzen AI NPUs.

</details>


### [8] [Reproducibility and Standardization in gem5 Resources v25.0](https://arxiv.org/abs/2512.13479)
*Kunal Pai,Harshil Patel,Erin Le,Noah Krim,Mahyar Samani,Bobby R. Bruce,Jason Lowe-Power*

Main category: cs.AR

TL;DR: 论文改进了gem5模拟器和gem5 Resources资源库，通过标准化磁盘镜像创建、重构退出事件系统、引入并行模拟功能，解决了计算机架构研究中仿真可重复性的关键问题。


<details>
  <summary>Details</summary>
Motivation: 基于仿真的计算机架构研究需要协调磁盘镜像、内核和基准测试等工件，但现有工作流程不一致。gem5 Resources虽然支持工件共享，但研究人员仍面临创建自定义磁盘镜像复杂、主机-客机通信功能有限、多工作负载模拟需要外部脚本协调等问题，导致可重复性差。

Method: 1. 使用Packer标准化x86、ARM和RISC-V的磁盘镜像创建，提供预标注基准测试套件的验证基础镜像；2. 重构退出事件系统为基于类的模型，引入hypercalls增强主机-客机通信；3. 实现Suites和MultiSim功能，支持从gem5配置脚本进行并行全系统模拟。

Result: 提供了12个新磁盘镜像、6个新内核和跨三种ISA的200多个工作负载。增强了gem5的通信和监控能力，包括远程监控工具和gem5-bridge驱动程序。消除了多工作负载模拟对外部脚本的依赖，实现了更可靠的并行模拟。

Conclusion: 这些改进降低了设置复杂性，提供了可扩展的验证资源，显著提高了基于仿真的计算机架构研究的可重复性和标准化水平，使研究人员能够更高效地创建、共享和复现实验。

Abstract: Reproducibility in simulation-based computer architecture research requires coordinating artifacts like disk images, kernels, and benchmarks, but existing workflows are inconsistent. We improve gem5, an open-source simulator with over 1600 forks, and gem5 Resources, a centralized repository of over 2000 pre-packaged artifacts, to address these issues. While gem5 Resources enables artifact sharing, researchers still face challenges. Creating custom disk images is complex and time-consuming, with no standardized process across ISAs, making it difficult to extend and share images. gem5 provides limited guest-host communication features through a set of predefined exit events that restrict researchers' ability to dynamically control and monitor simulations. Lastly, running simulations with multiple workloads requires researchers to write custom external scripts to coordinate multiple gem5 simulations which creates error-prone and hard-to-reproduce workflows. To overcome this, we introduce several features in gem5 and gem5 Resources. We standardize disk-image creation across x86, ARM, and RISC-V using Packer, and provide validated base images with pre-annotated benchmark suites (NPB, GAPBS). We provide 12 new disk images, 6 new kernels, and over 200 workloads across three ISAs. We refactor the exit event system to a class-based model and introduce hypercalls for enhanced guest-host communication that allows researchers to define custom behavior for their exit events. We also provide a utility to remotely monitor simulations and the gem5-bridge driver for user-space m5 operations. Additionally, we implemented Suites and MultiSim to enable parallel full-system simulations from gem5 configuration scripts, eliminating the need for external scripting. These features reduce setup complexity and provide extensible, validated resources that improve reproducibility and standardization.

</details>


### [9] [Lyra: A Hardware-Accelerated RISC-V Verification Framework with Generative Model-Based Processor Fuzzing](https://arxiv.org/abs/2512.13686)
*Juncheng Huo,Yunfan Gao,Xinxin Liu,Sa Wang,Yungang Bao,Xitong Gao,Kan Shi*

Main category: cs.AR

TL;DR: Lyra是一个异构RISC-V验证框架，结合硬件加速和ISA感知生成模型，显著提升验证覆盖率和速度


<details>
  <summary>Details</summary>
Motivation: 处理器设计日益复杂，但验证仍受限于软件模拟速度慢和随机测试刺激质量低。现有硬件验证中的软件模糊测试方法依赖语义盲随机突变，生成浅层、低质量刺激，难以探索复杂行为，导致覆盖收敛慢、验证成本高

Method: Lyra采用异构验证框架：1) 在FPGA SoC上并行执行被测设计和参考模型，实现高吞吐差分检查和硬件级覆盖收集；2) 训练领域专用生成模型LyraGen，具有内在语义感知能力，生成高质量、语义丰富的指令序列

Result: Lyra实现高达1.27倍的覆盖率提升，端到端验证加速达107倍到3343倍（相比最先进软件模糊测试），且始终表现出更低的收敛难度

Conclusion: Lyra通过硬件加速验证和语义感知生成模型的结合，有效解决了硬件验证中的覆盖收敛慢和验证成本高的问题，为复杂处理器设计验证提供了高效解决方案

Abstract: As processor designs grow more complex, verification remains bottlenecked by slow software simulation and low-quality random test stimuli. Recent research has applied software fuzzers to hardware verification, but these rely on semantically blind random mutations that may generate shallow, low-quality stimuli unable to explore complex behaviors. These limitations result in slow coverage convergence and prohibitively high verification costs. In this paper, we present Lyra, a heterogeneous RISC-V verification framework that addresses both challenges by pairing hardware-accelerated verification with an ISA-aware generative model. Lyra executes the DUT and reference model concurrently on an FPGA SoC, enabling high-throughput differential checking and hardware-level coverage collection. Instead of creating verification stimuli randomly or through simple mutations, we train a domain-specialized generative model, LyraGen, with inherent semantic awareness to generate high-quality, semantically rich instruction sequences. Empirical results show Lyra achieves up to $1.27\times$ higher coverage and accelerates end-to-end verification by up to $107\times$ to $3343\times$ compared to state-of-the-art software fuzzers, while consistently demonstrating lower convergence difficulty.

</details>
