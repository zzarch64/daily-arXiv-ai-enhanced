{"id": "2512.06093", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06093", "abs": "https://arxiv.org/abs/2512.06093", "authors": ["Boyu Li", "Zongwei Zhu", "Yi Xiong", "Qianyue Cao", "Jiawei Geng", "Xiaonan Zhang", "Xi Li"], "title": "Compass: Mapping Space Exploration for Multi-Chiplet Accelerators Targeting LLM Inference Serving Workloads", "comment": null, "summary": "Large Language Models (LLMs) impose massive computational demands, driving the need for scalable multi-chiplet accelerators. However, existing mapping space exploration efforts for such accelerators primarily focus on traditional CNN/Transformer workloads and fail to adequately support the dynamic behaviors of mixed request types and variable sequence lengths in real-world LLM inference serving. To bridge this gap, we first propose a computation execution graph-based mapping encoding scheme that decouples micro-batches and layers, enabling fine-grained execution control on heterogeneous chiplets and flexibly representing various parallelism strategies. Second, building upon this scheme, we develop the Compass framework, which integrates an evaluation engine and a genetic algorithm-based mapping generation engine to achieve efficient mapping search. Compared to state-of-the-art works, our solution achieves an average EDP reduction of 63.12%.", "AI": {"tldr": "Compass\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u5fae\u6279\u6b21\u548c\u5c42\u7684\u6620\u5c04\u7f16\u7801\u65b9\u6848\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u641c\u7d22\uff0c\u4e3aLLM\u63a8\u7406\u670d\u52a1\u5728\u591a\u82af\u7247\u52a0\u901f\u5668\u4e0a\u5b9e\u73b0\u9ad8\u6548\u6620\u5c04\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747EDP\u964d\u4f4e63.12%", "motivation": "\u73b0\u6709\u6620\u5c04\u7a7a\u95f4\u63a2\u7d22\u4e3b\u8981\u9488\u5bf9\u4f20\u7edfCNN/Transformer\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u65e0\u6cd5\u5145\u5206\u652f\u6301\u771f\u5b9eLLM\u63a8\u7406\u670d\u52a1\u4e2d\u6df7\u5408\u8bf7\u6c42\u7c7b\u578b\u548c\u53ef\u53d8\u5e8f\u5217\u957f\u5ea6\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848", "method": "1) \u63d0\u51fa\u57fa\u4e8e\u8ba1\u7b97\u6267\u884c\u56fe\u7684\u6620\u5c04\u7f16\u7801\u65b9\u6848\uff0c\u89e3\u8026\u5fae\u6279\u6b21\u548c\u5c42\uff0c\u5b9e\u73b0\u5f02\u6784\u82af\u7247\u4e0a\u7684\u7ec6\u7c92\u5ea6\u6267\u884c\u63a7\u5236\uff1b2) \u5f00\u53d1Compass\u6846\u67b6\uff0c\u96c6\u6210\u8bc4\u4f30\u5f15\u64ce\u548c\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u6620\u5c04\u751f\u6210\u5f15\u64ce", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cCompass\u6846\u67b6\u5e73\u5747\u5b9e\u73b063.12%\u7684EDP\uff08\u80fd\u8017\u5ef6\u8fdf\u79ef\uff09\u964d\u4f4e", "conclusion": "\u63d0\u51fa\u7684\u6620\u5c04\u7f16\u7801\u65b9\u6848\u548cCompass\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3LLM\u63a8\u7406\u670d\u52a1\u5728\u591a\u82af\u7247\u52a0\u901f\u5668\u4e0a\u7684\u6620\u5c04\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548"}}
{"id": "2512.06113", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06113", "abs": "https://arxiv.org/abs/2512.06113", "authors": ["Bin Xu", "Ayan Banerjee", "Sandeep Gupta"], "title": "Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures", "comment": null, "summary": "Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.", "AI": {"tldr": "MERINDA\u662f\u4e00\u4e2aFPGA\u52a0\u901f\u7684\u6a21\u578b\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u6570\u636e\u6d41\u7ba1\u9053\u91cd\u6784\u8ba1\u7b97\uff0c\u76f8\u6bd4GPU\u65b9\u6848\u89e3\u51b3\u4e86\u8fed\u4ee3\u4f9d\u8d56\u3001\u5185\u6838\u542f\u52a8\u5f00\u9500\u7b49\u95ee\u9898\uff0c\u5728\u4ee3\u8868\u6027MR\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u6bd4FPGA\u57fa\u51c6\u65b9\u6848\u51cf\u5c11\u6700\u591a6.3\u500d\u5468\u671f\u3002", "motivation": "\u6a21\u578b\u6062\u590d\uff08MR\uff09\u662f\u7269\u7406AI\u548c\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u7684\u6838\u5fc3\u539f\u8bed\uff0c\u4f46GPU\u6267\u884cMR\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u8fed\u4ee3\u4f9d\u8d56\u3001\u5185\u6838\u542f\u52a8\u5f00\u9500\u3001\u5185\u5b58\u5e26\u5bbd\u672a\u5145\u5206\u5229\u7528\u4ee5\u53ca\u9ad8\u6570\u636e\u79fb\u52a8\u5ef6\u8fdf\u3002", "method": "MERINDA\u5c06\u8ba1\u7b97\u91cd\u6784\u4e3a\u6d41\u5f0f\u6570\u636e\u6d41\u7ba1\u9053\uff0c\u5229\u7528BRAM\u5206\u5757\u3001\u5b9a\u70b9\u5185\u6838\u3001\u5e76\u53d1\u4f7f\u7528LUT\u7ed3\u6784\u548c\u8fdb\u4f4d\u94fe\u52a0\u6cd5\u5668\u6765\u5229\u7528\u7247\u4e0a\u5c40\u90e8\u6027\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u5e76\u884c\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u7247\u5916\u6d41\u91cf\u3002", "result": "\u5728\u4ee3\u8868\u6027MR\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0cMERINDA\u6bd4\u57fa\u4e8eFPGA\u7684LTC\u57fa\u51c6\u65b9\u6848\u51cf\u5c11\u4e86\u6700\u591a6.3\u500d\u7684\u5468\u671f\uff0c\u4e3a\u65f6\u95f4\u5173\u952e\u7684\u7269\u7406\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "MERINDA\u7684\u786c\u4ef6\u611f\u77e5\u516c\u5f0f\u6d88\u9664\u4e86\u540c\u6b65\u74f6\u9888\uff0c\u5728MR\u7684\u8fed\u4ee3\u66f4\u65b0\u4e2d\u7ef4\u6301\u9ad8\u541e\u5410\u91cf\uff0c\u4e3a\u7269\u7406AI\u548c\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684FPGA\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06177", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06177", "abs": "https://arxiv.org/abs/2512.06177", "authors": ["Jiahan Xie", "Evan Williams", "Adrian Sampson"], "title": "From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators", "comment": "5 pages, 3 figures", "summary": "We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f00\u6e90\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u80fd\u591f\u4ecePyTorch\u7f16\u5199\u7684ML\u6a21\u578b\u751f\u6210\u53ef\u7efc\u5408\u7684SystemVerilog\u4ee3\u7801", "motivation": "\u4e3a\u673a\u5668\u5b66\u4e60\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u66ff\u4ee3\u95ed\u6e90\u7684\u5546\u4e1a\u5de5\u5177\u5982Vitis HLS\uff0c\u964d\u4f4e\u786c\u4ef6\u8bbe\u8ba1\u95e8\u69db", "method": "\u57fa\u4e8eAllo\u52a0\u901f\u5668\u8bbe\u8ba1\u8bed\u8a00\u3001Calyx\u786c\u4ef6\u4e2d\u95f4\u8868\u793a\u548cLLVM\u7684CIRCT\u9879\u76ee\u6784\u5efa\u5de5\u5177\u94fe\uff0c\u5b9e\u73b0\u5185\u5b58\u5206\u533a\u7b49\u7f16\u8bd1\u5668\u4f18\u5316", "result": "\u80fd\u591f\u6709\u6548\u751f\u6210\u4f18\u5316\u7684FPGA\u53ef\u5b9e\u73b0\u7684\u786c\u4ef6\u8bbe\u8ba1\uff0c\u6027\u80fd\u4e0e\u95ed\u6e90\u5de5\u4e1a\u7ea7\u5de5\u5177Vitis HLS\u76f8\u5f53", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5f00\u6e90ML\u5230\u786c\u4ef6\u7f16\u8bd1\u5668\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5f00\u6e90\u5de5\u5177\u5728\u786c\u4ef6\u8bbe\u8ba1\u9886\u57df\u7684\u53ef\u884c\u6027"}}
{"id": "2512.06208", "categories": ["cs.AR", "cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2512.06208", "abs": "https://arxiv.org/abs/2512.06208", "authors": ["Ho Fung Tsoi", "Dylan Rankin", "Vladimir Loncar", "Philip Harris"], "title": "SparsePixels: Efficient Convolution for Sparse Data on FPGAs", "comment": "Under review", "summary": "Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $\u03bc$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\\times 73$ inference speedup to 0.665 $\u03bc$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.", "AI": {"tldr": "SparsePixels\uff1a\u9488\u5bf9\u7a7a\u95f4\u7a00\u758f\u56fe\u50cf\u6570\u636e\u7684FPGA\u9ad8\u6548\u5377\u79ef\u6846\u67b6\uff0c\u901a\u8fc7\u53ea\u8ba1\u7b97\u6d3b\u8dc3\u50cf\u7d20\u5b9e\u73b073\u500d\u63a8\u7406\u52a0\u901f", "motivation": "\u6807\u51c6CNN\u5728FPGA\u4e0a\u63a8\u7406\u65f6\uff0c\u7531\u4e8e\u9700\u8981\u5bf9\u6bcf\u4e2a\u8f93\u5165\u50cf\u7d20\u8fdb\u884c\u5bc6\u96c6\u5377\u79ef\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u957f\u542f\u52a8\u95f4\u9694\u3002\u8bb8\u591a\u56fe\u50cf\u6570\u636e\u5177\u6709\u7a7a\u95f4\u7a00\u758f\u6027\uff0c\u8bed\u4e49\u4fe1\u606f\u53ea\u5360\u4e00\u5c0f\u90e8\u5206\u50cf\u7d20\uff0c\u5927\u90e8\u5206\u8ba1\u7b97\u6d6a\u8d39\u5728\u7a7a\u533a\u57df\u4e0a\u3002", "method": "\u63d0\u51faSparsePixels\u6846\u67b6\uff0c\u5b9e\u73b0\u7279\u6b8a\u7c7b\u522b\u7684CNN\uff0c\u53ea\u9009\u62e9\u6027\u5730\u4fdd\u7559\u548c\u8ba1\u7b97\u6d3b\u8dc3\u50cf\u7d20\u5b50\u96c6\uff0c\u5ffd\u7565\u5176\u4f59\u50cf\u7d20\u3002\u5f00\u53d1\u4e86\u652f\u6301\u6784\u5efa\u7a00\u758fCNN\u7684\u5e93\uff08\u5305\u542b\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff09\u548c\u7528\u4e8eFPGA\u90e8\u7f72\u7684HLS\u5b9e\u73b0\u3002", "result": "\u5728\u4e2d\u5fae\u5b50\u7269\u7406\u6570\u636e\u96c6\u4e2d\uff0c\u6807\u51c6CNN\uff084k\u53c2\u6570\uff09\u63a8\u7406\u5ef6\u8fdf\u4e3a48.665\u03bcs\uff0c\u800c\u7a00\u758fCNN\uff08\u8ba1\u7b97\u5c11\u4e8e1%\u8f93\u5165\u50cf\u7d20\uff09\u5b9e\u73b073\u500d\u52a0\u901f\u81f30.665\u03bcs\uff0c\u8d44\u6e90\u5229\u7528\u7387\u5728\u7247\u4e0a\u9884\u7b97\u5185\uff0c\u4ec5\u635f\u5931\u5c11\u91cf\u6027\u80fd\u3002\u5728\u5176\u4ed6\u7a00\u758f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u4e5f\u5c55\u793a\u51fa\u81f3\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73b0\u4ee3\u5b9e\u9a8c\uff08\u5982CERN\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u7684\u89e6\u53d1\u548c\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\uff09\u4e2d\u7684\u5feb\u901f\u9ad8\u6548\u6570\u636e\u8bfb\u51fa\u7b97\u6cd5\u5f00\u53d1\u63d0\u4f9b\u652f\u6301\uff0c\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u7684\u7a7a\u95f4\u7a00\u758f\u6027\u5b9e\u73b0\u5fae\u79d2\u7ea7\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2512.06362", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06362", "abs": "https://arxiv.org/abs/2512.06362", "authors": ["Junyi Yang", "Xinyu Luo", "Ye Ke", "Zheng Wang", "Hongyang Shang", "Shuai Dong", "Zhengnan Fu", "Xiaofeng Yang", "Hongjie Liu", "Arindam Basu"], "title": "A 33.6-136.2 TOPS/W Nonlinear Analog Computing-In-Memory Macro for Multi-bit LSTM Accelerator in 65 nm CMOS", "comment": null, "summary": "The energy efficiency of analog computing-in-memory (ACIM) accelerator for recurrent neural networks, particularly long short-term memory (LSTM) network, is limited by the high proportion of nonlinear (NL) operations typically executed digitally. To address this, we propose an LSTM accelerator incorporating an ACIM macro with reconfigurable (1-5 bit) nonlinear in-memory (NLIM) analog-to-digital converter (ADC) to compute NL activations directly in the analog domain using: 1) a dual 9T bitcell with decoupled read/write paths for signed inputs and ternary weight operations; 2) a read-word-line underdrive Cascode (RUDC) technique achieving 2.8X higher read-bitline dynamic range than single-transistor designs (1.4X better over conventional Cascode structure with 7X lower current variation); 3) a dual-supply 6T-SRAM array for efficient multi-bit weight operations and reducing both bitcell count (7.8X) and latency (4X) for 5-bit weight operations. We experimentally demonstrate 5-bit NLIM ADC for approximating NL activations in LSTM cells, achieving average error <1 LSB. Simulation confirms the robustness of NLIM ADC against temperature variations thanks to the replica bias strategy. Our design achieves 92.0% on-chip inference accuracy for a 12-class keyword-spotting task while demonstrating 2.2X higher system-level normalized energy efficiency and 1.6X better normalized area efficiency than state-of-the-art works. The results combine physical measurements of a macro unit-accounting for the majority of LSTM operations (99% linear and 80% nonlinear operations)-with simulations of the remaining components, including additional LSTM and fully connected layers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u53ef\u91cd\u6784\u975e\u7ebf\u6027\u5185\u5b58ADC\u7684LSTM\u52a0\u901f\u5668\uff0c\u76f4\u63a5\u5728\u6a21\u62df\u57df\u8ba1\u7b97\u975e\u7ebf\u6027\u6fc0\u6d3b\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387", "motivation": "\u4f20\u7edf\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\u52a0\u901f\u5668\u5728\u5904\u7406RNN/LSTM\u65f6\uff0c\u7531\u4e8e\u5927\u91cf\u975e\u7ebf\u6027\u64cd\u4f5c\u9700\u8981\u6570\u5b57\u5904\u7406\uff0c\u9650\u5236\u4e86\u80fd\u6548\u63d0\u5347", "method": "\u91c7\u7528\u53ef\u91cd\u67841-5\u4f4d\u975e\u7ebf\u6027\u5185\u5b58ADC\uff0c\u5305\u542b\uff1a1)\u53cc9T\u4f4d\u5355\u5143\u652f\u6301\u6709\u7b26\u53f7\u8f93\u5165\u548c\u4e09\u5143\u6743\u91cd\uff1b2)RUDC\u6280\u672f\u63d0\u5347\u8bfb\u53d6\u52a8\u6001\u8303\u56f4\uff1b3)\u53cc\u7535\u6e906T-SRAM\u9635\u5217\u51cf\u5c11\u4f4d\u5355\u5143\u6570\u91cf\u548c\u5ef6\u8fdf", "result": "5\u4f4dNLIM ADC\u5e73\u5747\u8bef\u5dee<1 LSB\uff0c\u5bf9\u6e29\u5ea6\u53d8\u5316\u9c81\u68d2\uff1b\u572812\u7c7b\u5173\u952e\u8bcd\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u523092.0%\u7247\u4e0a\u63a8\u7406\u51c6\u786e\u7387\uff0c\u7cfb\u7edf\u7ea7\u80fd\u6548\u63d0\u53472.2\u500d\uff0c\u9762\u79ef\u6548\u7387\u63d0\u53471.6\u500d", "conclusion": "\u63d0\u51fa\u7684LSTM\u52a0\u901f\u5668\u901a\u8fc7\u6a21\u62df\u57df\u76f4\u63a5\u8ba1\u7b97\u975e\u7ebf\u6027\u6fc0\u6d3b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387\uff0c\u4e3aRNN\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2512.06537", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06537", "abs": "https://arxiv.org/abs/2512.06537", "authors": ["A. M. H. H. Alahakoon", "Hassaan Saadat", "Darshana Jayasinghe", "Sri Parameswaran"], "title": "Approximate Multiplier Induced Error Propagation in Deep Neural Networks", "comment": "7 pages, Submitted to Design and Automation Conference (DAC 2026)", "summary": "Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u5c06\u8fd1\u4f3c\u4e58\u6cd5\u5668\u7684\u7edf\u8ba1\u8bef\u5dee\u7279\u6027\u4e0eDNN\u7cbe\u5ea6\u635f\u5931\u5173\u8054\u8d77\u6765\uff0c\u8bc1\u660e\u8bef\u5dee\u5747\u503c\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7FPGA\u5b9e\u73b0\u9a8c\u8bc1\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f9d\u8d56\u5bc6\u96c6\u7b97\u672f\u8fd0\u7b97\uff0c\u8fd1\u4f3c\u4e58\u6cd5\u5668\u53ef\u964d\u4f4e\u80fd\u8017\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u8bef\u5dee\u5206\u5e03\u5982\u4f55\u5f71\u54cdDNN\u7cbe\u5ea6\u7684\u4e25\u683c\u6570\u5b66\u5206\u6790\u3002", "method": "\u5efa\u7acb\u5206\u6790\u6846\u67b6\uff0c\u5c06AxM\u7684\u7edf\u8ba1\u8bef\u5dee\u77e9\u4e0eGEMM\u4e2d\u7684\u5931\u771f\u5173\u8054\uff0c\u4f7f\u7528Frobenius\u8303\u6570\u63a8\u5bfc\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u8bef\u5dee\u6ce8\u5165\u548cFPGA\u5b9e\u73b0\u9a8c\u8bc1\u3002", "result": "\u9884\u6d4b\u7684\u5931\u771f\u4e0e\u89c2\u6d4b\u5230\u7684\u7cbe\u5ea6\u4e0b\u964d\u5f3a\u76f8\u5173\uff0c\u8bef\u5dee\u53ef\u914d\u7f6e\u7684AxM\u5728FPGA\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5206\u6790\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u884c\u4e3a\u6216\u786c\u4ef6\u7ea7\u4eff\u771f\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5feb\u901f\u4f30\u8ba1AxM\u5bf9DNN\u63a8\u7406\u8d28\u91cf\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.06854", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06854", "abs": "https://arxiv.org/abs/2512.06854", "authors": ["Qijun Zhang", "Yao Lu", "Mengming Li", "Shang Liu", "Zhiyao Xie"], "title": "ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design", "comment": "Published in NeurIPS'25 Dataset and Benchmark Track", "summary": "Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.", "AI": {"tldr": "ArchPower\u662f\u9996\u4e2a\u7528\u4e8e\u67b6\u6784\u7ea7\u5904\u7406\u5668\u529f\u8017\u5efa\u6a21\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5305\u542b200\u4e2aCPU\u6570\u636e\u6837\u672c\uff0c\u6db5\u76d625\u79cdCPU\u914d\u7f6e\u548c8\u79cd\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u63d0\u4f9b\u8d85\u8fc7100\u4e2a\u67b6\u6784\u7279\u5f81\u548c\u7ec6\u7c92\u5ea6\u529f\u8017\u6807\u7b7e\u3002", "motivation": "\u5f53\u524dCPU\u529f\u8017\u8bc4\u4f30\u9700\u8981\u8017\u65f6\u7684IC\u5b9e\u73b0\u6d41\u7a0b\uff0c\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u7684\u4f20\u7edf\u529f\u8017\u6a21\u578b\u4e0d\u51c6\u786e\uff0c\u800c\u57fa\u4e8eML\u7684\u67b6\u6784\u7ea7\u529f\u8017\u6a21\u578b\u9762\u4e34\u6570\u636e\u53ef\u7528\u6027\u6311\u6218\uff0c\u7f3a\u4e4f\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u53cd\u6620\u771f\u5b9eCPU\u8bbe\u8ba1\u573a\u666f\u3002", "method": "\u901a\u8fc7\u590d\u6742\u4e14\u771f\u5b9e\u7684\u8bbe\u8ba1\u6d41\u7a0b\u6536\u96c6CPU\u67b6\u6784\u4fe1\u606f\u4f5c\u4e3a\u7279\u5f81\uff0c\u4f7f\u7528\u6a21\u62df\u529f\u8017\u4f5c\u4e3a\u6807\u7b7e\u3002\u6570\u636e\u96c6\u5305\u542b200\u4e2aCPU\u6570\u636e\u6837\u672c\uff0c\u6765\u81ea25\u79cd\u4e0d\u540cCPU\u914d\u7f6e\u6267\u884c8\u79cd\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6bcf\u4e2a\u6837\u672c\u6709\u8d85\u8fc7100\u4e2a\u67b6\u6784\u7279\u5f81\uff08\u786c\u4ef6\u548c\u4e8b\u4ef6\u53c2\u6570\uff09\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u5f00\u6e90\u67b6\u6784\u7ea7\u5904\u7406\u5668\u529f\u8017\u5efa\u6a21\u6570\u636e\u96c6ArchPower\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u529f\u8017\u4fe1\u606f\uff1a\u603b\u8bbe\u8ba1\u529f\u8017\u548c11\u4e2a\u7ec4\u4ef6\u7684\u529f\u8017\uff0c\u6bcf\u4e2a\u529f\u8017\u503c\u8fdb\u4e00\u6b65\u5206\u89e3\u4e3a\u7ec4\u5408\u903b\u8f91\u3001\u65f6\u5e8f\u903b\u8f91\u3001\u5b58\u50a8\u5668\u548c\u65f6\u949f\u529f\u8017\u56db\u4e2a\u7ec6\u7c92\u5ea6\u7ec4\u3002", "conclusion": "ArchPower\u586b\u8865\u4e86\u67b6\u6784\u7ea7\u5904\u7406\u5668\u529f\u8017\u5efa\u6a21\u9886\u57df\u5f00\u6e90\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3aML-based\u529f\u8017\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u7ec6\u7c92\u5ea6\u7684\u771f\u5b9e\u6570\u636e\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u52a0\u901fCPU\u8bbe\u8ba1\u6d41\u7a0b\u3002"}}
{"id": "2512.07312", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07312", "abs": "https://arxiv.org/abs/2512.07312", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Yuhang Gu", "Wei Zhang"], "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411AI\u52a0\u901f\u5668\u7684\u5171\u4eab\u7cfb\u7edf\u7ea7\u7f13\u5b58\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5e94\u7528\u611f\u77e5\u7684\u7ba1\u7406\u7b56\u7565\uff08\u5305\u62ec\u7f13\u5b58\u66ff\u6362\u3001\u6b7b\u5757\u9884\u6d4b\u548c\u65c1\u8def\u51b3\u7b56\uff09\u6765\u7b80\u5316\u7f16\u7a0b\uff0c\u76f8\u6bd4\u4f20\u7edf\u7f13\u5b58\u67b6\u6784\u83b7\u5f97\u4e86\u6700\u9ad81.8\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u91c7\u7528\uff0cAI\u52a0\u901f\u5668\u8bbe\u8ba1\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u7279\u522b\u662f\u5177\u6709\u6df1\u5ea6\u5c42\u6b21\u5316\u6682\u5b58\u5668\u5185\u5b58\uff08SPMs\uff09\u53ca\u5176\u5f02\u6b65\u7ba1\u7406\u7684\u67b6\u6784\u589e\u52a0\u4e86\u8f6f\u4ef6\u5f00\u53d1\u96be\u5ea6\u3002\u4f5c\u8005\u63a2\u7d22\u76f8\u53cd\u7684\u8bbe\u8ba1\u65b9\u5411\uff1a\u91c7\u7528\u5171\u4eab\u7cfb\u7edf\u7ea7\u7f13\u5b58\u548c\u57fa\u4e8e\u5e94\u7528\u611f\u77e5\u7684\u7ba1\u7406\u7b56\u7565\uff0c\u4ee5\u4fdd\u6301\u7f16\u7a0b\u7b80\u5355\u6027\u3002", "method": "\u63d0\u51fa\u591a\u6838AI\u52a0\u901f\u5668\u67b6\u6784\uff0c\u914d\u5907\u5171\u4eab\u7cfb\u7edf\u7ea7\u7f13\u5b58\uff0c\u5229\u7528\u8f6f\u4ef6\u6808\u4e2d\u7684\u6570\u636e\u6d41\u4fe1\u606f\u6307\u5bfc\u7f13\u5b58\u66ff\u6362\uff08\u5305\u62ec\u6b7b\u5757\u9884\u6d4b\uff09\uff0c\u7ed3\u5408\u65c1\u8def\u51b3\u7b56\u548c\u7f13\u89e3\u7f13\u5b58\u98a0\u7c38\u7684\u673a\u5236\u3002\u901a\u8fc7\u5468\u671f\u7cbe\u786e\u6a21\u62df\u5668\u8bc4\u4f30\uff0c\u5e76\u5efa\u7acb\u8003\u8651\u5b9e\u9645\u91cd\u53e0\u884c\u4e3a\u7684\u5206\u6790\u6a21\u578b\u6765\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u7f13\u5b58\u67b6\u6784\u83b7\u5f97\u6700\u9ad81.80\u500d\u7684\u6027\u80fd\u52a0\u901f\u3002\u65c1\u8def\u548c\u98a0\u7c38\u7f13\u89e3\u7b56\u7565\u80fd\u6709\u6548\u5904\u7406\u6709\u65e0\u6838\u95f4\u6570\u636e\u5171\u4eab\u7684\u573a\u666f\u3002RTL\u5b9e\u73b0\u663e\u793a\u572815nm\u5de5\u827a\u4e0b\u9762\u79ef\u4e3a0.064mm\u00b2\uff0c\u53ef\u8fd0\u884c\u57282GHz\u65f6\u949f\u9891\u7387\u3002", "conclusion": "\u5171\u4eab\u7f13\u5b58\u8bbe\u8ba1\u5c55\u793a\u4e86\u7b80\u5316AI\u52a0\u901f\u5668\u5f00\u53d1\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5e94\u7528\u611f\u77e5\u7684\u7ba1\u7406\u7b56\u7565\u5728\u4fdd\u6301\u7f16\u7a0b\u7b80\u5355\u6027\u7684\u540c\u65f6\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u672a\u6765AI\u52a0\u901f\u5668\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.07520", "categories": ["cs.AR", "cs.CR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.07520", "abs": "https://arxiv.org/abs/2512.07520", "authors": ["No\u00e9 Amiot", "Quentin L. Meunier", "Karine Heydemann", "Emmanuelle Encrenaz"], "title": "aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \\& Software Formal Verification", "comment": null, "summary": "Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs", "AI": {"tldr": "aLEAKator\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63a9\u7801\u52a0\u5bc6\u786c\u4ef6\u52a0\u901f\u5668\u548cCPU\u8f6f\u4ef6\u5b9e\u73b0\uff0c\u652f\u6301\u591a\u79cd\u6cc4\u6f0f\u6a21\u578b\u548c\u4fe1\u53f7\u7c92\u5ea6\uff0c\u65e0\u9700\u76ee\u6807CPU\u67b6\u6784\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u53ea\u80fd\u5904\u7406\u5c0f\u578b\u786c\u4ef6\u6a21\u5757\u6216CPU\u4e0a\u7684\u5c0f\u89c4\u6a21\u7a0b\u5e8f\uff08\u5982S\u76d2\uff09\uff0c\u8981\u4e48\u53d7\u9650\u4e8e\u7279\u5b9a\u6cc4\u6f0f\u6a21\u578b\uff0c\u6216\u8005\u9700\u8981\u786c\u4ef6\u7279\u5b9a\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u5728\u8003\u8651\u6bdb\u523a\u3001\u8f6c\u6362\u548cCPU\u5fae\u67b6\u6784\u7279\u6027\u7b49\u9ad8\u7ea7\u6cc4\u6f0f\u6a21\u578b\u65f6\uff0c\u9a8c\u8bc1\u63a9\u7801\u5b9e\u73b0\u7684\u5b89\u5168\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faaLEAKator\u6846\u67b6\uff0c\u91c7\u7528\u6df7\u5408\u57df\u4eff\u771f\u65b9\u6cd5\uff0c\u80fd\u591f\u7cbe\u786e\u5efa\u6a21\u548c\u9a8c\u8bc1\u5404\u79cd\uff08\u5305\u62ec\u9c81\u68d2\u548c\u5bbd\u677e\u7684\uff091-\u63a2\u6d4b\u6cc4\u6f0f\u6a21\u578b\uff0c\u652f\u6301\u53ef\u53d8\u4fe1\u53f7\u7c92\u5ea6\u800c\u4e0d\u9650\u4e8e1\u4f4d\u7ebf\u3002\u8be5\u6846\u67b6\u8fd8\u652f\u6301\u67e5\u627e\u8868\u5b58\u5728\u65f6\u7684\u9a8c\u8bc1\uff0c\u4e14\u4e0d\u9700\u8981\u76ee\u6807CPU\u67b6\u6784\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "aLEAKator\u901a\u8fc7\u73b0\u6709\u5de5\u5177\u548c\u5b9e\u9645\u6d4b\u91cf\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u4e86\u521b\u65b0\u6027\u7ed3\u679c\uff0c\u5982\u5728\u5404\u79cdCPU\u4e0a\u9a8c\u8bc1\u5b8c\u6574\u7684\u4e00\u9636\u63a9\u7801AES\u5b9e\u73b0\u3002", "conclusion": "aLEAKator\u662f\u4e00\u4e2a\u6709\u6548\u7684\u81ea\u52a8\u5316\u9a8c\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u89e3\u51b3\u63a9\u7801\u786c\u4ef6\u548c\u8f6f\u4ef6\u5b9e\u73b0\u5b89\u5168\u9a8c\u8bc1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ea7\u6cc4\u6f0f\u6a21\u578b\u4e0b\uff0c\u4e3a\u5b9e\u9645\u52a0\u5bc6\u5b9e\u73b0\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.07622", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.07622", "abs": "https://arxiv.org/abs/2512.07622", "authors": ["Martha Semken", "Mariano Vargas", "Ignacio Tula", "Giuliana Zorzoli", "Andr\u00e9s Rojas Paredes"], "title": "An\u00e1lisis de rendimiento y eficiencia energ\u00e9tica en el cluster Raspberry Pi Cronos", "comment": "in Spanish language", "summary": "This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.", "AI": {"tldr": "\u8bc4\u4f30\u57fa\u4e8eRaspberry Pi\u7684\u6559\u80b2\u96c6\u7fa4Cronos\u7684\u8ba1\u7b97\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4f7f\u7528HPL\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e0d\u540c\u8282\u70b9\u914d\u7f6e\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u3001\u7a33\u5b9a\u6027\u548c\u529f\u8017\u3002", "motivation": "\u4e3a\u6559\u80b2\u7814\u7a76\u73af\u5883\u8bbe\u8ba1\u548c\u8bc4\u4f30\u4f4e\u6210\u672cARM\u96c6\u7fa4\uff0c\u5206\u6790\u5176\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u80fd\u6548\u8868\u73b0\u3002", "method": "\u4f7f\u7528High Performance Linpack (HPL)\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u914d\u7f6eSlurm\u8d44\u6e90\u7ba1\u7406\u548cOpen MPI\u5e76\u884c\u901a\u4fe1\u7684\u73af\u5883\u4e2d\uff0c\u5bf9\u4e0d\u540c\u8282\u70b9\u914d\u7f6e\uff08\u5305\u62ecRaspberry Pi4\u548c3b\uff09\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u3002", "result": "6\u4e2aRaspberry Pi4\u8282\u70b9\u7684\u540c\u6784\u914d\u7f6e\u8fbe\u52306.91 GFLOPS\u6027\u80fd\uff1b\u5f02\u6784\u8282\u70b9\uff08\u5305\u542bPi3b\uff09\u4f1a\u964d\u4f4e\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff1b\u6d4b\u91cf\u4e86\u7cfb\u7edf\u603b\u529f\u8017\u5e76\u8ba1\u7b97\u4e86\u6027\u80fd\u529f\u8017\u6bd4(GFLOPS/W)\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6559\u80b2\u7814\u7a76\u73af\u5883\u4e2d\u4f4e\u6210\u672cARM\u96c6\u7fa4\u7684\u8bbe\u8ba1\u3001\u8bc4\u4f30\u548c\u4f7f\u7528\u63d0\u4f9b\u4e86\u5177\u4f53\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8ba1\u7b97\u6027\u80fd\u548c\u80fd\u6548\u65b9\u9762\u7684\u8868\u73b0\u3002"}}
