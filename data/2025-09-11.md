<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Analyzing the capabilities of HLS and RTL tools in the design of an FPGA Montgomery Multiplier](https://arxiv.org/abs/2509.08067)
*Rares Ifrim,Decebal Popescu*

Main category: cs.AR

TL;DR: 这篇论文分析了使用CIOS方法的BLS12-381椭圆曲线Montgomery模乘器的多种FPGA实现方案，比较了Verilog简单方式、手动优化DSP原语和高级综合方案的性能差异，并与Rust软件实现进行对比。


<details>
  <summary>Details</summary>
Motivation: 为BLS12-381椭圆曲线加密操作开发高频率、高吞吐量的Montgomery模乘器，作为点加法和点乘法等基础操作的核心基础。

Method: 采用Coarsely Integrated Operand Scanning(CIOS)方法，在AMD-Xilinx工具和Alveo FPGA板卡上实现三类设计：Verilog简单方式（自动DSP选择）、Verilog优化方式（手动DSP原语实例化）和高级综合方案，并与Rust软件实现进行对比。

Result: 分析了不同设计选择和工具配置对频率、延迟和资源消耗的影响，展示了如何通过正确的DSP原语使用来提高性能并减少LUT和FF资源消耗。

Conclusion: 手动DSP原语优化的Verilog方案能够实现最佳性能，适当的流水线设计可以在高频率下运行，为椭圆曲线加密操作提供了高速计算基础。

Abstract: We present the analysis of various FPGA design implementations of a
Montgomery Modular Multiplier, compatible with the BLS12-381 elliptic curve,
using the Coarsely Integrated Operand Scanning approach of working with
complete partial products on different digit sizes. The scope of the
implemented designs is to achieve a high-frequency, high-throughput solution
capable of computing millions of operations per second, which can provide a
strong foundation for different Elliptic Curve Cryptography operations such as
point addition and point multiplication. One important constraint for our
designs was to only use FPGA DSP primitives for the arithmetic operations
between digits employed in the CIOS algorithm as these primitives, when
pipelined properly, can operate at a high frequency while also relaxing the
resource consumption of FPGA LUTs and FFs. The target of the analysis is to see
how different design choices and tool configurations influence the frequency,
latency and resource consumption when working with the latest AMD-Xilinx tools
and Alveo FPGA boards in an RTL-HLS hybrid approach. We compare three
categories of designs: a Verilog naive approach where we rely on the Vivado
synthesizer to automatically choose when and where to use DSPs, a Verilog
optimized approach by manually instantiating the DSP primitives ourselves and a
complete High-Level Synthesis approach. We also compare the FPGA
implementations with an optimized software implementation of the same
Montgomery multiplier written in Rust.

</details>


### [2] [Lifetime-Aware Design of Item-Level Intelligence](https://arxiv.org/abs/2509.08193)
*Shvetank Prakash,Andrew Cheng,Olof Kindgren,Ashiq Ahamed,Graham Knight,Jed Kufel,Francisco Rodriguez,Arya Tschand,David Kong,Mariam Elgamal,Jerry Huang,Emma Chen,Gage Hills,Richard Price,Emre Ozer,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: FlexiFlow是一个面向物品级智能的寿命感知设计框架，通过柔性电子技术实现低成本计算集成，在考虑万亿级部署规模时优化碳足迹。


<details>
  <summary>Details</summary>
Motivation: 传统计算部署模式无法适应物品级智能应用中1000倍的操作寿命差异，需要在考虑可持续性的前提下重新设计架构。

Method: 框架包含FlexiBench工作负载套件、FlexiBits面积优化的RISC-V核心（1/4/8位数据路径）以及基于部署特性的碳感知模型选择最优架构。

Result: 寿命感知微架构设计可减少1.62倍碳足迹，算法决策可减少14.5倍碳足迹，通过柔性电子PDK实现30.9kHz操作频率。

Conclusion: FlexiFlow推动了极端边缘计算的发展，需要重新评估传统设计方法以适应新的约束条件。

Abstract: We present FlexiFlow, a lifetime-aware design framework for item-level
intelligence (ILI) where computation is integrated directly into disposable
products like food packaging and medical patches. Our framework leverages
natively flexible electronics which offer significantly lower costs than
silicon but are limited to kHz speeds and several thousands of gates. Our
insight is that unlike traditional computing with more uniform deployment
patterns, ILI applications exhibit 1000X variation in operational lifetime,
fundamentally changing optimal architectural design decisions when considering
trillion-item deployment scales. To enable holistic design and optimization, we
model the trade-offs between embodied carbon footprint and operational carbon
footprint based on application-specific lifetimes. The framework includes: (1)
FlexiBench, a workload suite targeting sustainability applications from
spoilage detection to health monitoring; (2) FlexiBits, area-optimized RISC-V
cores with 1/4/8-bit datapaths achieving 2.65X to 3.50X better energy
efficiency per workload execution; and (3) a carbon-aware model that selects
optimal architectures based on deployment characteristics. We show that
lifetime-aware microarchitectural design can reduce carbon footprint by 1.62X,
while algorithmic decisions can reduce carbon footprint by 14.5X. We validate
our approach through the first tape-out using a PDK for flexible electronics
with fully open-source tools, achieving 30.9kHz operation. FlexiFlow enables
exploration of computing at the Extreme Edge where conventional design
methodologies must be reevaluated to account for new constraints and
considerations.

</details>


### [3] [FASE: FPGA-Assisted Syscall Emulation for Rapid End-to-End Processor Performance Validation](https://arxiv.org/abs/2509.08405)
*Chengzhen Meng,Xiuzhuang Chen,Hongjun Dai*

Main category: cs.AR

TL;DR: FASE框架是首个在FPGA平台上实现系统调用仿真的工作，通过最小化硬件接口、优化通信协议和远程处理系统调用，使复杂多线程基准测试能够在早期阶段直接在处理器设计上运行，无需集成SoC或目标操作系统，实现了高效准确的性能验证。


<details>
  <summary>Details</summary>
Motivation: 传统工作流程将验证过程推迟到RTL设计和SoC集成完成后，显著延长了开发和迭代周期。AI工作负载和领域特定架构的快速发展导致处理器微架构日益多样化，需要快速准确的性能验证方法。

Method: 提出FASE框架（FPGA辅助系统调用仿真），包含三个关键创新：1）仅暴露最小CPU接口，保持其他硬件组件不变；2）提出主机-目标协议（HTP）最小化跨设备数据流量；3）提出主机端运行时远程处理Linux风格系统调用。

Result: 在Xilinx FPGA上使用开源RISC-V SMP处理器Rocket进行实验。单线程CoreMark引入小于1%的性能误差，相比Proxy Kernel效率提高2000倍以上。复杂OpenMP基准测试显示，对于大多数单线程工作负载验证准确率超过96%，多线程工作负载超过91.5%。

Conclusion: FASE框架显著降低了开发复杂性和反馈时间，所有组件均已开源发布，为早期阶段处理器设计提供了高效准确的性能验证解决方案。

Abstract: The rapid advancement of AI workloads and domain-specific architectures has
led to increasingly diverse processor microarchitectures, whose design
exploration requires fast and accurate performance validation. However,
traditional workflows defer validation process until RTL design and SoC
integration are complete, significantly prolonging development and iteration
cycle.
  In this work, we present FASE framework, FPGA-Assisted Syscall Emulation, the
first work for adapt syscall emulation on FPGA platforms, enabling complex
multi-thread benchmarks to directly run on the processor design without
integrating SoC or target OS for early-stage performance validation. FASE
introduces three key innovations to address three critical challenges for
adapting FPGA-based syscall emulation: (1) only a minimal CPU interface is
exposed, with other hardware components untouched, addressing the lack of a
unified hardware interface in FPGA systems; (2) a Host-Target Protocol (HTP) is
proposed to minimize cross-device data traffic, mitigating the low-bandwidth
and high-latency communication between FPGA and host; and (3) a host-side
runtime is proposed to remotely handle Linux-style system calls, addressing the
challenge of cross-device syscall delegation.
  Experiments ware conducted on Xilinx FPGA with open-sourced RISC-V SMP
processor Rocket. With single-thread CoreMark, FASE introduces less than 1%
performance error and achieves over 2000x higher efficiency compared to Proxy
Kernel due to FPGA acceleration. With complex OpenMP benchmarks, FASE
demonstrates over 96% performance validation accuracy for most single-thread
workloads and over 91.5% for most multi-thread workloads compared to full SoC
validation, significantly reducing development complexity and time-to-feedback.
All components of FASE framework are released as open-source.

</details>


### [4] [AutoVeriFix: Automatically Correcting Errors and Enhancing Functional Correctness in LLM-Generated Verilog Code](https://arxiv.org/abs/2509.08416)
*Yan Tan,Xiangchen Meng,Zijun Jiang,Yangdi Lyu*

Main category: cs.AR

TL;DR: AutoVeriFix是一个两阶段框架，使用Python参考模型和自动化测试来提升LLM生成的Verilog代码功能正确性


<details>
  <summary>Details</summary>
Motivation: 解决LLM在硬件描述语言Verilog代码生成中因训练数据稀缺导致的功能错误问题

Method: 两阶段方法：首先生成Python参考模型定义电路行为，然后利用这些模型创建自动化测试来指导Verilog RTL实现，通过仿真差异迭代修正错误

Result: 实验结果表明该方法在提升生成Verilog代码功能正确性方面显著优于现有最先进方法

Conclusion: AutoVeriFix框架通过Python辅助的两阶段方法有效提高了LLM生成Verilog代码的功能准确性和可靠性

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
generating software code for high-level programming languages such as Python
and C++. However, their application to hardware description languages, such as
Verilog, is challenging due to the scarcity of high-quality training data.
Current approaches to Verilog code generation using LLMs often focus on
syntactic correctness, resulting in code with functional errors. To address
these challenges, we present AutoVeriFix, a novel Python-assisted two-stage
framework designed to enhance the functional correctness of LLM-generated
Verilog code. In the first stage, LLMs are employed to generate high-level
Python reference models that define the intended circuit behavior. In the
second stage, these Python models facilitate the creation of automated tests
that guide the generation of Verilog RTL implementations. Simulation
discrepancies between the reference model and the Verilog code are iteratively
used to identify and correct errors, thereby improving the functional accuracy
and reliability of the LLM-generated Verilog code. Experimental results
demonstrate that our approach significantly outperforms existing
state-of-the-art methods in improving the functional correctness of generated
Verilog code.

</details>


### [5] [BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference](https://arxiv.org/abs/2509.08542)
*Wenlun Zhang,Xinyu Li,Shimpei Ando,Kentaro Yoshioka*

Main category: cs.AR

TL;DR: BitROM是首个基于CiROM的加速器，通过1.58位量化模型和三项关键创新，解决了LLM在边缘设备部署的面积效率问题，在65nm工艺下实现20.8 TOPS/W和4,967 kB/mm²的比特密度。


<details>
  <summary>Details</summary>
Motivation: 传统CiROM加速器由于LLM参数量巨大而难以扩展，如LLaMA-7B即使在先进CMOS节点也需要超过1,000 cm²的硅面积，限制了在边缘设备上的实际应用。

Method: 1) 双向ROM阵列，每个晶体管存储两个三元权重；2) 三元权重计算优化的三模式局部累加器；3) 集成解码-刷新eDRAM，支持片上KV缓存管理；4) 集成LoRA适配器实现跨下游任务的高效迁移学习。

Result: 在65nm CMOS工艺下实现20.8 TOPS/W的能效和4,967 kB/mm²的比特密度，面积效率比之前的数字CiROM设计提升10倍，DR eDRAM减少43.6%的外部DRAM访问。

Conclusion: BitROM通过硬件-算法协同设计，成功解决了CiROM加速器在LLM部署中的可扩展性问题，为边缘设备上的高效LLM推理提供了实用解决方案。

Abstract: Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy
efficiency for CNNs by eliminating runtime weight updates. However, their
scalability to Large Language Models (LLMs) is fundamentally constrained by
their vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA
series - demands more than 1,000 cm2 of silicon area even in advanced CMOS
nodes. This paper presents BitROM, the first CiROM-based accelerator that
overcomes this limitation through co-design with BitNet's 1.58-bit quantization
model, enabling practical and efficient LLM inference at the edge. BitROM
introduces three key innovations: 1) a novel Bidirectional ROM Array that
stores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator
optimized for ternary-weight computations; and 3) an integrated Decode-Refresh
(DR) eDRAM that supports on-die KV-cache management, significantly reducing
external memory access during decoding. In addition, BitROM integrates
LoRA-based adapters to enable efficient transfer learning across various
downstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit
density of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over
prior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%
reduction in external DRAM access, further enhancing deployment efficiency for
LLMs in edge applications.

</details>
