<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Design Environment of Quantization-Aware Edge AI Hardware for Few-Shot Learning](https://arxiv.org/abs/2602.12295)
*R. Kanda,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: 该研究通过Brevitas量化模块实现定点数据处理，在边缘AI硬件的小样本学习设计中确保全流程精度一致性，验证了5-6位定点数即可达到浮点运算精度，显著减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 为了在边缘AI硬件的小样本学习实现中，确保从预训练到评估的整个设计流程中精度的一致性，需要实现定点数据处理来优化硬件实现。

Method: 使用Brevitas量化模块实现定点数据处理，支持任意指定整数和小数部分的位宽。采用量化感知训练（QAT）和训练后量化（PTQ）两种方法，并与当前设计流程中的Tensil工具进行比较。

Result: 性能验证表明，即使使用6位或5位的整数和小数部分，也能保持与浮点运算相当的精度，而Tensil要求8位或16位。这表明有进一步减少计算资源的潜力。

Conclusion: 该研究为小样本学习的边缘AI硬件创建了一个通用的设计和评估环境，通过定点量化实现了精度保持和资源优化的平衡。

Abstract: This study aims to ensure consistency in accuracy throughout the entire design flow in the implementation of edge AI hardware for few-shot learning, by implementing fixed-point data processing in the pre-training and evaluation phases. Specifically, the quantization module, called Brevitas, is applied to implement fixed-point data processing, which allows for arbitrary specification of the bit widths for the integer and fractional parts. Two methods of fixed-point data quantization, quantization-aware training (QAT) and post-training quantization (PTQ), are utilized in Brevitas. With Tensil, which is used in the current design flow, the bit widths of the integer and fractional parts need to be 8 bits each or 16 bits each when implemented in hardware, but performance validation has shown that accuracy comparable to floating-point operations can be maintained even with 6 bits or 5 bits each, indicating potential for further reduction in computational resources. These results clearly contribute to the creation of a versatile design and evaluation environment for edge AI hardware for few-shot learning.

</details>


### [2] [CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement](https://arxiv.org/abs/2602.12422)
*Kaushal Mhapsekar,Azam Ghanbari,Bita Aslrousta,Samira Mirbagher-Ajorpaz*

Main category: cs.AR

TL;DR: CacheMind：首个基于RAG和LLM的对话式缓存分析工具，支持自然语言查询缓存跟踪数据，提供语义化答案，显著提升缓存性能分析效率。


<details>
  <summary>Details</summary>
Motivation: 传统缓存替换策略依赖手工启发式方法，性能有限；缓存数据分析需要解析数百万条跟踪记录并进行手动过滤，过程缓慢且非交互式。需要一种更智能、交互性强的分析工具。

Method: 提出CacheMind系统，结合检索增强生成（RAG）和大语言模型（LLMs），实现缓存跟踪数据的语义推理。使用SIEVE和RANGER检索器，支持自然语言查询，提供基于跟踪数据的可读答案。

Result: 在CacheMindBench基准测试中：SIEVE在75个未见跟踪问题中达到66.67%准确率，在25个策略推理任务中达到84.80%；RANGER分别达到89.33%和64.80%。RANGER在6个类别中的4个实现100%准确率。实际应用显示：旁路用例提升缓存命中率7.66%、加速2.04%；软件修复用例加速76%；Mockingjay替换策略用例加速0.7%。

Conclusion: CacheMind首次实现了基于自然语言的缓存跟踪语义分析，显著优于现有RAG方法（LlamaIndex仅10%检索成功率）。该系统为微架构师提供了强大的交互式分析工具，能够从复杂缓存数据中提取可操作的性能洞察。

Abstract: Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, "Why is the memory access associated with PC X causing more evictions?", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface.

</details>


### [3] [MXFormer: A Microscaling Floating-Point Charge-Trap Transistor Compute-in-Memory Transformer Accelerator](https://arxiv.org/abs/2602.12480)
*George Karfakis,Samyak Chakrabarty,Vinod Kurian Jacob,Siyun Qiao,Subramanian S. Iyer,Sudhakar Pamarti,Puneet Gupta*

Main category: cs.AR

TL;DR: MXFormer是一种基于电荷捕获晶体管计算内存阵列的混合权重固定Transformer加速器，通过消除权重移动实现高吞吐和高能效，在保持接近数字精度的同时大幅提升计算密度。


<details>
  <summary>Details</summary>
Motivation: Transformer模型部署面临计算和内存带宽的巨大需求限制，需要高效加速器来支持大规模固定模型的短序列推理。

Method: 采用超密集电荷捕获晶体管构建MXFP4计算内存阵列，实现全权重固定存储；设计静态分区架构，12个Transformer块通过深度流水线数据流连接；静态权重层在模拟CTT阵列执行，动态计算在数字块中处理。

Result: MXFormer在ViT-L/32上达到58275 FPS（双芯片），ViT-B/16上达到41269 FPS（单芯片）；相比非FWS加速器计算密度提升3.3-60.5倍，能效提升1.7-2.5倍；相比FWS加速器计算密度提升20.9倍，权重存储密度提升2倍，精度损失小于1%。

Conclusion: MXFormer通过创新的混合权重固定计算内存架构，为大规模Transformer模型推理提供了高吞吐、高能效的解决方案，在保持精度的同时显著提升了硬件效率。

Abstract: The proliferation of Transformer models is often constrained by the significant computational and memory bandwidth demands of deployment. To address this, we present MXFormer, a novel, hybrid, weight-stationary Compute-in-Memory (CIM) accelerator that provides high throughput and efficiency for fixed-model inference on large short-sequence Transformers. Our architecture's foundation is the use of ultra-dense Charge-Trap Transistors (CTTs) in Microscaling MXFP4 CIM arrays, uniquely enabling the on-chip storage of up to hundreds of millions of parameters in Fully Weight Stationary (FWS) fashion.
  We introduce a statically partitioned design with 12 Transformer blocks connected by a deeply pipelined dataflow. Static-weight layers (MLPs and linear projections) execute on highly parallel analog CTT arrays using an MXFP4-native flow with per-block exponent alignment and a 10-bit SAR ADC. Dynamic computations are handled in fully accurate digital blocks that utilize MXFP-enabled systolic arrays for scaled dot-product attention and vector units for LayerNorm and FlashAttention-style Softmax.
  By eliminating all weight movement, the deeply pipelined MXFormer architecture yields very high single-stream throughput and efficiency, processing 58275 FPS on ViT-L/32 (dual-chip) or 41269 FPS on ViT-B/16 (single chip). MXFormer outperforms comparable state-of-the-art non-FWS digital, hybrid and photonic Transformer accelerators ~3.3x-60.5x in compute density and ~1.7x-2.5x in energy efficiency. Against FWS accelerators, MXFormer improves compute density by ~20.9x and resident weight storage density by ~2x, while preserving near-digital accuracy (drop of <1%) without any model retraining.

</details>


### [4] [Arcalis: Accelerating Remote Procedure Calls Using a Lightweight Near-Cache Solution](https://arxiv.org/abs/2602.12596)
*Johnson Umeike,Pongstorn Maidee,Bahar Asgari*

Main category: cs.AR

TL;DR: Arcalis是一种近缓存RPC加速器，通过在末级缓存旁放置轻量级硬件引擎，显著提升微服务RPC性能，相比CPU基线获得1.79-4.16倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着网络带宽扩展，RPC处理中的CPU开销（特别是序列化、反序列化和协议处理）已成为关键瓶颈。现有软件优化和FPGA卸载方案距离CPU内存层次较远，导致不必要的数据移动和缓存污染。

Method: 设计Arcalis近缓存RPC加速器，将轻量级硬件引擎放置在末级缓存旁，使用专用微引擎在接收和发送路径上卸载RPC处理，以缓存行延迟运行，同时保持可编程性。

Result: 相比CPU基线实现1.79-4.16倍端到端加速，微架构开销降低高达88%，吞吐量比现有解决方案提升高达1.62倍。

Conclusion: 近缓存RPC加速是高性能微服务部署的实用解决方案，通过将RPC处理逻辑解耦、支持微服务特定执行，并靠近LLC以立即消费网卡注入的数据，显著提升性能。

Abstract: Modern microservices increasingly depend on high-performance remote procedure calls (RPCs) to coordinate fine-grained, distributed computation. As network bandwidths continue to scale, the CPU overhead associated with RPC processing, particularly serialization, deserialization, and protocol handling, has become a critical bottleneck. This challenge is exacerbated by fast user-space networking stacks such as DPDK, which expose RPC processing as the dominant performance limiter. While prior work has explored software optimizations and FPGA-based offload engines, these approaches remain physically distant from the CPU's memory hierarchy, incurring unnecessary data movement and cache pollution. We present Arcalis, a near-cache RPC accelerator that positions a lightweight hardware engine adjacent to the last-level cache (LLC). Arcalis offloads RPC processing to dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. By decoupling RPC processing logic, enabling microservice-specific execution, and positioning itself near the LLC to immediately consume data injected by network cards, Arcalis achieves 1.79-4.16$\times$ end-to-end speedup compared to the CPU baseline, while significantly reducing microarchitectural overhead by up to 88%, and achieves up to a 1.62$\times$ higher throughput than prior solutions. These results highlight the potential of near-cache RPC acceleration as a practical solution for high-performance microservice deployment.

</details>


### [5] [DPUConfig: Optimizing ML Inference in FPGAs Using Reinforcement Learning](https://arxiv.org/abs/2602.12847)
*Alexandros Patras,Spyros Lalis,Christos D. Antonopoulos,Nikolaos Bellas*

Main category: cs.AR

TL;DR: 本文提出DPUConfig框架，使用强化学习代理动态选择FPGA中深度学习处理器单元(DPU)的最优配置，以提升能效。


<details>
  <summary>Details</summary>
Motivation: 异构嵌入式系统（如FPGA）为ML推理提供了快速灵活的平台，但在FPGA系统中为深度学习应用高效分配计算资源具有挑战性。DPU作为可参数化的FPGA加速模块，支持多种ML模型并能多次实例化实现并发执行，但需要智能的运行时管理来优化配置。

Method: 提出DPUConfig运行时管理框架，基于自定义强化学习(RL)代理，通过监控实时遥测数据、系统利用率、功耗和应用性能，动态选择最优DPU配置。

Result: 实验评估显示，在Xilinx Zynq UltraScale+ MPSoC ZCU102平台上，对于多个CNN模型，RL代理实现的能效平均达到可达最优能效的95%。

Conclusion: DPUConfig框架通过强化学习驱动的动态配置选择，有效解决了FPGA系统中深度学习应用的资源分配问题，显著提升了能效性能。

Abstract: Heterogeneous embedded systems, with diverse computing elements and accelerators such as FPGAs, offer a promising platform for fast and flexible ML inference, which is crucial for services such as autonomous driving and augmented reality, where delays can be costly. However, efficiently allocating computational resources for deep learning applications in FPGA-based systems is a challenging task. A Deep Learning Processor Unit (DPU) is a parameterizable FPGA-based accelerator module optimized for ML inference. It supports a wide range of ML models and can be instantiated multiple times within a single FPGA to enable concurrent execution. This paper introduces DPUConfig, a novel runtime management framework, based on a custom Reinforcement Learning (RL) agent, that dynamically selects optimal DPU configurations by leveraging real-time telemetry data monitoring, system utilization, power consumption, and application performance to inform its configuration selection decisions. The experimental evaluation demonstrates that the RL agent achieves energy efficiency 95% (on average) of the optimal attainable energy efficiency for several CNN models on the Xilinx Zynq UltraScale+ MPSoC ZCU102.

</details>


### [6] [TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design](https://arxiv.org/abs/2602.12962)
*Jonghun Lee,Junghoon Lee,Hyeonjin Kim,Seoho Jeon,Jisup Yoon,Hyunbin Park,Meejeong Park,Heonjae Ha*

Main category: cs.AR

TL;DR: TriGen是一种通过软硬件协同设计为资源受限环境定制的新型NPU架构，针对LLM推理优化，采用低精度计算、LUT替代非线性操作专用硬件，以及调度技术最大化计算利用率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的端侧设备上运行大型语言模型面临挑战：LLM参数量大但参数复用率低，传统NPU架构难以高效执行端到端推理。

Method: 1) 采用微缩放(MX)低精度计算，在保持精度同时提供优化机会；2) 使用快速准确的查找表(LUT)替代非线性操作专用硬件，联合优化线性和非线性操作；3) 考虑实际硬件约束，采用调度技术最大化有限片上内存下的计算利用率。

Result: 在各种LLM上评估显示，TriGen相比基线NPU设计平均实现2.73倍性能加速，内存传输减少52%，精度损失可忽略。

Conclusion: TriGen通过软硬件协同设计有效解决了资源受限环境下LLM推理的挑战，显著提升性能并减少内存开销，为端侧AI推理提供了高效解决方案。

Abstract: Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of parameter reuse compared to conventional CNNs, making end-to-end execution on resource-limited devices extremely challenging. To address these challenges, we propose TriGen, a novel NPU architecture tailored for resource-constrained environments through software-hardware co-design. Firstly, TriGen adopts low-precision computation using microscaling (MX) to enable additional optimization opportunities while preserving accuracy, and resolves the issues that arise by employing such precision. Secondly, to jointly optimize both nonlinear and linear operations, TriGen eliminates the need for specialized hardware for essential nonlinear operations by using fast and accurate LUT, thereby maximizing performance gains and reducing hardware-cost in on-device environments, and finally, by taking practical hardware constraints into account, further employs scheduling techniques to maximize computational utilization even under limited on-chip memory capacity. We evaluate the performance of TriGen on various LLMs and show that TriGen achieves an average 2.73x performance speedup and 52% less memory transfer over the baseline NPU design with negligible accuracy loss.

</details>
