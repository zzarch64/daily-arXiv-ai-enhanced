{"id": "2511.10760", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.10760", "abs": "https://arxiv.org/abs/2511.10760", "authors": ["Emad Haque", "Pragnya Sudershan Nalla", "Jeff Zhang", "Sachin S. Sapatnekar", "Chaitali Chakrabarti", "Yu Cao"], "title": "Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity", "comment": null, "summary": "The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets."}
{"id": "2511.10909", "categories": ["cs.AR", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.10909", "abs": "https://arxiv.org/abs/2511.10909", "authors": ["Peichen Xie", "Yang Wang", "Fan Yang", "Mao Yang"], "title": "MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores", "comment": null, "summary": "The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.\n  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors."}
{"id": "2511.11248", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11248", "abs": "https://arxiv.org/abs/2511.11248", "authors": ["Jianyu Wei", "Qingtao Li", "Shijie Cao", "Lingxiao Ma", "Zixu Hao", "Yanyong Zhang", "Xiaoyan Hu", "Ting Cao"], "title": "T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man."}
