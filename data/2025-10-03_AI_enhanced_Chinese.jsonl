{"id": "2510.01730", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.01730", "abs": "https://arxiv.org/abs/2510.01730", "authors": ["Ashiyana Abdul Majeed", "Mahmoud Meribout", "Safa Mohammed Sali"], "title": "Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI Reconstruction and Analysis", "comment": "11 pages. 14 figures. This work has been submitted to IEEE for\n  possible publication", "summary": "Advancements in AI have greatly enhanced the medical imaging process, making\nit quicker to diagnose patients. However, very few have investigated the\noptimization of a multi-model system with hardware acceleration. As specialized\nedge devices emerge, the efficient use of their accelerators is becoming\nincreasingly crucial. This paper proposes a hardware-accelerated method for\nsimultaneous reconstruction and diagnosis of \\ac{MRI} from \\ac{CT} images.\nReal-time performance of achieving a throughput of nearly 150 frames per second\nwas achieved by leveraging hardware engines available in modern NVIDIA edge\nGPU, along with scheduling techniques. This includes the GPU and the \\ac{DLA}\navailable in both Jetson AGX Xavier and Jetson AGX Orin, which were considered\nin this paper. The hardware allocation of different layers of the multiple AI\nmodels was done in such a way that the ideal time between the hardware engines\nis reduced. In addition, the AI models corresponding to the \\ac{GAN} model were\nfine-tuned in such a way that no fallback execution into the GPU engine is\nrequired without compromising accuracy. Indeed, the accuracy corresponding to\nthe fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of\n5\\%. A further hardware allocation of two fine-tuned GPU-aware GAN models\nproves they can double the performance over the original model, leveraging\nadequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The\nresults prove the effectiveness of employing hardware-aware models in parallel\nfor medical image analysis and diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u52a0\u901f\u7684\u591a\u6a21\u578b\u7cfb\u7edf\uff0c\u7528\u4e8e\u4eceCT\u56fe\u50cf\u540c\u65f6\u8fdb\u884cMRI\u91cd\u5efa\u548c\u8bca\u65ad\uff0c\u5728NVIDIA\u8fb9\u7f18GPU\u4e0a\u5b9e\u73b0\u4e86\u8fd1150\u5e27/\u79d2\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u4e13\u7528\u8fb9\u7f18\u8bbe\u5907\u7684\u51fa\u73b0\uff0c\u9ad8\u6548\u5229\u7528\u5176\u52a0\u901f\u5668\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5f88\u5c11\u6709\u7814\u7a76\u5173\u6ce8\u786c\u4ef6\u52a0\u901f\u7684\u591a\u6a21\u578b\u7cfb\u7edf\u4f18\u5316\u3002", "method": "\u5229\u7528\u73b0\u4ee3NVIDIA\u8fb9\u7f18GPU\u7684\u786c\u4ef6\u5f15\u64ce\u548c\u8c03\u5ea6\u6280\u672f\uff0c\u5bf9\u591a\u4e2aAI\u6a21\u578b\u7684\u4e0d\u540c\u5c42\u8fdb\u884c\u786c\u4ef6\u5206\u914d\u4ee5\u51cf\u5c11\u786c\u4ef6\u5f15\u64ce\u95f4\u7684\u7406\u60f3\u65f6\u95f4\uff0c\u5e76\u5bf9GAN\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u907f\u514d\u56de\u9000\u5230GPU\u5f15\u64ce\u6267\u884c\u3002", "result": "\u5728Jetson AGX Xavier\u548cOrin\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u8fd1150\u5e27/\u79d2\u7684\u541e\u5410\u91cf\uff0c\u5fae\u8c03\u540e\u7684\u8fb9\u7f18GPU\u611f\u77e5AI\u6a21\u578b\u51c6\u786e\u7387\u63d0\u5347\u4e865%\uff0c\u4e24\u4e2a\u5fae\u8c03\u6a21\u578b\u7684\u5e76\u884c\u5206\u914d\u4f7f\u6027\u80fd\u6bd4\u539f\u59cb\u6a21\u578b\u7ffb\u500d\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\u4e86\u5728\u533b\u7597\u56fe\u50cf\u5206\u6790\u548c\u8bca\u65ad\u4e2d\u91c7\u7528\u786c\u4ef6\u611f\u77e5\u5e76\u884c\u6a21\u578b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.02099", "categories": ["cs.AR", "cs.NE", "B.3; B.7; I.4"], "pdf": "https://arxiv.org/pdf/2510.02099", "abs": "https://arxiv.org/abs/2510.02099", "authors": ["Felix Zeller", "John Reuben", "Dietmar Fey"], "title": "Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic", "comment": "9 pages, 10 figures", "summary": "Vector-Matrix Multiplication (VMM) is the fundamental and frequently required\ncomputation in inference of Neural Networks (NN). Due to the large data\nmovement required during inference, VMM can benefit greatly from in-memory\ncomputing. However, ADC/DACs required for in-memory VMM consume significant\npower and area. `Distributed Arithmetic (DA)', a technique in computer\narchitecture prevalent in 1980s was used to achieve inner product or dot\nproduct of two vectors without using a hard-wired multiplier when one of the\nvectors is a constant. In this work, we extend the DA technique to multiply an\ninput vector with a constant matrix. By storing the sum of the weights in\nmemory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM\nmemory. We verify functional and also estimate non-functional properties\n(latency, energy, area) by performing transistor-level simulations. Using\nenergy-efficient sensing and fine grained pipelining, our approach achieves 4.5\nx less latency and 12 x less energy than VMM performed in memory conventionally\nby bit slicing. Furthermore, DA completely eliminated the need for power-hungry\nADCs which are the main source of area and energy consumption in the current\nVMM implementations in memory.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5f0f\u7b97\u6cd5(DA)\u7684\u5411\u91cf\u77e9\u9635\u4e58\u6cd5(VMM)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728ReRAM\u5185\u5b58\u4e2d\u5b58\u50a8\u6743\u91cd\u548c\uff0c\u4f7f\u7528\u79fb\u4f4d\u52a0\u7535\u8def\u5b9e\u73b0VMM\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u529f\u8017\u5927\u7684ADC\u9700\u6c42\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u7684\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4f20\u8f93\uff0c\u5185\u5b58\u8ba1\u7b97\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6027\u80fd\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684ADC/DAC\u6d88\u8017\u5927\u91cf\u529f\u8017\u548c\u9762\u79ef\u3002", "method": "\u6269\u5c55\u5206\u5e03\u5f0f\u7b97\u6cd5(DA)\u6280\u672f\uff0c\u5728ReRAM\u5185\u5b58\u4e2d\u5b58\u50a8\u6743\u91cd\u548c\uff0c\u4f7f\u7528\u79fb\u4f4d\u52a0\u5916\u56f4\u7535\u8def\u5b9e\u73b0\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\uff0c\u907f\u514d\u4e86\u786c\u4ef6\u4e58\u6cd5\u5668\u7684\u4f7f\u7528\u3002", "result": "\u901a\u8fc7\u6676\u4f53\u7ba1\u7ea7\u4eff\u771f\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4f20\u7edf\u4f4d\u5207\u7247\u65b9\u6cd5\uff0c\u5ef6\u8fdf\u51cf\u5c114.5\u500d\uff0c\u80fd\u8017\u51cf\u5c1112\u500d\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u529f\u8017\u5927\u7684ADC\u9700\u6c42\u3002", "conclusion": "\u57fa\u4e8eDA\u7684VMM\u65b9\u6cd5\u5728\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u9762\u79ef\u65b9\u9762\u90fd\u4f18\u4e8e\u4f20\u7edf\u5185\u5b58\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u5e94\u7528\u3002"}}
