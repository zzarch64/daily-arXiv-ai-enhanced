<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [A 16 nm 1.60TOPS/W High Utilization DNN Accelerator with 3D Spatial Data Reuse and Efficient Shared Memory Access](https://arxiv.org/abs/2602.11357)
*Xiaoling Yi,Ryan Antonio,Yunhao Deng,Fanchen Kong,Joren Dumoulin,Jun Yin,Marian Verhelst*

Main category: cs.AR

TL;DR: Voltra芯片采用3D空间数据流和灵活共享内存架构，提升DNN加速器计算利用率，相比传统2D设计空间利用率提升2倍，系统能效达1.60 TOPS/W。


<details>
  <summary>Details</summary>
Motivation: 提高AI工作负载的计算利用率对于通用DNN加速器的效率至关重要，需要解决传统架构在空间数据重用和内存访问效率方面的限制。

Method: 提出Voltra芯片架构，采用3D空间数据流实现三维平衡数据重用，结合灵活数据流处理器支持混合粒度硬件预取和动态内存分配。

Result: 相比传统2D设计空间利用率提升2.0倍，时间利用率提升2.12-2.94倍，总延迟加速1.15-2.36倍，16nm工艺下系统能效1.60 TOPS/W，面积效率1.25 TOPS/mm²。

Conclusion: Voltra芯片通过创新的3D空间数据流和灵活内存架构，在保持与最先进方案竞争力的同时，实现了跨多样化工作负载的高计算利用率。

Abstract: Achieving high compute utilization across a wide range of AI workloads is crucial for the efficiency of versatile DNN accelerators. This paper presents the Voltra chip and its utilization-optimised DNN accelerator architecture, which leverages 3-Dimensional (3D) spatial data reuse along with efficient and flexible shared memory access. The 3D spatial dataflow enables balanced spatial data reuse across three dimensions, improving spatial utilization by up to 2.0x compared to a conventional 2D design. Inside the shared memory access architecture, Voltra incorporates flexible data streamers that enable mixed-grained hardware data pre-fetching and dynamic memory allocation, further improving the temporal utilization by 2.12-2.94x and achieving 1.15-2.36x total latency speedup compared with the non-prefetching and separated memory architecture, respectively. Fabricated in 16nm technology, our chip achieves 1.60 TOPS/W peak system energy efficiency and 1.25 TOPS/mm2 system area efficiency, which is competitive with state-of-the-art solutions while achieving high utilization across diverse workloads.

</details>


### [2] [EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement & Routing for RF Circuits](https://arxiv.org/abs/2602.11461)
*Yilun Huang,Asal Mehradfar,Salman Avestimehr,Hamidreza Aghasi*

Main category: cs.AR

TL;DR: 提出基于机器学习的射频物理合成框架，可将电路网表转换为可制造的GDSII版图，解决了现有ML方法无法生成可制造版图的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在射频电路设计中虽在拓扑选择和参数优化方面取得成功，但无法生成可制造的版图，主要原因是组件模型过于简化且缺乏布线能力。

Method: 通过三个关键技术：1）基于18,210个电感几何结构的神经网络框架，预测Q因子误差小于2%；2）智能P-Cell优化器减少版图面积并保持DRC合规；3）完整的布局布线引擎，支持频率相关的电磁间距规则和DRC感知合成。

Result: 神经网络电感模型在1-100 GHz范围内表现出卓越精度，实现实时电磁精确组件合成；框架成功生成DRC感知的GDSII版图，高Q布局成功率达93.77%。

Conclusion: 该框架代表了向自动化射频物理设计迈出的重要一步，能够生成可制造的射频电路版图，解决了现有ML方法的局限性。

Abstract: This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to oversimplified component models and lack of routing capabilities. Our framework addresses these limitations through three key innovations: (1) a neural network framework trained on 18,210 inductor geometries with frequency sweeps from 1-100 GHz, generating 7.5 million training samples, that predicts inductor Q-factor with less than 2% error and enables fast gradient-based layout optimization with a 93.77% success rate in producing high-Q layouts; (2) an intelligent P-Cell optimizer that reduces layout area while maintaining design-rule-check (DRC) compliance; and (3) a complete placement and routing engine with frequency-dependent EM spacing rules and DRC-aware synthesis. The neural inductor model demonstrates superior accuracy across 1-100 GHz, enabling EM-accurate component synthesis with real-time inference. The framework successfully generates DRC-aware GDSII layouts for RF circuits, representing a significant step toward automated RF physical design.

</details>


### [3] [PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System](https://arxiv.org/abs/2602.11521)
*Lian Liu,Shixin Zhao,Yutian Zhou,Yintao He,Mengdi Wang,Yinhe Han,Ying Wang*

Main category: cs.AR

TL;DR: PAM：一种基于异构内存层次结构的KV中心化LLM服务系统，通过协调PIM内存设备解决KV相关操作的内存带宽和容量瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，KV相关操作（注意力计算和KV缓存存储）已成为关键瓶颈，需要大量内存带宽和容量。现有LLM服务系统针对计算密集型负载优化，无法有效处理这些内存密集型操作，即使使用PIM技术，单级内存设计也无法同时满足带宽和容量需求。

Method: 1. 利用KV访问模式中的上下文局部性，在内存层次结构中智能分配KV令牌；2. 提出PAMattention算法，在异构PIM设备间实现细粒度并行注意力计算；3. 包含设备内KV映射、设备间KV迁移接口和在线KV调度算法，动态平衡计算负载。

Result: PAM通过同时满足带宽和容量需求，显著提升了LLM服务系统的效率和可扩展性，为大规模AI时代提供了经济高效的高性能解决方案。

Conclusion: PAM系统通过协调异构PIM内存设备的层次架构，解决了LLM服务中KV相关操作的内存瓶颈问题，为大规模AI应用提供了有效的服务系统解决方案。

Abstract: The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.
  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.

</details>


### [4] [Benchmarking for Single Feature Attribution with Microarchitecture Cliffs](https://arxiv.org/abs/2602.11580)
*Hao Zhen,Qingxuan Kang,Yungang Bao,Trevor E. Carlson*

Main category: cs.AR

TL;DR: 提出Microarchitecture Cliffs基准生成方法，用于暴露模拟器与RTL之间的微架构行为差异，通过针对性校准将XS-GEM5模拟器的性能误差从59.2%降低到1.4%。


<details>
  <summary>Details</summary>
Motivation: 架构模拟器在早期微架构探索中至关重要，但其有效性常受限于保真度问题：模拟器可能与最终RTL的行为存在偏差，导致性能估计不可靠。因此，需要模型校准来对齐模拟器与RTL的行为。

Method: 提出Cliff基准生成方法：1)识别需要校准的关键架构组件；2)通过一组基准精确归因微架构差异到单个微架构特性；3)开发自动化工具提升Cliff工作流程效率。应用于XS-GEM5模拟器与XS-RTL CPU的校准。

Result: 1) 在Cliff基准上，XS-GEM5性能误差从59.2%降至1.4%；2) 代表性紧密耦合微架构特性的相对误差降低48.03%；3) SPECint2017和SPECfp2017的绝对性能误差分别降低15.1%和21.0%。

Conclusion: Microarchitecture Cliffs方法能有效暴露模拟器与RTL之间的微架构行为差异，通过针对性校准显著提升模拟器保真度，为架构模拟器的准确性能建模提供有效工具。

Abstract: Architectural simulators play a critical role in early microarchitectural exploration due to their flexibility and high productivity. However, their effectiveness is often constrained by fidelity: simulators may deviate from the behavior of the final RTL, leading to unreliable performance estimates. Consequently, model calibration, which aligns simulator behavior with the RTL as the ground-truth microarchitecture, becomes essential for achieving accurate performance modeling.
  To facilitate model calibration accuracy, we propose Microarchitecture Cliffs, a benchmark generation methodology designed to expose mismatches in microarchitectural behavior between the simulator and RTL. After identifying the key architectural components that require calibration, the Cliff methodology enables precise attribution of microarchitectural differences to a single microarchitectural feature through a set of benchmarks. In addition, we develop a set of automated tools to improve the efficiency of the Cliff workflow.
  We apply the Cliff methodology to calibrate the XiangShan version of gem5 (XS-GEM5) against the XiangShan open-source CPU (XS-RTL). We reduce the performance error of XS-GEM5 from 59.2% to just 1.4% on the Cliff benchmarks. Meanwhile, the calibration guided by Cliffs effectively reduces the relative error of a representative tightly coupled microarchitectural feature by 48.03%. It also substantially lowers the absolute performance error, with reductions of 15.1% and 21.0% on SPECint2017 and SPECfp2017, respectively.

</details>


### [5] [Device-Circuit Co-Design of Variation-Resilient Read and Write Drivers for Antiferromagnetic Tunnel Junction (AFMTJ) Memories](https://arxiv.org/abs/2602.11614)
*Yousuf Choudhary,Tosiron Adegbija*

Main category: cs.AR

TL;DR: 开发了针对反铁磁隧道结（AFMTJ）优化的器件-电路协同设计读写接口，解决了AFMTJ超快动态和低隧穿磁阻（TMR）带来的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: AFMTJ具有皮秒级开关速度和高度集成密度，适用于内存计算，但其超快动态和低TMR使得现有MRAM接口不可靠，需要专门优化的读写接口。

Method: 使用校准的SPICE AFMTJ模型作为基准，分析传统驱动器的局限性，提出不对称脉冲驱动器（PD）实现确定性皮秒开关，以及具有动态触发点调谐的自定时感测放大器（STSA）用于低TMR检测。

Result: SPICE和蒙特卡洛评估实验表明，所提出的电路在保持AFMTJ延迟和能量优势的同时，在现实的PVT和3D集成寄生参数下实现了稳健的读写良率，优于相同条件下的标准MRAM前端。

Conclusion: 通过器件-电路协同设计方法开发的专用读写接口成功解决了AFMTJ的可靠性问题，为AFMTJ在内存计算应用中的实际部署提供了可行的解决方案。

Abstract: Antiferromagnetic Tunnel Junctions (AFMTJs) offer picosecond switching and high integration density for in-memory computing, but their ultrafast dynamics and low tunnel magnetoresistance (TMR) make state-of-the-art MRAM interfaces unreliable. This work develops a device-circuit co-designed read/write interface optimized for AFMTJ behavior. Using a calibrated SPICE AFMTJ model as a baseline, we identify the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. Our experiments using SPICE and Monte Carlo evaluations demonstrate that the proposed circuits preserve AFMTJ latency and energy benefits while achieving robust read/write yield under realistic PVT and 3D integration parasitics, outperforming standard MRAM front-ends under the same conditions.

</details>


### [6] [MING: An Automated CNN-to-Edge MLIR HLS framework](https://arxiv.org/abs/2602.11966)
*Jiahong Bi,Lars Schütze,Jeronimo Castrillon*

Main category: cs.AR

TL;DR: MING是一个基于MLIR的FPGA HLS框架，专门针对边缘设备的资源约束进行优化，采用流式架构和缓冲区管理，相比现有框架在CNN内核上实现15-200倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用向边缘计算迁移，FPGA因其能效优势被广泛采用。现有基于MLIR的HLS框架虽然易于使用，但往往忽视了边缘设备严格的资源约束，需要专门针对资源受限环境的设计方案。

Method: 提出MING框架，基于MLIR抽象和自动化HLS设计流程。采用流式架构配合精心管理的缓冲区，专门针对资源约束进行设计，同时确保低延迟处理。

Result: 与现有框架相比，MING在标准CNN内核（最多四层）上平均实现15倍加速，单层内核最高可达200倍加速。对于大输入尺寸的内核，MING能够生成符合硬件资源约束的高效设计，而现有框架难以满足这些约束。

Conclusion: MING框架成功解决了边缘设备资源约束下的FPGA设计挑战，通过MLIR-based流式架构实现了显著的性能提升，特别适合资源受限的边缘计算场景。

Abstract: Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use. However, existing state-of-the-art frameworks often overlook the stringent resource constraints of edge devices. To address this limitation, we propose MING, an Multi-Level Intermediate Representation (MLIR)-based framework that abstracts and automates the HLS design process. Within this framework, we adopt a streaming architecture with carefully managed buffers, specifically designed to handle resource constraints while ensuring low-latency. In comparison with recent frameworks, our approach achieves on average 15x speedup for standard Convolutional Neural Network (CNN) kernels with up to four layers, and up to 200x for single-layer kernels. For kernels with larger input sizes, MING is capable of generating efficient designs that respect hardware resource constraints, whereas state-of-the-art frameworks struggle to meet.

</details>
