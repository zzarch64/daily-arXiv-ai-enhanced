<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving](https://arxiv.org/abs/2510.05245)
*Yue Pan,Zihan Xia,Po-Kai Hsu,Lanxiang Hu,Hyungyo Kim,Janak Sharda,Minxuan Zhou,Nam Sung Kim,Shimeng Yu,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: Stratum是一个系统-硬件协同设计方法，结合了单片3D堆叠DRAM、近内存处理和GPU加速，专门针对MoE模型推理的硬件部署挑战，实现了高达8.29倍的解码吞吐量提升和7.66倍的能效改进。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过稀疏门控实现了亿级参数容量和较小推理成本，但其MoE层引入的大数据量给硬件部署带来了挑战，需要解决内存带宽和处理效率问题。

Method: 采用单片3D堆叠DRAM提供比HBM更高的内部带宽，结合近内存处理和GPU加速；通过混合键合连接逻辑和DRAM芯片，硅中介层连接DRAM堆栈和GPU；构建内部内存层级并根据访问可能性分配数据，利用基于主题的专家使用预测来提升NMP吞吐量。

Result: Stratum系统在各种基准测试中相比GPU基线实现了高达8.29倍的解码吞吐量提升和7.66倍的能效改进。

Conclusion: Stratum通过系统-硬件协同设计成功解决了MoE模型部署中的内存带宽和处理效率瓶颈，为大规模MoE模型的硬件加速提供了有效解决方案。

Abstract: As Large Language Models (LLMs) continue to evolve, Mixture of Experts (MoE)
architecture has emerged as a prevailing design for achieving state-of-the-art
performance across a wide range of tasks. MoE models use sparse gating to
activate only a handful of expert sub-networks per input, achieving
billion-parameter capacity with inference costs akin to much smaller models.
However, such models often pose challenges for hardware deployment due to the
massive data volume introduced by the MoE layers. To address the challenges of
serving MoE models, we propose Stratum, a system-hardware co-design approach
that combines the novel memory technology Monolithic 3D-Stackable DRAM (Mono3D
DRAM), near-memory processing (NMP), and GPU acceleration. The logic and Mono3D
DRAM dies are connected through hybrid bonding, whereas the Mono3D DRAM stack
and GPU are interconnected via silicon interposer. Mono3D DRAM offers higher
internal bandwidth than HBM thanks to the dense vertical interconnect pitch
enabled by its monolithic structure, which supports implementations of
higher-performance near-memory processing. Furthermore, we tackle the latency
differences introduced by aggressive vertical scaling of Mono3D DRAM along the
z-dimension by constructing internal memory tiers and assigning data across
layers based on access likelihood, guided by topic-based expert usage
prediction to boost NMP throughput. The Stratum system achieves up to 8.29x
improvement in decoding throughput and 7.66x better energy efficiency across
various benchmarks compared to GPU baselines.

</details>


### [2] [DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base](https://arxiv.org/abs/2510.05327)
*Zahin Ibnat,Paul E. Calzada,Rasin Mohammed Ihtemam,Sujan Kumar Saha,Jingbo Zhou,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: DeepV是一个模型无关的RAG框架，用于生成RTL设计，通过高质量数据集增强上下文，无需RTL特定训练，在VerilogEval基准测试中性能提升近17%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在硬件设计自动化中存在不足：无法整合新型IP到知识库，导致代码生成质量差；基于旧模型的微调方法无法与通用LLM竞争；现有RAG技术使用低质量代码库或计算成本高。

Method: 提出DeepV框架，采用模型无关的检索增强生成(RAG)方法，利用大规模高质量数据集增强上下文，无需RTL特定训练，直接应用于RTL生成步骤。

Result: 在VerilogEval基准测试中，使用最新商业LLM OpenAI GPT-5时性能提升近17%，框架已在Hugging Face上开源。

Conclusion: DeepV框架有效解决了现有方法在RTL代码生成中的局限性，通过高质量RAG方法显著提升了生成性能，为硬件设计自动化提供了实用解决方案。

Abstract: As large language models (LLMs) continue to be integrated into modern
technology, there has been an increased push towards code generation
applications, which also naturally extends to hardware design automation.
LLM-based solutions for register transfer level (RTL) code generation for
intellectual property (IP) designs have grown, especially with fine-tuned LLMs,
prompt engineering, and agentic approaches becoming popular in literature.
However, a gap has been exposed in these techniques, as they fail to integrate
novel IPs into the model's knowledge base, subsequently resulting in poorly
generated code. Additionally, as general-purpose LLMs continue to improve,
fine-tuned methods on older models will not be able to compete to produce more
accurate and efficient designs. Although some retrieval augmented generation
(RAG) techniques exist to mitigate challenges presented in fine-tuning
approaches, works tend to leverage low-quality codebases, incorporate
computationally expensive fine-tuning in the frameworks, or do not use RAG
directly in the RTL generation step. In this work, we introduce DeepV: a
model-agnostic RAG framework to generate RTL designs by enhancing context
through a large, high-quality dataset without any RTL-specific training. Our
framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17%
increase in performance on the VerilogEval benchmark. We host DeepV for use by
the community in a Hugging Face (HF) Space:
https://huggingface.co/spaces/FICS-LLM/DeepV.

</details>


### [3] [From Principles to Practice: A Systematic Study of LLM Serving on Multi-core NPUs](https://arxiv.org/abs/2510.05632)
*Tianhao Zhu,Dahu Feng,Erhu Feng,Yubin Xia*

Main category: cs.AR

TL;DR: 提出针对多核NPU的LLM推理优化方案，包括张量并行策略、核心放置策略和内存管理方法，相比现有设计可获得1.32-6.03倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛应用，对高性能推理服务的需求增长。现有AI加速器多采用多核架构但缺乏SIMT架构的灵活性，容易导致计算资源利用不足和推理性能不佳。

Method: 开发了多级仿真框架（事务级和性能模型仿真），系统分析并提出了张量并行策略、核心放置策略、内存管理方法以及PD解聚与融合选择的最优解。

Result: 在代表性LLM和多种NPU配置上的实验表明，相比最先进的多核NPU设计，解决方案可获得1.32-6.03倍的加速。

Conclusion: 为多核NPU在各种LLM工作负载下设计最优硬件架构和服务策略提供了指导。

Abstract: With the widespread adoption of Large Language Models (LLMs), the demand for
high-performance LLM inference services continues to grow. To meet this demand,
a growing number of AI accelerators have been proposed, such as Google TPU,
Huawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators
adopt multi-core architectures to achieve enhanced scalability, but lack the
flexibility of SIMT architectures. Therefore, without careful configuration of
the hardware architecture, as well as deliberate design of tensor parallelism
and core placement strategies, computational resources may be underutilized,
resulting in suboptimal inference performance.
  To address these challenges, we first present a multi-level simulation
framework with both transaction-level and performance-model-based simulation
for multi-core NPUs. Using this simulator, we conduct a systematic analysis and
further propose the optimal solutions for tensor parallelism strategies, core
placement policies, memory management methods, as well as the selection between
PD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive
experiments on representative LLMs and various NPU configurations. The
evaluation results demonstrate that, our solution can achieve 1.32x-6.03x
speedup compared to SOTA designs for multi-core NPUs across different hardware
configurations. As for LLM serving, our work offers guidance on designing
optimal hardware architectures and serving strategies for multi-core NPUs
across various LLM workloads.

</details>


### [4] [An opportunity to improve Data Center Efficiency: Optimizing the Server's Upgrade Cycle](https://arxiv.org/abs/2510.05787)
*Panagiota Nikolaou,Freddy Gabbay,Jawad Haj-Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 通过优化服务器升级时机来提升数据中心效率，研究发现全局升级计划比局部升级计划表现更好


<details>
  <summary>Details</summary>
Motivation: 提高数据中心的效率，通过确定替换旧服务器的最优时机来优化服务器升级计划

Method: 基于历史服务器数据的研究，利用服务器进入年份、性能和功耗等信息，制定覆盖整个生命周期的全局升级计划

Result: 全局升级计划可能涉及非固定时间段的升级，在QPS/(TCOxCO2)指标上表现优于局部升级计划

Conclusion: 在数据中心设计阶段制定全局升级计划能显著提升效率，考虑未来服务器发布信息的全局计划优于仅基于当前可用服务器的局部计划

Abstract: This work aims to improve a data center's efficiency by optimizing the server
upgrade plan: determine the optimal timing for replacing old servers with new
ones. The opportunity presented by this approach is demonstrated through a
study based on historical server data. The study establishes a significant
opportunity to increase the QPS/(TCOxCO2) metric by formulating a global
upgrade plan at the data center's design time covering its entire life cycle.
This plan leverages information, such as server entry year, performance, and
active power consumption for both existing and future servers. Our findings
reveal that an optimal global upgrade plan, may involve upgrades at non fixed
time periods and outperforms local upgrade plans. Local upgrade plans follow a
fixed, equal-length cycle and make decisions based only on currently available
server models. These local plans select the best available server at each
upgrade cycle without accounting for future server releases.

</details>
