<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations](https://arxiv.org/abs/2509.14388)
*Lennart Bamberg,Filippo Minnella,Roberto Bosio,Fabrizio Ottati,Yuebin Wang,Jongmin Lee,Luciano Lavagno,Adam Fuks*

Main category: cs.AR

TL;DR: 本文介绍了eIQ Neutron高效NPU架构，通过数据驱动设计和编译器优化，在相同TOPS和内存资源下比领先嵌入式NPU性能提升1.8倍，甚至在面对双倍计算资源的NPU时仍能实现3.3倍性能优势。


<details>
  <summary>Details</summary>
Motivation: 传统NPU的峰值TOPS指标不能真实反映实际性能，且通常与更高的硅成本相关。需要在保持灵活性的同时最大化计算利用率。

Method: 采用灵活的、数据驱动的架构设计，配合编译器使用约束编程方法，根据工作负载特性优化计算和数据移动。

Result: 在标准AI基准测试中，相比领先的嵌入式NPU和编译器堆栈，在相同TOPS和内存资源下实现平均1.8倍加速（峰值4倍）。即使面对计算和内存资源翻倍的NPU，仍能提供最高3.3倍的性能提升。

Conclusion: eIQ Neutron NPU通过架构和编译器的协同设计，在保持灵活性的同时显著提升了计算效率，证明了在边缘AI推理中优化实际性能而非峰值指标的重要性。

Abstract: Neural Processing Units (NPUs) are key to enabling efficient AI inference in
resource-constrained edge environments. While peak tera operations per second
(TOPS) is often used to gauge performance, it poorly reflects real-world
performance and typically rather correlates with higher silicon cost. To
address this, architects must focus on maximizing compute utilization, without
sacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU,
integrated into a commercial flagship MPU, alongside co-designed compiler
algorithms. The architecture employs a flexible, data-driven design, while the
compiler uses a constrained programming approach to optimize compute and data
movement based on workload characteristics. Compared to the leading embedded
NPU and compiler stack, our solution achieves an average speedup of 1.8x (4x
peak) at equal TOPS and memory resources across standard AI-benchmarks. Even
against NPUs with double the compute and memory resources, Neutron delivers up
to 3.3x higher performance.

</details>


### [2] [Shift-Left Techniques in Electronic Design Automation: A Survey](https://arxiv.org/abs/2509.14551)
*Xinyue Wu,Zixuan Li,Fan Hu,Ting Lin,Xiaotian Zhao,Runxi Wang,Xinfei Guo*

Main category: cs.AR

TL;DR: 本文对EDA领域中的Shift-Left方法进行了全面调查，分析了现有和新兴的研究范式，重点关注AI技术和开源设计流程如何增强预测建模能力，推动传统串行设计过程向虚拟设计环境的转变。


<details>
  <summary>Details</summary>
Motivation: 随着芯片设计复杂度增加，传统串行设计流程效率低下。Shift-Left方法通过创建数字孪生和融合多个设计步骤，使设计师能够更早建立强相关性并优化设计，但准确复制下游行为和确定采用时机仍存在挑战。

Method: 采用文献调查方法，系统梳理EDA领域中Shift-Left相关研究，包括AI技术的应用、开源设计流程的发展，以及数据驱动方法在EDA社区中的重要性。

Result: 研究发现AI技术和开源设计流程显著增强了预测和建模能力，数据驱动方法在EDA社区中变得越来越相关，这反过来增强了当前工具中的Shift-Left功能。

Conclusion: Shift-Left方法在EDA领域取得了重要进展，但仍面临挑战。未来需要继续发展智能EDA工具和技术，推动设计生态系统向更高效、更集成的方向发展。

Abstract: The chip design process involves numerous steps, beginning with defining
product requirements and progressing through architectural planning,
system-level design, and the physical layout of individual circuit blocks. As
the enablers of large-scale chip development, Electronic Design Automation
(EDA) tools play a vital role in helping designers achieve high-quality
results. The Shift-Left methodology introduces a pathway toward creating
digital twins and fusing multiple design steps, thereby transitioning
traditionally sequential, physically-aware processes into virtual design
environments. This shift allows designers to establish stronger correlations
earlier and optimize designs more effectively. However, challenges remain,
especially in accurately replicating downstream behaviors and determining the
right scope and timing for adoption. These challenges, in turn, have revealed
new opportunities for EDA vendors, physical designers, and logic designers
alike. As the industry advances toward intelligent EDA tools and techniques, it
is timely to reflect on Shift-Left progress made and the challenges that
remain. The rise of AI techniques and the momentum of open-source design flows
have significantly strengthened prediction and modeling capabilities, making
data-driven methods increasingly relevant to the EDA community. This, in turn,
enhances the ''Shift-Left'' features embedded in current tools. In this paper,
we present a comprehensive survey of existing and emerging paradigms in
Shift-Left research within EDA and the broader design ecosystem. Our goal is to
provide a unique perspective on the state of the field and its future
directions. Relevant papers mentioned are organized in
https://github.com/iCAS-SJTU/Shift-Left-EDA-Papers.

</details>


### [3] [DeepAssert: An LLM-Aided Verification Framework with Fine-Grained Assertion Generation for Modules with Extracted Module Specifications](https://arxiv.org/abs/2509.14668)
*Yonghao Wang,Jiaxin Zhou,Hongqin Lyu,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: DeepAssert是一个基于LLM的验证框架，能够分析模块间的调用关系并提取模块级规范，自动生成细粒度的深度断言，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有断言生成方法只能针对顶层设计或依赖黄金RTL模型的局限性，无法生成针对模块内部功能的深度断言的问题。

Method: 通过分析模块间调用关系，提取各模块的I/O端口信息和独立规范，利用LLM自动生成细粒度的深度断言。

Result: DeepAssert在生成高质量深度断言方面显著优于AssertLLM和Spec2Assertion等方法，并能提升这些方法的整体断言质量。

Conclusion: 该框架能够实现更全面有效的验证过程，为模块级验证提供了新的解决方案。

Abstract: Assertion-Based Verification (ABV) is a crucial method for ensuring that
logic designs conform to their architectural specifications. However, existing
assertion generation methods primarily rely on information either from the
design specification, or register-transfer level (RTL) code. The former methods
are typically limited to generating assertions for the top-level design. As the
top-level design is composed of different modules without module-level
specifications, they are unable to generate deep assertions that target the
internal functionality of modules. The latter methods often rely on a golden
RTL model, which is difficult to obtain. To address the above limitations, this
paper presents a novel large language model (LLM)-aided verification framework
named DeepAssert. DeepAssert is capable of analyzing the invocation
relationships between modules and extracting independent specifications for
each module with its I/O port information. These extracted specifications are
subsequently used to guide LLMs to automatically generate fine-grained deep
assertions for these modules. Our evaluation demonstrates that DeepAssert
significantly outperforms existing methods such as AssertLLM and Spec2Assertion
in generating high-quality deep assertions for modules. Furthermore, when
integrated with these methods, DeepAssert can enhance the overall quality of
the assertions generated. This allows for a more comprehensive and effective
verification process.

</details>


### [4] [LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism](https://arxiv.org/abs/2509.14781)
*Yimin Wang,Yue Jiet Chong,Xuanyao Fong*

Main category: cs.AR

TL;DR: LEAP是一种非冯·诺依曼架构的LLM推理加速器，通过内存计算(PIM)和片上计算网络(NoC)的协同设计，在Llama模型上相比A100 GPU实现了2.55倍吞吐量提升和71.94倍能效提升


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理需求日益增长，但其巨大的张量规模和计算复杂度给内存、计算和数据总线带来了挑战，需要新的硬件架构来解决这些问题

Method: 提出计算/内存/通信协同设计的非冯·诺依曼加速器，结合PIM和计算NoC，根据数据动态性将矩阵乘法分配到PIM或NoC，通过启发式设计空间探索优化模型划分和映射，采用细粒度并行和分块技术实现高吞吐数据流

Result: 在Llama 1B/8B/13B模型上评估，相比A100 GPU实现了约2.55倍的吞吐量(tokens/sec)提升和约71.94倍的能效(tokens/Joule)提升

Conclusion: LEAP架构通过计算/内存/通信协同设计，有效解决了LLM推理中的内存和计算瓶颈，显著提升了吞吐量和能效，为LLM硬件加速提供了有前景的解决方案

Abstract: Large language model (LLM) inference has been a prevalent demand in daily
life and industries. The large tensor sizes and computing complexities in LLMs
have brought challenges to memory, computing, and databus. This paper proposes
a computation/memory/communication co-designed non-von Neumann accelerator by
aggregating processing-in-memory (PIM) and computational network-on-chip (NoC),
termed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC
based on the data dynamicity to maximize data locality. Model partition and
mapping are optimized by heuristic design space exploration. Dedicated
fine-grained parallelism and tiling techniques enable high-throughput dataflow
across the distributed resources in PIM and NoC. The architecture is evaluated
on Llama 1B/8B/13B models and shows $\sim$2.55$\times$ throughput (tokens/sec)
improvement and $\sim$71.94$\times$ energy efficiency (tokens/Joule) boost
compared to the A100 GPU.

</details>


### [5] [NEURAL: An Elastic Neuromorphic Architecture with Hybrid Data-Event Execution and On-the-fly Attention Dataflow](https://arxiv.org/abs/2509.15036)
*Yuehai Chen,Farhad Merchant*

Main category: cs.AR

TL;DR: NEURAL是一种新型神经形态架构，采用混合数据-事件执行范式，通过解耦稀疏感知处理和神经元计算，结合弹性FIFO和W2TTFS机制，实现了高效的脉冲神经网络加速，在准确率和能效方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有SNN硬件实现存在脉冲稀疏性和多时间步执行的固有问题，导致延迟增加和能效降低，需要新的架构设计来解决这些挑战。

Method: 提出NEURAL架构：1）混合数据-事件执行范式，解耦稀疏感知处理与神经元计算；2）弹性FIFO支持；3）W2TTFS机制替代平均池化；4）基于知识蒸馏的训练框架构建单时间步SNN模型。

Result: 在CIFAR-10上准确率提升3.20%，CIFAR-100上提升5.13%；与现有SNN加速器相比，资源利用率降低50%，能效提升1.97倍。

Conclusion: NEURAL架构通过创新的混合执行范式和硬件设计，有效解决了SNN的稀疏性和延迟问题，在保持高准确率的同时显著提升了能效和资源效率。

Abstract: Spiking neural networks (SNNs) have emerged as a promising alternative to
artificial neural networks (ANNs), offering improved energy efficiency by
leveraging sparse and event-driven computation. However, existing hardware
implementations of SNNs still suffer from the inherent spike sparsity and
multi-timestep execution, which significantly increase latency and reduce
energy efficiency. This study presents NEURAL, a novel neuromorphic
architecture based on a hybrid data-event execution paradigm by decoupling
sparsity-aware processing from neuron computation and using elastic
first-in-first-out (FIFO). NEURAL supports on-the-fly execution of spiking
QKFormer by embedding its operations within the baseline computing flow without
requiring dedicated hardware units. It also integrates a novel
window-to-time-to-first-spike (W2TTFS) mechanism to replace average pooling and
enable full-spike execution. Furthermore, we introduce a knowledge distillation
(KD)-based training framework to construct single-timestep SNN models with
competitive accuracy. NEURAL is implemented on a Xilinx Virtex-7 FPGA and
evaluated using ResNet-11, QKFResNet-11, and VGG-11. Experimental results
demonstrate that, at the algorithm level, the VGG-11 model trained with KD
improves accuracy by 3.20% on CIFAR-10 and 5.13% on CIFAR-100. At the
architecture level, compared to existing SNN accelerators, NEURAL achieves a
50% reduction in resource utilization and a 1.97x improvement in energy
efficiency.

</details>


### [6] [Voyager: An End-to-End Framework for Design-Space Exploration and Generation of DNN Accelerators](https://arxiv.org/abs/2509.15205)
*Kartik Prabhu,Jeffrey Yu,Xinyuan Allen Pan,Zhouhua Xie,Abigail Aleshire,Zihan Chen,Ammar Ali Ratnani,Priyanka Raina*

Main category: cs.AR

TL;DR: Voyager是一个基于高层次综合(HLS)的框架，用于深度神经网络加速器的设计空间探索和自动生成，解决了现有方法在参数化、高性能设计生成、数据类型支持和端到端编译器集成方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络需要硬件加速器来高效运行，但手动设计加速器耗时耗力且难以扩展。现有自动化方法存在参数化有限、无法生成高性能设计、数据类型支持不足以及缺乏端到端编译器等问题。

Method: 基于高层次综合(HLS)框架，提供跨技术节点、时钟频率和规模的可配置性，支持多种数据类型和量化方案，包括浮点数、posit和整数格式，以及用户自定义格式。集成PyTorch编译器进行端到端网络映射。

Result: 在先进视觉和语言模型上评估，Voyager实现高达99.8%的利用率，相比先前生成器延迟降低61%、面积减少56%，与手工优化加速器性能相当但自动化程度更高。

Conclusion: Voyager框架能够高效自动化生成高性能DNN加速器，支持快速设计空间探索和完整数据集精度评估，在保持高性能的同时显著提高了设计自动化水平。

Abstract: While deep neural networks (DNNs) have achieved state-of-the-art performance
in fields from computer vision to natural language processing, efficiently
running these computationally demanding models requires hardware accelerators.
However, designing these accelerators is a time-consuming, labor-intensive
process that does not scale well. While prior efforts have sought to automate
DNN accelerator generation, they offer limited parameterization, cannot produce
high-performance, tapeout-ready designs, provide limited support for datatypes
and quantization schemes, and lack an integrated, end-to-end software compiler.
This work proposes Voyager, a high-level synthesis (HLS)-based framework for
design space exploration (DSE) and generation of DNN accelerators. Voyager
overcomes the limitations of prior work by offering extensive configurability
across technology nodes, clock frequencies, and scales, with customizable
parameters such as number of processing elements, on-chip buffer sizes, and
external memory bandwidth. Voyager supports a wider variety of datatypes and
quantization schemes versus prior work, including both built-in floating-point,
posit and integer formats, as well as user-defined formats with both per-tensor
scaling and microscaling quantization. Voyager's PyTorch-based compiler
efficiently maps networks end-to-end on the generated hardware, with support
for quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art
vision and language models. Voyager enables fast DSE with full-dataset accuracy
evaluation for datatypes and quantization schemes. Generated designs achieve a
high utilization across models and scales, up to 99.8%, and outperform prior
generators with up to 61% lower latency and 56% lower area. Compared to
hand-optimized accelerators, Voyager achieves comparable performance, while
offering much greater automation in design and workload mapping.

</details>
