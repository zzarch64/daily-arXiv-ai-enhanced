{"id": "2511.11895", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11895", "abs": "https://arxiv.org/abs/2511.11895", "authors": ["Thorben Schey", "Khaled Karoonlatifi", "Michael Weyrich", "Andrey Morozov"], "title": "Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing", "comment": "9 pages, 8 figures. this is the preprint version of the paper accepted for publication at ICCAD 2025", "summary": "This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments."}
{"id": "2511.11917", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11917", "abs": "https://arxiv.org/abs/2511.11917", "authors": ["Thorben Schey", "Khaled Karoonlatifi", "Michael Weyrich", "Andrey Morozov"], "title": "Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing", "comment": "6 pages, 5 figures, this is the preprint version of the paper accepted for publication at ATS 2025", "summary": "This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing."}
{"id": "2511.12035", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12035", "abs": "https://arxiv.org/abs/2511.12035", "authors": ["Wenxuan Miao", "Yulin Sun", "Aiyue Chen", "Jing Lin", "Yiwu Yao", "Yiming Gan", "Jieru Zhao", "Jingwen Leng", "Mingyi Guo", "Yu Feng"], "title": "TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space", "comment": null, "summary": "The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.\n  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\\% loss on VBench)."}
{"id": "2511.12152", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12152", "abs": "https://arxiv.org/abs/2511.12152", "authors": ["Jianyi Yu", "Yuxuan Wang", "Xiang Fu", "Fei Qiao", "Ying Wang", "Rui Yuan", "Liyuan Liu", "Cong Shi"], "title": "A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation", "comment": null, "summary": "Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications."}
{"id": "2511.12286", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12286", "abs": "https://arxiv.org/abs/2511.12286", "authors": ["Khyati Kiyawat", "Zhenxing Fan", "Yasas Seneviratne", "Morteza Baradaran", "Akhil Shekar", "Zihan Xia", "Mingu Kang", "Kevin Skadron"], "title": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing", "comment": null, "summary": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively."}
{"id": "2511.12349", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12349", "abs": "https://arxiv.org/abs/2511.12349", "authors": ["Divya Kiran Kadiyala", "Alexandros Daglis"], "title": "Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting", "comment": null, "summary": "The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.\n  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x."}
{"id": "2511.12544", "categories": ["cs.AR", "cs.ET", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12544", "abs": "https://arxiv.org/abs/2511.12544", "authors": ["Mukul Lokhande", "Akash Sankhe", "S. V. Jaya Chand", "Santosh Kumar Vishvakarma"], "title": "FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration", "comment": null, "summary": "The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads."}
{"id": "2511.12616", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12616", "abs": "https://arxiv.org/abs/2511.12616", "authors": ["Arya Parameshwara"], "title": "SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration", "comment": "10 pages, 7 figures, conference-style formatting", "summary": "This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures."}
{"id": "2511.12860", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12860", "abs": "https://arxiv.org/abs/2511.12860", "authors": ["Yongjoo Jang", "Sangwoo Hwang", "Hojin Lee", "Sangwoo Jung", "Donghun Lee", "Wonbo Shim", "Jaeha Kung"], "title": "Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs", "comment": "This paper is accepted in the 43rd IEEE International Conference on Computer Design (ICCD), 2025", "summary": "The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead."}
{"id": "2511.12930", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12930", "abs": "https://arxiv.org/abs/2511.12930", "authors": ["Changhun Oh", "Seongryong Oh", "Jinwoo Hwang", "Yoonsung Kim", "Hardik Sharma", "Jongse Park"], "title": "Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration", "comment": null, "summary": "3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical."}
{"id": "2511.13139", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13139", "abs": "https://arxiv.org/abs/2511.13139", "authors": ["Zhiteng Chao", "Yonghao Wang", "Xinyu Zhang", "Jiaxin Zhou", "Tenghui Hua", "Husheng Han", "Tianmeng Yang", "Jianan Mu", "Bei Yu", "Rui Zhang", "Jing Ye", "Huawei Li"], "title": "Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT", "comment": "6 pages, 5 figures", "summary": "Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability."}
{"id": "2511.13343", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13343", "abs": "https://arxiv.org/abs/2511.13343", "authors": ["A Cormier", "David Roqui", "Fabrice Surma", "Martin Labouré", "Jean-Marc Vallet", "Odile Guillon", "N Grozavu", "Ann Bourgès"], "title": "Coliseum project: Correlating climate change data with the behavior of heritage materials", "comment": null, "summary": "Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site."}
{"id": "2511.13676", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13676", "abs": "https://arxiv.org/abs/2511.13676", "authors": ["Hyunwoo Oh", "KyungIn Nam", "Rajat Bhattacharjya", "Hanning Chen", "Tamoghno Das", "Sanggeon Yun", "Suyeon Jang", "Andrew Ding", "Nikil Dutt", "Mohsen Imani"], "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization", "comment": "Accepted to DATE 2026", "summary": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms."}
{"id": "2511.13679", "categories": ["cs.AR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13679", "abs": "https://arxiv.org/abs/2511.13679", "authors": ["Hyunwoo Oh", "Hanning Chen", "Sanggeon Yun", "Yang Ni", "Wenjun Huang", "Tamoghno Das", "Suyeon Jang", "Mohsen Imani"], "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention", "comment": "Accepted to DATE 2026", "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups."}
