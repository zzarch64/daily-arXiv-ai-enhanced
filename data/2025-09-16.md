<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [ReCross: Efficient Embedding Reduction Scheme for In-Memory Computing using ReRAM-Based Crossbar](https://arxiv.org/abs/2509.10627)
*Yu-Hong Lai,Chieh-Lin Tsai,Wen Sheng Lim,Han-Wen Hu,Tei-Wei Kuo,Yuan-Hao Chang*

Main category: cs.AR

TL;DR: ReCross是一种基于ReRAM的内存计算方案，通过智能分组和映射共现嵌入、复制频繁访问的嵌入以及动态选择内存处理操作，显著提升了DLRM嵌入缩减的执行时间和能效。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型中的大型稀疏嵌入层存在内存带宽瓶颈，导致推理时间增加和能耗上升。虽然ReRAM交叉阵列提供了快速节能的解决方案，但简单的嵌入映射会导致交叉阵列利用率低下。

Method: ReCross通过协同优化嵌入访问模式和ReRAM交叉阵列特性，包括智能分组映射共现嵌入、跨交叉阵列复制频繁访问的嵌入，以及使用新设计的动态开关ADC电路动态选择内存处理操作。

Result: 实验结果表明，与最先进的IMC方法相比，ReCross实现了3.97倍的执行时间减少和6.1倍的能效提升。

Conclusion: ReCross通过高效的ReRAM内存计算方案，有效解决了DLRM嵌入层的内存带宽瓶颈问题，显著提升了推理性能和能源效率。

Abstract: Deep learning-based recommendation models (DLRMs) are widely deployed in
commercial applications to enhance user experience. However, the large and
sparse embedding layers in these models impose substantial memory bandwidth
bottlenecks due to high memory access costs and irregular access patterns,
leading to increased inference time and energy consumption. While resistive
random access memory (ReRAM) based crossbars offer a fast and energy-efficient
solution through in-memory embedding reduction operations, naively mapping
embeddings onto crossbar arrays leads to poor crossbar utilization and thus
degrades performance. We present ReCross, an efficient ReRAM-based in-memory
computing (IMC) scheme designed to minimize execution time and enhance energy
efficiency in DLRM embedding reduction. ReCross co-optimizes embedding access
patterns and ReRAM crossbar characteristics by intelligently grouping and
mapping co-occurring embeddings, replicating frequently accessed embeddings
across crossbars, and dynamically selecting in-memory processing operations
using a newly designed dynamic switch ADC circuit that considers runtime energy
trade-offs. Experimental results demonstrate that ReCross achieves a 3.97x
reduction in execution time and a 6.1x improvement in energy efficiency
compared to state-of-the-art IMC approaches.

</details>


### [2] [DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators](https://arxiv.org/abs/2509.10702)
*Charles Hong,Qijing Huang,Grace Dinh,Mahesh Subedar,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: DOSA是一个基于梯度下降的硬件设计空间探索框架，通过可微分性能模型同时优化硬件参数和算法到硬件的映射，显著提升了DNN加速器的能效比。


<details>
  <summary>Details</summary>
Motivation: 传统方法将硬件设计空间和映射空间分开探索，导致组合爆炸问题，难以找到最优设计点。需要一种能够同时优化这两个高度非凸空间的方法。

Method: 提出DOSA框架，包含可微分性能模型和基于梯度下降的优化技术，能够同时探索硬件设计空间和映射空间，并支持与学习模型的模块化集成。

Result: DOSA在相同采样数量下，比随机搜索和贝叶斯优化分别提升2.80倍和12.59倍的能效延迟积。在真实DNN加速器上优化缓冲大小和映射，实现了1.82倍的能效延迟积改进。

Conclusion: DOSA通过可微分建模和梯度优化有效解决了硬件设计空间探索的组合爆炸问题，为DNN加速器设计提供了高效且灵活的优化方案。

Abstract: In the hardware design space exploration process, it is critical to optimize
both hardware parameters and algorithm-to-hardware mappings. Previous work has
largely approached this simultaneous optimization problem by separately
exploring the hardware design space and the mapspace - both individually large
and highly nonconvex spaces - independently. The resulting combinatorial
explosion has created significant difficulties for optimizers.
  In this paper, we introduce DOSA, which consists of differentiable
performance models and a gradient descent-based optimization technique to
simultaneously explore both spaces and identify high-performing design points.
Experimental results demonstrate that DOSA outperforms random search and
Bayesian optimization by 2.80x and 12.59x, respectively, in improving DNN model
energy-delay product, given a similar number of samples. We also demonstrate
the modularity and flexibility of DOSA by augmenting our analytical model with
a learned model, allowing us to optimize buffer sizes and mappings of a real
DNN accelerator and attain a 1.82x improvement in energy-delay product.

</details>


### [3] [Design and Analysis of Approximate Hardware Accelerators for VVC Intra Angular Prediction](https://arxiv.org/abs/2509.10751)
*Lucas M. Leipnitz de Fraga,Cláudio Machado Diniz*

Main category: cs.AR

TL;DR: 提出一种近似方法来减少VVC帧内预测中插值滤波器的系数数量，通过平均固定子集来降低MCM块大小，在保持编码效率的同时减少电路面积和功耗。


<details>
  <summary>Details</summary>
Motivation: VVC标准相比HEVC显著提高了压缩效率，但带来了更高的计算复杂度，特别是在帧内预测阶段。插值滤波器有50多个不同系数，使得无乘法器常数乘法(MCM)块的实现资源密集。

Method: 提出近似方法，通过平均固定子集的插值系数来减少系数数量，降低MCM块大小。引入了六种不同的MCM块架构，其中五种使用该近似方法。

Result: 实验结果显示，只有两种MCM实现超过了4%的BD-Rate增加，最坏情况下平均为2.6%。两种MCM实现分别减少了20%和44%的电路面积。三种架构的并行样本预测模块减少了30%的门面积，两种实现降低了能耗。

Conclusion: 该方法在可接受的编码效率损失下，显著降低了硬件实现的资源消耗和功耗，为VVC帧内预测的硬件加速提供了有效的优化方案。

Abstract: The Versatile Video Coding (VVC) standard significantly improves compression
efficiency over its predecessor, HEVC, but at the cost of substantially higher
computational complexity, particularly in intra-frame prediction. This stage
employs various directional modes, each requiring multiple multiplications
between reference samples and constant coefficients. To optimize these
operations at hardware accelerators, multiplierless constant multiplication
(MCM) blocks offer a promising solution. However, VVC's interpolation filters
have more than fifty distinct coefficients, making MCM implementations
resource-intensive. This work proposes an approximation method to reduce the
number of interpolation coefficients by averaging fixed subsets of them,
therefore decreasing MCM block size and potentially lowering circuit area and
power consumption. Six different MCM block architectures for angular intra
prediction are introduced, in which five use the approximation method
introduced in this work, and evaluate the trade-off between coefficient
reduction and coding efficiency compared with a conventional multiplier
architecture. Experimental results in ten videos demonstrate that only two MCM
implementations exceed a 4% BD-Rate increase and 2.6% on average in the worst
case, while two of the MCM implementations have circuit area reduction of 20%
and 44%. For three of the architectures, parallel sample prediction modules
were synthesized, showing a reduction of 30% gate area compared to single
sample processing units, and a reduction in energy consumption for two of the
implementations.

</details>


### [4] [always_comm: An FPGA-based Hardware Accelerator for Audio/Video Compression and Transmission](https://arxiv.org/abs/2509.11503)
*Rishab Parthasarathy,Akshay Attaluri,Gilford Ting*

Main category: cs.AR

TL;DR: 在Nexys4 DDR FPGA上实现的全硬件视频会议系统，使用M-JPEG编解码和UDP网络协议，支持30FPS实时视频流传输


<details>
  <summary>Details</summary>
Motivation: 设计一个完全在硬件上实现的可扩展视频会议系统，通过FPGA实现高效的视频压缩和网络传输，减少对计算机资源的依赖

Method: 使用M-JPEG编解码器压缩视频，UDP网络协议栈进行通信，FPGA处理视频编码和音频控制，计算机端用Python脚本解码和播放

Result: 系统能够实现30FPS的视频流传输，通过Cocotb仿真验证和Vivado综合部署，评估了端到端延迟和传输吞吐量

Conclusion: 该硬件实现的视频会议架构是可行的，能够满足实时视频传输的需求，为完全硬件化的视频通信系统提供了设计参考

Abstract: We present a design for an extensible video conferencing stack implemented
entirely in hardware on a Nexys4 DDR FPGA, which uses the M-JPEG codec to
compress video and a UDP networking stack to communicate between the FPGA and
the receiving computer. This networking stack accepts real-time updates from
both the video codec and the audio controller, which means that video will be
able to be streamed at 30 FPS from the FPGA to a computer. On the computer
side, a Python script reads the Ethernet packets and decodes the packets into
the video and the audio for real time playback. We evaluate this architecture
using both functional, simulation-driven verification in Cocotb and by
synthesizing SystemVerilog RTL code using Vivado for deployment on our Nexys4
DDR FPGA, where we evaluate both end-to-end latency and throughput of video
transmission.

</details>


### [5] [SuperUROP: An FPGA-Based Spatial Accelerator for Sparse Matrix Operations](https://arxiv.org/abs/2509.11529)
*Rishab Parthasarathy*

Main category: cs.AR

TL;DR: 本文提出了Azul加速器的FPGA实现，这是一种SRAM-only硬件加速器，用于解决稀疏线性方程组问题，通过优化内存访问和并行性来提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏线性方程组求解在数值方法中至关重要，但现有软件级迭代求解器在当前硬件上效率低下，主要原因是内存访问不规则和数据依赖复杂限制了并行性。

Method: 在FPGA上实现Azul加速器，采用基于RISC-V CPU核心的网格架构，每个tile包含处理单元和独立SRAM，通过NoC连接，并使用自定义RISC-V ISA扩展实现任务编程模型。

Result: 设计了分布式测试用例进行功能验证，确认FPGA实现的性能与Azul框架架构仿真等效。

Conclusion: FPGA实现的Azul加速器能够有效解决稀疏线性方程组求解中的内存带宽利用和算术强度问题，为硬件加速提供了可行方案。

Abstract: Solving sparse systems of linear equations is a fundamental problem in the
field of numerical methods, with applications spanning from circuit design to
urban planning. These problems can have millions of constraints, such as when
laying out transistors on a circuit, or trying to optimize traffic light
timings, making fast sparse solvers extremely important. However, existing
state-of-the-art software-level solutions for solving sparse linear systems,
termed iterative solvers, are extremely inefficient on current hardware. This
inefficiency can be attributed to two key reasons: (1) poor short-term data
reuse, which causes frequent, irregular memory accesses, and (2) complex data
dependencies, which limit parallelism. Hence, in this paper, we present an FPGA
implementation of the existing Azul accelerator, an SRAM-only hardware
accelerator that achieves both high memory bandwidth utilization and arithmetic
intensity. Azul features a grid of tiles, each of which is composed of a
processing element (PE) and a small independent SRAM memory, which are all
connected over a network on chip (NoC). We implement Azul on FPGA using simple
RISC-V CPU cores connected to a memory hierarchy of different FPGA memory
modules. We utilize custom RISC-V ISA augmentations to implement a task-based
programming model for the various PEs, allowing communication over the NoC.
Finally, we design simple distributed test cases so that we can functionally
verify the FPGA implementation, verifying equivalent performance to an
architectural simulation of the Azul framework.

</details>


### [6] [LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications](https://arxiv.org/abs/2509.12053)
*Yujun Lin,Zhekai Zhang,Song Han*

Main category: cs.AR

TL;DR: LEGO框架是一个自动生成张量应用空间架构设计和可综合RTL代码的框架，解决了现有框架在灵活性和生产力之间的权衡问题，相比Gemmini实现了3.2倍加速和2.4倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 现代张量应用（特别是基础模型和生成式AI应用）需要多模态输入，对灵活加速器架构需求增加。现有框架要么局限于少数手工模板，要么无法自动生成RTL，存在设计灵活性和RTL生成生产力之间的权衡问题。

Method: LEGO框架基于仿射变换架构表示，前端分析功能单元互连、合成存储系统、基于数据重用分析融合不同空间数据流设计；后端将硬件转换为原始级图进行低级优化，应用线性规划算法优化流水线寄存器插入和减少未使用逻辑开销。

Result: 评估显示LEGO相比之前工作Gemmini实现了3.2倍加速和2.4倍能效提升，能够为生成式AI应用中的多样化现代基础模型生成统一架构。

Conclusion: LEGO框架成功解决了张量加速器设计中灵活性与生产力之间的权衡问题，通过自动化RTL生成和优化技术，为多模态AI应用提供了高效的硬件设计解决方案。

Abstract: Modern tensor applications, especially foundation models and generative AI
applications require multiple input modalities (both vision and language),
which increases the demand for flexible accelerator architecture. Existing
frameworks suffer from the trade-off between design flexibility and
productivity of RTL generation: either limited to very few hand-written
templates or cannot automatically generate the RTL. To address this challenge,
we propose the LEGO framework, which targets tensor applications and
automatically generates spatial architecture design and outputs synthesizable
RTL code without handwritten RTL design templates. Leveraging the
affine-transformation-based architecture representation, LEGO front end finds
interconnections between function units, synthesizes the memory system, and
fuses different spatial dataflow designs based on data reuse analysis. LEGO
back end then translates the hardware in a primitive-level graph to perform
lower-level optimizations, and applies a set of linear-programming algorithms
to optimally insert pipeline registers and reduce the overhead of unused logic
when switching spatial dataflows. Our evaluation demonstrates that LEGO can
achieve 3.2x speedup and 2.4x energy efficiency compared to previous work
Gemmini, and can generate one architecture for diverse modern foundation models
in generative AI applications.

</details>
