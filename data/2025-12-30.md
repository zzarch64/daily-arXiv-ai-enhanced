<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator](https://arxiv.org/abs/2512.22131)
*Sheng Lu,Qianhou Qu,Sungyong Jung,Qilian Liang,Chenyun Pan*

Main category: cs.AR

TL;DR: 提出基于可重构场效应晶体管(RFET)的随机计算神经网络(SCNN)架构，通过器件级可重构性设计高效紧凑的随机数生成器和累加并行计数器，显著降低硬件资源消耗。


<details>
  <summary>Details</summary>
Motivation: 随机计算神经网络虽然能降低传统卷积神经网络的硬件复杂度，但仍受限于随机数生成器和累加并行计数器等高资源消耗组件，限制了性能提升。

Method: 利用RFET的器件级可重构特性，设计高效紧凑的随机数生成器、累加并行计数器等核心模块，并开发专用的SCNN加速器架构进行系统级仿真。

Result: 实验结果显示，在相同技术节点下，基于RFET的SCNN加速器相比FinFET设计在面积、延迟和能耗方面均实现显著降低。

Conclusion: RFET技术为随机计算神经网络提供了有效的硬件优化方案，通过器件级可重构性实现了更高效、更紧凑的神经网络加速器设计。

Abstract: Stochastic computing (SC) offers significant reductions in hardware complexity for traditional convolutional neural networks (CNNs), but stochastic computing neural networks (SCNNs) still suffer from high resource usage due to components such as stochastic number generators (SNGs) and accumulative parallel counters (APCs), which limit performance. This paper introduces a novel SCNN architecture based on reconfigurable field-effect transistors (RFETs), whose device-level reconfigurability enables the design of highly efficient and compact SNGs, APCs, and other core modules. A dedicated SCNN accelerator architecture is also developed for system-level simulation. Using publicly available open-source standard cell libraries, experimental results show that the proposed RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.

</details>


### [2] [AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience](https://arxiv.org/abs/2512.22435)
*Zining Wang,Jian Gao,Weimin Fu,Xiaolong Guo,Xuan Zhang*

Main category: cs.AR

TL;DR: AnalogSAGE是一个开源的自进化多智能体框架，通过分层记忆和三阶段智能体探索实现模拟电路自动化设计，相比现有方法显著提升了通过率和搜索效率。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计严重依赖人类经验和直觉，现有基于LLM的方法通常局限于提示驱动的网表生成或预定义拓扑模板，难以满足复杂规格要求。

Method: 提出AnalogSAGE框架，通过四个分层记忆层协调三阶段智能体探索，利用仿真反馈进行迭代优化，支持可重现性和通用性。

Result: 在SKY130 PDK和ngspice环境下，AnalogSAGE相比现有框架实现了10倍整体通过率、48倍Pass@1，并将参数搜索空间减少4倍。

Conclusion: 分层记忆和基于仿真的推理显著提升了模拟设计自动化的可靠性和自主性，为复杂规格驱动的模拟电路设计提供了有效解决方案。

Abstract: Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\times$ overall pass rate, a 48$\times$ Pass@1, and a 4$\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.

</details>


### [3] [TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators](https://arxiv.org/abs/2512.23062)
*Soham Pramanik,Vimal William,Arnab Raha,Debayan Das,Amitava Mukherjee,Janet L. Paluh*

Main category: cs.AR

TL;DR: TYTAN是一种基于泰勒级数的非线性激活引擎，通过可重构硬件设计和动态近似算法优化边缘AI推理中的激活函数，实现2倍性能提升、56%功耗降低和35倍面积减少。


<details>
  <summary>Details</summary>
Motivation: 边缘AI部署面临计算成本和能耗等资源约束，特别是GEMM和激活函数等高功耗操作需要优化。需要针对边缘设备的领域特定架构来提升AI推理的加速和能效。

Method: 提出TYTAN系统，包含可重构硬件设计和专门算法，通过泰勒级数动态估计每个激活函数所需近似，实现最小化与基准精度的偏差。

Result: 在Silvaco FreePDK45工艺节点上验证，时钟频率>950MHz，相比NVDLA基准实现：性能提升约2倍，功耗降低56%，面积减少35倍。

Conclusion: TYTAN能有效支持边缘AI推理的加速和能效优化，通过硬件-算法协同设计解决了激活函数计算的高功耗问题。

Abstract: The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.

</details>
