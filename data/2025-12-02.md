<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 31]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [VeriPy - A New Python-Based Approach for SDR Pipelined/Unrolled Hardware Accelerator Generation](https://arxiv.org/abs/2512.00006)
*Yuqin Zhao,Linghui Ye,Haihang Xia,Luke Seed,Tiantai Deng*

Main category: cs.AR

TL;DR: VeriPy：面向SDR工程师的Python HLS工具，无需硬件知识即可生成Verilog硬件加速器设计，性能优于Vivado HLS，接近手工编码实现


<details>
  <summary>Details</summary>
Motivation: SDR应用需要硬件加速器提升性能，但通信工程师缺乏硬件专业知识。现有HLS工具门槛高，需要硬件描述语言和底层硬件知识，阻碍了SDR工程师使用硬件加速技术。

Method: 开发基于Python的HLS工具VeriPy，支持生成两种主流硬件加速器架构（展开设计和流水线设计），提供自动测试平台生成、可扩展硬件库、性能资源估计，并在算法和硬件层面进行优化。

Result: VeriPy生成的硬件设计相比pragma优化的Vivado HLS设计，工作频率最高提升70%，资源消耗合理增加；性能与资源消耗接近手工编码实现。代码复杂度低，无需pragma和硬件知识。

Conclusion: VeriPy成功降低了SDR工程师使用硬件加速器的门槛，提供高性能硬件设计生成能力，填补了通信领域专用HLS工具的空白。

Abstract: Software-defined radio (SDR) plays an important role in the communication field by providing a flexible and customized communication system for different purposes according to the needs. To enhance the performance of SDR applications, hardware accelerators have been widely deployed in recent years. In facing this obstacle, a necessity arises for a high-level synthesis (HLS) tool specifically designed for communication engineers without detailed hardware knowledge. To lower the barrier between SDR engineers and hardware development, this work proposed a Python-based HLS tool, VeriPy, which can generate both mainstream architecture for hardware accelerators in Verilog specifically for SDR designs including unrolled design and pipelined design, requiring no detailed digital hardware knowledge or Hardware Description Languages (HDL). Furthermore, VeriPy supports automatic testbench generation with random input stimulus, an extensible hardware library, performance and resource estimation, and offers strong optimisation potential at both the algorithmic and digital hardware levels. The generated hardware design by VeriPy can achieve up to 70% faster operating frequency compared to pragma-optimised Vivado HLS designs with a reasonably higher resource con-sumption while delivering comparable performance and resource consumption to hand-coded implementations. Regarding code complexity, VeriPy requires no pragmas, completely eliminating the need for low-level hardware knowledge. For straightforward algorithms, the input code length remains comparable to that of Vivado HLS.

</details>


### [2] [Architect in the Loop Agentic Hardware Design and Verification](https://arxiv.org/abs/2512.00016)
*Mubarek Mohammed*

Main category: cs.AR

TL;DR: 提出基于智能体与工程师协同的处理器自动化设计与验证方法，利用大语言模型分层生成HDL代码和测试，成功设计并验证了两个处理器（LEGv8和RISC-V），成本效益高且可扩展。


<details>
  <summary>Details</summary>
Motivation: 硬件设计日益复杂，需要改进设计与验证方法。现有AI方法主要针对小型组件，缺乏完整的处理器设计自动化方案。本文旨在利用分层模块化设计原则，实现处理器全流程的自动化设计与验证。

Method: 采用智能体与工程师协同的自动化设计验证框架：智能体根据规范将设计分解为子组件，生成HDL代码和cocotb测试，在调试和综合阶段引入工程师指导。结合推理模型（如gemini-pro）和非推理模型（如gpt-5-mini），根据任务复杂度选择合适模型。

Result: 成功设计并验证了两个处理器：1) LEGv8类似处理器，在DE-10 Lite FPGA上验证、综合并编程；2) RISC-V类似32位处理器，以相同方式设计验证并综合。每个处理器约使用100万推理token，成本效益高。方法可扩展，已尝试系统级芯片设计。

Conclusion: 提出的智能体自动化处理器设计与验证方法有效可行，无需专用硬件即可低成本进行硬件设计实验。该方法具有可扩展性，为未来系统级芯片设计奠定了基础。

Abstract: The ever increasing complexity of the hardware design process demands improved hardware design and verification methodologies. With the advent of generative AI various attempts have been made to automate parts of the design and verification process. Large language models (LLMs) as well as specialized models generate hdl and testbenches for small components, having a few leaf level components. However, there are only a few attempts to automate the entire processor design process. Hardware design demands hierarchical and modular design processes. We utilized this best practice systematically and effectively. We propose agentic automated processor design and verification with engineers in the loop. The agent with optional specification tries to break down the design into sub-components, generate HDL and cocotb tests, and verifies the components involving engineer guidance, especially during debugging and synthesis. We designed various digital systems using this approach. However, we selected two simple processors for demonstration purposes in this work. The first one is a LEGv8 like a simple processor verified, synthesized and programmed for the DE-10 Lite FPGA. The second one is a RISC-V like 32-bit processor designed and verified in similar manner and synthesized. However, it is not programmed into the DE-10 Lite. This process is accomplished usually using around a million inference tokens per processor, using a combination of reasoning (e.g gemini-pro) and non-reasoning models (eg. gpt-5-mini) based on the complexity of the task. This indicates that hardware design and verification experimentation can be done cost effectively without using any specialized hardware. The approach is scalable, we even attempted system-on-chip, which we want to experiment in our future work.

</details>


### [3] [Hardware-Aware DNN Compression for Homogeneous Edge Devices](https://arxiv.org/abs/2512.00017)
*Kunlong Zhang,Guiying Li,Ning Lu,Peng Yang,Ke Tang*

Main category: cs.AR

TL;DR: HDAP提出了一种针对同构边缘设备的硬件感知DNN压缩框架，通过设备聚类和代理评估来解决设备性能差异问题，实现跨设备的最优平均性能。


<details>
  <summary>Details</summary>
Motivation: 同构边缘设备（相同型号）在实际部署后，由于用户配置、环境条件、制造差异、电池退化等因素，性能会变得不同。现有的DNN压缩方法未考虑这种情况，无法保证在所有同构设备上都有良好的压缩效果。

Method: 提出HDAP框架：1）将大量同构设备聚类成几个设备簇，减少需要评估的设备数量；2）使用基于代理的评估代替实时硬件评估；3）目标是实现压缩模型在所有设备上的最优平均性能。

Result: 在多种设备类型（Jetson Xavier NX和Jetson Nano）和任务类型（图像分类和物体检测）上的实验表明，HDAP相比最先进方法能实现更低的平均延迟和竞争性准确率，显著加速（如ResNet50在1.0G FLOPs下达到2.86倍加速）。

Conclusion: HDAP为同构边缘设备提供了可扩展、高性能的DNN部署解决方案，有效解决了设备性能差异带来的压缩挑战。

Abstract: Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Extensive experiments on multiple device types (Jetson Xavier NX and Jetson Nano) and task types (image classification with ResNet50, MobileNetV1, ResNet56, VGG16; object detection with YOLOv8n) demonstrate that HDAP consistently achieves lower average latency and competitive accuracy compared to state-of-the-art methods, with significant speedups (e.g., 2.86$\times$ on ResNet50 at 1.0G FLOPs). HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices.

</details>


### [4] [Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead](https://arxiv.org/abs/2512.00020)
*Guang Yang,Wei Zheng,Xiang Chen,Dong Liang,Peng Hu,Yukui Yang,Shaohang Peng,Zhenghan Li,Jiahui Feng,Xiao Wei,Kexin Sun,Deyuan Ma,Haotian Cheng,Yiheng Shen,Xing Hu,Terry Yue Zhuo,David Lo*

Main category: cs.AR

TL;DR: 这是一篇关于大语言模型在Verilog代码生成领域应用的系统性文献综述，涵盖了102篇相关论文，分析了现有方法、数据集、评估指标和技术分类，并指出了研究局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: Verilog作为硬件描述语言在数字电路设计中至关重要，而大语言模型在软件工程和人工智能领域的成功应用促使研究者探索其在硬件设计自动化中的应用。然而，尽管已有大量研究关注LLM在Verilog代码生成方面的应用，但缺乏对这些发展的系统性综述。

Method: 通过系统性文献综述方法，收集了来自软件工程、人工智能和电子设计自动化领域的70篇会议/期刊论文和32篇高质量预印本论文，总计102篇。通过回答四个关键研究问题来组织分析。

Result: 综述识别了用于Verilog生成的LLM模型，检查了评估中使用的数据集和指标，对Verilog生成技术进行了分类，并分析了LLM对齐方法。同时指出了现有研究的局限性。

Conclusion: 该综述填补了LLM在Verilog代码生成领域系统性综述的空白，为研究者提供了全面的现状分析，并提出了未来研究的路线图，有助于推动自动化硬件设计的发展。

Abstract: Code generation has emerged as a critical research area at the intersection of Software Engineering (SE) and Artificial Intelligence (AI), attracting significant attention from both academia and industry. Within this broader landscape, Verilog, as a representative hardware description language (HDL), plays a fundamental role in digital circuit design and verification, making its automated generation particularly significant for Electronic Design Automation (EDA). Consequently, recent research has increasingly focused on applying Large Language Models (LLMs) to Verilog code generation, particularly at the Register Transfer Level (RTL), exploring how these AI-driven techniques can be effectively integrated into hardware design workflows. Despite substantial research efforts have explored LLM applications in this domain, a comprehensive survey synthesizing these developments remains absent from the literature. This review fill addresses this gap by providing a systematic literature review of LLM-based methods for Verilog code generation, examining their effectiveness, limitations, and potential for advancing automated hardware design. The review encompasses research work from conferences and journals in the fields of SE, AI, and EDA, encompassing 70 papers published on venues, along with 32 high-quality preprint papers, bringing the total to 102 papers. By answering four key research questions, we aim to (1) identify the LLMs used for Verilog generation, (2) examine the datasets and metrics employed in evaluation, (3) categorize the techniques proposed for Verilog generation, and (4) analyze LLM alignment approaches for Verilog generation. Based on our findings, we have identified a series of limitations of existing studies. Finally, we have outlined a roadmap highlighting potential opportunities for future research endeavors in LLM-assisted hardware design.

</details>


### [5] [ML-PCM : Machine Learning Technique for Write Optimization in Phase Change Memory (PCM)](https://arxiv.org/abs/2512.00026)
*Mahek Desai,Rowena Quinn,Marjan Asadinia*

Main category: cs.AR

TL;DR: 使用神经网络模型预测相变存储器（PCM）的写入延迟、能耗和耐久性，以优化性能并提高其作为DRAM替代方案的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着DRAM等晶体管存储技术接近可扩展性极限，需要探索替代存储方案。相变存储器（PCM）因可扩展性、快速访问速度和零泄漏功耗而备受关注，但其耐久性有限和写入能耗高的问题阻碍了广泛应用。

Method: 提出使用神经网络模型，通过监测实时运行条件和设备特性，预测PCM的关键参数（写入延迟、能耗和耐久性），从而优化写入设置并提升性能。

Result: 神经网络预测取得了显著成果：耐久性预测的平均绝对百分比误差（MAPE）为0.0073%，总写入延迟为0.23%，总写入能耗为4.92%。

Conclusion: 基于神经网络的预测方法能有效提升PCM的性能和效率，使其在频繁写入的应用中成为更实用、更高效的数据存储替代方案。

Abstract: As transistor-based memory technologies like dynamic random access memory (DRAM) approach their scalability limits, the need to explore alternative storage solutions becomes increasingly urgent. Phase-change memory (PCM) has gained attention as a promising option due to its scalability, fast access speeds, and zero leakage power compared to conventional memory systems. However, despite these advantages, PCM faces several challenges that impede its broader adoption, particularly its limited lifespan due to material degradation during write operations, as well as the high energy demands of these processes. For PCM to become a viable storage alternative, enhancing its endurance and reducing the energy required for write operations are essential. This paper proposes the use of a neural network (NN) model to predict critical parameters such as write latency, energy consumption, and endurance by monitoring real-time operating conditions and device characteristics. These predictions are key to improving PCM performance and identifying optimal write settings, making PCM a more practical and efficient option for data storage in applications with frequent write operations. Our approach leads to significant improvements, with NN predictions achieving a Mean Absolute Percentage Error (MAPE) of 0.0073% for endurance, 0.23% for total write latency, and 4.92% for total write energy.

</details>


### [6] [Analysis of Single Event Induced Bit Faults in a Deep Neural Network Accelerator Pipeline](https://arxiv.org/abs/2512.00028)
*Naïn Jonckers,Toon Vinck,Peter Karsmakers,Jeffrey Prinzie*

Main category: cs.AR

TL;DR: 该论文分析了辐射粒子引起的单比特故障对基于脉动阵列的DNN加速器的影响，通过RTL级故障注入模拟评估硬件模块敏感性，并提出高效容错加固策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI和DNN应用增长，专用DNN加速器在辐射环境（如太空、核设施）中面临辐射粒子引发的故障挑战，需要评估其容错性并设计高效加固方案。

Method: 采用RTL级故障注入模拟，分析脉动阵列DNN加速器计算流水线中寄存器组的单比特故障敏感性，使用MNIST和CIFAR-10数据集评估三种DNN工作负载。

Result: 提出了故障传播概率和错误分类概率两个敏感性指标，识别了不同硬件模块的故障敏感性差异，为针对性加固提供了数据支持。

Conclusion: 基于敏感性分析设计了兼顾面积和功耗开销的高效容错加固策略，为辐射环境下DNN加速器的可靠部署提供了解决方案。

Abstract: In recent years, the increased interest and the growth in application domains of Artificial Intelligence (AI), and more specifically Deep Neural Networks (DNNs), has led to an extensive usage of domain specific DNN accelerator processors to improve the computational efficiency of DNN inference. However, like any digital circuit, these processors are prone to faults induced by radiation particles such as heavy ions, protons, etc., making their use in harsh radiation environments a challenge. This work presents an in-depth analysis of the impact of such faults on the computational pipeline of a Systolic Array based Deep Neural Network accelerator (SA-DNN accelerator) by means of a Register Transfer Level (RTL) Fault Injection (FI) simulation in order to improve the observability of each hardware block. From this analysis, we present the sensitivity to single bit faults of register groups in the pipeline for three different DNN workloads utilising two datasets, namely MNIST and CIFAR-10. These sensitivity figures are presented in terms of Fault Propagation Probability ($P(f_{non-crit})$) and False Classification Probability ($P(f_{crit})$) which respectively show the probability that an injected fault causes a non-critical error (numerical offset) or a critical error (classification fault). From these results, we devise a fault mitigation strategy to harden the SA-DNN accelerator in an efficient way, both in terms of area and power overhead.

</details>


### [7] [Hardware-Aware Neural Network Compilation with Learned Optimization: A RISC-V Accelerator Approach](https://arxiv.org/abs/2512.00031)
*Ravindra Ganti,Steve Xu*

Main category: cs.AR

TL;DR: XgenSilicon ML编译器是一个全自动端到端编译框架，可将高级ML模型转换为针对定制ASIC加速器优化的RISC-V汇编代码，通过五项关键技术实现显著的PPA改进。


<details>
  <summary>Details</summary>
Motivation: 现有ML模型部署到硬件加速器时，通常需要手动优化和硬件专业知识，缺乏统一的软件-硬件成本模型，导致无法充分利用定制ASIC的潜力。

Method: 1) 多算法自动调优框架结合学习成本模型；2) 集成量化框架支持FP32到二进制的极端精度；3) 硬件感知验证确保ISA合规和内存约束；4) 动态形状支持与多配置专业化；5) 高级缓存感知成本建模与多级缓存层次分析。

Result: 相比基线实现，生成的ASIC性能提升2.5-4.5倍，功耗降低3-6倍，面积减少40-60%，支持100多个ONNX算子，实现RISC-V向量优化，生成可直接用于ASIC综合的硬件验证汇编代码。

Conclusion: XgenSilicon ML编译器通过统一的软件-硬件成本模型和五项关键技术，实现了从ML模型到优化ASIC的全自动编译流程，显著提升了PPA指标，为定制AI加速器开发提供了高效解决方案。

Abstract: We present XgenSilicon ML Compiler, a fully automated end-to-end compilation framework that transforms high-level machine learning models into optimized RISC-V assembly code for custom ASIC accelerators. By unifying the system's cost model across software and hardware, the compiler achieves significant improvements in Power, Performance, and Area (PPA) metrics compared to standard off-the-shelf components and hand-designed chips through five key innovations: (1) a multi-algorithm auto-tuning framework with five search strategies (Bayesian Optimization, Genetic Algorithm, Simulated Annealing, Random Search, Grid Search) combined with a learned cost model, (2) an integrated quantization framework supporting extreme precisions from FP32 to Binary with full KL divergence calibration (2048-bin histogram optimization) and momentum-based QAT gradient updates, (3) hardware-aware validation ensuring 100 percent ISA compliance and memory constraint satisfaction, (4) dynamic shape support with multi-configuration specialization, and (5) advanced cache-aware cost modeling with multi-level cache hierarchy analysis. Our evaluation demonstrates that ASICs produced by this compiler achieve 2.5-4.5x better performance, 3-6x lower power consumption, and 40-60 percent area reduction compared to baseline implementations. The compiler supports more than 100 ONNX operators across 12 categories, implements advanced RISC-V Vector optimizations, and generates hardware-validated assembly code suitable for direct ASIC synthesis. All compilation steps are fully automated, requiring zero manual intervention from model input to ASIC-ready output.

</details>


### [8] [Decoupled Control Flow and Data Access in RISC-V GPGPUs](https://arxiv.org/abs/2512.00032)
*Giuseppe M. Sarda,Nimish Shah,Abubakr Nada,Debjyoti Bhattacharjee,Marian Verhelst*

Main category: cs.AR

TL;DR: 本文针对开源GPGPU平台Vortex的性能瓶颈，提出通过解耦控制流和数据访问的微架构改进，包括硬件控制流管理器和解耦内存流通道，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: Vortex作为基于RISC-V的开源GPGPU平台，为GPGPU研究提供了商业GPU之外的替代方案，但目前性能不及商业GPU，特别是在控制流管理和内存访问方面存在瓶颈，这限制了其在机器学习等应用中的广泛采用。

Method: 提出两种微架构改进：1）硬件控制流管理器，用于加速常规循环执行中的分支和谓词操作；2）解耦内存流通道，通过有用计算进一步隐藏内存延迟。

Result: 评估结果显示：执行速度提升8倍，动态指令数减少10倍，整体性能从0.35提升到1.63 GFLOP/s/mm²。

Conclusion: 通过简单的微架构改进显著提升了Vortex的性能，使其成为下一代机器学习GPGPU研究的理想平台。

Abstract: Vortex, a newly proposed open-source GPGPU platform based on the RISC-V ISA, offers a valid alternative for GPGPU research over the broadly-used modeling platforms based on commercial GPUs. Similarly to the push originating from the RISC-V movement for CPUs, Vortex can enable a myriad of fresh research directions for GPUs. However, as a young hardware platform, it currently lacks the performance competitiveness of commercial GPUs, which is crucial for widespread adoption. State-of-the-art GPUs, in fact, rely on complex architectural features, still unavailable in Vortex, to hide the micro-code overheads linked to control flow (CF) management and memory orchestration for data access. In particular, these components account for the majority of the dynamic instruction count in regular, memory-intensive kernels, such as linear algebra routines, which form the basis of many applications, including Machine Learning. To address these challenges with simple yet powerful micro-architecture modifications, this paper introduces decoupled CF and data access through 1.) a hardware CF manager to accelerate branching and predication in regular loop execution and 2.) decoupled memory streaming lanes to further hide memory latency with useful computation. The evaluation results for different kernels show 8$\times$ faster execution, 10$\times$ reduction in dynamic instruction count, and overall performance improvement from 0.35 to 1.63 $\mathrm{GFLOP/s/mm^2}$. Thanks to these enhancements, Vortex can become an ideal playground to enable GPGPU research for the next generation of Machine Learning.

</details>


### [9] [WebAssembly on Resource-Constrained IoT Devices: Performance, Efficiency, and Portability](https://arxiv.org/abs/2512.00035)
*Mislav Has,Tao Xiong,Fehmi Ben Abdesslem,Mario Kušek*

Main category: cs.AR

TL;DR: 该论文评估了WebAssembly在嵌入式物联网设备上的性能、内存占用和能耗，发现虽然原生C代码性能更优，但WASM在跨平台兼容性和安全沙箱执行方面提供了可接受的权衡，是嵌入式物联网应用的可行选择。


<details>
  <summary>Details</summary>
Motivation: 物联网硬件和软件的异构性给资源受限设备的软件可移植性、可维护性和部署带来了重大挑战。WebAssembly作为最初为Web设计的可移植、安全、高效的运行时环境，有望解决这些挑战。

Method: 在三种代表性微控制器（Raspberry Pi Pico、ESP32 C6、nRF5340）上评估WASM的性能、内存占用和能耗。比较两种轻量级WASM运行时（WAMR和wasm3）与原生C执行的差异。

Result: 原生执行在速度和能效方面仍然更优，但WASM在跨平台兼容性和沙箱执行方面提供了可接受的权衡。WASM是嵌入式物联网应用的可行选择，特别是在可移植性和安全性优先于严格性能约束的场景中。

Conclusion: WebAssembly是嵌入式物联网系统的可行选项，当可移植性和安全性比严格性能约束更重要时。进一步的运行时优化可以扩展其在该领域的实用性。

Abstract: The increasing heterogeneity of hardware and software in the Internet of Things (IoT) poses a major challenge for the portability, maintainability and deployment of software on devices with limited resources. WebAssembly (WASM), originally designed for the web, is increasingly recognized as a portable, secure and efficient runtime environment that can overcome these challenges. This paper explores the feasibility of using WASM in embedded IoT systems by evaluating its performance, memory footprint and energy consumption on three representative microcontrollers: the Raspberry Pi Pico, the ESP32 C6 and the nRF5340. Two lightweight WASM runtimes, WAMR and wasm3, are compared with the native C execution. The results show that while the native execution remains superior in terms of speed and energy efficiency, WASM offers acceptable trade-offs in return for cross-platform compatibility and sandbox execution. The results highlight that WASM is a viable option for embedded IoT applications when portability and security outweigh strict performance constraints, and that further runtime optimization could extend its practicality in this area.

</details>


### [10] [Critical Path Aware Timing-Driven Global Placement for Large-Scale Heterogeneous FPGAs](https://arxiv.org/abs/2512.00038)
*He Jiang,Yi Guo,Shikai Guo,Huijiang Liu,Xiaochen Li,Ning Wang,Zhixiong Di*

Main category: cs.AR

TL;DR: TD-Placer是一个面向FPGA的时序驱动全局布局框架，通过图表示和轻量级算法优化关键路径延迟，在WNS和CPD上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着FPGA设计规模扩大和异构资源增加，密集互连带来的电阻电容效应使时序收敛变得困难。现有方法在多因素非线性约束、负载和串扰耦合效应下难以构建准确的时序模型。

Method: 提出TD-Placer框架：1）使用基于图的表示捕捉全局网络交互；2）采用非线性模型整合多种时序相关特征进行精确延迟预测；3）采用二次布局目标最小化线长，同时通过轻量级算法构建时序项；4）采用细粒度加权方案处理网络级时序竞争。

Result: 在7个真实FPGA项目（LUT数量60K-400K）上的实验显示：相比SOTA方法，平均WNS改善10%，CPD减少5%；在五个Vivado版本（2020.2-2024.2）上，平均CPD与商业工具相当（*1.01）。

Conclusion: TD-Placer通过有效的时序驱动全局布局框架，显著改善了FPGA设计的时序性能，在关键路径延迟优化方面表现出色，代码和数据集已公开。

Abstract: Timing optimization during global placement is critical for achieving optimal circuit performance and remains a key challenge in modern Field Programmable Gate Array (FPGA) design. As FPGA designs scale and heterogeneous resources increase, dense interconnects introduce significant resistive and capacitive effects, making timing closure increasingly difficult. Existing methods face challenges in constructing accurate timing models due to multi-factor nonlinear constraints as well as load and crosstalk coupling effects arising in multi-pin driving scenarios. To address these challenges, we propose TD-Placer, a critical path aware, timing-driven global placement framework. It leverages graph-based representations to capture global net interactions and employs a nonlinear model to integrate diverse timing-related features for precise delay prediction, thereby improving the overall placement quality for FPGAs. TD-Placer adopts a quadratic placement objective that minimizes wirelength while incorporating a timing term constructed by a lightweight algorithm, enabling efficient and high-quality timing optimization. Regarding net-level timing contention, it also employs a finer-grained weighting scheme to facilitate smooth reduction of the Critical Path Delay (CPD). Extensive experiments were carried out on seven real-world open-source FPGA projects with LUT counts ranging from 60K to 400K. The results demonstrate that TD-Placer achieves an average 10% improvement in Worst Negative Slack (WNS) and a 5% reduction in CPD compared to the state-of-the-art method, with an average CPD comparable (*1.01) to the commercial AMD Vivado across five versions (2020.2-2024.2). Its code and dataset are publicly available.

</details>


### [11] [SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning](https://arxiv.org/abs/2512.00044)
*Junzhuo Zhou,Ziwen Wang,Haoxuan Xia,Yuxin Yan,Chengyu Zhu,Ting-Jung Lin,Wei Xing,Lei He*

Main category: cs.AR

TL;DR: SetupKit是一个用于加速芯片时序库表征的框架，通过统计智能、电路分析和主动学习，将CPU时间从720天减少到290天（2.4倍加速）。


<details>
  <summary>Details</summary>
Motivation: 现代芯片时序闭合需要准确的建立/保持时间表征，但依赖数百万次SPICE仿真和多个PVT角点，导致数周甚至数月的瓶颈。现有方法收敛慢且探索效率低，特别是在多角点设置下。

Method: SetupKit整合三个关键创新：1) BEIRA - 基于统计误差建模的偏置增强插值搜索，加速收敛并克服停滞问题；2) 通过电路分析进行初始搜索区间估计；3) 使用高斯过程的主动学习策略，智能学习PVT-时序相关性，引导昂贵仿真到最有信息量的角点。

Result: 在工业22nm标准单元库的16个PVT角点上评估，SetupKit实现了2.4倍的整体CPU时间减少（从单核720天减少到290天），显著缩短了表征时间。

Conclusion: SetupKit为库表征提供了一个基于学习的原理性方法，解决了关键的EDA挑战，并为更智能的仿真管理铺平了道路。

Abstract: Accurate setup/hold time characterization is crucial for modern chip timing closure, but its reliance on potentially millions of SPICE simulations across diverse process-voltagetemperature (PVT) corners creates a major bottleneck, often lasting weeks or months. Existing methods suffer from slow search convergence and inefficient exploration, especially in the multi-corner setting. We introduce SetupKit, a novel framework designed to break this bottleneck using statistical intelligence, circuit analysis and active learning (AL). SetupKit integrates three key innovations: BEIRA, a bias-enhanced interpolation search derived from statistical error modeling to accelerate convergence by overcoming stagnation issues, initial search interval estimation by circuit analysis and AL strategy using Gaussian Process. This AL component intelligently learns PVT-timing correlations, actively guiding the expensive simulations to the most informative corners, thus minimizing redundancy in multicorner characterization. Evaluated on industrial 22nm standard cells across 16 PVT corners, SetupKit demonstrates a significant 2.4x overall CPU time reduction (from 720 to 290 days on a single core) compared to standard practices, drastically cutting characterization time. SetupKit offers a principled, learningbased approach to library characterization, addressing a critical EDA challenge and paving the way for more intelligent simulation management.

</details>


### [12] [Assessing Large Language Models in Generating RTL Design Specifications](https://arxiv.org/abs/2512.00045)
*Hung-Ming Huang,Yu-Hsin Yang,Fu-Chieh Chang,Yun-Chia Hsu,Yin-Yu Lin,Ming-Fang Tsai,Chun-Chih Yang,Pei-Yuan Wu*

Main category: cs.AR

TL;DR: 研究如何利用LLM从RTL代码自动生成规格说明，提出评估指标并比较不同LLM的性能


<details>
  <summary>Details</summary>
Motivation: 随着IC设计复杂度增加，工程师需要手动解读RTL代码并编写规格说明，这个过程缓慢且容易出错。虽然已有研究使用LLM从规格生成RTL，但从RTL自动生成规格说明的研究仍然不足，主要缺乏可靠的评估方法。

Method: 研究提示策略对RTL到规格生成质量的影响，引入用于忠实评估生成规格的指标，并对开源和商业LLM进行基准测试。

Result: 提供了评估生成规格的指标框架，比较了不同LLM在RTL到规格生成任务上的性能，为IC设计中更自动化和高效的规格工作流程奠定了基础。

Conclusion: 通过研究提示策略和引入评估指标，为RTL代码自动生成规格说明提供了方法论基础，推动了IC设计中规格工作流程的自动化和效率提升。

Abstract: As IC design grows more complex, automating comprehension and documentation of RTL code has become increasingly important. Engineers currently should manually interpret existing RTL code and write specifications, a slow and error-prone process. Although LLMs have been studied for generating RTL from specifications, automated specification generation remains underexplored, largely due to the lack of reliable evaluation methods. To address this gap, we investigate how prompting strategies affect RTL-to-specification quality and introduce metrics for faithfully evaluating generated specs. We also benchmark open-source and commercial LLMs, providing a foundation for more automated and efficient specification workflows in IC design.

</details>


### [13] [A Configurable Mixed-Precision Fused Dot Product Unit for GPGPU Tensor Computation](https://arxiv.org/abs/2512.00053)
*Nikhil Rout,Blaise Tine*

Main category: cs.AR

TL;DR: 提出了一种可扩展的混合精度点积单元，将浮点和整数算术流水线集成在单一融合架构中，作为开源RISC-V Vortex GPGPU张量核心单元的扩展，支持多种低精度格式和高精度累加。


<details>
  <summary>Details</summary>
Motivation: 现有开源RTL实现的内积操作依赖离散算术单元，导致吞吐量不理想和资源利用率低下，需要更高效的混合精度MMA操作来加速深度学习工作负载。

Method: 设计了一个可扩展的混合精度点积单元，将浮点和整数算术流水线集成在单一融合架构中，支持FP16/BF16/FP8/BF8/INT8/UINT4等低精度格式的乘法，以及FP32/INT32的高精度累加，并提供了可扩展框架以支持未来自定义表示。

Result: 在AMD Xilinx Alveo U55C FPGA上实现了4周期操作延迟和306.6 MHz时钟频率，在每warp 4线程配置下实现了9.812 GFLOPS的理想流水线吞吐量。

Conclusion: 提出的融合架构解决了现有离散算术单元的问题，提供了高效、可扩展的混合精度点积单元，显著提升了深度学习工作负载的加速性能。

Abstract: Efficient mixed-precision MMA operations are critical for accelerating Deep Learning workloads on GPGPUs. However, existing open-source RTL implementations of inner dot products rely on discrete arithmetic units, leading to suboptimal throughput and poor resource utilization. To address these challenges, we propose a scalable mixed-precision dot product unit that integrates floating-point and integer arithmetic pipelines within a singular fused architecture, implemented as part of the open-source RISC-V based Vortex GPGPU's Tensor Core Unit extension. Our design supports low-precision multiplication in (FP16/BF16/FP8/BF8/INT8/UINT4) formats and higher-precision accumulation in (FP32/INT32), with an extensible framework for adding and evaluating other custom representations in the future. Experimental results demonstrate 4-cycle operation latency at 306.6 MHz clock frequency on the AMD Xilinx Alveo U55C FPGA, delivering an ideal filled pipeline throughput of 9.812 GFLOPS in a 4-thread per warp configuration.

</details>


### [14] [KAN-SAs: Efficient Acceleration of Kolmogorov-Arnold Networks on Systolic Arrays](https://arxiv.org/abs/2512.00055)
*Sohaib Errabii,Olivier Sentieys,Marcello Traiola*

Main category: cs.AR

TL;DR: KAN-SAs：一种基于脉动阵列的新型KAN加速器，通过非递归B样条实现和利用KAN稀疏性，相比传统SA提升100%利用率和减少50%时钟周期


<details>
  <summary>Details</summary>
Motivation: KANs因其参数效率和可解释性受到关注，但其核心的B样条激活函数难以加速。脉动阵列(SA)作为高效的DNN加速器，在KAN加速方面的适用性和效率从未被评估。

Method: 提出KAN-SAs加速器：1) 采用非递归B样条实现；2) 利用KAN固有的稀疏性；3) 在传统SA基础上增强以支持高效KAN推理。

Result: 在28nm FD-SOI工艺上的硬件综合结果显示：相比同等面积的传统SA，KAN-SAs实现100% SA利用率，时钟周期减少50%。在不同KAN应用上的评估证实了其效率提升。

Conclusion: KAN-SAs通过专门针对B样条特性和KAN稀疏性优化，显著提升了KAN推理效率，同时保持对传统DNN的加速能力，为KAN硬件加速提供了有效解决方案。

Abstract: Kolmogorov-Arnold Networks (KANs) have garnered significant attention for their promise of improved parameter efficiency and explainability compared to traditional Deep Neural Networks (DNNs). KANs' key innovation lies in the use of learnable non-linear activation functions, which are parametrized as splines. Splines are expressed as a linear combination of basis functions (B-splines). B-splines prove particularly challenging to accelerate due to their recursive definition. Systolic Array (SA)based architectures have shown great promise as DNN accelerators thanks to their energy efficiency and low latency. However, their suitability and efficiency in accelerating KANs have never been assessed. Thus, in this work, we explore the use of SA architecture to accelerate the KAN inference. We show that, while SAs can be used to accelerate part of the KAN inference, their utilization can be reduced to 30%. Hence, we propose KAN-SAs, a novel SA-based accelerator that leverages intrinsic properties of B-splines to enable efficient KAN inference. By including a nonrecursive B-spline implementation and leveraging the intrinsic KAN sparsity, KAN-SAs enhances conventional SAs, enabling efficient KAN inference, in addition to conventional DNNs. KAN-SAs achieves up to 100% SA utilization and up to 50% clock cycles reduction compared to conventional SAs of equivalent area, as shown by hardware synthesis results on a 28nm FD-SOI technology. We also evaluate different configurations of the accelerator on various KAN applications, confirming the improved efficiency of KAN inference provided by KAN-SAs.

</details>


### [15] [SafeCiM: Investigating Resilience of Hybrid Floating-Point Compute-in-Memory Deep Learning Accelerators](https://arxiv.org/abs/2512.00059)
*Swastik Bhattacharya,Sanjay Das,Anand Menon,Shamik Kundu,Arnab Raha,Kanad Basu*

Main category: cs.AR

TL;DR: 该论文系统分析了浮点计算内存（FP-CiM）架构中的硬件故障影响，针对LLM等生成式AI应用，提出了一种名为SafeCiM的容错设计，显著降低了故障对推理精度的影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM等复杂DNN模型参数规模不断增长，传统加速器面临数据传输瓶颈，计算内存（CiM）架构成为解决方案。浮点运算（FP）因动态范围和精度优势更适合生成式AI应用，但FP-CiM对硬件故障的脆弱性尚未充分研究，这在关键任务环境中构成重大可靠性问题。

Method: 通过系统分析FP-CiM中的硬件故障影响，在关键计算阶段（数字乘法器、CiM存储单元、数字加法器树）引入比特翻转故障。使用CNN（如AlexNet）和先进LLM（如LLaMA-3.2-1B和Qwen-0.3B-Base）进行实验，评估各阶段故障对推理精度的影响。基于分析结果，提出了名为SafeCiM的故障弹性设计。

Result: 实验显示，单个加法器故障可使LLM精度降至0%。提出的SafeCiM设计相比基础FP-CiM架构显著降低了故障影响，例如在4096个MAC单元配置下，对单个加法器故障的精度下降减少了49倍。

Conclusion: FP-CiM架构对硬件故障高度敏感，特别是在加法器阶段。SafeCiM设计通过有效的故障缓解机制，显著提升了FP-CiM在关键任务环境中的可靠性，为生成式AI应用的硬件加速提供了更可靠的解决方案。

Abstract: Deep Neural Networks (DNNs) continue to grow in complexity with Large Language Models (LLMs) incorporating vast numbers of parameters. Handling these parameters efficiently in traditional accelerators is limited by data-transmission bottlenecks, motivating Compute-in-Memory (CiM) architectures that integrate computation within or near memory to reduce data movement. Recent work has explored CiM designs using Floating-Point (FP) and Integer (INT) operations. FP computations typically deliver higher output quality due to their wider dynamic range and precision, benefiting precision-sensitive Generative AI applications. These include models such as LLMs, thus driving advancements in FP-CiM accelerators. However, the vulnerability of FP-CiM to hardware faults remains underexplored, posing a major reliability concern in mission-critical settings. To address this gap, we systematically analyze hardware fault effects in FP-CiM by introducing bit-flip faults at key computational stages, including digital multipliers, CiM memory cells, and digital adder trees. Experiments with Convolutional Neural Networks (CNNs) such as AlexNet and state-of-the-art LLMs including LLaMA-3.2-1B and Qwen-0.3B-Base reveal how faults at each stage affect inference accuracy. Notably, a single adder fault can reduce LLM accuracy to 0%. Based on these insights, we propose a fault-resilient design, SafeCiM, that mitigates fault impact far better than a naive FP-CiM with a pre-alignment stage. For example, with 4096 MAC units, SafeCiM reduces accuracy degradation by up to 49x for a single adder fault compared to the baseline FP-CiM architecture.

</details>


### [16] [A CNN-Based Technique to Assist Layout-to-Generator Conversion for Analog Circuits](https://arxiv.org/abs/2512.00070)
*Sungyu Jeong,Minsu Kim,Byungsub Kim*

Main category: cs.AR

TL;DR: 提出一种基于CNN的自动化方法，用于检测模拟电路参考布局中可由现有生成器脚本生成的子单元，显著提高布局生成效率


<details>
  <summary>Details</summary>
Motivation: 传统手动检查模拟电路布局中哪些子单元可由现有生成器生成非常耗时（88分钟），需要自动化方法来提高效率

Method: 使用卷积神经网络（CNN）模型自动检测参考布局中的子单元，识别哪些可由库中现有生成器脚本生成，并在生成器软件中分层正确位置建议使用

Result: CNN模型在高速有线接收器电路（包含4,885个子单元实例，145种不同设计）上实现99.3%的精确度，将检查时间从88分钟减少到18秒，并能正确分类与训练数据差异很大的陌生子单元

Conclusion: 提出的CNN方法能有效自动化子单元生成器识别过程，大幅提高模拟电路布局生成效率，具有良好的泛化能力

Abstract: We propose a technique to assist in converting a reference layout of an analog circuit into the procedural layout generator by efficiently reusing available generators for sub-cell creation. The proposed convolutional neural network (CNN) model automatically detects sub-cells that can be generated by available generator scripts in the library, and suggests using them in the hierarchically correct places of the generator software. In experiments, the CNN model examined sub-cells of a high-speed wireline receiver that has a total of 4,885 sub-cell instances including different 145 sub-cell designs. The CNN model classified the sub-cell instances into 51 generatable and one not-generatable classes. One not-generatable class indicates that no available generator can generate the classified sub-cell. The CNN model achieved 99.3% precision in examining the 145 different sub-cell designs. The CNN model greatly reduced the examination time to 18 seconds from 88 minutes required in manual examination. Also, the proposed CNN model could correctly classify unfamiliar sub-cells that are very different from the training dataset.

</details>


### [17] [InF-ATPG: Intelligent FFR-Driven ATPG with Advanced Circuit Representation Guided Reinforcement Learning](https://arxiv.org/abs/2512.00079)
*Bin Sun,Rengang Zhang,Zhiteng Chao,Zizhen Liu,Jianan Mu,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: InF-ATPG是一种基于强化学习和图神经网络的智能测试模式生成框架，通过FFR分区和QGNN架构，显著减少回溯次数并提高故障覆盖率。


<details>
  <summary>Details</summary>
Motivation: 随着半导体技术进步，传统ATPG方法生成测试模式时间长，影响芯片上市时间。现有机器学习方法如强化学习和图神经网络存在奖励延迟和电路表示不足的问题。

Method: 提出InF-ATPG框架：1) 将电路划分为扇出自由区域(FFR)；2) 将ATPG特定特征融入新颖的QGNN架构；3) 使用改进的电路表示指导强化学习。

Result: 实验结果显示：与传统方法相比平均减少55.06%回溯次数，与机器学习方法相比减少38.31%回溯次数，同时提高了故障覆盖率。

Conclusion: InF-ATPG通过先进的电路表示和FFR驱动方法，有效解决了传统ATPG和现有机器学习方法的局限性，显著提升了测试模式生成效率。

Abstract: Automatic test pattern generation (ATPG) is a crucial process in integrated circuit (IC) design and testing, responsible for efficiently generating test patterns. As semiconductor technology progresses, traditional ATPG struggles with long execution times to achieve the expected fault coverage, which impacts the time-to-market of chips. Recent machine learning techniques, like reinforcement learning (RL) and graph neural networks (GNNs), show promise but face issues such as reward delay in RL models and inadequate circuit representation in GNN-based methods. In this paper, we propose InF-ATPG, an intelligent FFR-driven ATPG framework that overcomes these challenges by using advanced circuit representation to guide RL. By partitioning circuits into fanout-free regions (FFRs) and incorporating ATPG-specific features into a novel QGNN architecture, InF-ATPG enhances test pattern generation efficiency. Experimental results show InF-ATPG reduces backtracks by 55.06\% on average compared to traditional methods and 38.31\% compared to the machine learning approach, while also improving fault coverage.

</details>


### [18] [LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling](https://arxiv.org/abs/2512.00083)
*Zhongchun Zhou,Chengtao Lai,Wei Zhang*

Main category: cs.AR

TL;DR: LLaMCAT：针对LLM推理的LLC优化方案，通过MSHR感知的缓存仲裁和线程节流解决KV Cache访问中的带宽瓶颈和缓存停顿问题，在特定场景下实现1.26-1.58倍加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理时对内存系统提出了巨大挑战，特别是KV Cache访问导致的带宽需求和缓存停顿问题。现有工作未充分针对LLM解码特有的MSHR竞争问题，需要专门优化方案。

Method: 提出LLaMCAT方法：1）结合MSHR感知和负载均衡感知的缓存仲裁；2）线程节流技术；3）混合仿真框架（分析模型+周期级模拟器+内存追踪），平衡架构细节和效率。

Result: 在miss处理吞吐量为主要瓶颈时，平均加速1.26倍；在缓存大小也受限时，相比未优化版本加速1.58倍，相比最佳基线（dyncta）提升1.26倍。首次针对LLM解码特有的MSHR竞争问题。

Conclusion: LLaMCAT是首个针对LLM解码特定MSHR竞争的优化方案，为未来硬件平台上的LLM推理加速提供了实用解决方案，填补了先前工作的空白。

Abstract: Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.
  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms.

</details>


### [19] [Modeling and Simulation Frameworks for Processing-in-Memory Architectures](https://arxiv.org/abs/2512.00096)
*Mahdi Aghaei,Saba Ebrahimi,Mohammad Saleh Arafati,Elham Cheshmikhani,Dara Rahmati,Saeid Gorgin,Jungrae Kim*

Main category: cs.AR

TL;DR: 本章全面综述了存内计算（PIM）仿真方法和工具，对现有模拟器按抽象层次、设计目标和评估指标进行分类，并讨论了常用基准测试套件和仿真方法学的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 存内计算作为突破冯·诺依曼架构瓶颈的新兴计算范式，需要有效的仿真工具来评估和优化设计方案。现有PIM模拟器在保真度、可扩展性、支持技术等方面差异显著，研究人员需要系统了解这些权衡以选择合适的仿真工具。

Method: 本章采用分类综述方法，将PIM模拟器按抽象层次（从底层器件模型到应用层环境）、设计目标和评估指标进行分类，突出代表性实例。为提升可访问性，部分内容在不同上下文中重复出现以指导不同背景的读者。

Result: 系统梳理了PIM仿真工具的分类框架，识别了各类模拟器的特点和应用场景，调查了PIM研究中常用的基准测试套件，并指出了仿真方法学中存在的开放挑战。

Conclusion: PIM仿真在存内计算研究中具有关键作用，本章的综述为研究人员选择合适仿真工具提供了系统指导，并指出了未来需要更可靠、可扩展和高效的PIM建模方法的发展方向。

Abstract: Processing-in-Memory (PIM) has emerged as a promising computing paradigm to address the memory wall and the fundamental bottleneck of the von Neumann architecture by reducing costly data movement between memory and processing units. As with any engineering challenge, identifying the most effective solutions requires thorough exploration of diverse architectural proposals, device technologies, and application domains. In this context, simulation plays a critical role in enabling researchers to evaluate, compare, and refine PIM designs prior to fabrication. Over the past decade, a variety of PIM simulators have been introduced, spanning low-level device models, architectural frameworks, and application-oriented environments. These tools differ significantly in fidelity, scalability, supported memory/compute technologies, and benchmark compatibility. Understanding these trade-offs is essential for researchers to select appropriate simulators that accurately map and validate their research efforts. This chapter provides a comprehensive overview of PIM simulation methodologies and tools. We categorize simulators according to abstraction levels, design objectives, and evaluation metrics, highlighting representative examples. To improve accessibility, some content may appear in multiple contexts to guide readers with different backgrounds. We also survey benchmark suites commonly employed in PIM studies and discuss open challenges in simulation methodology, paving the way for more reliable, scalable, and efficient PIM modeling.

</details>


### [20] [An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache](https://arxiv.org/abs/2512.00112)
*Elham Cheshmikhani,Hamed Farbeh*

Main category: cs.AR

TL;DR: 本文提出了一种分析模型来确定缓存标签分区的最优分割点k，以最大化标签读取减少效果，解决了先前研究中k值选择缺乏理论依据的问题。


<details>
  <summary>Details</summary>
Motivation: 关联缓存对处理器性能和能耗有重要影响，但缓存标签阵列作为硬件管理的核心组件，既是能耗的主要来源，也容易受到故障影响。标签分区技术虽然被广泛使用来降低标签访问能耗和提高可靠性，但现有研究对关键参数k（标签分割点）的选择缺乏理论依据，通常凭直觉、随机或经验确定，导致效果不佳且无法跨配置通用。

Method: 通过分析推导出基于缓存配置参数确定最优分割点k的数学公式。该公式是凸函数且可微分，能够精确量化任意k值和配置下的标签分区效率。通过广泛的缓存设计实验验证模型准确性。

Result: 实验结果表明，分析模型与实验结果高度一致，能够准确预测标签分区效率和最优k值。模型显示选择过大或过小的k值都会显著降低标签分区效果。

Conclusion: 提出的分析模型使设计者和研究人员能够即时计算最优标签分割点，并准确估计标签读取减少量，为缓存标签分区设计提供了理论指导。

Abstract: Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.
  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction.

</details>


### [21] [From RISC-V Cores to Neuromorphic Arrays: A Tutorial on Building Scalable Digital Neuromorphic Processors](https://arxiv.org/abs/2512.00113)
*Amirreza Yousefzadeh*

Main category: cs.AR

TL;DR: 本文以SENECA平台为例，系统介绍了全数字神经形态处理器的架构设计原则，从基础RISC-V核心阵列逐步演进到专用神经处理单元，重点讨论架构权衡、性能瓶颈和软件映射技术。


<details>
  <summary>Details</summary>
Motivation: 数字神经形态处理器在低功耗、常开边缘AI应用中具有巨大潜力，但缺乏系统性的架构设计指导。本文旨在为希望设计自己数字神经形态处理器的学生和从业者提供连贯的、逐步的架构视角。

Method: 以SENECA平台为案例，从灵活的RISC-V处理核心阵列开始，逐步演进架构：从事件驱动的全连接网络实现，到添加专用神经处理单元和循环控制器，同时讨论尖峰分组、事件驱动深度优先卷积等软件映射技术。

Result: 本文未呈现新的实验结果，而是综合了先前SENECA出版物中的发现，提供了系统化的架构设计视角，重点关注架构权衡、性能瓶颈和能量瓶颈分析。

Conclusion: 数字神经形态处理器设计需要平衡灵活性和专用加速，通过渐进式架构演进和优化的软件映射技术，可以实现高效的边缘AI处理，为相关领域的设计者提供了有价值的参考框架。

Abstract: Digital neuromorphic processors are emerging as a promising computing substrate for low-power, always-on EdgeAI applications. In this tutorial paper, we outline the main architectural design principles behind fully digital neuromorphic processors and illustrate them using the SENECA platform as a running example. Starting from a flexible array of tiny RISC-V processing cores connected by a simple Network-on-Chip (NoC), we show how to progressively evolve the architecture: from a baseline event-driven implementation of fully connected networks, to versions with dedicated Neural Processing Elements (NPEs) and a loop controller that offloads fine-grained control from the general-purpose cores. Along the way, we discuss software and mapping techniques such as spike grouping, event-driven depth-first convolution for convolutional networks, and hard-attention style processing for high-resolution event-based vision. The focus is on architectural trade-offs, performance and energy bottlenecks, and on leveraging flexibility to incrementally add domain-specific acceleration. This paper assumes familiarity with basic neuromorphic concepts (spikes, event-driven computation, sparse activation) and deep neural network workloads. It does not present new experimental results; instead, it synthesizes and contextualizes findings previously reported in our SENECA publications to provide a coherent, step-by-step architectural perspective for students and practitioners who wish to design their own digital neuromorphic processors.

</details>


### [22] [Ternary-Input Binary-Weight CNN Accelerator Design for Miniature Object Classification System with Query-Driven Spatial DVS](https://arxiv.org/abs/2512.00138)
*Yuyang Li,Swasthik Muloor,Jack Laudati,Nickolas Dematteis,Yidam Park,Hana Kim,Nathan Chang,Inhee Lee*

Main category: cs.AR

TL;DR: 本文提出了一种用于微型成像系统的CNN硬件加速器，采用三值DVS输出和三元输入-二值权重神经网络，显著降低数据大小和计算需求。


<details>
  <summary>Details</summary>
Motivation: 微型成像系统受限于内存和功耗约束，而机器学习虽然能减少数据大小，但其高能耗需求往往超过小型电池的容量，因此需要设计低功耗的硬件加速器。

Method: 设计CNN硬件加速器，处理来自空间动态视觉传感器(DVS)的数据，可通过像素共享重新配置为时间DVS以减少传感器面积。采用三值DVS输出和三元输入-二值权重神经网络，降低计算和内存需求。

Result: 在28nm CMOS工艺下制造，数据大小减少81%，MAC操作减少27%。在仅1.6mW功耗下实现440ms推理时间，相比先前微型系统CNN加速器的品质因数(FoM)提升7.3倍。

Conclusion: 该硬件加速器通过创新的传感器设计和神经网络量化方法，为微型成像系统提供了高效、低功耗的物体分类解决方案，显著提升了能效比。

Abstract: Miniature imaging systems are essential for space-constrained applications but are limited by memory and power constraints. While machine learning can reduce data size by extracting key features, its high energy demands often exceed the capacity of small batteries. This paper presents a CNN hardware accelerator optimized for object classification in miniature imaging systems. It processes data from a spatial Dynamic Vision Sensor (DVS), reconfigurable to a temporal DVS via pixel sharing, minimizing sensor area. By using ternary DVS outputs and a ternary-input, binary-weight neural network, the design reduces computation and memory needs. Fabricated in 28 nm CMOS, the accelerator cuts data size by 81% and MAC operations by 27%. It achieves 440 ms inference time at just 1.6 mW power consumption, improving the Figure-of-Merit (FoM) by 7.3x over prior CNN accelerators for miniature systems.

</details>


### [23] [Variable Point: A Number Format for Area- and Energy-Efficient Multiplication of High-Dynamic-Range Numbers](https://arxiv.org/abs/2512.00186)
*Seyed Hadi Mirfarshbafan,Nicolas Filliol,Oscar Castañeda,Christoph Studer*

Main category: cs.AR

TL;DR: 提出一种新型数字格式——可变点(VP)，在保持硬件复杂度不显著增加的同时，比定点数提供更大的动态范围，适用于高动态范围信号处理。


<details>
  <summary>Details</summary>
Motivation: 定点数表示在硬件效率要求高的VLSI设计中常用，但其动态范围有限；浮点数虽提供更大动态范围但硬件复杂度高。需要一种折中方案，既能扩大动态范围又不显著增加硬件复杂度。

Method: 提出可变点(VP)数字格式，在类似位宽下比定点数提供更大动态范围。通过多天线无线通信系统中的空间均衡矩阵向量乘法引擎来验证VP格式的有效性。

Result: 后布局VLSI实现结果显示，基于VP的设计相比完全优化的定点设计，面积节省20%，功耗节省10%，且没有明显的性能下降。

Conclusion: VP数字格式为高动态范围信号处理提供了一种硬件高效的解决方案，在保持性能的同时显著降低了面积和功耗。

Abstract: Fixed-point number representation is commonly employed in digital VLSI designs that have stringent hardware efficiency constraints. However, fixed-point numbers cover a relatively small dynamic range for a given bitwidth. In contrast, floating-point numbers offer a larger dynamic range at the cost of increased hardware complexity. In this paper, we propose a novel number format called variable-point (VP). VP numbers cover a larger dynamic range than fixed-point numbers with similar bitwidth, without notably increasing hardware complexity -- this allows for a more efficient representation of signals with high dynamic range. To demonstrate the efficacy of the proposed VP number format, we consider a matrix-vector multiplication engine for spatial equalization in multi-antenna wireless communication systems involving high-dynamic-range signals. Through post-layout VLSI implementation results, we demonstrate that the proposed VP-based design achieves 20% and 10% area and power savings, respectively, compared to a fully optimized fixed-point design, without incurring any noticeable performance degradation.

</details>


### [24] [Efficient Kernel Mapping and Comprehensive System Evaluation of LLM Acceleration on a CGLA](https://arxiv.org/abs/2512.00335)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次全面评估了非AI专用粗粒度线性阵列(CGLA)加速器在Qwen大语言模型上的性能，相比GPU实现了更高的能效比


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求大，GPU能耗高，需要寻找能效与可编程性平衡的替代方案

Method: 使用非AI专用粗粒度线性阵列(CGLA)加速器，通过FPGA原型实现，基于llama.cpp框架评估，并投影到28nm ASIC性能

Result: 相比RTX 4090和Jetson AGX Orin，CGLA加速器分别提升功率延迟积44.4倍和13.6倍，降低能量延迟积11.5倍

Conclusion: 粗粒度可重构阵列是功率受限环境下LLM推理的合适平台，无需局限于特定算法，主机-加速器数据传输是主要性能瓶颈

Abstract: Large Language Models (LLMs) demand substantial computational resources, resulting in high energy consumption on GPUs. To address this challenge, we focus on Coarse-Grained Reconfigurable Arrays (CGRAs) as an effective alternative that provides a trade-off between energy efficiency and programmability. This paper presents the first comprehensive, end-to-end evaluation of a non-AI-specialized Coarse-Grained Linear Array (CGLA) accelerator for the state-of-the-art Qwen LLM family. The architecture has a general-purpose, task-agnostic design, yet its flexible instruction set allows for domain-specific adaptations. This flexibility enables the architecture to achieve high efficiency for sustainable LLM inference. We assess the performance of our architecture on an FPGA prototype using the widely adopted llama.cpp framework. We then project its potential as a 28nm ASIC and compare it against a high-performance GPU (NVIDIA RTX 4090) and an edge AI device (NVIDIA Jetson AGX Orin). While GPUs exhibit lower latency, our non-AI-specific accelerator achieves higher energy efficiency, improving the Power-Delay Product (PDP) by up to 44.4x and 13.6x compared with the RTX 4090 and Jetson, respectively. Similarly, it reduces the Energy-Delay Product (EDP) by up to 11.5x compared to the high-performance GPU, demonstrating a favorable performance-energy trade-off. Critically, our system-level analysis identifies host-accelerator data transfer as the primary performance bottleneck, a factor often overlooked in kernel-level studies. These findings provide design guidance for next-generation LLM accelerators. This work validates CGRAs as a suitable platform for LLM inference in power-constrained environments, without being confined to specific algorithms.

</details>


### [25] [A Novel 8T SRAM-Based In-Memory Computing Architecture for MAC-Derived Logical Functions](https://arxiv.org/abs/2512.00441)
*Amogh K M,Sunita M S*

Main category: cs.AR

TL;DR: 提出基于8T SRAM阵列的内存计算架构，支持多比特并行MAC运算和标准存储处理，通过电荷共享和新型解码方案实现逻辑运算


<details>
  <summary>Details</summary>
Motivation: 传统6T SRAM设计存在可靠性限制，需要开发既能执行内存计算又能保持标准存储功能的高性能架构

Method: 使用8x8 8T SRAM阵列，通过专用读位线电荷共享实现MAC运算，采用新型模数解码方案将电压输出转换为数字计数，支持多种逻辑运算

Result: 在90nm CMOS工艺、1.8V电压下，实现142.85MHz频率、0.7ns延迟、56.56fJ/bit能耗的8位MAC运算，吞吐量达15.8M操作/秒

Conclusion: 8T SRAM架构成功克服了6T设计的可靠性限制，实现了高效的内存计算和逻辑运算集成，为SRAM技术的内存计算应用提供了可行方案

Abstract: This paper presents an in-memory computing (IMC) architecture developed on an 8x8 array of 8T SRAM cells. This architecture enables both multi-bit parallel Multiply-Accumulate (MAC) operations and standard memory processing through charge-sharing on dedicated read bit-lines. By leveraging the maturity of SRAM technology, this work introduces an 8T SRAM-based IMC architecture that decouples read and write paths, thereby overcoming the reliability limitations of prior 6T SRAM designs. A novel analog-to-digital decoding scheme converts the MAC voltage output into digital counts, which are subsequently interpreted to realize fundamental logic functions including AND/NAND, NOR/OR, XOR/XNOR, and 1-bit addition within the same array. Simulated in a 90 nm CMOS process at 1.8 V supply voltage, the proposed design achieves 8-bit MAC and logical operations at a frequency of 142.85 MHz, with a latency of 0.7 ns and energy consumption of 56.56 fJ/bit per MAC operation and throughput of 15.8 M operations/s.

</details>


### [26] [Partial Cross-Compilation and Mixed Execution for Accelerating Dynamic Binary Translation](https://arxiv.org/abs/2512.00487)
*Yuhao Gu,Zhongchun Zheng,Nong Xiao,Yutong Lu,Xianwei Zhang*

Main category: cs.AR

TL;DR: 提出混合执行系统，结合编译与仿真，通过选择性函数卸载机制减少动态二进制翻译开销，实现最高13倍加速


<details>
  <summary>Details</summary>
Motivation: 随着指令集架构多样化，跨ISA程序执行变得普遍。动态二进制翻译性能差，交叉编译受限于"全有或全无"模式，完整交叉编译常不可行，导致高仿真开销

Method: 提出混合执行系统，结合编译与仿真，采用选择性函数卸载机制建立跨环境调用通道，将符合条件的函数卸载到主机进行原生执行以减少DBT开销，并优化卸载成本。基于LLVM和QEMU构建，自动适用于应用程序和库

Result: 评估显示系统相比现有DBT实现最高13倍加速，具有强大实用价值

Conclusion: 提出的混合执行系统有效解决了跨ISA执行中的性能问题，通过选择性函数卸载机制在编译与仿真间取得平衡，显著提升性能

Abstract: With the growing diversity of instruction set architectures (ISAs), cross-ISA program execution has become common. Dynamic binary translation (DBT) is the main solution but suffers from poor performance. Cross-compilation avoids emulation costs but is constrained by an "all-or-nothing" model-programs are either fully cross-compiled or entirely emulated. Complete cross-compilation is often unfeasible due to ISA-specific code or missing dependencies, leaving programs with high emulation overhead.
  We propose a hybrid execution system that combines compilation and emulation, featuring a selective function offloading mechanism. This mechanism establishes cross-environment calling channels, offloading eligible functions to the host for native execution to reduce DBT overhead. Key optimizations address offloading costs, enabling efficient hybrid operation. Built on LLVM and QEMU, the system works automatically for both applications and libraries. Evaluations show it achieves up to 13x speedups over existing DBT, with strong practical value.

</details>


### [27] [A WASM-Subset Stack Architecture for Low-cost FPGAs using Open-Source EDA Flows](https://arxiv.org/abs/2512.00974)
*Aradhya Chakrabarti*

Main category: cs.AR

TL;DR: 本文提出了一种针对资源受限FPGA优化的32位双栈微处理器架构，采用WebAssembly子集ISA实现高代码密度，使用开源工具链，通过XIP机制直接从SPI Flash执行，在Gowin GW1NR-9 FPGA上实现27MHz稳定运行。


<details>
  <summary>Details</summary>
Motivation: 传统软核处理器在资源受限FPGA上存在代码密度低、依赖专有工具链的问题。本文旨在设计一种优化的微处理器架构，解决这些限制，提供高代码密度和开源工具链支持。

Method: 设计32位双栈微处理器架构（数据栈和返回栈），采用WebAssembly子集作为指令集架构，使用开源综合流程，实现从SPI Flash的XIP执行机制，分析栈深度参数化权衡，优化FSM设计解决时序冒险。

Result: 在Gowin GW1NR-9 FPGA上实现，8项分布式RAM配置在逻辑资源利用率（约80%）和布线拥塞间取得平衡，稳定运行频率27MHz（受Flash延迟限制），成功执行单/多位中缀计算器等简单应用。

Conclusion: 该双栈微处理器架构成功解决了资源受限FPGA上软核处理器的代码密度和工具链依赖问题，通过开源流程和优化设计实现了实用性能，为低成本嵌入式系统提供了可行解决方案。

Abstract: Soft-core processors on resource-constrained FPGAs often suffer from low code density and reliance on proprietary toolchains. This paper details the design, implementation, and evaluation of a 32-bit dual-stack microprocessor architecture optimized for low-cost, resource-constrained Field-Programmable Gate Arrays (FPGAs). Implemented on the Gowin GW1NR-9 (Tang Nano 9K), the processor utilizes an instruction set architecture (ISA) inspired from a subset of the WebAssembly (WASM) specification to achieve high code density. Unlike traditional soft-cores that often rely on proprietary vendor toolchains and opaque IP blocks, this design is synthesized and routed utilizing an open-source flow, providing transparency and portability. The architecture features a dual-stack model (Data and Return), executing directly from SPI Flash via an Execute-in-Place (XIP) mechanism to conserve scarce Block RAM on the intended target device. An analysis of the trade-offs involved in stack depth parametrization is presented, demonstrating that an 8-entry distributed RAM implementation provides a balance between logic resource utilization ($\sim 80\%$) and routing congestion. Furthermore, timing hazards in single-cycle stack operations are identified and resolved through a refined Finite State Machine (FSM) design. The system achieves a stable operating frequency of 27 MHz, limited by Flash latency, and successfully executes simple applications including a single and multi-digit infix calculator.

</details>


### [28] [Leveraging Recurrent Patterns in Graph Accelerators](https://arxiv.org/abs/2512.01193)
*Masoud Rahimi,Sébastien Le Beux*

Main category: cs.AR

TL;DR: 提出一种基于ReRAM的图加速器优化方法，通过识别频繁子图模式并静态分配给计算引擎，减少内存写入操作，提升性能、降低能耗并延长电路寿命


<details>
  <summary>Details</summary>
Motivation: 现有基于ReRAM的图加速器在处理大规模稀疏图时，由于大量图分区导致内存访问开销大，增加了执行时间、能耗，并缩短了电路寿命

Method: 识别频繁出现的子图模式，将其静态分配给图计算引擎，使大多数子图处理无需重新配置交叉开关，从而最小化内存写入操作

Result: 相比最先进的加速器，实现了最高2.38倍的加速和7.23倍的能耗节省，同时将电路寿命延长了2倍

Conclusion: 通过减少内存写入操作，提出的方法显著提升了图处理性能、降低了能耗，并有效延长了ReRAM图加速器的电路寿命

Abstract: Graph accelerators have emerged as a promising solution for processing large-scale sparse graphs, leveraging the in-situ compu-tation of ReRAM-based crossbars to maximize computational efficiency. However, existing designs suffer from memristor access overhead due to the large number of graph partitions. This leads to increased execution time, higher energy consumption, and re-duced circuit lifetime. This paper proposes a graph processing method that minimizes memristor write operations by identifying frequent subgraph patterns and assigning them to graph engines, referred to as static, allowing most subgraphs to be processed without a need for crossbar reconfiguration. Experimental results show speed up to 2.38x speedup and 7.23x energy savings com-pared to state-of-the-art accelerators. Furthermore, our method extends the circuit lifetime by 2x compared to state-of-the-art ReRAM graph accelerators.

</details>


### [29] [hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware](https://arxiv.org/abs/2512.01463)
*Jan-Frederik Schulte,Benjamin Ramhorst,Chang Sun,Jovan Mitrevski,Nicolò Ghielmetti,Enrico Lupi,Dimitrios Danopoulos,Vladimir Loncar,Javier Duarte,David Burnette,Lauri Laatu,Stylianos Tzelepis,Konstantinos Axiotis,Quentin Berthet,Haoyan Wang,Paul White,Suleyman Demirsoy,Marco Colombo,Thea Aarrestad,Sioni Summers,Maurizio Pierini,Giuseppe Di Guglielmo,Jennifer Ngadiuba,Javier Campos,Ben Hawks,Abhijith Gandrakota,Farah Fahim,Nhan Tran,George Constantinides,Zhiqiang Que,Wayne Luk,Alexander Tapper,Duc Hoang,Noah Paladino,Philip Harris,Bo-Cheng Lai,Manuel Valentin,Ryan Forelli,Seda Ogrenci,Lino Gerlach,Rian Flynn,Mia Liu,Daniel Diaz,Elham Khoda,Melissa Quinnan,Russell Solares,Santosh Parajuli,Mark Neubauer,Christian Herwig,Ho Fung Tsoi,Dylan Rankin,Shih-Chieh Hsu,Scott Hauck*

Main category: cs.AR

TL;DR: hls4ml是一个开源平台，可将深度学习模型转换为HLS代码，用于FPGA/ASIC部署，支持多种框架和编译器，适用于低延迟、低功耗应用。


<details>
  <summary>Details</summary>
Motivation: 在商业和科学应用中，机器学习推理需要低延迟、低资源使用和低功耗，而传统软件实现难以满足这些要求。需要将ML模型高效部署到FPGA或ASIC硬件上。

Method: 开发了一个模块化、灵活的hls4ml平台，能够从现代深度学习框架（如TensorFlow、PyTorch等）翻译机器学习模型，生成高级综合（HLS）代码，支持多种HLS编译器（Vitis HLS、Intel oneAPI、Catapult HLS）。

Result: hls4ml支持多种深度学习框架和HLS编译器，已成功应用于广泛的商业和科学应用，实现了ML推理的硬件加速，满足了低延迟、低资源使用和低功耗的需求。

Conclusion: hls4ml是一个有效的软硬件协同设计平台，能够将机器学习模型高效部署到FPGA/ASIC硬件上，为需要高性能、低功耗ML推理的应用提供了重要解决方案。

Abstract: We present hls4ml, a free and open-source platform that translates machine learning (ML) models from modern deep learning frameworks into high-level synthesis (HLS) code that can be integrated into full designs for field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs). With its flexible and modular design, hls4ml supports a large number of deep learning frameworks and can target HLS compilers from several vendors, including Vitis HLS, Intel oneAPI and Catapult HLS. Together with a wider eco-system for software-hardware co-design, hls4ml has enabled the acceleration of ML inference in a wide range of commercial and scientific applications where low latency, resource usage, and power consumption are critical. In this paper, we describe the structure and functionality of the hls4ml platform. The overarching design considerations for the generated HLS code are discussed, together with selected performance results.

</details>


### [30] [RoMe: Row Granularity Access Memory System for Large Language Models](https://arxiv.org/abs/2512.01541)
*Hwayong Nam,Seungmin Baek,Jumin Kim,Michael Jaemin Kim,Jung Ho Ahn*

Main category: cs.AR

TL;DR: RoMe提出了一种面向大语言模型的新型内存架构，通过以行粒度访问DRAM、简化内存接口设计，在最小硬件开销下提升带宽12.5%


<details>
  <summary>Details</summary>
Motivation: 现代HBM内存系统保持缓存行粒度访问，引入了bank组和伪通道等复杂结构，增加了内存控制器调度复杂度。大语言模型需要连续数据块传输（KB到MB级），但在传统HBM系统中被分割成数百个32B缓存行事务，导致调度效率低下

Method: RoMe以行粒度访问DRAM，从内存接口中移除列、bank组和伪通道，简化内存调度逻辑。释放的引脚被聚合形成额外通道，在最小额外引脚开销下增加带宽

Result: RoMe在最小硬件开销下实现了12.5%的带宽提升，显著简化了内存调度逻辑，为下一代HBM内存系统提供了替代方案

Conclusion: RoMe展示了针对大语言模型工作负载可以显著简化内存调度逻辑，并提出了一种以最小硬件开销增加带宽的下一代HBM内存系统设计方法

Abstract: Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.
  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.

</details>


### [31] [A Systematic Characterization of LLM Inference on GPUs](https://arxiv.org/abs/2512.01644)
*Haonan Wang,Xuxin Xiao,Mingyu Yan,Zhuoyuan Zhu,Dengke Han,Duo Wang,Wenming Li,Xiaochun Ye,Cunchen Hu,Hongyang Chen,Guangyu Sun*

Main category: cs.AR

TL;DR: 该研究系统性地分析了大型语言模型推理性能，提出了四维分析框架，从观察到预测，为LLM推理提供实证基础和优化指导。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型推理的理解较为碎片化，缺乏系统性分析。研究旨在建立全面的分析框架，从现象观察到根本原因分析，为LLM推理提供可靠的实证基础和实践指导。

Method: 通过综合实验建立四维分析框架：1) 两阶段异构性观察；2) 微架构根本原因分析；3) 系统扩展原则；4) 新兴范式边界。研究从观察到预测系统推进：识别性能现象、揭示硬件原因、验证系统行为、探索新范式。

Result: 建立了可靠的LLM推理实证基础，提供了新的发现和实用的优化指导。研究不仅巩固了现有研究的实证基础，还为LLM推理系统设计提供了系统性分析框架。

Conclusion: 该研究通过系统性分析LLM推理，建立了从观察到预测的四维分析框架，为LLM推理提供了可靠的实证基础、新的发现和实用的优化指导，填补了该领域系统性分析的空白。

Abstract: This work presents a systematic characterization of Large Language Model (LLM) inference to address fragmented understanding. Through comprehensive experiments, we establish a four-dimensional analytical framework: (1) Two-Phase Heterogeneity Observation; (2) Microarchitectural Root Cause Analysis; (3) System Scaling Principles; and (4) Emerging Paradigm Boundaries. Our investigation progresses systematically from observation to foresight: identifying performance phenomena, revealing hardware causes, validating system behavior, and exploring new paradigms. This study not only consolidates a reliable empirical foundation for existing research but also provides new discoveries and practical optimization guidance for LLM inference.

</details>
