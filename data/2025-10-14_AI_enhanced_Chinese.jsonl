{"id": "2510.10225", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10225", "abs": "https://arxiv.org/abs/2510.10225", "authors": ["Jialin Sun", "Yuchen Hu", "Dean You", "Yushu Du", "Hui Wang", "Xinwei Fang", "Weiwei Shan", "Nan Guan", "Zhe Jiang"], "title": "ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism", "comment": null, "summary": "Functional verification is a critical bottleneck in integrated circuit\ndevelopment, with CPU verification being especially time-intensive and\nlabour-consuming. Industrial practice relies on differential testing for CPU\nverification, yet faces bottlenecks at nearly each stage of the framework\npipeline: front-end stimulus generation lacks micro-architectural awareness,\nyielding low-quality and redundant tests that impede coverage closure and miss\ncorner cases. Meanwhile, back-end simulation infrastructure, even with FPGA\nacceleration, often stalls on long-running tests and offers limited visibility,\ndelaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a\nfull-stack, Large Language Model (LLM)-aided CPU verification framework with\nFPGA parallelism, from bug categorisation and stimulus generation to simulation\ninfrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's\nfront-end, infused with micro-architectural knowledge and historical bug\npatterns, generating highly targeted tests that rapidly achieve coverage goals\nand capture elusive corner cases. In ISAAC's back-end, we introduce a\nlightweight forward-snapshot mechanism and a decoupled co-simulation\narchitecture between the Instruction Set Simulator (ISS) and the Design Under\nTest (DUT), enabling a single ISS to drive multiple DUTs in parallel. By\neliminating long-tail test bottlenecks and exploiting FPGA parallelism, the\nsimulation throughput is significantly improved. As a demonstration, we used\nISAAC to verify a mature CPU that has undergone multiple successful tape-outs.\nResults show up to 17,536x speed-up over software RTL simulation, while\ndetecting several previously unknown bugs, two of which are reported in this\npaper.", "AI": {"tldr": "ISAAC\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684CPU\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u6fc0\u52b1\u751f\u6210\u548cFPGA\u5e76\u884c\u4eff\u771f\uff0c\u663e\u8457\u63d0\u5347\u9a8c\u8bc1\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u4f20\u7edfCPU\u9a8c\u8bc1\u65b9\u6cd5\u5b58\u5728\u74f6\u9888\uff1a\u524d\u7aef\u6fc0\u52b1\u751f\u6210\u7f3a\u4e4f\u5fae\u67b6\u6784\u611f\u77e5\u5bfc\u81f4\u6d4b\u8bd5\u8d28\u91cf\u4f4e\uff0c\u540e\u7aef\u4eff\u771f\u57fa\u7840\u8bbe\u65bd\u5373\u4f7f\u4f7f\u7528FPGA\u52a0\u901f\u4e5f\u4f1a\u56e0\u957f\u65f6\u95f4\u6d4b\u8bd5\u800c\u505c\u6ede\uff0c\u53cd\u9988\u5ef6\u8fdf\u4e14\u8c03\u8bd5\u5468\u671f\u957f\u3002", "method": "ISAAC\u91c7\u7528\u5168\u6808LLM\u8f85\u52a9\u9a8c\u8bc1\u6846\u67b6\uff1a\u524d\u7aef\u4f7f\u7528\u6ce8\u5165\u5fae\u67b6\u6784\u77e5\u8bc6\u548c\u5386\u53f2bug\u6a21\u5f0f\u7684\u591a\u4ee3\u7406\u6fc0\u52b1\u5f15\u64ce\u751f\u6210\u9488\u5bf9\u6027\u6d4b\u8bd5\uff1b\u540e\u7aef\u5f15\u5165\u8f7b\u91cf\u7ea7\u524d\u5411\u5feb\u7167\u673a\u5236\u548c\u89e3\u8026\u7684\u534f\u540c\u4eff\u771f\u67b6\u6784\uff0c\u4f7f\u5355\u4e2a\u6307\u4ee4\u96c6\u4eff\u771f\u5668\u53ef\u5e76\u884c\u9a71\u52a8\u591a\u4e2a\u88ab\u6d4b\u8bbe\u8ba1\u3002", "result": "\u5728\u6210\u719fCPU\u9a8c\u8bc1\u4e2d\uff0c\u76f8\u6bd4\u8f6f\u4ef6RTL\u4eff\u771f\u5b9e\u73b0\u6700\u9ad817,536\u500d\u52a0\u901f\uff0c\u5e76\u68c0\u6d4b\u5230\u591a\u4e2a\u5148\u524d\u672a\u77e5\u7684bug\u3002", "conclusion": "ISAAC\u901a\u8fc7\u6d88\u9664\u957f\u5c3e\u6d4b\u8bd5\u74f6\u9888\u548c\u5229\u7528FPGA\u5e76\u884c\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eff\u771f\u541e\u5410\u91cf\uff0c\u4e3aCPU\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10623", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10623", "abs": "https://arxiv.org/abs/2510.10623", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration", "comment": null, "summary": "Transformers are at the core of modern AI nowadays. They rely heavily on\nmatrix multiplication and require efficient acceleration due to their\nsubstantial memory and computational requirements. Quantization plays a vital\nrole in reducing memory usage, and can be exploited for computations by\ndesigning reconfigurable architectures that enhance matrix multiplication by\ndynamically adjusting the precision. This paper proposes ADiP, a novel\nadaptive-precision systolic array architecture designed for efficient matrix\nmultiplication acceleration.The proposed architecture consists of NxN\nadaptive-precision processing elements (PEs) and shared accumulators. ADiP\nsupports multiple computation modes, including symmetric single-matrix\nmultiplication as well as asymmetric multi-matrix multiplication with a shared\ninput matrix, thereby improving data-reuse and PE utilization. In addition,\nADiP maximizes the computational density by adapting to different precisions,\nsuch as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed\nfor ADiP architecture, including latency and throughput for versatile\narchitecture configurations. A comprehensive hardware design space exploration\nis demonstrated using 22nm commercial technology, achieving up to a 4x higher\ncomputational throughput. Furthermore, ADiP is evaluated on different\ntransformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,\ndelivering latency improvement up to 53.6%, and energy improvement up to 24.4%\nfor BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a\npeak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,\n8bitx4bit, and 8bitx2bit operations, respectively.", "AI": {"tldr": "ADiP\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u67b6\u6784\uff0c\u4e13\u95e8\u4e3a\u9ad8\u6548\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u800c\u8bbe\u8ba1\uff0c\u652f\u6301\u591a\u79cd\u8ba1\u7b97\u6a21\u5f0f\u548c\u7cbe\u5ea6\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347Transformer\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u6548\u3002", "motivation": "Transformer\u6a21\u578b\u5bf9\u77e9\u9635\u4e58\u6cd5\u6709\u5de8\u5927\u9700\u6c42\uff0c\u9700\u8981\u9ad8\u6548\u52a0\u901f\u6765\u5e94\u5bf9\u5176\u5185\u5b58\u548c\u8ba1\u7b97\u8981\u6c42\u3002\u91cf\u5316\u6280\u672f\u53ef\u4ee5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u4f46\u9700\u8981\u53ef\u91cd\u6784\u67b6\u6784\u6765\u52a8\u6001\u8c03\u6574\u7cbe\u5ea6\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u3002", "method": "\u63d0\u51faADiP\u67b6\u6784\uff0c\u5305\u542bNxN\u81ea\u9002\u5e94\u7cbe\u5ea6\u5904\u7406\u5355\u5143\u548c\u5171\u4eab\u7d2f\u52a0\u5668\uff0c\u652f\u6301\u5bf9\u79f0\u5355\u77e9\u9635\u4e58\u6cd5\u548c\u4e0d\u5bf9\u79f0\u591a\u77e9\u9635\u4e58\u6cd5\uff0c\u80fd\u591f\u9002\u5e948bitx8bit\u30018bitx4bit\u30018bitx2bit\u7b49\u4e0d\u540c\u7cbe\u5ea6\u3002", "result": "\u572822nm\u5546\u7528\u6280\u672f\u4e0b\uff0cADiP\u5b9e\u73b0\u4e86\u9ad8\u8fbe4\u500d\u7684\u8ba1\u7b97\u541e\u5410\u91cf\u63d0\u5347\u3002\u5728GPT-2 Medium\u3001BERT Large\u548cBitNet-1.58B\u6a21\u578b\u4e0a\uff0c\u5ef6\u8fdf\u6539\u5584\u6700\u9ad8\u8fbe53.6%\uff0cBitNet-1.58B MHA\u5de5\u4f5c\u8d1f\u8f7d\u7684\u80fd\u6548\u6539\u5584\u8fbe24.4%\u300264x64\u89c4\u6a21\u4e0b\u5cf0\u503c\u541e\u5410\u91cf\u5206\u522b\u4e3a8.192 TOPS\u300116.384 TOPS\u548c32.768 TOPS\u3002", "conclusion": "ADiP\u67b6\u6784\u901a\u8fc7\u81ea\u9002\u5e94\u7cbe\u5ea6\u548c\u591a\u79cd\u8ba1\u7b97\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u77e9\u9635\u4e58\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u6548\uff0c\u7279\u522b\u9002\u7528\u4e8e\u73b0\u4ee3AI\u4e2d\u7684Transformer\u6a21\u578b\u52a0\u901f\u3002"}}
{"id": "2510.10676", "categories": ["cs.AR", "cs.CL", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10676", "abs": "https://arxiv.org/abs/2510.10676", "authors": ["Mukul Lokhande", "Tanushree Dewangan", "Mohd Sharik Mansoori", "Tejas Chaudhari", "Akarsh J.", "Damayanti Lokhande", "Adam Teman", "Santosh Kumar Vishvakarma"], "title": "Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation", "comment": null, "summary": "This paper introduces Bhasha-Rupantarika, a light and efficient multilingual\ntranslation system tailored through algorithm-hardware codesign for\nresource-limited settings. The method investigates model deployment at\nsub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental\nresults indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in\ninference speed, which correlates with an increased throughput of 66 tokens/s\n(improvement by 4.8x). This underscores the importance of ultra-low precision\nquantization for real-time deployment in IoT devices using FPGA accelerators,\nachieving performance on par with expectations. Our evaluation covers\nbidirectional translation between Indian and international languages,\nshowcasing its adaptability in low-resource linguistic contexts. The FPGA\ndeployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,\nresulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x\nenhancement compared to HPTA. Overall, the evaluation provides a viable\nsolution based on quantisation-aware translation along with hardware efficiency\nsuitable for deployable multilingual AI systems. The entire codes\n[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for\nreproducibility are publicly available, facilitating rapid integration and\nfurther development by researchers.", "AI": {"tldr": "Bhasha-Rupantarika\u662f\u4e00\u4e2a\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u9ad8\u6548\u591a\u8bed\u8a00\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u4e13\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4f18\u5316\u3002\u8be5\u7cfb\u7edf\u5728FPGA\u4e0a\u90e8\u7f72\u8d85\u4f4e\u7cbe\u5ea6\u91cf\u5316\u6a21\u578b\uff08FP8/INT8/INT4/FP4\uff09\uff0c\u5b9e\u73b0\u4e864.1\u500d\u6a21\u578b\u538b\u7f29\u548c4.2\u500d\u63a8\u7406\u52a0\u901f\uff0c\u7279\u522b\u9002\u7528\u4e8eIoT\u8bbe\u5907\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982IoT\u8bbe\u5907\uff09\u4e2d\u591a\u8bed\u8a00\u7ffb\u8bd1\u7cfb\u7edf\u7684\u90e8\u7f72\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4f20\u7edf\u6a21\u578b\u5728\u8ba1\u7b97\u8d44\u6e90\u548c\u5b58\u50a8\u65b9\u9762\u8981\u6c42\u8f83\u9ad8\uff0c\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7814\u7a76\u5728FPGA\u52a0\u901f\u5668\u4e0a\u90e8\u7f72\u8d85\u4f4e\u7cbe\u5ea6\u91cf\u5316\u6a21\u578b\uff08FP8\u3001INT8\u3001INT4\u3001FP4\uff09\u3002\u7cfb\u7edf\u652f\u6301\u5370\u5ea6\u8bed\u8a00\u4e0e\u56fd\u9645\u8bed\u8a00\u4e4b\u95f4\u7684\u53cc\u5411\u7ffb\u8bd1\uff0c\u7279\u522b\u5173\u6ce8\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u3002", "result": "FP4\u91cf\u5316\u5b9e\u73b04.1\u500d\u6a21\u578b\u5927\u5c0f\u7f29\u51cf\u548c4.2\u500d\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u541e\u5410\u91cf\u8fbe66 tokens/s\uff08\u63d0\u53474.8\u500d\uff09\u3002FPGA\u90e8\u7f72\u51cf\u5c111.96\u500dLUTs\u548c1.65\u500dFFs\uff0c\u76f8\u6bd4OPU\u548cHPTA\u5206\u522b\u63d0\u53472.2\u500d\u548c4.6\u500d\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8d85\u4f4e\u7cbe\u5ea6\u91cf\u5316\u5728FPGA\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u53ef\u90e8\u7f72\u7684\u591a\u8bed\u8a00AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\uff0c\u4fbf\u4e8e\u7814\u7a76\u4eba\u5458\u5feb\u901f\u96c6\u6210\u548c\u8fdb\u4e00\u6b65\u5f00\u53d1\u3002"}}
{"id": "2510.10872", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10872", "abs": "https://arxiv.org/abs/2510.10872", "authors": ["Sumukh Pinge", "Ashkan Moradifirouzabadi", "Keming Fan", "Prasanna Venkatesan Ravindran", "Tanvir H. Pantha", "Po-Kai Hsu", "Zheyu Li", "Weihong Xu", "Zihan Xia", "Flavio Ponzina", "Winston Chern", "Taeyoung Song", "Priyankka Ravikumar", "Mengkun Tian", "Lance Fernandes", "Huy Tran", "Hari Jayasankar", "Hang Chen", "Chinsung Park", "Amrit Garlapati", "Kijoon Kim", "Jongho Woo", "Suhwan Lim", "Kwangsoo Kim", "Wanki Kim", "Daewon Ha", "Duygu Kuzum", "Shimeng Yu", "Sourav Dutta", "Asif Khan", "Tajana Rosing", "Mingu Kang"], "title": "FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash", "comment": null, "summary": "The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of\nterabytes, poses significant challenges for efficient, large-scale library\nsearch - a critical component for drug discovery. Traditional processors\nstruggle to handle this data volume efficiently, making in-storage computing\n(ISP) a promising alternative. This work introduces an ISP architecture\nleveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly\nhigher density, faster speeds, and lower voltage requirements compared to\ntraditional NAND flash. Despite its superior density, the NAND structure has\nnot been widely utilized in ISP applications due to limited throughput\nassociated with row-by-row reads from serially connected cells. To overcome\nthese limitations, we integrate hyperdimensional computing (HDC), a\nbrain-inspired paradigm that enables highly parallel processing with simple\noperations and strong error tolerance. By combining HDC with the proposed\ndual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND\nstructure, we parallelize vector computations to enable efficient MS spectral\nlibrary search, achieving 43x speedup and 21x higher energy efficiency over\nstate-of-the-art 3D NAND methods, while maintaining comparable accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u94c1\u7535NAND\u5b58\u50a8\u5668\u548c\u8d85\u7ef4\u8ba1\u7b97\u67b6\u6784\u7684\u8d28\u8c31\u6570\u636e\u641c\u7d22\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8643\u500d\u52a0\u901f\u548c21\u500d\u80fd\u6548\u63d0\u5347", "motivation": "\u8d28\u8c31\u6570\u636e\u5feb\u901f\u589e\u957f\u81f3\u6570\u767eTB\u89c4\u6a21\uff0c\u4f20\u7edf\u5904\u7406\u5668\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u5e93\u641c\u7d22\uff0c\u9700\u8981\u65b0\u7684\u5b58\u50a8\u8ba1\u7b97\u67b6\u6784", "method": "\u7ed3\u54083D\u94c1\u7535NAND\u5b58\u50a8\u5668\u548c\u8d85\u7ef4\u8ba1\u7b97\uff0c\u91c7\u7528\u53cc\u8fb9\u754c\u8fd1\u4f3c\u5339\u914d\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5e76\u884c\u5316\u5411\u91cf\u8ba1\u7b97", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u76843D NAND\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8643\u500d\u901f\u5ea6\u63d0\u5347\u548c21\u500d\u80fd\u6548\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u51c6\u786e\u6027", "conclusion": "\u8be5\u5b58\u50a8\u8ba1\u7b97\u67b6\u6784\u4e3a\u5927\u89c4\u6a21\u8d28\u8c31\u6570\u636e\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.11192", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11192", "abs": "https://arxiv.org/abs/2510.11192", "authors": ["Jo\u00e3o Paulo Cardoso de Lima", "Marc Dietrich", "Jeronimo Castrillon", "Asif Ali Khan"], "title": "Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs", "comment": "8 pages, to appear in IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Structured sparsity enables deploying large language models (LLMs) on\nresource-constrained systems. Approaches like dense-to-sparse fine-tuning are\nparticularly compelling, achieving remarkable structured sparsity by reducing\nthe model size by over 6.7x, while still maintaining acceptable accuracy.\nDespite this reduction, LLM inference, especially the decode stage being\ninherently memory-bound, is extremely expensive on conventional Von-Neumann\narchitectures. Compute-in-memory (CIM) architectures mitigate this by\nperforming computations directly in memory, and when paired with sparse LLMs,\nenable storing and computing the entire model in memory, eliminating the data\nmovement on the off-chip bus and improving efficiency. Nonetheless, naively\nmapping sparse matrices onto CIM arrays leads to poor array utilization and\ndiminished computational efficiency. In this paper, we present an automated\nframework with novel mapping and scheduling strategies to accelerate sparse LLM\ninference on CIM accelerators. By exploiting block-diagonal sparsity, our\napproach improves CIM array utilization by over 50%, achieving more than 4x\nreduction in both memory footprint and the number of required floating-point\noperations.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6620\u5c04\u548c\u8c03\u5ea6\u7b56\u7565\u5728\u5b58\u5185\u8ba1\u7b97\u52a0\u901f\u5668\u4e0a\u52a0\u901f\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u5229\u7528\u5757\u5bf9\u89d2\u7a00\u758f\u6027\u63d0\u9ad8\u9635\u5217\u5229\u7528\u7387\u8d85\u8fc750%\uff0c\u51cf\u5c114\u500d\u4ee5\u4e0a\u5185\u5b58\u5360\u7528\u548c\u6d6e\u70b9\u8fd0\u7b97\u3002", "motivation": "\u7ed3\u6784\u5316\u7a00\u758f\u6027\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u4e0a\u90e8\u7f72\uff0c\u4f46\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u4e0a\u63a8\u7406\u6210\u672c\u9ad8\u6602\u3002\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u901a\u8fc7\u5185\u5b58\u4e2d\u76f4\u63a5\u8ba1\u7b97\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u7a00\u758f\u77e9\u9635\u5728CIM\u9635\u5217\u4e0a\u7684\u7b80\u5355\u6620\u5c04\u4f1a\u5bfc\u81f4\u9635\u5217\u5229\u7528\u7387\u4f4e\u548c\u8ba1\u7b97\u6548\u7387\u4e0b\u964d\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u91c7\u7528\u65b0\u9896\u7684\u6620\u5c04\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u5229\u7528\u5757\u5bf9\u89d2\u7a00\u758f\u6027\u6765\u4f18\u5316\u7a00\u758fLLM\u5728CIM\u52a0\u901f\u5668\u4e0a\u7684\u90e8\u7f72\u3002", "result": "CIM\u9635\u5217\u5229\u7528\u7387\u63d0\u9ad8\u8d85\u8fc750%\uff0c\u5185\u5b58\u5360\u7528\u548c\u6d6e\u70b9\u8fd0\u7b97\u6570\u91cf\u5747\u51cf\u5c114\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758fLLM\u5728CIM\u52a0\u901f\u5668\u4e0a\u7684\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
