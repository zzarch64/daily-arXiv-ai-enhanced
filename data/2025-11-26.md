<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [CAMformer: Associative Memory is All You Need](https://arxiv.org/abs/2511.19740)
*Tergel Molom-Ochir,Benjamin F. Morris,Mark Horton,Chiyue Wei,Cong Guo,Brady Taylor,Peter Liu,Shan X. Wang,Deliang Fan,Hai Helen Li,Yiran Chen*

Main category: cs.AR

TL;DR: CAMformer是一种新型Transformer加速器，通过将注意力重新解释为关联内存操作，使用电压域二元注意力内容可寻址存储器(BA-CAM)计算注意力分数，实现恒定时间相似性搜索，显著提升能效和吞吐量。


<details>
  <summary>Details</summary>
Motivation: Transformer面临可扩展性挑战，因为注意力机制需要查询和键之间的密集相似性计算，导致二次计算成本。

Method: 采用BA-CAM通过模拟电荷共享计算注意力分数，用物理相似性感知替代数字算术；集成层次化两阶段top-k过滤、流水线执行和高精度上下文化。

Result: 在BERT和Vision Transformer工作负载上评估，CAMformer实现了超过10倍的能效提升、高达4倍的吞吐量提升，以及6-8倍的面积减少，同时保持近乎无损的精度。

Conclusion: CAMformer通过重新设计注意力计算范式，在保持算法精度的同时实现了显著的架构效率提升，为Transformer的可扩展性提供了有效解决方案。

Abstract: Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.

</details>


### [2] [Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher](https://arxiv.org/abs/2511.19973)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: Pickle Prefetcher是一个可编程的LLC预取器，通过软件定义预取策略来处理不规则内存访问模式，相比传统预取技术显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代高性能架构使用大型末级缓存，但传统基于预测的预取器难以处理现代应用中普遍存在的不规则内存访问模式。

Method: 采用软件可编程接口，让软件定义自己的预取策略，而不依赖静态启发式或复杂预测算法，将硬件预测的逻辑复杂性转化为软件可编程性。

Result: 在gem5全系统模拟中，Pickle Prefetcher在GAPBS广度优先搜索实现上比基线系统获得最高1.74倍加速，与私有缓存预取器结合时比仅使用私有缓存预取器的系统提供最高1.40倍加速。

Conclusion: 通过软件可编程性，Pickle Prefetcher能够有效处理不规则内存访问模式，在保持硬件资源效率的同时显著提升性能。

Abstract: Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.

</details>


### [3] [R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation](https://arxiv.org/abs/2511.20090)
*Zizhang Luo,Fan Cui,Kexing Zhou,Runlin Guo,Mile Xia,Hongyuan Hou,Yun Lian*

Main category: cs.AR

TL;DR: R3A是一个基于大语言模型的RTL自动程序修复框架，通过随机树搜索和多智能体故障定位提高修复可靠性，在RTL-repair数据集上修复了90.6%的bug，比传统方法多修复45%的bug。


<details>
  <summary>Details</summary>
Motivation: 传统自动程序修复方法依赖固定模板，只能处理有限的bug；而大语言模型虽然能理解代码语义，但由于随机性和RTL代码及波形的长输入上下文，修复结果不可靠。

Method: 提出R3A框架：1）使用随机树搜索方法控制补丁生成智能体探索验证解决方案；2）基于启发式函数采样搜索状态以平衡探索与利用；3）多智能体故障定位方法找到故障候选点作为补丁生成的起点。

Result: 在给定时间限制内修复了RTL-repair数据集中90.6%的bug，比传统方法和其他基于LLM的方法多修复45%的bug，平均达到86.7%的pass@5率。

Conclusion: R3A通过结合随机树搜索和多智能体故障定位，显著提高了基于LLM的RTL程序修复的可靠性，在修复率和成功率方面都优于现有方法。

Abstract: Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.

</details>
