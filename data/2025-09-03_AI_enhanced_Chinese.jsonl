{"id": "2509.00433", "categories": ["cs.AR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00433", "abs": "https://arxiv.org/abs/2509.00433", "authors": ["Houshu He", "Naifeng Jing", "Li Jiang", "Xiaoyao Liang", "Zhuoran Song"], "title": "AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection", "comment": "15 pages", "summary": "Simultaneous Localization and Mapping (SLAM) is a critical task that enables\nautonomous vehicles to construct maps and localize themselves in unknown\nenvironments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting\n(3DGS) to achieve exceptional reconstruction fidelity. However, existing\n3DGS-SLAM systems provide insufficient throughput due to the need for multiple\ntraining iterations per frame and the vast number of Gaussians.\n  In this paper, we propose AGS, an algorithm-hardware co-design framework to\nboost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems\nprocess frames in a streaming manner, where adjacent frames exhibit high\nsimilarity that can be utilized for acceleration. On the software level: 1) We\npropose a coarse-then-fine-grained pose tracking method with respect to the\nrobot's movement. 2) We avoid redundant computations of Gaussians by sharing\ntheir contribution information across frames. On the hardware level, we propose\na frame covisibility detection engine to extract intermediate data from the\nvideo CODEC. We also implement a pose tracking engine and a mapping engine with\nworkload schedulers to efficiently deploy the AGS algorithm. Our evaluation\nshows that AGS achieves up to $17.12\\times$, $6.71\\times$, and $5.41\\times$\nspeedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS\naccelerator, GSCore.", "AI": {"tldr": "AGS\u662f\u4e00\u4e2a\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528SLAM\u7cfb\u7edf\u4e2d\u76f8\u90bb\u5e27\u7684\u9ad8\u76f8\u4f3c\u6027\u6765\u52a0\u901f3D\u9ad8\u65af\u6e85\u5c04SLAM\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u76843DGS-SLAM\u7cfb\u7edf\u7531\u4e8e\u6bcf\u5e27\u9700\u8981\u591a\u6b21\u8bad\u7ec3\u8fed\u4ee3\u548c\u6d77\u91cf\u9ad8\u65af\u8ba1\u7b97\uff0c\u5bfc\u81f4\u541e\u5410\u91cf\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "method": "\u8f6f\u4ef6\u5c42\u9762\uff1a1\uff09\u63d0\u51fa\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u4f4d\u59ff\u8ddf\u8e2a\u65b9\u6cd5\uff1b2\uff09\u901a\u8fc7\u8de8\u5e27\u5171\u4eab\u9ad8\u65af\u8d21\u732e\u4fe1\u606f\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u3002\u786c\u4ef6\u5c42\u9762\uff1a\u8bbe\u8ba1\u5e27\u5171\u89c6\u6027\u68c0\u6d4b\u5f15\u64ce\u3001\u4f4d\u59ff\u8ddf\u8e2a\u5f15\u64ce\u548c\u5efa\u56fe\u5f15\u64ce\uff0c\u5e76\u914d\u5907\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u3002", "result": "AGS\u76f8\u6bd4\u79fb\u52a8\u548c\u9ad8\u6027\u80fdGPU\u4ee5\u53ca\u6700\u5148\u8fdb\u76843DGS\u52a0\u901f\u5668GSCore\uff0c\u5206\u522b\u5b9e\u73b0\u4e8617.12\u500d\u30016.71\u500d\u548c5.41\u500d\u7684\u52a0\u901f\u6bd4\u3002", "conclusion": "AGS\u6846\u67b6\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u5229\u7528\u4e86SLAM\u7cfb\u7edf\u4e2d\u5e27\u95f4\u76f8\u4f3c\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6e85\u5c04SLAM\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00500", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.00500", "abs": "https://arxiv.org/abs/2509.00500", "authors": ["Yizhi Chen", "Jingwei Li", "Wenyao Zhu", "Zhonghai Lu"], "title": "Bit Transition Reduction by Data Transmission Ordering in NoC-based DNN Accelerator", "comment": "Accepted to IEEE SoCC 2025 (38th IEEE International System-on-Chip\n  Conference)", "summary": "As Deep Neural Networks (DNN) are becoming essential, Network-on-Chip\n(NoC)-based DNN accelerators gained increasing popularity. To save link power\nin NoC, many researchers focus on reducing the Bit Transition (BT). We propose\n'1'-bit count-based ordering method to reduce BT for DNN workloads. We provide\na mathematical proof of the efficacy of proposed ordering. We evaluate our\nmethod through experiments without NoC and with NoC. Without NoC, our proposed\nordering method achieves up to 20.38% BT reduction for floating-point-32 data\nand 55.71% for fixed-point-8 data, respectively. We propose two data ordering\nmethods, affiliated-ordering and separated-ordering to process weight and input\njointly or individually and apply them to run full DNNs in NoC-based DNN\naccelerator. We evaluate our approaches under various configurations, including\ndifferent DNN models such as LeNet and DarkNet, various NoC sizes with\ndifferent numbers of memory controllers, random weights and trained weights,\nand different data precision. Our approach efficiently reduces the link power\nby achieving up to 32.01% BT reduction for floating-point-32 data and 40.85% BT\nreduction for fixed-point-8 data.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e'1'\u6bd4\u7279\u8ba1\u6570\u7684\u6392\u5e8f\u65b9\u6cd5\u6765\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u4e2dNoC\u94fe\u8def\u7684\u6bd4\u7279\u7ffb\u8f6c\uff0c\u964d\u4f4e\u529f\u8017", "motivation": "\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u91c7\u7528NoC\u67b6\u6784\uff0c\u51cf\u5c11\u94fe\u8def\u6bd4\u7279\u7ffb\u8f6c\u4ee5\u8282\u7701\u529f\u8017\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u65b9\u5411", "method": "\u63d0\u51fa\u4e24\u79cd\u6570\u636e\u6392\u5e8f\u65b9\u6cd5\uff08\u5173\u8054\u6392\u5e8f\u548c\u5206\u79bb\u6392\u5e8f\uff09\u6765\u5904\u7406\u6743\u91cd\u548c\u8f93\u5165\u6570\u636e\uff0c\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u6392\u5e8f\u65b9\u6cd5\u7684\u6709\u6548\u6027", "result": "\u65e0NoC\u65f6\u6d6e\u70b932\u6570\u636e\u51cf\u5c1120.38%\u6bd4\u7279\u7ffb\u8f6c\uff0c\u5b9a\u70b98\u6570\u636e\u51cf\u5c1155.71%\uff1b\u5728NoC\u4e2d\u6d6e\u70b932\u6570\u636e\u51cf\u5c1132.01%\uff0c\u5b9a\u70b98\u6570\u636e\u51cf\u5c1140.85%", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540cDNN\u6a21\u578b\u3001NoC\u914d\u7f6e\u548c\u6570\u636e\u7cbe\u5ea6\u4e0b\u90fd\u80fd\u6709\u6548\u964d\u4f4e\u94fe\u8def\u529f\u8017"}}
{"id": "2509.00589", "categories": ["cs.AR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00589", "abs": "https://arxiv.org/abs/2509.00589", "authors": ["Shafayet M. Anik", "D. G. Perera"], "title": "Real-Time Piano Note Frequency Detection Using FPGA and FFT Core", "comment": "20 pages, 11 Figures", "summary": "Real-time frequency analysis of musical instruments, such as the piano, is an\nessential feature in areas like electronic tuners, music visualizers, and live\nsound monitoring. Traditional methods often rely on software-based digital\nsignal processing (DSP), which may introduce latency and require significant\ncomputational power. In contrast, hardware platforms such as FPGAs (Field\nProgrammable Gate Arrays) offer the ability to perform such analyses with\ngreater speed and determinism due to their parallel processing capabilities.\nThe primary objective of this project was to analyze analog audio signals from\na digital piano using an FPGA-based real-time Fast Fourier Transform (FFT)\nsystem.", "AI": {"tldr": "\u4f7f\u7528FPGA\u5b9e\u73b0\u94a2\u7434\u97f3\u9891\u4fe1\u53f7\u7684\u5b9e\u65f6FFT\u9891\u7387\u5206\u6790\uff0c\u76f8\u6bd4\u4f20\u7edf\u8f6f\u4ef6DSP\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u5ef6\u8fdf\u548c\u66f4\u9ad8\u8ba1\u7b97\u6548\u7387", "motivation": "\u4f20\u7edf\u8f6f\u4ef6DSP\u65b9\u6cd5\u5728\u5b9e\u65f6\u97f3\u4e50\u9891\u7387\u5206\u6790\u4e2d\u5b58\u5728\u5ef6\u8fdf\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u800cFPGA\u7684\u5e76\u884c\u5904\u7406\u80fd\u529b\u53ef\u4ee5\u63d0\u4f9b\u66f4\u5feb\u3001\u66f4\u786e\u5b9a\u6027\u7684\u5206\u6790\u6027\u80fd", "method": "\u91c7\u7528\u57fa\u4e8eFPGA\u7684\u5b9e\u65f6\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362(FFT)\u7cfb\u7edf\u6765\u5206\u6790\u6570\u5b57\u94a2\u7434\u7684\u6a21\u62df\u97f3\u9891\u4fe1\u53f7", "result": "FPGA\u5e73\u53f0\u80fd\u591f\u5b9e\u73b0\u9ad8\u901f\u3001\u4f4e\u5ef6\u8fdf\u7684\u97f3\u4e50\u9891\u7387\u5206\u6790\uff0c\u9002\u7528\u4e8e\u7535\u5b50\u8c03\u97f3\u5668\u3001\u97f3\u4e50\u53ef\u89c6\u5316\u5668\u548c\u73b0\u573a\u58f0\u97f3\u76d1\u63a7\u7b49\u5e94\u7528", "conclusion": "FPGA\u786c\u4ef6\u5e73\u53f0\u5728\u5b9e\u65f6\u97f3\u9891\u9891\u7387\u5206\u6790\u65b9\u9762\u76f8\u6bd4\u8f6f\u4ef6DSP\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u5feb\u901f\u54cd\u5e94\u548c\u786e\u5b9a\u6027\u6027\u80fd\u7684\u97f3\u4e50\u5e94\u7528\u573a\u666f"}}
{"id": "2509.00599", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.00599", "abs": "https://arxiv.org/abs/2509.00599", "authors": ["Shubham Negi", "Manik Singhal", "Aayush Ankit", "Sudeep Bhoja", "Kaushik Roy"], "title": "COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives", "comment": null, "summary": "Modern machine learning accelerators are designed to efficiently execute deep\nneural networks (DNNs) by optimizing data movement, memory hierarchy, and\ncompute throughput. However, emerging DNN models such as large language models,\nstate space models increasingly rely on compound operations-structured\ncompositions of multiple basic operations-which introduce new challenges for\ndataflow optimization and minimizing off-chip memory traffic. Moreover, as\nmodel size continues to grow, deployment across spatially distributed compute\nclusters becomes essential, requiring frequent and complex collective\ncommunication. Existing dataflow optimization frameworks and performance models\neither focus on single operations or lack explicit modeling of collective\ncommunication cost, limiting their applicability to modern workloads.\n  To address these limitations, we propose, a framework for modeling and\noptimizing dataflow for compound operations on machine learning accelerators.\nCOMET introduces a novel representation that explicitly models collective\ncommunication across spatial clusters, along with latency and energy cost\nmodels that account for both GEMM and non-GEMM operation level dependencies\nwithin compound operations. We demonstrate COMET's capabilities to analyze and\noptimize dataflows for compound operations such as GEMM--Softmax,\nGEMM--LayerNorm, and self-attention, across both edge and cloud accelerator\nconfigurations. Our collective-aware modeling enables exploration of a broader\nmapping space, leading to improved performance and energy efficiency.\nSpecifically, our optimized dataflows achieve up to 1.42$\\times$ speedup for\nGEMM-Softmax, 3.46$\\times$ for GEMM-LayerNorm and 1.82$\\times$ for\nself-attention compared to unfused baselines.", "AI": {"tldr": "COMET\u662f\u4e00\u4e2a\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u52a0\u901f\u5668\u4e2d\u590d\u5408\u64cd\u4f5c\u7684\u6570\u636e\u6d41\u5efa\u6a21\u548c\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8de8\u7a7a\u95f4\u96c6\u7fa4\u7684\u96c6\u4f53\u901a\u4fe1\uff0c\u5b9e\u73b0\u4e86\u6bd4\u672a\u878d\u5408\u57fa\u7ebf\u6700\u9ad83.46\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u52a0\u901f\u5668\u4e3b\u8981\u4f18\u5316\u5355\u4e00\u64cd\u4f5c\uff0c\u4f46\u65b0\u5174DNN\u6a21\u578b\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u3001\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u590d\u5408\u64cd\u4f5c\uff0c\u73b0\u6709\u6846\u67b6\u7f3a\u4e4f\u5bf9\u96c6\u4f53\u901a\u4fe1\u6210\u672c\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u73b0\u4ee3\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u63d0\u51faCOMET\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u9896\u7684\u8868\u793a\u65b9\u6cd5\u663e\u5f0f\u5efa\u6a21\u8de8\u7a7a\u95f4\u96c6\u7fa4\u7684\u96c6\u4f53\u901a\u4fe1\uff0c\u5efa\u7acb\u5305\u542bGEMM\u548c\u975eGEMM\u64cd\u4f5c\u7ea7\u4f9d\u8d56\u5173\u7cfb\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u6210\u672c\u6a21\u578b\uff0c\u652f\u6301\u590d\u5408\u64cd\u4f5c\u7684\u6570\u636e\u6d41\u5206\u6790\u548c\u4f18\u5316\u3002", "result": "\u4f18\u5316\u7684\u6570\u636e\u6d41\u5728GEMM-Softmax\u4e0a\u5b9e\u73b01.42\u500d\u52a0\u901f\uff0cGEMM-LayerNorm\u5b9e\u73b03.46\u500d\u52a0\u901f\uff0c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b01.82\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u672a\u878d\u5408\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u80fd\u6548\u3002", "conclusion": "COMET\u6846\u67b6\u901a\u8fc7\u96c6\u4f53\u901a\u4fe1\u611f\u77e5\u5efa\u6a21\u6269\u5c55\u4e86\u6620\u5c04\u7a7a\u95f4\u63a2\u7d22\u80fd\u529b\uff0c\u4e3a\u73b0\u4ee3\u590d\u5408\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u4f18\u5316\u65b9\u6848\uff0c\u5728\u8fb9\u7f18\u548c\u4e91\u7aef\u52a0\u901f\u5668\u914d\u7f6e\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.00633", "categories": ["cs.AR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.00633", "abs": "https://arxiv.org/abs/2509.00633", "authors": ["Mehdi Elahi", "Mohamed R. Elshamy", "Abdel-Hameed A. Badawy", "Ahmad Patooghy"], "title": "On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures", "comment": null, "summary": "3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance\nmemory interactions to address the well-known performance challenge, namely the\nmemory wall. However, these architectures are susceptible to thermal\nvulnerabilities due to the inherent vertical adjacency that occurs during the\nmanufacturing process of HBM architectures. We anticipate that adversaries may\nexploit the intense vertical and lateral adjacency to design and develop\nthermal performance degradation attacks on the memory banks that host\ndata/instructions from victim applications. In such attacks, the adversary\nmanages to inject short and intense heat pulses from vertically and/or\nlaterally adjacent memory banks, creating a convergent thermal wave that\nmaximizes impact and delays the victim application from accessing its\ndata/instructions. As the attacking application does not access any\nout-of-range memory locations, it can bypass both design-time security tests\nand the operating system's memory management policies. In other words, since\nthe attack mimics legitimate workloads, it will be challenging to detect.", "AI": {"tldr": "\u57fa\u4e8e3D\u5806\u53e0\u9ad8\u5e26\u5bbd\u5185\u5b58(HBM)\u67b6\u6784\u7684\u70ed\u653b\u51fb\uff0c\u5229\u7528\u5782\u76f4\u548c\u6c34\u5e73\u90bb\u8fd1\u6027\u5411\u76ee\u6807\u5185\u5b58\u6ce2\u6ce8\u5165\u77ed\u66b4\u70ed\u6ce2\uff0c\u5f15\u8d77\u6027\u80fd\u52a3\u5316\u4f46\u80fd\u7a7f\u8d8a\u5b89\u5168\u68c0\u6d4b", "motivation": "HBM\u67b6\u6784\u867d\u63d0\u4f9b\u9ad8\u6027\u80fd\u5185\u5b58\u8bbf\u95ee\uff0c\u4f46\u5782\u76f4\u90bb\u8fd1\u6027\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u70ed\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u5229\u7528\u8fd9\u4e00\u5f31\u70b9\u8fdb\u884c\u9690\u85cf\u6027\u5f3a\u7684\u6027\u80fd\u52a3\u5316\u653b\u51fb", "method": "\u901a\u8fc7\u5782\u76f4\u548c/\u6216\u6c34\u5e73\u90bb\u8fd1\u7684\u5185\u5b58\u6b3e\u5411\u76ee\u6807\u5185\u5b58\u6ce2\u6ce8\u5165\u77ed\u66b4\u70ed\u6ce2\uff0c\u4ea7\u751f\u805a\u7126\u70ed\u6ce2\u6700\u5927\u5316\u5f71\u54cd\u6548\u679c\uff0c\u5ef6\u8fdf\u76ee\u6807\u5e94\u7528\u8bbf\u95ee\u6570\u636e/\u6307\u4ee4", "result": "\u653b\u51fb\u80fd\u591f\u7a7f\u8d8a\u8bbe\u8ba1\u65f6\u5b89\u5168\u6d4b\u8bd5\u548c\u64cd\u4f5c\u7cfb\u7edf\u5185\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u56e0\u4e3a\u653b\u51fb\u6a21\u4eff\u5408\u6cd5\u5de5\u4f5c\u8d1f\u8377\u800c\u96be\u4ee5\u68c0\u6d4b", "conclusion": "HBM\u67b6\u6784\u5b58\u5728\u65b0\u578b\u70ed\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u53d1\u5c55\u65b0\u7684\u70ed\u5b89\u5168\u9632\u62a4\u673a\u5236\u6765\u5e94\u5bf9\u8fd9\u79cd\u9690\u85cf\u5728\u5408\u6cd5\u5de5\u4f5c\u8d1f\u8377\u4e2d\u7684\u6027\u80fd\u52a3\u5316\u653b\u51fb"}}
{"id": "2509.00764", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00764", "abs": "https://arxiv.org/abs/2509.00764", "authors": ["Pragun Jaswal", "L. Hemanth Krishna", "B. Srinivasu"], "title": "Low Power Approximate Multiplier Architecture for Deep Neural Networks", "comment": null, "summary": "This paper proposes an low power approximate multiplier architecture for deep\nneural network (DNN) applications. A 4:2 compressor, introducing only a single\ncombination error, is designed and integrated into an 8x8 unsigned multiplier.\nThis integration significantly reduces the usage of exact compressors while\npreserving low error rates. The proposed multiplier is employed within a custom\nconvolution layer and evaluated on neural network tasks, including image\nrecognition and denoising. Hardware evaluation demonstrates that the proposed\ndesign achieves up to 30.24% energy savings compared to the best among existing\nmultipliers. In image denoising, the custom approximate convolution layer\nachieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity\nIndex Measure (SSIM) compared to other approximate designs. Additionally, when\napplied to handwritten digit recognition, the model maintains high\nclassification accuracy. These results demonstrate that the proposed\narchitecture offers a favorable balance between energy efficiency and\ncomputational precision, making it suitable for low-power AI hardware\nimplementations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u7684\u4f4e\u529f\u8017\u8fd1\u4f3c\u4e58\u6cd5\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4ec5\u4ea7\u751f\u5355\u4e00\u7ec4\u5408\u8bef\u5dee\u76844:2\u538b\u7f29\u5668\uff0c\u5728\u4fdd\u6301\u4f4e\u9519\u8bef\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u5bf9\u8ba1\u7b97\u80fd\u6548\u8981\u6c42\u9ad8\uff0c\u4f20\u7edf\u7cbe\u786e\u4e58\u6cd5\u5668\u529f\u8017\u5927\uff0c\u9700\u8981\u5f00\u53d1\u5728\u4fdd\u6301\u8ba1\u7b97\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u7684\u8fd1\u4f3c\u4e58\u6cd5\u5668\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u4ec5\u4ea7\u751f\u5355\u4e00\u7ec4\u5408\u8bef\u5dee\u76844:2\u538b\u7f29\u5668\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u52308x8\u65e0\u7b26\u53f7\u4e58\u6cd5\u5668\u4e2d\uff0c\u51cf\u5c11\u7cbe\u786e\u538b\u7f29\u5668\u7684\u4f7f\u7528\uff0c\u6784\u5efa\u81ea\u5b9a\u4e49\u5377\u79ef\u5c42\u5e76\u5728\u795e\u7ecf\u7f51\u7edc\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u786c\u4ef6\u8bc4\u4f30\u663e\u793a\u80fd\u8017\u8282\u7701\u8fbe30.24%\uff0c\u56fe\u50cf\u53bb\u566a\u4efb\u52a1\u4e2dPSNR\u548cSSIM\u6307\u6807\u4f18\u4e8e\u5176\u4ed6\u8fd1\u4f3c\u8bbe\u8ba1\uff0c\u624b\u5199\u6570\u5b57\u8bc6\u522b\u4fdd\u6301\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u67b6\u6784\u5728\u80fd\u6548\u548c\u8ba1\u7b97\u7cbe\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u5408\u4f4e\u529f\u8017AI\u786c\u4ef6\u5b9e\u73b0\u3002"}}
{"id": "2509.00778", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00778", "abs": "https://arxiv.org/abs/2509.00778", "authors": ["Pragun Jaswal", "L. Hemanth Krishna", "B. Srinivasu"], "title": "Energy Efficient Exact and Approximate Systolic Array Architecture for Matrix Multiplication", "comment": "Submitted to 39th International Conference on VLSI Design, 2026", "summary": "Deep Neural Networks (DNNs) require highly efficient matrix multiplication\nengines for complex computations. This paper presents a systolic array\narchitecture incorporating novel exact and approximate processing elements\n(PEs), designed using energy-efficient positive partial product and negative\npartial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit\nexact and approximate PE designs are employed in a 8x8 systolic array, which\nachieves a energy savings of 22% and 32%, respectively, compared to the\nexisting design. To demonstrate their effectiveness, the proposed PEs are\nintegrated into a systolic array (SA) for Discrete Cosine Transform (DCT)\ncomputation, achieving high output quality with a PSNR of 38.21,dB.\nFurthermore, in an edge detection application using convolution, the\napproximate PE achieves a PSNR of 30.45,dB. These results highlight the\npotential of the proposed design to deliver significant energy efficiency while\nmaintaining competitive output quality, making it well-suited for\nerror-resilient image and vision processing applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u65b0\u578b\u7cbe\u786e\u548c\u8fd1\u4f3c\u5904\u7406\u5355\u5143\u7684\u8109\u52a8\u9635\u5217\u67b6\u6784\uff0c\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u80fd\u77e9\u9635\u4e58\u6cd5\u8ba1\u7b97\uff0c\u76f8\u6bd4\u73b0\u6709\u8bbe\u8ba1\u5206\u522b\u5b9e\u73b022%\u548c32%\u7684\u80fd\u8017\u8282\u7701\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5904\u7406\u5e94\u7528\u4e2d\u4fdd\u6301\u826f\u597d\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u9ad8\u6548\u7684\u77e9\u9635\u4e58\u6cd5\u5f15\u64ce\u6765\u5904\u7406\u590d\u6742\u8ba1\u7b97\uff0c\u4f20\u7edf\u8bbe\u8ba1\u5728\u80fd\u8017\u65b9\u9762\u5b58\u5728\u4f18\u5316\u7a7a\u95f4\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u8282\u7701\u80fd\u91cf\u53c8\u80fd\u4fdd\u6301\u8ba1\u7b97\u7cbe\u5ea6\u7684\u65b0\u578b\u5904\u7406\u67b6\u6784\u3002", "method": "\u91c7\u7528\u80fd\u91cf\u9ad8\u6548\u7684\u6b63\u90e8\u5206\u79ef\u5355\u5143(PPC)\u548c\u8d1f\u90e8\u5206\u79ef\u5355\u5143(NPPC)\u8bbe\u8ba18\u4f4d\u7cbe\u786e\u548c\u8fd1\u4f3c\u5904\u7406\u5355\u5143(PE)\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u52308x8\u8109\u52a8\u9635\u5217\u4e2d\uff0c\u5e94\u7528\u4e8e\u79bb\u6563\u4f59\u5f26\u53d8\u6362(DCT)\u8ba1\u7b97\u548c\u8fb9\u7f18\u68c0\u6d4b\u5377\u79ef\u5e94\u7528\u3002", "result": "\u7cbe\u786ePE\u5b9e\u73b022%\u80fd\u8017\u8282\u7701\uff0c\u8fd1\u4f3cPE\u5b9e\u73b032%\u80fd\u8017\u8282\u7701\uff1bDCT\u8ba1\u7b97\u83b7\u5f9738.21dB PSNR\u9ad8\u8d28\u91cf\u8f93\u51fa\uff1b\u8fb9\u7f18\u68c0\u6d4b\u5e94\u7528\u8fbe\u523030.45dB PSNR\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u80fd\u591f\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u4f9b\u663e\u8457\u7684\u80fd\u6548\u4f18\u52bf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5bb9\u9519\u56fe\u50cf\u548c\u89c6\u89c9\u5904\u7406\u5e94\u7528\u3002"}}
{"id": "2509.00911", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00911", "abs": "https://arxiv.org/abs/2509.00911", "authors": ["Joongho Jo", "Jongsun Park"], "title": "GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency", "comment": "DAC 2025", "summary": "3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to\nneural radiance fields (NeRF) as it offers high speed as well as high image\nquality in novel view synthesis. Despite these advancements, 3D-GS still\nstruggles to meet the frames per second (FPS) demands of real-time\napplications. In this paper, we introduce GS-TG, a tile-grouping-based\naccelerator that enhances 3D-GS rendering speed by reducing redundant sorting\noperations and preserving rasterization efficiency. GS-TG addresses a critical\ntrade-off issue in 3D-GS rendering: increasing the tile size effectively\nreduces redundant sorting operations, but it concurrently increases unnecessary\nrasterization computations. So, during sorting of the proposed approach, GS-TG\ngroups small tiles (for making large tiles) to share sorting operations across\ntiles within each group, significantly reducing redundant computations. During\nrasterization, a bitmask assigned to each Gaussian identifies relevant small\ntiles, to enable efficient sharing of sorting results. Consequently, GS-TG\nenables sorting to be performed as if a large tile size is used by grouping\ntiles during the sorting stage, while allowing rasterization to proceed with\nthe original small tiles by using bitmasks in the rasterization stage. GS-TG is\na lossless method requiring no retraining or fine-tuning and it can be\nseamlessly integrated with previous 3D-GS optimization techniques. Experimental\nresults show that GS-TG achieves an average speed-up of 1.54 times over\nstate-of-the-art 3D-GS accelerators.", "AI": {"tldr": "GS-TG\u662f\u4e00\u79cd\u57fa\u4e8e\u74e6\u7247\u5206\u7ec4\u76843D\u9ad8\u65af\u6cfc\u6e85\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u6392\u5e8f\u64cd\u4f5c\u5e76\u4fdd\u6301\u5149\u6805\u5316\u6548\u7387\uff0c\u5b9e\u73b0\u4e861.54\u500d\u7684\u5e73\u5747\u52a0\u901f\u6548\u679c\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u867d\u7136\u6bd4NeRF\u901f\u5ea6\u5feb\uff0c\u4f46\u4ecd\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u7684\u5e27\u7387\u9700\u6c42\uff0c\u5b58\u5728\u74e6\u7247\u5927\u5c0f\u9009\u62e9\u4e0a\u7684\u6743\u8861\u95ee\u9898\uff1a\u5927\u74e6\u7247\u51cf\u5c11\u6392\u5e8f\u5197\u4f59\u4f46\u589e\u52a0\u5149\u6805\u5316\u8ba1\u7b97\uff0c\u5c0f\u74e6\u7247\u5219\u76f8\u53cd\u3002", "method": "\u63d0\u51fa\u74e6\u7247\u5206\u7ec4\u65b9\u6cd5\uff1a\u5728\u6392\u5e8f\u9636\u6bb5\u5c06\u5c0f\u74e6\u7247\u5206\u7ec4\u5f62\u6210\u5927\u74e6\u7247\u6765\u5171\u4eab\u6392\u5e8f\u64cd\u4f5c\uff1b\u5728\u5149\u6805\u5316\u9636\u6bb5\u4f7f\u7528\u4f4d\u63a9\u7801\u6807\u8bc6\u76f8\u5173\u5c0f\u74e6\u7247\uff0c\u4fdd\u6301\u5c0f\u74e6\u7247\u7684\u5149\u6805\u5316\u6548\u7387\u3002\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65e0\u635f\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aGS-TG\u76f8\u6bd4\u6700\u5148\u8fdb\u76843D-GS\u52a0\u901f\u5668\u5b9e\u73b0\u4e86\u5e73\u57471.54\u500d\u7684\u52a0\u901f\u6548\u679c\u3002", "conclusion": "GS-TG\u6210\u529f\u89e3\u51b3\u4e863D-GS\u6e32\u67d3\u4e2d\u7684\u5173\u952e\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u74e6\u7247\u5206\u7ec4\u548c\u4f4d\u63a9\u7801\u6280\u672f\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\uff0c\u4e14\u80fd\u4e0e\u73b0\u6709\u4f18\u5316\u6280\u672f\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2509.01020", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.01020", "abs": "https://arxiv.org/abs/2509.01020", "authors": ["Elena Espinosa", "Rub\u00e9n Rodr\u00edguez \u00c1lvarez", "Jos\u00e9 Miranda", "Rafael Larrosa", "Miguel Pe\u00f3n-Quir\u00f3s", "Oscar Plata", "David Atienza"], "title": "GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs", "comment": null, "summary": "The advent of next-generation sequencing (NGS) has revolutionized genomic\nresearch by enabling high-throughput data generation through parallel\nsequencing of a diverse range of organisms at significantly reduced costs. This\nbreakthrough has unleashed a \"Cambrian explosion\" in genomic data volume and\ndiversity. This volume of workloads places genomics among the top four big data\nchallenges anticipated for this decade. In this context, pairwise sequence\nalignment represents a very time- and energy-consuming step in common\nbioinformatics pipelines. Speeding up this step requires the implementation of\nheuristic approaches, optimized algorithms, and/or hardware acceleration.\n  Whereas state-of-the-art CPU and GPU implementations have demonstrated\nsignificant performance gains, recent field programmable gate array (FPGA)\nimplementations have shown improved energy efficiency. However, the latter\noften suffer from limited scalability due to constraints on hardware resources\nwhen aligning longer sequences. In this work, we present a scalable and\nflexible FPGA-based accelerator template that implements Myers's algorithm\nusing high-level synthesis and a worker-based architecture. GeneTEK, an\ninstance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA,\noutperforms state-of-the-art CPU and GPU implementations in both speed and\nenergy efficiency, while overcoming scalability limitations of current FPGA\napproaches. Specifically, GeneTEK achieves at least a 19.4% increase in\nexecution speed and up to 62x reduction in energy consumption compared to\nleading CPU and GPU solutions, while fitting comparison matrices up to 72%\nlarger compared to previous FPGA solutions. These results reaffirm the\npotential of FPGAs as an energy-efficient platform for scalable genomic\nworkloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GeneTEK\uff0c\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u53ef\u6269\u5c55\u52a0\u901f\u5668\u6a21\u677f\uff0c\u7528\u4e8e\u57fa\u56e0\u7ec4\u5e8f\u5217\u6bd4\u5bf9\uff0c\u5728\u901f\u5ea6\u548c\u80fd\u8017\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709CPU\u548cGPU\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u6d4b\u5e8f\u6280\u672f\u4ea7\u751f\u4e86\u6d77\u91cf\u57fa\u56e0\u7ec4\u6570\u636e\uff0c\u5e8f\u5217\u6bd4\u5bf9\u4f5c\u4e3a\u751f\u7269\u4fe1\u606f\u5b66\u6d41\u7a0b\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u8ba1\u7b97\u8017\u65f6\u4e14\u80fd\u8017\u9ad8\u3002\u73b0\u6709FPGA\u89e3\u51b3\u65b9\u6848\u5728\u6bd4\u5bf9\u957f\u5e8f\u5217\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002", "method": "\u91c7\u7528\u9ad8\u7ea7\u7efc\u5408\u6280\u672f\u548c\u57fa\u4e8e\u5de5\u4f5c\u8005\u7684\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86Myers\u7b97\u6cd5\u7684FPGA\u52a0\u901f\u5668\u6a21\u677fGeneTEK\uff0c\u90e8\u7f72\u5728Xilinx Zynq UltraScale+ FPGA\u4e0a\u3002", "result": "GeneTEK\u76f8\u6bd4\u9886\u5148\u7684CPU\u548cGPU\u89e3\u51b3\u65b9\u6848\uff0c\u6267\u884c\u901f\u5ea6\u81f3\u5c11\u63d0\u534719.4%\uff0c\u80fd\u8017\u964d\u4f4e\u9ad8\u8fbe62\u500d\uff0c\u540c\u65f6\u652f\u6301\u6bd4\u5148\u524dFPGA\u89e3\u51b3\u65b9\u6848\u592772%\u7684\u6bd4\u5bf9\u77e9\u9635\u3002", "conclusion": "FPGA\u4f5c\u4e3a\u53ef\u6269\u5c55\u57fa\u56e0\u7ec4\u5de5\u4f5c\u8d1f\u8f7d\u7684\u80fd\u6548\u5e73\u53f0\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cGeneTEK\u6210\u529f\u514b\u670d\u4e86\u5f53\u524dFPGA\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002"}}
{"id": "2509.01339", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.01339", "abs": "https://arxiv.org/abs/2509.01339", "authors": ["Bochen Ye", "Gustavo Naspolini", "Kimmo Salo", "Manil Dev Gomony"], "title": "LinkBo: An Adaptive Single-Wire, Low-Latency, and Fault-Tolerant Communications Interface for Variable-Distance Chip-to-Chip Systems", "comment": "This paper is full version of SOCC'2025 conference", "summary": "Cost-effective embedded systems necessitate utilizing the single-wire\ncommunication protocol for inter-chip communication, thanks to its reduced pin\ncount in comparison to the multi-wire I2C or SPI protocols. However, current\nsingle-wire protocols suffer from increased latency, restricted throughput, and\nlack of robustness. This paper presents LinkBo, an innovative single-wire\nprotocol that offers reduced latency, enhanced throughput, and greater\nrobustness with hardware-interrupt for variable-distance inter-chip\ncommunication. The LinkBo protocol-level guarantees that high-priority messages\nare delivered with an error detection feature in just 50.4 $\\mu$s, surpassing\ncurrent commercial options, 1-wire and UNI/O by at least 20X and 6.3X,\nrespectively. In addition, we present the hardware architecture for this new\nprotocol and its performance evaluation on a hardware platform consisting of\ntwo FPGAs. Our findings demonstrate that the protocol reliably supports wire\nlengths up to 15 meters with a data rate of 300 kbps, while reaching a maximum\ndata rate of 7.5 Mbps over an 11 cm wire, providing reliable performance for\nvarying inter-chip communication distances.", "AI": {"tldr": "LinkBo\u662f\u4e00\u79cd\u521b\u65b0\u7684\u5355\u7ebf\u901a\u4fe1\u534f\u8bae\uff0c\u76f8\u6bd4\u73b0\u6709\u76841-wire\u548cUNI/O\u534f\u8bae\uff0c\u5ef6\u8fdf\u964d\u4f4e20\u500d\u4ee5\u4e0a\uff0c\u541e\u5410\u91cf\u63d0\u5347\u663e\u8457\uff0c\u652f\u6301\u6700\u957f15\u7c73\u7ebf\u7f06\u8ddd\u79bb\u548c\u6700\u9ad87.5 Mbps\u6570\u636e\u901f\u7387", "motivation": "\u73b0\u6709\u5355\u7ebf\u901a\u4fe1\u534f\u8bae\u5b58\u5728\u5ef6\u8fdf\u9ad8\u3001\u541e\u5410\u91cf\u53d7\u9650\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5bf9\u9ad8\u6548\u53ef\u9760\u901a\u4fe1\u7684\u9700\u6c42", "method": "\u63d0\u51faLinkBo\u534f\u8bae\uff0c\u91c7\u7528\u786c\u4ef6\u4e2d\u65ad\u673a\u5236\u5b9e\u73b0\u53ef\u53d8\u8ddd\u79bb\u82af\u7247\u95f4\u901a\u4fe1\uff0c\u63d0\u4f9b\u9519\u8bef\u68c0\u6d4b\u529f\u80fd\u548c\u9ad8\u4f18\u5148\u7ea7\u6d88\u606f\u4f20\u8f93\u4fdd\u8bc1\uff0c\u5e76\u5728FPGA\u786c\u4ef6\u5e73\u53f0\u4e0a\u5b9e\u73b0\u548c\u8bc4\u4f30", "result": "\u9ad8\u4f18\u5148\u7ea7\u6d88\u606f\u4f20\u8f93\u5ef6\u8fdf\u4ec550.4\u03bcs\uff0c\u6bd41-wire\u548cUNI/O\u5206\u522b\u5feb20\u500d\u548c6.3\u500d\uff1b\u652f\u6301300 kbps\u6570\u636e\u901f\u7387\u4e0b15\u7c73\u7ebf\u7f06\u957f\u5ea6\uff0c11\u5398\u7c73\u7ebf\u7f06\u4e0b\u53ef\u8fbe7.5 Mbps", "conclusion": "LinkBo\u534f\u8bae\u5728\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u9c81\u68d2\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5546\u7528\u5355\u7ebf\u534f\u8bae\uff0c\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u7684\u82af\u7247\u95f4\u901a\u4fe1\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.02369", "categories": ["cs.AR", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.02369", "abs": "https://arxiv.org/abs/2509.02369", "authors": ["Zacharia A. Rudge", "Dario Izzo", "Moritz Fieback", "Anteneh Gebregiorgis", "Said Hamdioui", "Dominik Dold"], "title": "Guidance and Control Neural Network Acceleration using Memristors", "comment": "4 pages, SPAICE 2024 conference", "summary": "In recent years, the space community has been exploring the possibilities of\nArtificial Intelligence (AI), specifically Artificial Neural Networks (ANNs),\nfor a variety of on board applications. However, this development is limited by\nthe restricted energy budget of smallsats and cubesats as well as radiation\nconcerns plaguing modern chips. This necessitates research into neural network\naccelerators capable of meeting these requirements whilst satisfying the\ncompute and performance needs of the application. This paper explores the use\nof Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM)\nmemristors for on-board in-memory computing AI acceleration in space\napplications. A guidance and control neural network (G\\&CNET) accelerated using\nmemristors is simulated in a variety of scenarios and with both device types to\nevaluate the performance of memristor-based accelerators, considering device\nnon-idealities such as noise and conductance drift. We show that the memristive\naccelerator is able to learn the expert actions, though challenges remain with\nthe impact of noise on accuracy. We also show that re-training after\ndegradation is able to restore performance to nominal levels. This study\nprovides a foundation for future research into memristor-based AI accelerators\nfor space, highlighting their potential and the need for further investigation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u76f8\u53d8\u5185\u5b58(PCM)\u548c\u963b\u53d8\u5b58\u50a8\u5668(RRAM)\u5728\u592a\u7a7a\u5e94\u7528\u4e2d\u5b9e\u73b0\u5185\u5b58\u8ba1\u7b97AI\u52a0\u901f\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u6307\u5bfc\u548c\u63a7\u5236\u795e\u7ecf\u7f51\u7edc\u6765\u8bc4\u4f30\u6027\u80fd\u548c\u9762\u5bf9\u8bbe\u5907\u975e\u7406\u60f3\u6027\u7684\u5f3a\u9510\u6027\u3002", "motivation": "\u592a\u7a7a\u5e94\u7528\u4e2dAI\u6280\u672f\u53d1\u5c55\u53d7\u9650\u4e8e\u5c0f\u5360\u5360\u548c\u7acb\u65b9\u5361\u5360\u7684\u6709\u9650\u80fd\u6e90\u9884\u7b97\u4ee5\u53ca\u73b0\u4ee3\u82af\u7247\u7684\u653e\u5c04\u95ee\u9898\uff0c\u9700\u8981\u7814\u7a76\u80fd\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u4f7f\u7528PCM\u548cRRAM\u963b\u53d8\u5b58\u50a8\u5668\u52a0\u901f\u7684\u6307\u5bfc\u63a7\u5236\u795e\u7ecf\u7f51\u7edc(G&CNET)\uff0c\u8003\u8651\u5668\u4ef6\u7684\u975e\u7406\u60f3\u6027\u5982\u566a\u58f0\u548c\u5bfc\u7535\u7387\u98d8\u79fb\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u963b\u53d8\u5b58\u50a8\u5668\u52a0\u901f\u5668\u80fd\u591f\u5b66\u4e60\u4e13\u5bb6\u884c\u4e3a\uff0c\u4f46\u566a\u58f0\u5bf9\u51c6\u786e\u6027\u7684\u5f71\u54cd\u4ecd\u662f\u6311\u6218\uff1b\u5728\u6027\u80fd\u9000\u5316\u540e\u91cd\u65b0\u8bad\u7ec3\u80fd\u591f\u6062\u590d\u5230\u6807\u79f0\u6c34\u5e73\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u672a\u6765\u592a\u7a7a\u5e94\u7528\u4e2d\u963b\u53d8\u5b58\u50a8\u5668\u57faAI\u52a0\u901f\u5668\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u5e76\u6307\u51fa\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u9886\u57df\u3002"}}
