<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations](https://arxiv.org/abs/2510.13147)
*Faraz Tahmasebi,Michael Pelluer,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 本文提出D-com加速器，通过输入分解、渐进分解算法和协同加速器架构，解决了大语言模型分解过程中的延迟问题，实现了22%的端到端延迟改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算和内存成本持续增长，传统权重分解方法导致运行时分解延迟过高，超过了分解带来的收益。

Method: 采用渐进分解算法（Lanczos算法），设计协同加速器架构，引入计算复制方法解决内存瓶颈，开发输出形状保持计算方案，并使用多通道分解方法处理异常通道。

Result: 实现了6.2倍加速，端到端延迟比A100 GPU提升22%，模型质量损失较小（在AI2推理挑战任务上仅3%）。

Conclusion: 通过适当的分解算法和硬件支持，输入分解可以显著有益，D-com加速器在保持模型质量的同时显著提升了性能。

Abstract: The computation and memory costs of large language models kept increasing
over last decade, which reached over the scale of 1T parameters. To address the
challenges from the large scale models, model compression techniques such as
low-rank decomposition have been explored. Previous model decomposition works
have focused on weight decomposition to avoid costly runtime decomposition,
whose latency often significantly exceeds the benefits from decomposition
(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K
sequence length with activation decomposition compared to no decomposition). In
this work, we debunk such observations and report that the input decomposition
can be significantly beneficial with a proper choice of decomposition algorithm
and hardware support. We adopt progressive decomposition algorithm, Lanczos
algorithm, and design a co-accelerator architecture for the decomposition
algorithm. To address the memory- boundness of the decomposition operation, we
introduce a novel compute replication methodology that moves the op- eration
toward compute-bound region, which enables 6.2x speedup in our evaluation. We
also develop an output shape- preserving computation scheme that eliminates
decomposi- tion costs in consecutive layers. To compensate model quality loss
from compression, we introduce a multi-track decom- position approach that
separately handles outlier channels for high accuracy and low perplexity with
minimal compu- tational costs. Combined together, our accelerator, D-com,
provides 22% end-to-end latency improvements compared to A100 GPU at the cost
of small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

</details>


### [2] [Energy-Efficient FPGA Framework for Non-Quantized Convolutional Neural Networks](https://arxiv.org/abs/2510.13362)
*Angelos Athanasiadis,Nikolaos Tampouratzis,Ioannis Papaefstathiou*

Main category: cs.AR

TL;DR: 提出一个基于Darknet的FPGA框架，用于在保持神经网络全精度的情况下高效实现CNN计算，在CPU-FPGA异构系统中提供与量化方案相当的性能和能效。


<details>
  <summary>Details</summary>
Motivation: AI应用中CNN的实时处理需求日益增长，传统处理器在性能、功耗和延迟方面难以平衡，特别是在嵌入式系统和边缘计算平台中。FPGA提供了高性能、高能效和可重构的替代方案。

Method: 基于广泛使用的Darknet框架开发，允许设计者使用类似Darknet的输入，在CPU-FPGA异构系统中高效实现CNN，同时保持所有神经网络参数的全精度。

Result: 与支持量化的FPGA框架相比，该解决方案旨在提供相似的性能和/或能效，而不会降低神经网络精度。

Conclusion: 该框架为CNN在FPGA上的实现提供了一种有效的全精度计算方法，在保持准确性的同时实现了良好的性能和能效平衡。

Abstract: The growing demand for real-time processing in artificial intelligence
applications, particularly those involving Convolutional Neural Networks
(CNNs), has highlighted the need for efficient computational solutions.
Conventional processors, very often, fall short in balancing performance, power
consumption, and latency, especially in embedded systems and edge computing
platforms. Field-Programmable Gate Arrays (FPGAs) offer a promising
alternative, combining high performance with energy efficiency and
reconfigurability. The presented framework addresses the complex and demanding
computations of CNNs on FPGAs maintaining full precision in all neural network
parameters. Specifically, our framework is based on Darknet which is very
widely used for the design of CNNs and allows the designer, by using a similar
input to that given to Darknet, to efficiently implement a CNN in a
heterogeneous system comprising of CPUs and FPGAs. When compared with the FPGA
frameworks that support quantization, our solution aims to offer similar
performance and/or energy efficiency without any degradation on the NN
accuracy.

</details>


### [3] [F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs](https://arxiv.org/abs/2510.13401)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 提出F-BFQ加速器，支持动态切换两种BFP量化变体，在AMD Kria板上实现1.4倍推理加速，达到5.2 tokens/秒的性能


<details>
  <summary>Details</summary>
Motivation: LLM在边缘设备部署需要量化技术，但混合BFP量化需要专用加速器支持不同量化变体而无需重新配置

Method: 设计灵活的块浮点量化(F-BFQ)加速器，能够动态切换两种BFP量化变体并执行矩阵乘法操作

Result: 在AMD Kria板上部署，相比Arm NEON CPU执行平均减少1.4倍推理时间，达到5.2 tokens/秒（约3.9 words/秒）

Conclusion: F-BFQ加速器有效支持混合BFP量化LLM的推理加速，为边缘设备部署提供高效解决方案

Abstract: Large Language Models (LLMs) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of LLM inference frameworks, such as
llama.cpp, which support optimizations such as KV-caching and quantization, it
is now easier than ever to deploy LLMs on edge devices. Quantization is
fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) quantization to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run LLMs. LLMs are typically quantized with mixed BFP
quantization across the model layers to reduce the loss of model accuracy due
to quantization. Therefore, to efficiently accelerate across the layers of
BFP-quantized LLMs, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP quantization variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per
second (~3.9 words per second).

</details>
