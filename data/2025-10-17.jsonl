{"id": "2510.14172", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.14172", "abs": "https://arxiv.org/abs/2510.14172", "authors": ["Yuchao Su", "Srikar Chundury", "Jiajia Li", "Frank Mueller"], "title": "DIAMOND: Systolic Array Acceleration of Sparse Matrix Multiplication for Quantum Simulation", "comment": null, "summary": "Hamiltonian simulation is a key workload in quantum computing, enabling the\nstudy of complex quantum systems and serving as a critical tool for classical\nverification of quantum devices. However, it is computationally challenging\nbecause the Hilbert space dimension grows exponentially with the number of\nqubits. The growing dimensions make matrix exponentiation, the key kernel in\nHamiltonian simulations, increasingly expensive. Matrix exponentiation is\ntypically approximated by the Taylor series, which contains a series of matrix\nmultiplications. Since Hermitian operators are often sparse, sparse matrix\nmultiplication accelerators are essential for improving the scalability of\nclassical Hamiltonian simulation. Yet, existing accelerators are primarily\ndesigned for machine learning workloads and tuned to their characteristic\nsparsity patterns, which differ fundamentally from those in Hamiltonian\nsimulations that are often dominated by structured diagonals.\n  In this work, we present \\name, the first diagonal-optimized quantum\nsimulation accelerator. It exploits the diagonal structure commonly found in\nproblem-Hamiltonian (Hermitian) matrices and leverages a restructured systolic\narray dataflow to transform diagonally sparse matrices into dense computations,\nenabling high utilization and performance. Through detailed cycle-level\nsimulation of diverse benchmarks in HamLib, \\name{} demonstrates average\nperformance improvements of $10.26\\times$, $33.58\\times$, and $53.15\\times$\nover SIGMA, Outer Product, and Gustavson's algorithm, respectively, with peak\nspeedups up to $127.03\\times$ while reducing energy consumption by an average\nof $471.55\\times$ and up to $4630.58\\times$ compared to SIGMA."}
{"id": "2510.14379", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.14379", "abs": "https://arxiv.org/abs/2510.14379", "authors": ["Ming-Han Lin", "Tian-Sheuan Chang"], "title": "Computing-In-Memory Aware Model Adaption For Edge Devices", "comment": "9 pages", "summary": "Computing-in-Memory (CIM) macros have gained popularity for deep learning\nacceleration due to their highly parallel computation and low power\nconsumption. However, limited macro size and ADC precision introduce throughput\nand accuracy bottlenecks. This paper proposes a two-stage CIM-aware model\nadaptation process. The first stage compresses the model and reallocates\nresources based on layer importance and macro size constraints, reducing model\nweight loading latency while improving resource utilization and maintaining\naccuracy. The second stage performs quantization-aware training, incorporating\npartial sum quantization and ADC precision to mitigate quantization errors in\ninference. The proposed approach enhances CIM array utilization to 90\\%,\nenables concurrent activation of up to 256 word lines, and achieves up to 93\\%\ncompression, all while preserving accuracy comparable to previous methods."}
{"id": "2510.14393", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14393", "abs": "https://arxiv.org/abs/2510.14393", "authors": ["Ching-Lin Hsiung", "Tian-Sheuan Chang"], "title": "Low Power Vision Transformer Accelerator with Hardware-Aware Pruning and Optimized Dataflow", "comment": "10 pages; IEEE Transactions on Circuits and Systems I: Regular Papers", "summary": "Current transformer accelerators primarily focus on optimizing self-attention\ndue to its quadratic complexity. However, this focus is less relevant for\nvision transformers with short token lengths, where the Feed-Forward Network\n(FFN) tends to be the dominant computational bottleneck. This paper presents a\nlow power Vision Transformer accelerator, optimized through algorithm-hardware\nco-design. The model complexity is reduced using hardware-friendly dynamic\ntoken pruning without introducing complex mechanisms. Sparsity is further\nimproved by replacing GELU with ReLU activations and employing dynamic FFN2\npruning, achieving a 61.5\\% reduction in operations and a 59.3\\% reduction in\nFFN2 weights, with an accuracy loss of less than 2\\%. The hardware adopts a\nrow-wise dataflow with output-oriented data access to eliminate data\ntransposition, and supports dynamic operations with minimal area overhead.\nImplemented in TSMC's 28nm CMOS technology, our design occupies 496.4K gates\nand includes a 232KB SRAM buffer, achieving a peak throughput of 1024 GOPS at\n1GHz, with an energy efficiency of 2.31 TOPS/W and an area efficiency of 858.61\nGOPS/mm2."}
{"id": "2510.14750", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14750", "abs": "https://arxiv.org/abs/2510.14750", "authors": ["İsmail Emir Yüksel", "Ataberk Olgun", "F. Nisa Bostancı", "Haocong Luo", "A. Giray Yağlıkçı", "Onur Mutlu"], "title": "ColumnDisturb: Understanding Column-based Read Disturbance in Real DRAM Chips and Implications for Future Systems", "comment": "Extended version of our publication at the 58th IEEE/ACM\n  International Symposium on Microarchitecture (MICRO-58), 2025", "summary": "We experimentally demonstrate a new widespread read disturbance phenomenon,\nColumnDisturb, in real commodity DRAM chips. By repeatedly opening or keeping a\nDRAM row (aggressor row) open, we show that it is possible to disturb DRAM\ncells through a DRAM column (i.e., bitline) and induce bitflips in DRAM cells\nsharing the same columns as the aggressor row (across multiple DRAM subarrays).\nWith ColumnDisturb, the activation of a single row concurrently disturbs cells\nacross as many as three subarrays (e.g., 3072 rows) as opposed to\nRowHammer/RowPress, which affect only a few neighboring rows of the aggressor\nrow in a single subarray. We rigorously characterize ColumnDisturb and its\ncharacteristics under various operational conditions using 216 DDR4 and 4 HBM2\nchips from three major manufacturers. Among our 27 key experimental\nobservations, we highlight two major results and their implications.\n  First, ColumnDisturb affects chips from all three major manufacturers and\nworsens as DRAM technology scales down to smaller node sizes (e.g., the minimum\ntime to induce the first ColumnDisturb bitflip reduces by up to 5.06x). We\nobserve that, in existing DRAM chips, ColumnDisturb induces bitflips within a\nstandard DDR4 refresh window (e.g., in 63.6 ms) in multiple cells. We predict\nthat, as DRAM technology node size reduces, ColumnDisturb would worsen in\nfuture DRAM chips, likely causing many more bitflips in the standard refresh\nwindow. Second, ColumnDisturb induces bitflips in many (up to 198x) more rows\nthan retention failures. Therefore, ColumnDisturb has strong implications for\nretention-aware refresh mechanisms that leverage the heterogeneity in cell\nretention times: our detailed analyses show that ColumnDisturb greatly reduces\nthe benefits of such mechanisms."}
