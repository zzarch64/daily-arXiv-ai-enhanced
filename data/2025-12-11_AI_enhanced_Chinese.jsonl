{"id": "2512.09304", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09304", "abs": "https://arxiv.org/abs/2512.09304", "authors": ["Siyuan Ma", "Jiajun Hu", "Jeeho Ryoo", "Aman Arora", "Lizy Kurian John"], "title": "RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference", "comment": null, "summary": "In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.", "AI": {"tldr": "RACAM\uff1a\u9996\u4e2a\u652f\u6301\u6570\u636e\u91cd\u7528\u548c\u51cf\u5c11\u5197\u4f59\u4f20\u8f93\u7684DRAM\u5185\u4f4d\u4e32\u884c\u5904\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u4e13\u7528\u5c40\u90e8\u6027\u7f13\u51b2\u533a\u548c\u6620\u5c04\u673a\u5236\uff0c\u5728LLM\u63a8\u7406\u4e2d\u76f8\u6bd4GPU\u548c\u73b0\u6709PIM\u7cfb\u7edf\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u73b0\u6709DRAM\u5904\u7406\u5185\u5b58\uff08DRAM-PIM\uff09\u67b6\u6784\u5b58\u5728\u6570\u636e\u91cd\u7528\u4e0d\u8db3\u3001\u5197\u4f59\u6570\u636e\u4f20\u8f93\u91cf\u5927\u3001\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u652f\u6301\u4e0d\u5145\u5206\u7b49\u4e3b\u8981\u9650\u5236\uff0c\u8fd9\u5f71\u54cd\u4e86\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff09\u7684\u6548\u7387\u3002", "method": "\u63d0\u51faRACAM\u67b6\u6784\uff0c\u5305\u542b\u4e13\u7528\u5c40\u90e8\u6027\u7f13\u51b2\u533a\u3001\u4f4d\u4e32\u884c\u5904\u7406\u5355\u5143\u3001popcount\u5f52\u7ea6\u5355\u5143\u548c\u5e7f\u64ad\u5355\u5143\uff0c\u4ee5\u5b9e\u73b0\u6570\u636e\u91cd\u7528\u5e76\u51cf\u5c11\u5197\u4f59\u6570\u636e\u4f20\u8f93\u3002\u540c\u65f6\u63d0\u51fa\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u673a\u5236\uff0c\u5145\u5206\u5229\u7528DRAM\u67b6\u6784\u7684\u5927\u89c4\u6a21\u5e76\u884c\u6027\u3002", "result": "\u5728\u7aef\u5230\u7aefLLM\u63a8\u7406\u8bc4\u4f30\u4e2d\uff0cRACAM\u76f8\u6bd4GPU\u5b9e\u73b09-102\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684DRAM-PIM\u7cfb\u7edfProteus\uff0c\u5728GPT3\u6848\u4f8b\u4e2d\u5b9e\u73b0\u6bcf\u5e73\u65b9\u6beb\u7c73233\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "RACAM\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u6620\u5c04\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709DRAM-PIM\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff09\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09427", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09427", "abs": "https://arxiv.org/abs/2512.09427", "authors": ["Guoqiang Zou", "Wanyu Wang", "Hao Zheng", "Longxiang Yin", "Yinhe Han"], "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators", "comment": "10 pages, 5 figures", "summary": "Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.", "AI": {"tldr": "ODMA\u662f\u4e00\u4e2a\u9488\u5bf9\u968f\u673a\u8bbf\u95ee\u53d7\u9650\u5185\u5b58\uff08RACM\uff09\u52a0\u901f\u5668\u7684\u6309\u9700\u5185\u5b58\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u957f\u5ea6\u9884\u6d4b\u5668\u548c\u52a8\u6001\u6876\u5206\u533a\uff0c\u663e\u8457\u63d0\u5347LLM\u670d\u52a1\u7684\u5185\u5b58\u5229\u7528\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5728\u968f\u673a\u8bbf\u95ee\u5e26\u5bbd\u8f83\u5dee\u7684\u52a0\u901f\u5668\uff08\u5982\u57fa\u4e8eLPDDR5\u7684Cambricon MLU370\uff09\u4e0a\u670d\u52a1\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u73b0\u6709\u5185\u5b58\u7ba1\u7406\u65b9\u6848\u5b58\u5728\u5c40\u9650\u6027\uff1a\u9759\u6001\u9884\u5206\u914d\u6d6a\u8d39\u5185\u5b58\uff0c\u800c\u7ec6\u7c92\u5ea6\u5206\u9875\uff08\u5982PagedAttention\uff09\u7531\u4e8e\u9ad8\u968f\u673a\u8bbf\u95ee\u6210\u672c\u800c\u4e0d\u9002\u7528\u3002\u73b0\u6709HBM\u4e2d\u5fc3\u89e3\u51b3\u65b9\u6848\u672a\u80fd\u5145\u5206\u5229\u7528RACM\u52a0\u901f\u5668\u7684\u7279\u6027\u3002", "method": "ODMA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u957f\u5ea6\u9884\u6d4b\u5668\u3001\u52a8\u6001\u6876\u5206\u533a\u548c\u5927\u6876\u4fdd\u62a4\u673a\u5236\u6765\u89e3\u51b3\u5206\u5e03\u6f02\u79fb\u548c\u91cd\u5c3e\u8bf7\u6c42\u95ee\u9898\u3002\u8fb9\u754c\u6839\u636e\u5b9e\u65f6\u8ddf\u8e2a\u5b9a\u671f\u66f4\u65b0\u4ee5\u6700\u5927\u5316\u5185\u5b58\u5229\u7528\u7387\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9RACM\u52a0\u901f\u5668\u7684\u786c\u4ef6\u7279\u6027\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728Alpaca\u548cGoogle-NQ\u6570\u636e\u96c6\u4e0a\uff0cODMA\u5c06\u9884\u6d4b\u51c6\u786e\u7387\u4ece82.68%\u663e\u8457\u63d0\u5347\u81f393.36%\u3002\u5728Cambricon MLU370-X4\u4e0a\u670d\u52a1DeepSeek-R1-Distill-Qwen-7B\u6a21\u578b\u65f6\uff0c\u5185\u5b58\u5229\u7528\u7387\u4ece55.05%\u63d0\u5347\u523072.45%\uff0cRPS\u548cTPS\u5206\u522b\u6bd4\u9759\u6001\u57fa\u7ebf\u63d0\u9ad8\u4e8629%\u548c27%\u3002", "conclusion": "\u786c\u4ef6\u611f\u77e5\u7684\u5185\u5b58\u5206\u914d\u80fd\u591f\u89e3\u9501RACM\u5e73\u53f0\u4e0a\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u3002ODMA\u6846\u67b6\u901a\u8fc7\u9488\u5bf9\u968f\u673a\u8bbf\u95ee\u53d7\u9650\u5185\u5b58\u7279\u6027\u7684\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u5229\u7528\u7387\u548c\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u5728\u7c7b\u4f3c\u786c\u4ef6\u4e0a\u90e8\u7f72LLM\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
