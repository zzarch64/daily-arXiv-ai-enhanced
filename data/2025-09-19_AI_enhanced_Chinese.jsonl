{"id": "2509.14388", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14388", "abs": "https://arxiv.org/abs/2509.14388", "authors": ["Lennart Bamberg", "Filippo Minnella", "Roberto Bosio", "Fabrizio Ottati", "Yuebin Wang", "Jongmin Lee", "Luciano Lavagno", "Adam Fuks"], "title": "eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations", "comment": "Submitted to IEEE Transactions on Computers", "summary": "Neural Processing Units (NPUs) are key to enabling efficient AI inference in\nresource-constrained edge environments. While peak tera operations per second\n(TOPS) is often used to gauge performance, it poorly reflects real-world\nperformance and typically rather correlates with higher silicon cost. To\naddress this, architects must focus on maximizing compute utilization, without\nsacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU,\nintegrated into a commercial flagship MPU, alongside co-designed compiler\nalgorithms. The architecture employs a flexible, data-driven design, while the\ncompiler uses a constrained programming approach to optimize compute and data\nmovement based on workload characteristics. Compared to the leading embedded\nNPU and compiler stack, our solution achieves an average speedup of 1.8x (4x\npeak) at equal TOPS and memory resources across standard AI-benchmarks. Even\nagainst NPUs with double the compute and memory resources, Neutron delivers up\nto 3.3x higher performance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86eIQ Neutron\u9ad8\u6548NPU\u67b6\u6784\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u8bbe\u8ba1\u548c\u7f16\u8bd1\u5668\u4f18\u5316\uff0c\u5728\u76f8\u540cTOPS\u548c\u5185\u5b58\u8d44\u6e90\u4e0b\u6bd4\u9886\u5148\u5d4c\u5165\u5f0fNPU\u6027\u80fd\u63d0\u53471.8\u500d\uff0c\u751a\u81f3\u5728\u9762\u5bf9\u53cc\u500d\u8ba1\u7b97\u8d44\u6e90\u7684NPU\u65f6\u4ecd\u80fd\u5b9e\u73b03.3\u500d\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edfNPU\u7684\u5cf0\u503cTOPS\u6307\u6807\u4e0d\u80fd\u771f\u5b9e\u53cd\u6620\u5b9e\u9645\u6027\u80fd\uff0c\u4e14\u901a\u5e38\u4e0e\u66f4\u9ad8\u7684\u7845\u6210\u672c\u76f8\u5173\u3002\u9700\u8981\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u540c\u65f6\u6700\u5927\u5316\u8ba1\u7b97\u5229\u7528\u7387\u3002", "method": "\u91c7\u7528\u7075\u6d3b\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u914d\u5408\u7f16\u8bd1\u5668\u4f7f\u7528\u7ea6\u675f\u7f16\u7a0b\u65b9\u6cd5\uff0c\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u4f18\u5316\u8ba1\u7b97\u548c\u6570\u636e\u79fb\u52a8\u3002", "result": "\u5728\u6807\u51c6AI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u9886\u5148\u7684\u5d4c\u5165\u5f0fNPU\u548c\u7f16\u8bd1\u5668\u5806\u6808\uff0c\u5728\u76f8\u540cTOPS\u548c\u5185\u5b58\u8d44\u6e90\u4e0b\u5b9e\u73b0\u5e73\u57471.8\u500d\u52a0\u901f\uff08\u5cf0\u503c4\u500d\uff09\u3002\u5373\u4f7f\u9762\u5bf9\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u7ffb\u500d\u7684NPU\uff0c\u4ecd\u80fd\u63d0\u4f9b\u6700\u9ad83.3\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "eIQ Neutron NPU\u901a\u8fc7\u67b6\u6784\u548c\u7f16\u8bd1\u5668\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5728\u8fb9\u7f18AI\u63a8\u7406\u4e2d\u4f18\u5316\u5b9e\u9645\u6027\u80fd\u800c\u975e\u5cf0\u503c\u6307\u6807\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.14551", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.14551", "abs": "https://arxiv.org/abs/2509.14551", "authors": ["Xinyue Wu", "Zixuan Li", "Fan Hu", "Ting Lin", "Xiaotian Zhao", "Runxi Wang", "Xinfei Guo"], "title": "Shift-Left Techniques in Electronic Design Automation: A Survey", "comment": null, "summary": "The chip design process involves numerous steps, beginning with defining\nproduct requirements and progressing through architectural planning,\nsystem-level design, and the physical layout of individual circuit blocks. As\nthe enablers of large-scale chip development, Electronic Design Automation\n(EDA) tools play a vital role in helping designers achieve high-quality\nresults. The Shift-Left methodology introduces a pathway toward creating\ndigital twins and fusing multiple design steps, thereby transitioning\ntraditionally sequential, physically-aware processes into virtual design\nenvironments. This shift allows designers to establish stronger correlations\nearlier and optimize designs more effectively. However, challenges remain,\nespecially in accurately replicating downstream behaviors and determining the\nright scope and timing for adoption. These challenges, in turn, have revealed\nnew opportunities for EDA vendors, physical designers, and logic designers\nalike. As the industry advances toward intelligent EDA tools and techniques, it\nis timely to reflect on Shift-Left progress made and the challenges that\nremain. The rise of AI techniques and the momentum of open-source design flows\nhave significantly strengthened prediction and modeling capabilities, making\ndata-driven methods increasingly relevant to the EDA community. This, in turn,\nenhances the ''Shift-Left'' features embedded in current tools. In this paper,\nwe present a comprehensive survey of existing and emerging paradigms in\nShift-Left research within EDA and the broader design ecosystem. Our goal is to\nprovide a unique perspective on the state of the field and its future\ndirections. Relevant papers mentioned are organized in\nhttps://github.com/iCAS-SJTU/Shift-Left-EDA-Papers.", "AI": {"tldr": "\u672c\u6587\u5bf9EDA\u9886\u57df\u4e2d\u7684Shift-Left\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u5206\u6790\u4e86\u73b0\u6709\u548c\u65b0\u5174\u7684\u7814\u7a76\u8303\u5f0f\uff0c\u91cd\u70b9\u5173\u6ce8AI\u6280\u672f\u548c\u5f00\u6e90\u8bbe\u8ba1\u6d41\u7a0b\u5982\u4f55\u589e\u5f3a\u9884\u6d4b\u5efa\u6a21\u80fd\u529b\uff0c\u63a8\u52a8\u4f20\u7edf\u4e32\u884c\u8bbe\u8ba1\u8fc7\u7a0b\u5411\u865a\u62df\u8bbe\u8ba1\u73af\u5883\u7684\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740\u82af\u7247\u8bbe\u8ba1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u4f20\u7edf\u4e32\u884c\u8bbe\u8ba1\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u3002Shift-Left\u65b9\u6cd5\u901a\u8fc7\u521b\u5efa\u6570\u5b57\u5b6a\u751f\u548c\u878d\u5408\u591a\u4e2a\u8bbe\u8ba1\u6b65\u9aa4\uff0c\u4f7f\u8bbe\u8ba1\u5e08\u80fd\u591f\u66f4\u65e9\u5efa\u7acb\u5f3a\u76f8\u5173\u6027\u5e76\u4f18\u5316\u8bbe\u8ba1\uff0c\u4f46\u51c6\u786e\u590d\u5236\u4e0b\u6e38\u884c\u4e3a\u548c\u786e\u5b9a\u91c7\u7528\u65f6\u673a\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u6587\u732e\u8c03\u67e5\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406EDA\u9886\u57df\u4e2dShift-Left\u76f8\u5173\u7814\u7a76\uff0c\u5305\u62ecAI\u6280\u672f\u7684\u5e94\u7528\u3001\u5f00\u6e90\u8bbe\u8ba1\u6d41\u7a0b\u7684\u53d1\u5c55\uff0c\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728EDA\u793e\u533a\u4e2d\u7684\u91cd\u8981\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u6280\u672f\u548c\u5f00\u6e90\u8bbe\u8ba1\u6d41\u7a0b\u663e\u8457\u589e\u5f3a\u4e86\u9884\u6d4b\u548c\u5efa\u6a21\u80fd\u529b\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728EDA\u793e\u533a\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u76f8\u5173\uff0c\u8fd9\u53cd\u8fc7\u6765\u589e\u5f3a\u4e86\u5f53\u524d\u5de5\u5177\u4e2d\u7684Shift-Left\u529f\u80fd\u3002", "conclusion": "Shift-Left\u65b9\u6cd5\u5728EDA\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672a\u6765\u9700\u8981\u7ee7\u7eed\u53d1\u5c55\u667a\u80fdEDA\u5de5\u5177\u548c\u6280\u672f\uff0c\u63a8\u52a8\u8bbe\u8ba1\u751f\u6001\u7cfb\u7edf\u5411\u66f4\u9ad8\u6548\u3001\u66f4\u96c6\u6210\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2509.14668", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.14668", "abs": "https://arxiv.org/abs/2509.14668", "authors": ["Yonghao Wang", "Jiaxin Zhou", "Hongqin Lyu", "Zhiteng Chao", "Tiancheng Wang", "Huawei Li"], "title": "DeepAssert: An LLM-Aided Verification Framework with Fine-Grained Assertion Generation for Modules with Extracted Module Specifications", "comment": "7 pages, 8 figures", "summary": "Assertion-Based Verification (ABV) is a crucial method for ensuring that\nlogic designs conform to their architectural specifications. However, existing\nassertion generation methods primarily rely on information either from the\ndesign specification, or register-transfer level (RTL) code. The former methods\nare typically limited to generating assertions for the top-level design. As the\ntop-level design is composed of different modules without module-level\nspecifications, they are unable to generate deep assertions that target the\ninternal functionality of modules. The latter methods often rely on a golden\nRTL model, which is difficult to obtain. To address the above limitations, this\npaper presents a novel large language model (LLM)-aided verification framework\nnamed DeepAssert. DeepAssert is capable of analyzing the invocation\nrelationships between modules and extracting independent specifications for\neach module with its I/O port information. These extracted specifications are\nsubsequently used to guide LLMs to automatically generate fine-grained deep\nassertions for these modules. Our evaluation demonstrates that DeepAssert\nsignificantly outperforms existing methods such as AssertLLM and Spec2Assertion\nin generating high-quality deep assertions for modules. Furthermore, when\nintegrated with these methods, DeepAssert can enhance the overall quality of\nthe assertions generated. This allows for a more comprehensive and effective\nverification process.", "AI": {"tldr": "DeepAssert\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u5206\u6790\u6a21\u5757\u95f4\u7684\u8c03\u7528\u5173\u7cfb\u5e76\u63d0\u53d6\u6a21\u5757\u7ea7\u89c4\u8303\uff0c\u81ea\u52a8\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u6df1\u5ea6\u65ad\u8a00\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65ad\u8a00\u751f\u6210\u65b9\u6cd5\u53ea\u80fd\u9488\u5bf9\u9876\u5c42\u8bbe\u8ba1\u6216\u4f9d\u8d56\u9ec4\u91d1RTL\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u751f\u6210\u9488\u5bf9\u6a21\u5757\u5185\u90e8\u529f\u80fd\u7684\u6df1\u5ea6\u65ad\u8a00\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u5757\u95f4\u8c03\u7528\u5173\u7cfb\uff0c\u63d0\u53d6\u5404\u6a21\u5757\u7684I/O\u7aef\u53e3\u4fe1\u606f\u548c\u72ec\u7acb\u89c4\u8303\uff0c\u5229\u7528LLM\u81ea\u52a8\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u6df1\u5ea6\u65ad\u8a00\u3002", "result": "DeepAssert\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6df1\u5ea6\u65ad\u8a00\u65b9\u9762\u663e\u8457\u4f18\u4e8eAssertLLM\u548cSpec2Assertion\u7b49\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u5347\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6574\u4f53\u65ad\u8a00\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u66f4\u5168\u9762\u6709\u6548\u7684\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u4e3a\u6a21\u5757\u7ea7\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14781", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.14781", "abs": "https://arxiv.org/abs/2509.14781", "authors": ["Yimin Wang", "Yue Jiet Chong", "Xuanyao Fong"], "title": "LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism", "comment": "Accepted to the 2025 International Conference on Computer-Aided\n  Design (ICCAD'25)", "summary": "Large language model (LLM) inference has been a prevalent demand in daily\nlife and industries. The large tensor sizes and computing complexities in LLMs\nhave brought challenges to memory, computing, and databus. This paper proposes\na computation/memory/communication co-designed non-von Neumann accelerator by\naggregating processing-in-memory (PIM) and computational network-on-chip (NoC),\ntermed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC\nbased on the data dynamicity to maximize data locality. Model partition and\nmapping are optimized by heuristic design space exploration. Dedicated\nfine-grained parallelism and tiling techniques enable high-throughput dataflow\nacross the distributed resources in PIM and NoC. The architecture is evaluated\non Llama 1B/8B/13B models and shows $\\sim$2.55$\\times$ throughput (tokens/sec)\nimprovement and $\\sim$71.94$\\times$ energy efficiency (tokens/Joule) boost\ncompared to the A100 GPU.", "AI": {"tldr": "LEAP\u662f\u4e00\u79cd\u975e\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u7684LLM\u63a8\u7406\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5185\u5b58\u8ba1\u7b97(PIM)\u548c\u7247\u4e0a\u8ba1\u7b97\u7f51\u7edc(NoC)\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5728Llama\u6a21\u578b\u4e0a\u76f8\u6bd4A100 GPU\u5b9e\u73b0\u4e862.55\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c71.94\u500d\u80fd\u6548\u63d0\u5347", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u5176\u5de8\u5927\u7684\u5f20\u91cf\u89c4\u6a21\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7ed9\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u6570\u636e\u603b\u7ebf\u5e26\u6765\u4e86\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u786c\u4ef6\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "\u63d0\u51fa\u8ba1\u7b97/\u5185\u5b58/\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\u7684\u975e\u51af\u00b7\u8bfa\u4f9d\u66fc\u52a0\u901f\u5668\uff0c\u7ed3\u5408PIM\u548c\u8ba1\u7b97NoC\uff0c\u6839\u636e\u6570\u636e\u52a8\u6001\u6027\u5c06\u77e9\u9635\u4e58\u6cd5\u5206\u914d\u5230PIM\u6216NoC\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4f18\u5316\u6a21\u578b\u5212\u5206\u548c\u6620\u5c04\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u5e76\u884c\u548c\u5206\u5757\u6280\u672f\u5b9e\u73b0\u9ad8\u541e\u5410\u6570\u636e\u6d41", "result": "\u5728Llama 1B/8B/13B\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4A100 GPU\u5b9e\u73b0\u4e86\u7ea62.55\u500d\u7684\u541e\u5410\u91cf(tokens/sec)\u63d0\u5347\u548c\u7ea671.94\u500d\u7684\u80fd\u6548(tokens/Joule)\u63d0\u5347", "conclusion": "LEAP\u67b6\u6784\u901a\u8fc7\u8ba1\u7b97/\u5185\u5b58/\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u80fd\u6548\uff0c\u4e3aLLM\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.15036", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.15036", "abs": "https://arxiv.org/abs/2509.15036", "authors": ["Yuehai Chen", "Farhad Merchant"], "title": "NEURAL: An Elastic Neuromorphic Architecture with Hybrid Data-Event Execution and On-the-fly Attention Dataflow", "comment": "Accepted by ASP-DAC 2026; 7 pages, 10 figures", "summary": "Spiking neural networks (SNNs) have emerged as a promising alternative to\nartificial neural networks (ANNs), offering improved energy efficiency by\nleveraging sparse and event-driven computation. However, existing hardware\nimplementations of SNNs still suffer from the inherent spike sparsity and\nmulti-timestep execution, which significantly increase latency and reduce\nenergy efficiency. This study presents NEURAL, a novel neuromorphic\narchitecture based on a hybrid data-event execution paradigm by decoupling\nsparsity-aware processing from neuron computation and using elastic\nfirst-in-first-out (FIFO). NEURAL supports on-the-fly execution of spiking\nQKFormer by embedding its operations within the baseline computing flow without\nrequiring dedicated hardware units. It also integrates a novel\nwindow-to-time-to-first-spike (W2TTFS) mechanism to replace average pooling and\nenable full-spike execution. Furthermore, we introduce a knowledge distillation\n(KD)-based training framework to construct single-timestep SNN models with\ncompetitive accuracy. NEURAL is implemented on a Xilinx Virtex-7 FPGA and\nevaluated using ResNet-11, QKFResNet-11, and VGG-11. Experimental results\ndemonstrate that, at the algorithm level, the VGG-11 model trained with KD\nimproves accuracy by 3.20% on CIFAR-10 and 5.13% on CIFAR-100. At the\narchitecture level, compared to existing SNN accelerators, NEURAL achieves a\n50% reduction in resource utilization and a 1.97x improvement in energy\nefficiency.", "AI": {"tldr": "NEURAL\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u5f62\u6001\u67b6\u6784\uff0c\u91c7\u7528\u6df7\u5408\u6570\u636e-\u4e8b\u4ef6\u6267\u884c\u8303\u5f0f\uff0c\u901a\u8fc7\u89e3\u8026\u7a00\u758f\u611f\u77e5\u5904\u7406\u548c\u795e\u7ecf\u5143\u8ba1\u7b97\uff0c\u7ed3\u5408\u5f39\u6027FIFO\u548cW2TTFS\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\uff0c\u5728\u51c6\u786e\u7387\u548c\u80fd\u6548\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709SNN\u786c\u4ef6\u5b9e\u73b0\u5b58\u5728\u8109\u51b2\u7a00\u758f\u6027\u548c\u591a\u65f6\u95f4\u6b65\u6267\u884c\u7684\u56fa\u6709\u95ee\u9898\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\u548c\u80fd\u6548\u964d\u4f4e\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faNEURAL\u67b6\u6784\uff1a1\uff09\u6df7\u5408\u6570\u636e-\u4e8b\u4ef6\u6267\u884c\u8303\u5f0f\uff0c\u89e3\u8026\u7a00\u758f\u611f\u77e5\u5904\u7406\u4e0e\u795e\u7ecf\u5143\u8ba1\u7b97\uff1b2\uff09\u5f39\u6027FIFO\u652f\u6301\uff1b3\uff09W2TTFS\u673a\u5236\u66ff\u4ee3\u5e73\u5747\u6c60\u5316\uff1b4\uff09\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8bad\u7ec3\u6846\u67b6\u6784\u5efa\u5355\u65f6\u95f4\u6b65SNN\u6a21\u578b\u3002", "result": "\u5728CIFAR-10\u4e0a\u51c6\u786e\u7387\u63d0\u53473.20%\uff0cCIFAR-100\u4e0a\u63d0\u53475.13%\uff1b\u4e0e\u73b0\u6709SNN\u52a0\u901f\u5668\u76f8\u6bd4\uff0c\u8d44\u6e90\u5229\u7528\u7387\u964d\u4f4e50%\uff0c\u80fd\u6548\u63d0\u53471.97\u500d\u3002", "conclusion": "NEURAL\u67b6\u6784\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u6267\u884c\u8303\u5f0f\u548c\u786c\u4ef6\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86SNN\u7684\u7a00\u758f\u6027\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2509.15205", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.15205", "abs": "https://arxiv.org/abs/2509.15205", "authors": ["Kartik Prabhu", "Jeffrey Yu", "Xinyuan Allen Pan", "Zhouhua Xie", "Abigail Aleshire", "Zihan Chen", "Ammar Ali Ratnani", "Priyanka Raina"], "title": "Voyager: An End-to-End Framework for Design-Space Exploration and Generation of DNN Accelerators", "comment": null, "summary": "While deep neural networks (DNNs) have achieved state-of-the-art performance\nin fields from computer vision to natural language processing, efficiently\nrunning these computationally demanding models requires hardware accelerators.\nHowever, designing these accelerators is a time-consuming, labor-intensive\nprocess that does not scale well. While prior efforts have sought to automate\nDNN accelerator generation, they offer limited parameterization, cannot produce\nhigh-performance, tapeout-ready designs, provide limited support for datatypes\nand quantization schemes, and lack an integrated, end-to-end software compiler.\nThis work proposes Voyager, a high-level synthesis (HLS)-based framework for\ndesign space exploration (DSE) and generation of DNN accelerators. Voyager\novercomes the limitations of prior work by offering extensive configurability\nacross technology nodes, clock frequencies, and scales, with customizable\nparameters such as number of processing elements, on-chip buffer sizes, and\nexternal memory bandwidth. Voyager supports a wider variety of datatypes and\nquantization schemes versus prior work, including both built-in floating-point,\nposit and integer formats, as well as user-defined formats with both per-tensor\nscaling and microscaling quantization. Voyager's PyTorch-based compiler\nefficiently maps networks end-to-end on the generated hardware, with support\nfor quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art\nvision and language models. Voyager enables fast DSE with full-dataset accuracy\nevaluation for datatypes and quantization schemes. Generated designs achieve a\nhigh utilization across models and scales, up to 99.8%, and outperform prior\ngenerators with up to 61% lower latency and 56% lower area. Compared to\nhand-optimized accelerators, Voyager achieves comparable performance, while\noffering much greater automation in design and workload mapping.", "AI": {"tldr": "Voyager\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u5c42\u6b21\u7efc\u5408(HLS)\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u81ea\u52a8\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53c2\u6570\u5316\u3001\u9ad8\u6027\u80fd\u8bbe\u8ba1\u751f\u6210\u3001\u6570\u636e\u7c7b\u578b\u652f\u6301\u548c\u7aef\u5230\u7aef\u7f16\u8bd1\u5668\u96c6\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u786c\u4ef6\u52a0\u901f\u5668\u6765\u9ad8\u6548\u8fd0\u884c\uff0c\u4f46\u624b\u52a8\u8bbe\u8ba1\u52a0\u901f\u5668\u8017\u65f6\u8017\u529b\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u53c2\u6570\u5316\u6709\u9650\u3001\u65e0\u6cd5\u751f\u6210\u9ad8\u6027\u80fd\u8bbe\u8ba1\u3001\u6570\u636e\u7c7b\u578b\u652f\u6301\u4e0d\u8db3\u4ee5\u53ca\u7f3a\u4e4f\u7aef\u5230\u7aef\u7f16\u8bd1\u5668\u7b49\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9ad8\u5c42\u6b21\u7efc\u5408(HLS)\u6846\u67b6\uff0c\u63d0\u4f9b\u8de8\u6280\u672f\u8282\u70b9\u3001\u65f6\u949f\u9891\u7387\u548c\u89c4\u6a21\u7684\u53ef\u914d\u7f6e\u6027\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u91cf\u5316\u65b9\u6848\uff0c\u5305\u62ec\u6d6e\u70b9\u6570\u3001posit\u548c\u6574\u6570\u683c\u5f0f\uff0c\u4ee5\u53ca\u7528\u6237\u81ea\u5b9a\u4e49\u683c\u5f0f\u3002\u96c6\u6210PyTorch\u7f16\u8bd1\u5668\u8fdb\u884c\u7aef\u5230\u7aef\u7f51\u7edc\u6620\u5c04\u3002", "result": "\u5728\u5148\u8fdb\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cVoyager\u5b9e\u73b0\u9ad8\u8fbe99.8%\u7684\u5229\u7528\u7387\uff0c\u76f8\u6bd4\u5148\u524d\u751f\u6210\u5668\u5ef6\u8fdf\u964d\u4f4e61%\u3001\u9762\u79ef\u51cf\u5c1156%\uff0c\u4e0e\u624b\u5de5\u4f18\u5316\u52a0\u901f\u5668\u6027\u80fd\u76f8\u5f53\u4f46\u81ea\u52a8\u5316\u7a0b\u5ea6\u66f4\u9ad8\u3002", "conclusion": "Voyager\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u6027\u80fdDNN\u52a0\u901f\u5668\uff0c\u652f\u6301\u5feb\u901f\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u5b8c\u6574\u6570\u636e\u96c6\u7cbe\u5ea6\u8bc4\u4f30\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u81ea\u52a8\u5316\u6c34\u5e73\u3002"}}
