<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 14]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11895)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 提出一种基于扩展卡尔曼滤波的自适应闭环测试方法，用于高效测试高分辨率SAR ADC的线性度，显著减少测试时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法（如直方图法、正弦波测试等）依赖密集数据采集和离线后处理，导致测试时间长、复杂度高。

Method: 使用扩展卡尔曼滤波实时迭代优化行为模型，动态选择测量点以最大化信息增益，直接估计决定INL行为的电容失配参数。

Result: 实验结果表明该方法大幅减少了总测试时间和计算开销。

Conclusion: 该方法适合集成到生产环境中，为高分辨率SAR ADC提供高效的线性度测试解决方案。

Abstract: This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments.

</details>


### [2] [Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11917)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 增强型UGLMS方法显著加快SAR ADC线性度测试速度，通过秩-1 EKF更新、协方差膨胀策略、载波多项式扩展和基于轨迹的终止机制，在保持精度的同时将16位ADC测试时间缩短至36ms，速度提升8倍。


<details>
  <summary>Details</summary>
Motivation: 传统SAR ADC测试需要全范围扫描和离线后处理，耗时且不适合生产环境。需要开发实时、高效的测试方法。

Method: 1. 秩-1 EKF更新替代矩阵求逆；2. 测量对齐的协方差膨胀策略加速收敛；3. 扩展静态失配模型包含载波多项式捕获系统非线性；4. 基于轨迹的测试终止机制。

Result: 16位ADC完整INL/DNL重建仅需36ms，18位ADC小于70ms（多项式扩展后120ms），精度相同但速度提升8倍。

Conclusion: 增强型UGLMS实现了实时、生产就绪的SAR ADC线性度测试，显著提高了测试效率。

Abstract: This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing.

</details>


### [3] [TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space](https://arxiv.org/abs/2511.12035)
*Wenxuan Miao,Yulin Sun,Aiyue Chen,Jing Lin,Yiwu Yao,Yiming Gan,Jieru Zhao,Jingwen Leng,Mingyi Guo,Yu Feng*

Main category: cs.AR

TL;DR: 提出了一种基于时空相关性的轻量级自适应重用策略，通过在视频扩散变换器(vDiT)中重用时空相关token的部分注意力分数来加速自注意力计算，实现85%的计算节省且视频质量损失极小。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型主要基于vDiT架构，但自注意力机制导致显著的推理延迟。先前研究主要关注减少自注意力中的冗余计算，但忽视了视频流中固有的时空相关性。

Method: 利用潜在空间中的时空相关性，提出轻量级自适应重用策略，通过重用空间或时间相关token在单个通道上的部分注意力分数来近似注意力计算。

Result: 在4个vDiT模型上实现了85%的计算节省，同时视频质量损失极小（VBench上<0.06%）。

Conclusion: 该方法通过利用视频数据中的时空相关性，在保持视频质量的同时显著加速了vDiT的自注意力计算，计算效率优于现有技术。

Abstract: The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.
  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\% loss on VBench).

</details>


### [4] [A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation](https://arxiv.org/abs/2511.12152)
*Jianyi Yu,Yuxuan Wang,Xiang Fu,Fei Qiao,Ying Wang,Rui Yuan,Liyuan Liu,Cong Shi*

Main category: cs.AR

TL;DR: 提出了一种用于Transformer注意力计算的数字存内计算宏，通过重新设计注意力得分计算过程和使用比特串行移位加法，实现了高能效和面积效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统存内计算范式不适合动态矩阵乘法的问题，缓解计算单元和存储单元之间大量数据移动导致的功耗和延迟瓶颈。

Method: 基于组合QK权重矩阵重新制定注意力得分计算过程，将二项式矩阵乘法分解为4组比特串行移位和加法，采用零值比特跳过、数据驱动字线激活等技术优化电路能效。

Result: 在65nm工艺下实现，面积0.35mm²，峰值性能42.27 GOPS，功耗1.24mW，能效34.1 TOPS/W，面积效率120.77 GOPS/mm²。相比CPU和GPU分别提升25倍和13倍能效。

Conclusion: 该设计在能效和面积效率方面显著优于现有Transformer存内计算方案，展示了在边缘智能应用中的潜力。

Abstract: Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications.

</details>


### [5] [Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing](https://arxiv.org/abs/2511.12286)
*Khyati Kiyawat,Zhenxing Fan,Yasas Seneviratne,Morteza Baradaran,Akhil Shekar,Zihan Xia,Mingu Kang,Kevin Skadron*

Main category: cs.AR

TL;DR: Sangam是一个基于CXL连接的PIM-chiplet内存模块，通过将逻辑和内存解耦到不同工艺节点的chiplet中来解决现有内存计算方案的局限性，显著提升大语言模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型变得日益数据密集，推理阶段特别是解码阶段受限于内存带宽，而现有的内存计算方案面临内存容量减少和处理能力有限的问题。

Method: 提出chiplet-based内存模块，将逻辑和DRAM解耦到不同工艺节点的chiplet中，通过中介层连接，逻辑chiplet支持高带宽访问DRAM chiplet，集成脉动阵列和SRAM缓冲区来加速内存密集型GEMM操作。

Result: 相比H100 GPU，Sangam在LLaMA 2-7B、Mistral-7B和LLaMA 3-70B上分别实现了3.93、4.22、2.82倍的端到端查询延迟加速，10.3、9.5、6.36倍的解码吞吐量提升，以及数量级的能耗节省。

Conclusion: Sangam架构通过异构chiplet设计有效解决了现有PIM方案的局限性，为大语言模型推理提供了显著性能提升和能效优化。

Abstract: Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.

</details>


### [6] [Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting](https://arxiv.org/abs/2511.12349)
*Divya Kiran Kadiyala,Alexandros Daglis*

Main category: cs.AR

TL;DR: SURGE是一种软件支持的架构技术，通过利用空闲的I/O带宽资源来提升内存带宽可用性，解决服务器CPU内存带宽受限问题。


<details>
  <summary>Details</summary>
Motivation: 随着服务器CPU核心数量增加，内存系统面临带宽限制，而传统CPU设计中内存和I/O带宽固定分配导致资源利用率低下。

Method: 利用CXL等灵活互连技术，在相同处理器接口上动态复用内存和I/O流量，实现带宽资源灵活分配。

Result: SURGE增强的架构可以将内存密集型工作负载在带宽受限服务器上的性能提升高达1.3倍。

Conclusion: 通过实现内存和I/O带宽的可互换性，SURGE能够显著提高有限芯片外带宽资源的利用率。

Abstract: The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.
  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x.

</details>


### [7] [FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration](https://arxiv.org/abs/2511.12544)
*Mukul Lokhande,Akash Sankhe,S. V. Jaya Chand,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: FERMI-ML是一种用于TinyML加速的灵活资源高效内存原位SRAM宏，采用9T XNOR位单元和22T压缩树累加器，支持可变精度MAC和CAM操作，在65nm工艺下实现1.93 TOPS吞吐量和364 TOPS/W能效。


<details>
  <summary>Details</summary>
Motivation: AIoT设备对低功耗和面积高效的TinyML推理需求日益增长，需要最小化数据移动同时保持高计算效率的内存架构。

Method: 提出9T XNOR基RX9T位单元，集成5T存储单元和4T XNOR计算单元；采用22T压缩树累加器进行对数1-64位MAC计算；4KB宏支持原位计算和CAM查找操作，兼容Posit-4或FP-4精度。

Result: 65nm后布局结果显示在350MHz、0.9V下运行，吞吐量1.93 TOPS，能效364 TOPS/W，InceptionV4和ResNet-18的QoR超过97.5%。

Conclusion: FERMI-ML展示了紧凑、可重构、能量感知的数字内存原位宏，能够支持混合精度TinyML工作负载。

Abstract: The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.

</details>


### [8] [SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration](https://arxiv.org/abs/2511.12616)
*Arya Parameshwara*

Main category: cs.AR

TL;DR: SynapticCore-X是一个针对低成本FPGA优化的模块化神经处理架构，集成了轻量级RISC-V控制核心和可配置神经计算单元，提供完全开源的SystemVerilog微架构。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA加速器依赖重量级IP模块，缺乏开源和可配置性。本文旨在降低神经网络微架构研究的入门门槛，为学术和开源硬件研究提供可定制的解决方案。

Method: 设计集成了RV32IMC RISC-V控制核心和可配置神经计算单元，支持融合矩阵、激活和数据移动操作。采用可调并行度、暂存器深度和DMA突发行为，实现硬件-软件协同设计。

Result: 在Zynq-7020上实现100MHz时序收敛，仅消耗6.1% LUTs、32.5% DSPs和21.4% BRAMs。PYNQ-Z2硬件验证确认了寄存器级执行正确性、确定性控制路径行为和周期精确性能。

Conclusion: SynapticCore-X证明能在商用教育FPGA上实现类似NPU的高效能加速原型，为神经微架构研究降低了入门门槛。

Abstract: This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures.

</details>


### [9] [Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs](https://arxiv.org/abs/2511.12860)
*Yongjoo Jang,Sangwoo Hwang,Hojin Lee,Sangwoo Jung,Donghun Lee,Wonbo Shim,Jaeha Kung*

Main category: cs.AR

TL;DR: 提出将单批次token生成卸载到3D NAND闪存PIM设备，利用其高存储密度突破DRAM容量限制，实现高效LLM推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数规模达数十亿，内存和计算需求激增，传统硬件因DRAM容量有限和GPU成本高昂难以有效服务。

Method: 探索3D NAND闪存配置，重新设计带H-tree网络的PIM阵列，开发操作分块和LLM层映射方法。

Result: 相比4个RTX4090+vLLM实现2.4倍加速，与4个A100性能相当仅4.9%延迟开销，可在4.98mm²芯片面积内集成。

Conclusion: 3D NAND闪存PIM架构能有效解决LLM推理的存储瓶颈，提供高密度、低成本解决方案。

Abstract: The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead.

</details>


### [10] [Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration](https://arxiv.org/abs/2511.12930)
*Changhun Oh,Seongryong Oh,Jinwoo Hwang,Yoonsung Kim,Hardik Sharma,Jongse Park*

Main category: cs.AR

TL;DR: Neo提出了重用更新排序算法和硬件加速器，通过利用连续帧间高斯排序的时间冗余，显著降低了3D高斯泼溅渲染中的排序瓶颈，实现了比现有方案高达10倍的吞吐量提升和94.5%的DRAM流量减少。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上实现实时3D高斯泼溅渲染对于沉浸式AR/VR体验至关重要，但现有解决方案难以实现高帧率，特别是高分辨率渲染时，排序阶段成为主要瓶颈。

Method: 提出了重用更新排序算法，利用连续帧间高斯排序的时间冗余，通过跟踪和更新高斯深度排序而非从头重新排序；同时设计了针对该算法的硬件加速器。

Result: 实验结果显示，Neo相比最先进的边缘GPU和ASIC解决方案分别实现了10.0倍和5.6倍的吞吐量提升，同时将DRAM流量减少了94.5%和81.3%。

Conclusion: Neo的改进使得高质量、低延迟的设备端3D渲染更加实用可行。

Abstract: 3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.

</details>


### [11] [Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT](https://arxiv.org/abs/2511.13139)
*Zhiteng Chao,Yonghao Wang,Xinyu Zhang,Jiaxin Zhou,Tenghui Hua,Husheng Han,Tianmeng Yang,Jianan Mu,Bei Yu,Rui Zhang,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: VeriBToT是一种专门用于自动化Verilog生成的LLM推理范式，通过集成自上而下和设计验证方法，实现中间步骤的自解耦和自验证，构建具有形式化算子的回溯思维树。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂设计生成时缺乏针对性的解耦策略，且评估解耦子任务正确性困难。传统的CoT方法在自动化IC设计工作流中效果不佳，需要人工干预，主要问题是无法控制CoT推理方向和步骤粒度，与专家RTL设计知识不匹配。

Method: 提出VeriBToT范式，集成Top-down和设计验证(DFV)方法，实现自解耦和自验证中间步骤，构建具有形式化算子的回溯思维树。

Result: 与传统CoT范式相比，该方法增强了Verilog生成能力，同时通过灵活的模块化、层次化和可重用性优化了token成本。

Conclusion: VeriBToT为自动化IC设计提供了一种有效的LLM推理范式，解决了传统方法在控制推理方向和步骤粒度方面的局限性。

Abstract: Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability.

</details>


### [12] [Coliseum project: Correlating climate change data with the behavior of heritage materials](https://arxiv.org/abs/2511.13343)
*A Cormier,David Roqui,Fabrice Surma,Martin Labouré,Jean-Marc Vallet,Odile Guillon,N Grozavu,Ann Bourgès*

Main category: cs.AR

TL;DR: COLISEUM项目开发了一种多模态数据收集方法，结合人工智能模型预测文化遗产材料在气候变化下的风化行为。项目在法国三个不同气候和材料的遗址进行监测，通过微气候传感器、化学分析和科学成像等数据建立风化模型。


<details>
  <summary>Details</summary>
Motivation: 气候变化正在影响文化遗产材料，增加的气候变化减少了古迹的寿命。由于风化依赖于多种因素，很难将其进展与气候变化联系起来。需要收集气候数据并同时监测恶化进展来预测风化。

Method: 在法国三个文化遗址建立气候监测方法：斯特拉斯堡圣母大教堂、比布拉克特考古遗址和维尔弗朗什滨海圣皮埃尔教堂。使用微气候传感器连续记录参数变化，通过化学分析、制图测量和科学成像定期监测风化状态。数据通过计算风化指数收集到风化矩阵中，使用人工智能计算机模型实现风化预测。

Result: 本文介绍了仪器方法、初步诊断和以斯特拉斯堡大教堂遗址为例的首批结果。建立了多模态数据收集系统，能够将不同时间尺度的图像、文本等数据进行关联分析。

Conclusion: COLISEUM项目提供了一种预测文化遗产材料行为的系统方法，通过结合现场监测数据和IPCC气候变化情景，预测模型将能够利用已知数据预测未来材料行为，为文化遗产保护提供科学依据。

Abstract: Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site.

</details>


### [13] [T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization](https://arxiv.org/abs/2511.13676)
*Hyunwoo Oh,KyungIn Nam,Rajat Bhattacharjya,Hanning Chen,Tamoghno Das,Sanggeon Yun,Suyeon Jang,Andrew Ding,Nikil Dutt,Mohsen Imani*

Main category: cs.AR

TL;DR: T-SAR是首个在CPU上实现可扩展三元LLM推理的框架，通过重新利用SIMD寄存器文件进行动态寄存器内查找表生成，消除了内存瓶颈，显著提升了计算性能。


<details>
  <summary>Details</summary>
Motivation: LLM的快速发展超过了边缘平台的计算和内存能力，现有CPU解决方案依赖内存查找表限制了可扩展性，而FPGA或GPU加速器在边缘场景中不实用。

Method: 重新利用SIMD寄存器文件进行动态寄存器内查找表生成，仅需最小的硬件修改，消除内存瓶颈并最大化数据级并行性。

Result: 在GEMM延迟和GEMV吞吐量上分别实现5.6-24.5倍和1.1-86.2倍的提升，SIMD单元仅增加3.2%功耗和1.4%面积开销，能源效率达到NVIDIA Jetson AGX Orin的2.5-4.9倍。

Conclusion: T-SAR为边缘平台上的高效LLM推理提供了一种实用方法，通过最小硬件修改实现了显著性能提升。

Abstract: Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.

</details>


### [14] [QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention](https://arxiv.org/abs/2511.13679)
*Hyunwoo Oh,Hanning Chen,Sanggeon Yun,Yang Ni,Wenjun Huang,Tamoghno Das,Suyeon Jang,Mohsen Imani*

Main category: cs.AR

TL;DR: QUILL是一个针对可变形注意力机制优化的硬件加速器，通过距离排序查询和预取技术将不规则内存访问转化为缓存友好的单次计算，显著提升检测模型的硬件效率。


<details>
  <summary>Details</summary>
Motivation: 可变形变换器在检测任务中达到最先进性能，但由于不规则内存访问和低算术强度，在硬件上映射效果差，需要专门的加速方案。

Method: 采用距离排序查询(DOOQ)按空间邻近性排序查询，结合前瞻驱动的区域预取形成调度感知预取循环，融合MSDeformAttn引擎在单次计算中完成插值、Softmax、聚合和投影操作。

Result: 相比RTX 4090实现7.29倍吞吐量提升和47.3倍能效提升，相比现有加速器吞吐量提升3.26-9.82倍，能效提升2.01-6.07倍，混合精度量化下精度损失小于0.9 AP。

Conclusion: 通过将稀疏性转化为局部性，再将局部性转化为利用率，QUILL实现了端到端的持续加速效果。

Abstract: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.

</details>
