{"id": "2602.13434", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.13434", "abs": "https://arxiv.org/abs/2602.13434", "authors": ["Maccoy Merrell", "Daniel Puckett", "Gino Chacon", "Jeffrey Stuecheli", "Stavros Kalafatis", "Paul V. Gratz"], "title": "ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory", "comment": "15 pages, 19 figures", "summary": "Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%.", "AI": {"tldr": "Rowhammer\u7f13\u89e3\u673a\u5236\u4e0e\u786c\u4ef6\u9884\u53d6\u5668\u5b58\u5728\u4ea4\u53c9\u5e72\u6270\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9884\u53d6\u6027\u80fd\u6536\u76ca\u3002\u672c\u6587\u63d0\u51faORAP\u9884\u53d6\u5668\uff0c\u901a\u8fc7\u7f13\u5b58DRAM\u884c\u7f13\u51b2\u533a\u5185\u5bb9\u51cf\u5c11\u6fc0\u6d3b\u6b21\u6570\uff0c\u5728RFM\u7f13\u89e3\u7cfb\u7edf\u4e2d\u63d0\u5347\u6027\u80fd4.6%\uff0c\u5728PRAC\u7f13\u89e3\u7cfb\u7edf\u4e2d\u964d\u4f4e\u80fd\u801711.8%\u3002", "motivation": "\u73b0\u6709Rowhammer\u7f13\u89e3\u65b9\u6848\u8bc4\u4f30\u901a\u5e38\u5ffd\u7565\u786c\u4ef6\u9884\u53d6\u5668\u7684\u5f71\u54cd\u3002\u786c\u4ef6\u9884\u53d6\u5668\u4f1a\u589e\u52a0DRAM\u6fc0\u6d3b\u7387\uff0c\u800cRowhammer\u7f13\u89e3\u7684\u6027\u80fd\u5f00\u9500\u4e0e\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u76f4\u63a5\u76f8\u5173\uff0c\u4e24\u8005\u5b58\u5728\u4ea4\u53c9\u5e72\u6270\uff0c\u5bfc\u81f4\u9884\u53d6\u5668\u7684\u6027\u80fd\u6536\u76ca\u4e25\u91cd\u53d7\u635f\u3002", "method": "\u63d0\u51fa\u4f18\u5316\u884c\u8bbf\u95ee\u9884\u53d6\u5668(ORAP)\uff0c\u5229\u7528\u6700\u540e\u4e00\u7ea7\u7f13\u5b58(LLC)\u7a7a\u95f4\u7f13\u5b58DRAM\u884c\u7f13\u51b2\u533a\u7684\u5927\u90e8\u5206\u5185\u5bb9\uff0c\u51cf\u5c11\u672a\u6765\u7684\u884c\u6fc0\u6d3b\u9700\u6c42\u3002\u4e0e\u6700\u5148\u8fdb\u7684Berti\u9884\u53d6\u5668\u534f\u540c\u5de5\u4f5c\uff0c\u964d\u4f4eDRAM\u6fc0\u6d3b\u7387\u3002", "result": "\u5728RFM\u7f13\u89e3\u5185\u5b58\u7cfb\u7edf\u4e2d\uff0cORAP\u5c06DRAM\u6fc0\u6d3b\u7387\u964d\u4f4e51.3%\uff0c\u76f8\u6bd4Berti\u548cSPP-PPF\u9884\u53d6\u5668\u914d\u7f6e\u5b9e\u73b04.6%\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728PRAC\u7f13\u89e3\u7cfb\u7edf\u4e2d\uff0cORAP\u964d\u4f4e\u80fd\u8017\u5f00\u950011.8%\u3002", "conclusion": "Rowhammer\u7f13\u89e3\u4e0e\u786c\u4ef6\u9884\u53d6\u5668\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u4ea4\u53c9\u5e72\u6270\u95ee\u9898\u3002ORAP\u901a\u8fc7\u7f13\u5b58\u884c\u7f13\u51b2\u533a\u5185\u5bb9\u6709\u6548\u51cf\u5c11DRAM\u6fc0\u6d3b\uff0c\u65e2\u4fdd\u6301\u4e86\u9884\u53d6\u6027\u80fd\u6536\u76ca\uff0c\u53c8\u964d\u4f4e\u4e86Rowhammer\u7f13\u89e3\u7684\u5f00\u9500\uff0c\u4e3a\u7f13\u89e3\u673a\u5236\u4e0e\u9884\u53d6\u5668\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13825", "categories": ["cs.AR", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.13825", "abs": "https://arxiv.org/abs/2602.13825", "authors": ["Paras Tiwari", "Narendra Singh Dhakad", "Shalu Rani", "Sanjay Kumar", "Themis Prodromakis"], "title": "Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits", "comment": null, "summary": "In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5fc6\u963b\u5668\u7684\u57fa\u672c\u903b\u8f91\u95e8\u548c\u65f6\u5e8f\u903b\u8f91\u7535\u8def\u8bbe\u8ba1\uff0c\u91c7\u752890nm CMOS\u5de5\u827a\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u663e\u8457\u964d\u4f4e\u4e86\u9762\u79ef\u3001\u529f\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "\u63a2\u7d22\u5fc6\u963b\u5668\u5728\u903b\u8f91\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u3001\u7d27\u51d1\u7684\u7535\u8def\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u4f20\u7edfCMOS\u6280\u672f\u7684\u9650\u5236\u3002", "method": "\u5728Cadence Virtuoso\u7684SPECTRE\u4e2d\u8bbe\u8ba1\u548c\u4f18\u5316\u5fc6\u963b\u5668\u9a71\u52a8\u7684\u903b\u8f91\u95e8\uff08NOT\u3001AND\u3001NAND\u3001OR\u3001NOR\u3001XOR\uff09\u548c\u65f6\u5e8f\u903b\u8f91\u7535\u8def\uff08D\u3001T\u3001JK\u3001SR\u89e6\u53d1\u5668\uff09\uff0c\u91c7\u752890nm CMOS\u5de5\u827a\u96c6\u6210\uff0c\u5e76\u4f7f\u7528Y2O3\u57fa\u5fc6\u963b\u5668\u4ef6\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5fc6\u963b\u5668\u57fa\u8bbe\u8ba1\u76f8\u6bd4\u73b0\u6709\u65f6\u5e8f\u7535\u8def\u6280\u672f\uff0c\u9762\u79ef\u51cf\u5c11\u7ea624%\uff0c\u529f\u8017\u964d\u4f4e60%\uff0c\u5ef6\u8fdf\u51cf\u5c1158%\uff0c\u4e14\u5fc6\u963b\u5668\u5728D2D\u548cC2C\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u4f4e\u53d8\u5f02\u6027\u3002", "conclusion": "\u5fc6\u963b\u5668\u57fa\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u903b\u8f91\u7535\u8def\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5fc6\u963b\u5668\u5728\u5b9e\u73b0\u4f4e\u529f\u8017\u3001\u4f4e\u6210\u672c\u3001\u8d85\u5feb\u901f\u3001\u7d27\u51d1\u7535\u8def\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.14262", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14262", "abs": "https://arxiv.org/abs/2602.14262", "authors": ["Siddhartha Raman Sundara Raman", "Jaydeep P. Kulkarni"], "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute", "comment": null, "summary": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u5bc6\u96c6\u6210\u7684\u8fd1\u5185\u5b58GPU\u67b6\u6784\uff0c\u5728\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u76f8\u6bd4MIAOW GPU\u5b9e\u73b0\u4e866-16\u500d\u52a0\u901f\u548c6-13\u500d\u80fd\u6548\u63d0\u5347\uff0c\u652f\u6301\u53ef\u91cd\u6784\u8ba1\u7b97\u548c\u52a8\u6001\u5206\u8fa8\u7387\u66f4\u65b0\u3002", "motivation": "\u4f20\u7edfGPU\u67b6\u6784\u5728\u5904\u7406\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982CNN\u3001GCN\u3001\u7ebf\u6027\u89c4\u5212\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001Ising\u6a21\u578b\uff09\u65f6\u5b58\u5728\u80fd\u6548\u548c\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u66f4\u7d27\u5bc6\u96c6\u6210\u7684\u8fd1\u5185\u5b58\u67b6\u6784\u6765\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u6548\u3002", "method": "\u8bbe\u8ba1\u4e86\u7d27\u5bc6\u96c6\u6210\u7684\u7edf\u4e00\u8fd1\u5185\u5b58GPU\u67b6\u6784\uff0c\u5305\u542b\u5b9a\u5236\u5316\u7684\u7a00\u758f\u611f\u77e5\u8fd1\u5185\u5b58\u7535\u8def\u548c\u8f7b\u91cf\u7ea7softmax\u7535\u8def\uff0c\u652f\u6301INT16\u53ef\u91cd\u6784\u8ba1\u7b97\u548c\u52a8\u6001\u5206\u8fa8\u7387\u66f4\u65b0\uff0c\u80fd\u591f\u9ad8\u6548\u6269\u5c55\u5230\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u3002", "result": "\u76f8\u6bd4MIAOW GPU\u5b9e\u73b0\u4e866-16\u500d\u52a0\u901f\u548c6-13\u500d\u80fd\u6548\u63d0\u5347\uff1b\u7a00\u758f\u611f\u77e5\u7535\u8def\u63d0\u4f9b\u7ea61.5\u500d\u80fd\u6548\u63d0\u5347\uff0csoftmax\u7535\u8def\u63d0\u4f9b\u7ea61.6\u500d\u80fd\u6548\u63d0\u5347\uff1b\u5728ABI-enabled MI300\u548cBlackwell\u7cfb\u7edf\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e86\u7ea64.5\u500d\u52a0\u901f\u3002", "conclusion": "\u8be5\u7d27\u5bc6\u96c6\u6210\u7684\u8fd1\u5185\u5b58GPU\u67b6\u6784\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u652f\u6301\u53ef\u91cd\u6784\u8ba1\u7b97\u548c\u52a8\u6001\u5206\u8fa8\u7387\u66f4\u65b0\uff0c\u4e3a\u591a\u6837\u5316\u8ba1\u7b97\u9700\u6c42\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14393", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14393", "abs": "https://arxiv.org/abs/2602.14393", "authors": ["Zongle Huang", "Hongyang Jia", "Kaiwei Zou", "Yongpan Liu"], "title": "Scope: A Scalable Merged Pipeline Framework for Multi-Chip-Module NN Accelerators", "comment": "Accepted in ASP-DAC 2026", "summary": "Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.\n  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches.", "AI": {"tldr": "Scope\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u82af\u7247\u6a21\u5757\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u7684\u5408\u5e76\u6d41\u6c34\u7ebf\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u591a\u5c42\u7ef4\u5ea6\uff0c\u5728\u4fdd\u6301\u76f8\u4f3c\u80fd\u8017\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u9ad81.73\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u591a\u82af\u7247\u6a21\u5757\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u548c\u7247\u5916\u901a\u4fe1\u5f00\u9500\u7684\u6311\u6218\u3002\u4f20\u7edf\u7684\u5e76\u884c\u5316\u65b9\u6848\uff08\u5982\u5c42\u5185\u5e76\u884c\u548c\u5c42\u95f4\u6d41\u6c34\u7ebf\uff09\u65e0\u6cd5\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u9650\u5236\u4e86MCM\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5355\u72ec\u90e8\u7f72\u5404\u5c42\u800c\u975e\u8054\u5408\u8003\u8651\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8ba1\u7b97\u548c\u901a\u4fe1\u4e4b\u95f4\u7684\u59a5\u534f\u3002", "method": "\u63d0\u51faScope\u6846\u67b6\uff0c\u5f15\u5165\u88ab\u5ffd\u89c6\u7684\u591a\u5c42\u7ef4\u5ea6\u8fdb\u884c\u5408\u5e76\u6d41\u6c34\u7ebf\u4f18\u5316\u3002\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u641c\u7d22\u7b97\u6cd5\uff0c\u5c06\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u7684\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u7ebf\u6027\u7ea7\uff0c\u540c\u65f6\u80fd\u627e\u5230\u6027\u80fd\u6392\u540d\u524d0.05%\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cScope\u5728ResNet-152\u63a8\u7406\u4e2d\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u76f8\u4f3c\u80fd\u8017\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u9ad81.73\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u8003\u8651\u591a\u5c42\u7ef4\u5ea6\u5e76\u5f00\u53d1\u9ad8\u6548\u7684\u641c\u7d22\u7b97\u6cd5\uff0cScope\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u82af\u7247\u6a21\u5757\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u7684\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6709\u6548\u7f13\u89e3\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u5185\u5b58\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
