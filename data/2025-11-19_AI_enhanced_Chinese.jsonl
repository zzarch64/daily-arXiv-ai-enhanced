{"id": "2511.13950", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13950", "abs": "https://arxiv.org/abs/2511.13950", "authors": ["Lei Zhao", "Luca Buonanno", "Archit Gajjar", "John Moon", "Aishwarya Natarajan", "Sergey Serebryakov", "Ron M. Roth", "Xia Sheng", "Youtao Zhang", "Paolo Faraboschi", "Jim Ignowski", "Giacomo Pedretti"], "title": "NL-DPE: An Analog In-memory Non-Linear Dot Product Engine for Efficient CNN and LLM Inference", "comment": null, "summary": "Resistive Random Access Memory (RRAM) based in-memory computing (IMC) accelerators offer significant performance and energy advantages for deep neural networks (DNNs), but face three major limitations: (1) they support only \\textit{static} dot-product operations and cannot accelerate arbitrary non-linear functions or data-dependent multiplications essential to modern LLMs; (2) they demand large, power-hungry analog-to-digital converter (ADC) circuits; and (3) mapping model weights to device conductance introduces errors from cell nonidealities. These challenges hinder scalable and accurate IMC acceleration as models grow.\n  We propose NL-DPE, a Non-Linear Dot Product Engine that overcomes these barriers. NL-DPE augments crosspoint arrays with RRAM-based Analog Content Addressable Memory (ACAM) to execute arbitrary non-linear functions and data-dependent matrix multiplications in the analog domain by transforming them into decision trees, fully eliminating ADCs. To address device noise, NL-DPE uses software-based Noise Aware Fine-tuning (NAF), requiring no in-device calibration. Experiments show that NL-DPE delivers 28X energy efficiency and 249X speedup over a GPU baseline, and 22X energy efficiency and 245X speedup over existing IMC accelerators, while maintaining high accuracy.", "AI": {"tldr": "NL-DPE\u662f\u4e00\u79cd\u975e\u7ebf\u6027\u70b9\u79ef\u5f15\u64ce\uff0c\u901a\u8fc7\u7ed3\u5408RRAM\u4ea4\u53c9\u9635\u5217\u548c\u6a21\u62df\u5185\u5bb9\u53ef\u5bfb\u5740\u5b58\u50a8\u5668\uff0c\u5728\u6a21\u62df\u57df\u6267\u884c\u4efb\u610f\u975e\u7ebf\u6027\u51fd\u6570\u548c\u6570\u636e\u4f9d\u8d56\u77e9\u9635\u4e58\u6cd5\uff0c\u5b8c\u5168\u6d88\u9664ADC\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edfRRAM\u5185\u5b58\u8ba1\u7b97\u52a0\u901f\u5668\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u4ec5\u652f\u6301\u9759\u6001\u70b9\u79ef\u8fd0\u7b97\u3001\u9700\u8981\u5927\u529f\u7387ADC\u7535\u8def\u3001\u8bbe\u5907\u975e\u7406\u60f3\u6027\u5f15\u5165\u8bef\u5dee\uff0c\u8fd9\u963b\u788d\u4e86\u73b0\u4ee3LLM\u7684\u53ef\u6269\u5c55\u548c\u51c6\u786e\u52a0\u901f\u3002", "method": "NL-DPE\u5c06\u4ea4\u53c9\u9635\u5217\u4e0e\u57fa\u4e8eRRAM\u7684ACAM\u7ed3\u5408\uff0c\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u51fd\u6570\u548c\u6570\u636e\u4f9d\u8d56\u4e58\u6cd5\u8f6c\u6362\u4e3a\u51b3\u7b56\u6811\u5728\u6a21\u62df\u57df\u6267\u884c\uff1b\u91c7\u7528\u8f6f\u4ef6\u566a\u58f0\u611f\u77e5\u5fae\u8c03\u89e3\u51b3\u8bbe\u5907\u566a\u58f0\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793aNL-DPE\u76f8\u6bd4GPU\u57fa\u7ebf\u5b9e\u73b028\u500d\u80fd\u6548\u548c249\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u73b0\u6709IMC\u52a0\u901f\u5668\u5b9e\u73b022\u500d\u80fd\u6548\u548c245\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "NL-DPE\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edfRRAM IMC\u52a0\u901f\u5668\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u73b0\u4ee3LLM\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u53ef\u6269\u5c55\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14202", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.14202", "abs": "https://arxiv.org/abs/2511.14202", "authors": ["Weiping Yang", "Shilin Zhou", "Hui Xu", "Yujiao Nie", "Qimin Zhou", "Zhiwei Li", "Changlin Chen"], "title": "A Bit Level Weight Reordering Strategy Based on Column Similarity to Explore Weight Sparsity in RRAM-based NN Accelerator", "comment": "accepted by ICPADS 2025 (International Conference on Parallel and Distributed Systems)", "summary": "Compute-in-Memory (CIM) and weight sparsity are two effective techniques to reduce data movement during Neural Network (NN) inference. However, they can hardly be employed in the same accelerator simultaneously because CIM requires structural compute patterns which are disrupted in sparse NNs. In this paper, we partially solve this issue by proposing a bit level weight reordering strategy which can realize compact mapping of sparse NN weight matrices onto Resistive Random Access Memory (RRAM) based NN Accelerators (RRAM-Acc). In specific, when weights are mapped to RRAM crossbars in a binary complement manner, we can observe that, which can also be mathematically proven, bit-level sparsity and similarity commonly exist in the crossbars. The bit reordering method treats bit sparsity as a special case of bit similarity, reserve only one column in a pair of columns that have identical bit values, and then map the compressed weight matrices into Operation Units (OU). The performance of our design is evaluated with typical NNs. Simulation results show a 61.24% average performance improvement and 1.51x-2.52x energy savings under different sparsity ratios, with only slight overhead compared to the state-of-the-art design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4d\u7ea7\u6743\u91cd\u91cd\u6392\u5e8f\u7b56\u7565\uff0c\u80fd\u591f\u5728RRAM\u52a0\u901f\u5668\u4e0a\u5b9e\u73b0\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u7684\u7d27\u51d1\u6620\u5c04\uff0c\u89e3\u51b3\u4e86CIM\u548c\u6743\u91cd\u7a00\u758f\u6027\u96be\u4ee5\u540c\u65f6\u5e94\u7528\u7684\u95ee\u9898\u3002", "motivation": "\u8ba1\u7b97\u5185\u5b58(CIM)\u548c\u6743\u91cd\u7a00\u758f\u662f\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u6570\u636e\u79fb\u52a8\u7684\u4e24\u79cd\u6709\u6548\u6280\u672f\uff0c\u4f46\u7531\u4e8e\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u4f1a\u7834\u574fCIM\u6240\u9700\u7684\u7ed3\u6784\u5316\u8ba1\u7b97\u6a21\u5f0f\uff0c\u8fd9\u4e24\u79cd\u6280\u672f\u96be\u4ee5\u5728\u540c\u4e00\u4e2a\u52a0\u901f\u5668\u4e2d\u540c\u65f6\u4f7f\u7528\u3002", "method": "\u91c7\u7528\u4f4d\u7ea7\u6743\u91cd\u91cd\u6392\u5e8f\u7b56\u7565\uff0c\u5c06\u6743\u91cd\u4ee5\u4e8c\u8fdb\u5236\u8865\u7801\u5f62\u5f0f\u6620\u5c04\u5230RRAM\u4ea4\u53c9\u9635\u5217\uff0c\u5229\u7528\u4f4d\u7ea7\u7a00\u758f\u6027\u548c\u76f8\u4f3c\u6027\uff0c\u4fdd\u7559\u5177\u6709\u76f8\u540c\u4f4d\u503c\u7684\u5217\u5bf9\u4e2d\u4ec5\u4e00\u5217\uff0c\u7136\u540e\u5c06\u538b\u7f29\u540e\u7684\u6743\u91cd\u77e9\u9635\u6620\u5c04\u5230\u64cd\u4f5c\u5355\u5143\u4e2d\u3002", "result": "\u5728\u5178\u578b\u795e\u7ecf\u7f51\u7edc\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534761.24%\uff0c\u5728\u4e0d\u540c\u7a00\u758f\u5ea6\u4e0b\u5b9e\u73b01.51\u500d\u52302.52\u500d\u7684\u80fd\u91cf\u8282\u7701\uff0c\u4e0e\u6700\u5148\u8fdb\u8bbe\u8ba1\u76f8\u6bd4\u4ec5\u6709\u8f7b\u5fae\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86CIM\u548c\u6743\u91cd\u7a00\u758f\u6027\u96be\u4ee5\u540c\u65f6\u5e94\u7528\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f4d\u7ea7\u91cd\u6392\u5e8f\u5b9e\u73b0\u4e86\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u5728RRAM\u52a0\u901f\u5668\u4e0a\u7684\u9ad8\u6548\u6620\u5c04\u3002"}}
