<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [NL-DPE: An Analog In-memory Non-Linear Dot Product Engine for Efficient CNN and LLM Inference](https://arxiv.org/abs/2511.13950)
*Lei Zhao,Luca Buonanno,Archit Gajjar,John Moon,Aishwarya Natarajan,Sergey Serebryakov,Ron M. Roth,Xia Sheng,Youtao Zhang,Paolo Faraboschi,Jim Ignowski,Giacomo Pedretti*

Main category: cs.AR

TL;DR: NL-DPE是一种非线性点积引擎，通过结合RRAM交叉阵列和模拟内容可寻址存储器，在模拟域执行任意非线性函数和数据依赖矩阵乘法，完全消除ADC需求，显著提升能效和速度。


<details>
  <summary>Details</summary>
Motivation: 传统RRAM内存计算加速器存在三个主要限制：仅支持静态点积运算、需要大功率ADC电路、设备非理想性引入误差，这阻碍了现代LLM的可扩展和准确加速。

Method: NL-DPE将交叉阵列与基于RRAM的ACAM结合，通过将非线性函数和数据依赖乘法转换为决策树在模拟域执行；采用软件噪声感知微调解决设备噪声问题。

Result: 实验显示NL-DPE相比GPU基线实现28倍能效和249倍加速，相比现有IMC加速器实现22倍能效和245倍加速，同时保持高精度。

Conclusion: NL-DPE成功克服了传统RRAM IMC加速器的关键限制，为现代LLM提供了高效、准确的可扩展加速解决方案。

Abstract: Resistive Random Access Memory (RRAM) based in-memory computing (IMC) accelerators offer significant performance and energy advantages for deep neural networks (DNNs), but face three major limitations: (1) they support only \textit{static} dot-product operations and cannot accelerate arbitrary non-linear functions or data-dependent multiplications essential to modern LLMs; (2) they demand large, power-hungry analog-to-digital converter (ADC) circuits; and (3) mapping model weights to device conductance introduces errors from cell nonidealities. These challenges hinder scalable and accurate IMC acceleration as models grow.
  We propose NL-DPE, a Non-Linear Dot Product Engine that overcomes these barriers. NL-DPE augments crosspoint arrays with RRAM-based Analog Content Addressable Memory (ACAM) to execute arbitrary non-linear functions and data-dependent matrix multiplications in the analog domain by transforming them into decision trees, fully eliminating ADCs. To address device noise, NL-DPE uses software-based Noise Aware Fine-tuning (NAF), requiring no in-device calibration. Experiments show that NL-DPE delivers 28X energy efficiency and 249X speedup over a GPU baseline, and 22X energy efficiency and 245X speedup over existing IMC accelerators, while maintaining high accuracy.

</details>


### [2] [A Bit Level Weight Reordering Strategy Based on Column Similarity to Explore Weight Sparsity in RRAM-based NN Accelerator](https://arxiv.org/abs/2511.14202)
*Weiping Yang,Shilin Zhou,Hui Xu,Yujiao Nie,Qimin Zhou,Zhiwei Li,Changlin Chen*

Main category: cs.AR

TL;DR: 提出了一种位级权重重排序策略，能够在RRAM加速器上实现稀疏神经网络权重的紧凑映射，解决了CIM和权重稀疏性难以同时应用的问题。


<details>
  <summary>Details</summary>
Motivation: 计算内存(CIM)和权重稀疏是减少神经网络推理中数据移动的两种有效技术，但由于稀疏神经网络会破坏CIM所需的结构化计算模式，这两种技术难以在同一个加速器中同时使用。

Method: 采用位级权重重排序策略，将权重以二进制补码形式映射到RRAM交叉阵列，利用位级稀疏性和相似性，保留具有相同位值的列对中仅一列，然后将压缩后的权重矩阵映射到操作单元中。

Result: 在典型神经网络上的仿真结果显示，平均性能提升61.24%，在不同稀疏度下实现1.51倍到2.52倍的能量节省，与最先进设计相比仅有轻微开销。

Conclusion: 该方法成功解决了CIM和权重稀疏性难以同时应用的问题，通过位级重排序实现了稀疏神经网络在RRAM加速器上的高效映射。

Abstract: Compute-in-Memory (CIM) and weight sparsity are two effective techniques to reduce data movement during Neural Network (NN) inference. However, they can hardly be employed in the same accelerator simultaneously because CIM requires structural compute patterns which are disrupted in sparse NNs. In this paper, we partially solve this issue by proposing a bit level weight reordering strategy which can realize compact mapping of sparse NN weight matrices onto Resistive Random Access Memory (RRAM) based NN Accelerators (RRAM-Acc). In specific, when weights are mapped to RRAM crossbars in a binary complement manner, we can observe that, which can also be mathematically proven, bit-level sparsity and similarity commonly exist in the crossbars. The bit reordering method treats bit sparsity as a special case of bit similarity, reserve only one column in a pair of columns that have identical bit values, and then map the compressed weight matrices into Operation Units (OU). The performance of our design is evaluated with typical NNs. Simulation results show a 61.24% average performance improvement and 1.51x-2.52x energy savings under different sparsity ratios, with only slight overhead compared to the state-of-the-art design.

</details>
