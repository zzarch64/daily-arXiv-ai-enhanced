{"id": "2511.00075", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00075", "abs": "https://arxiv.org/abs/2511.00075", "authors": ["Qianhui Li", "Weiya Wang", "Qianqi Zhao", "Tong Qu", "Jing He", "Xuhong Qiang", "Jingwen Hou", "Ke Chen", "Bao Zhang", "Qi Wang"], "title": "PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories", "comment": null, "summary": "Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant\nstorage solution in the era of artificial intelligence. QLC 3D NAND flash\nstores 4 bit per cell to expand the storage density, resulting in narrower read\nmargins. Constrained to read margins, QLC always suffers from lateral charge\nmigration (LCM), which caused by non-uniform charge density across adjacent\nmemory cells. To suppress charge density gap between cells, there are some\nalgorithm in form of intra-page data mapping such as WBVM, DVDS. However, we\nobserve inter-page data arrangements also approach the suppression. Thus, we\nproposed an intelligent model PDA-LSTM to arrange intra-page data for LCM\nsuppression, which is a physics-knowledge-driven neural network model. PDA-LSTM\napplies a long-short term memory (LSTM) neural network to compute a data\narrangement probability matrix from input page data pattern. The arrangement is\nto minimize the global impacts derived from the LCM among wordlines. Since each\npage data can be arranged only once, we design a transformation from output\nmatrix of LSTM network to non-repetitive sequence generation probability matrix\nto assist training process. The arranged data pattern can decrease the bit\nerror rate (BER) during data retention. In addition, PDA-LSTM do not need extra\nflag bits to record data transport of 3D NAND flash compared with WBVM, DVDS.\nThe experiment results show that the PDA-LSTM reduces the average BER by 80.4%\ncompared with strategy without data arrangement, and by 18.4%, 15.2% compared\nrespectively with WBVM and DVDS with code-length 64.", "AI": {"tldr": "\u63d0\u51faPDA-LSTM\u6a21\u578b\uff0c\u901a\u8fc7LSTM\u795e\u7ecf\u7f51\u7edc\u4f18\u5316QLC 3D NAND\u95ea\u5b58\u4e2d\u7684\u6570\u636e\u6392\u5217\uff0c\u6291\u5236\u6a2a\u5411\u7535\u8377\u8fc1\u79fb\uff0c\u964d\u4f4e\u6bd4\u7279\u9519\u8bef\u7387\u3002", "motivation": "QLC 3D NAND\u95ea\u5b58\u7531\u4e8e\u6bcf\u5355\u5143\u5b58\u50a84\u6bd4\u7279\u5bfc\u81f4\u8bfb\u53d6\u88d5\u5ea6\u53d8\u7a84\uff0c\u6613\u53d7\u6a2a\u5411\u7535\u8377\u8fc1\u79fb\u5f71\u54cd\u3002\u73b0\u6709\u7b97\u6cd5\u5982WBVM\u3001DVDS\u4ec5\u5173\u6ce8\u9875\u5185\u6570\u636e\u6620\u5c04\uff0c\u800c\u9875\u95f4\u6570\u636e\u6392\u5217\u4e5f\u80fd\u6709\u6548\u6291\u5236\u7535\u8377\u8fc1\u79fb\u3002", "method": "\u8bbe\u8ba1PDA-LSTM\u6a21\u578b\uff0c\u4f7f\u7528LSTM\u795e\u7ecf\u7f51\u7edc\u4ece\u8f93\u5165\u9875\u9762\u6570\u636e\u6a21\u5f0f\u8ba1\u7b97\u6570\u636e\u6392\u5217\u6982\u7387\u77e9\u9635\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u5b57\u7ebf\u95f4LCM\u7684\u5168\u5c40\u5f71\u54cd\u6765\u4f18\u5316\u6570\u636e\u6392\u5217\uff0c\u5e76\u5c06LSTM\u8f93\u51fa\u77e9\u9635\u8f6c\u6362\u4e3a\u975e\u91cd\u590d\u5e8f\u5217\u751f\u6210\u6982\u7387\u77e9\u9635\u4ee5\u8f85\u52a9\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPDA-LSTM\u76f8\u6bd4\u65e0\u6570\u636e\u6392\u5217\u7b56\u7565\u5e73\u5747BER\u964d\u4f4e80.4%\uff0c\u76f8\u6bd4WBVM\u548cDVDS\uff08\u7801\u957f64\uff09\u5206\u522b\u964d\u4f4e18.4%\u548c15.2%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6807\u5fd7\u4f4d\u3002", "conclusion": "PDA-LSTM\u901a\u8fc7\u667a\u80fd\u6570\u636e\u6392\u5217\u6709\u6548\u6291\u5236QLC 3D NAND\u95ea\u5b58\u4e2d\u7684\u6a2a\u5411\u7535\u8377\u8fc1\u79fb\uff0c\u663e\u8457\u964d\u4f4e\u6bd4\u7279\u9519\u8bef\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.00295", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.00295", "abs": "https://arxiv.org/abs/2511.00295", "authors": ["Kosmas Alexandridis", "Giorgos Dimitrakopoulos"], "title": "H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention", "comment": "Accepted for publication at IEEE Transactions on Circuits and Systems\n  for Artificial Intelligence", "summary": "Transformers have significantly advanced AI and machine learning through\ntheir powerful attention mechanism. However, computing attention on long\nsequences can become a computational bottleneck. FlashAttention mitigates this\nby fusing the softmax and matrix operations into a tiled computation pattern\nthat decouples performance from sequence length. Though designed for GPUs, its\nsimplicity also makes it well suited for direct hardware acceleration. To\nimprove hardware implementation, we compute FlashAttention using a mixture of\nfloating-point and fixed-point logarithm domain representations. Floating-point\nis used to compute attention scores from query and key matrices, while\nlogarithmic computation simplifies the fused computation of softmax\nnormalization and the multiplication with the value matrix. This\ntransformation, called H-FA, replaces vector-wide floating-point multiplication\nand division operations by additions and subtractions implemented efficiently\nwith fixed-point arithmetic in the logarithm domain. Exponential function\nevaluations are effectively omitted and fused with the rest operations, and the\nfinal result is directly returned to floating-point arithmetic without any\nadditional hardware overhead. Hardware implementation results at 28nm\ndemonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction\nin power, on average, compared to FlashAttention parallel hardware\narchitectures built solely with floating-point datapaths, without hindering\nperformance.", "AI": {"tldr": "H-FA\u662f\u4e00\u79cd\u6539\u8fdb\u7684FlashAttention\u786c\u4ef6\u5b9e\u73b0\uff0c\u901a\u8fc7\u6df7\u5408\u4f7f\u7528\u6d6e\u70b9\u6570\u548c\u5b9a\u70b9\u6570\u5bf9\u6570\u57df\u8868\u793a\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u9762\u79ef\u548c\u529f\u8017\u3002", "motivation": "Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\uff0cFlashAttention\u901a\u8fc7\u5206\u5757\u8ba1\u7b97\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5176\u786c\u4ef6\u5b9e\u73b0\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u51cf\u5c11\u9762\u79ef\u548c\u529f\u8017\u65b9\u9762\u3002", "method": "\u91c7\u7528\u6d6e\u70b9\u6570\u8ba1\u7b97\u67e5\u8be2\u548c\u952e\u77e9\u9635\u7684\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5728\u5bf9\u6570\u57df\u4e2d\u4f7f\u7528\u5b9a\u70b9\u6570\u7b97\u672f\u7b80\u5316softmax\u5f52\u4e00\u5316\u4e0e\u503c\u77e9\u9635\u4e58\u6cd5\u7684\u878d\u5408\u8ba1\u7b97\uff0c\u7528\u52a0\u51cf\u6cd5\u66ff\u4ee3\u4e58\u9664\u6cd5\u64cd\u4f5c\u3002", "result": "\u572828nm\u5de5\u827a\u4e0b\uff0cH-FA\u76f8\u6bd4\u7eaf\u6d6e\u70b9\u6570\u6570\u636e\u8def\u5f84\u7684FlashAttention\u5e76\u884c\u786c\u4ef6\u67b6\u6784\uff0c\u5e73\u5747\u9762\u79ef\u51cf\u5c1126.5%\uff0c\u529f\u8017\u964d\u4f4e23.4%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "H-FA\u901a\u8fc7\u6df7\u5408\u6d6e\u70b9-\u5b9a\u70b9\u5bf9\u6570\u57df\u8ba1\u7b97\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4e86FlashAttention\u786c\u4ef6\u52a0\u901f\u5668\u7684\u9ad8\u6548\u4f18\u5316\uff0c\u4e3a\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00321", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00321", "abs": "https://arxiv.org/abs/2511.00321", "authors": ["Dowon Kim", "MinJae Lee", "Janghyeon Kim", "HyuckSung Kwon", "Hyeonggyu Jeong", "Sang-Soo Park", "Minyong Yoon", "Si-Dong Roh", "Yongsuk Kwon", "Jinin So", "Jungwook Choi"], "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits", "comment": null, "summary": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCXL\u7684PNM KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5185\u5b58\u8fd1\u5904\u7406\u6280\u672f\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\u95ee\u9898", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u6269\u5c55\u5230\u767e\u4e07token\u7ea7\u522b\u65f6\uff0cKV\u7f13\u5b58\u7ba1\u7406\u9762\u4e34\u4e25\u91cd\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u73b0\u6709CXL\u975e\u9a71\u9010\u6846\u67b6\u5728\u53ec\u56de\u975e\u9a7b\u7559KV token\u65f6\u4ecd\u5b58\u5728\u6602\u8d35\u7684\u6570\u636e\u4f20\u8f93\u6210\u672c", "method": "\u8bbe\u8ba1CXL\u542f\u7528\u7684KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u5c06token\u9875\u9762\u9009\u62e9\u5378\u8f7d\u5230CXL\u5185\u5b58\u4e2d\u7684PNM\u52a0\u901f\u5668\uff0c\u5f15\u5165\u6df7\u5408\u5e76\u884c\u5316\u7b56\u7565\u548c\u7a33\u6001token\u9009\u62e9\u673a\u5236", "result": "\u5728405B\u53c2\u6570\u548c1M token\u4e0a\u4e0b\u6587\u7684LLM\u4e0a\u5b9e\u73b0\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0cPNM-KV\u548cPnG-KV\u65b9\u6848\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b021.9\u500d\u541e\u5410\u91cf\u63d0\u5347\u300160\u500d\u6bcftoken\u80fd\u8017\u964d\u4f4e\u548c7.3\u500d\u603b\u6210\u672c\u6548\u7387\u63d0\u5347", "conclusion": "CXL\u542f\u7528\u7684\u591aPNM\u67b6\u6784\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u53ef\u6269\u5c55\u9aa8\u5e72"}}
{"id": "2511.01244", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.01244", "abs": "https://arxiv.org/abs/2511.01244", "authors": ["Wajid Ali", "Ayaz Akram", "Deepak Shankar"], "title": "Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim", "comment": null, "summary": "This paper focuses on the simulation of multi-die System-on-Chip (SoC)\narchitectures using VisualSim, emphasiz- ing chiplet-based system modeling and\nperformance analysis. Chiplet technology presents a promising alternative to\ntraditional monolithic chips, which face increasing challenges in manufactur-\ning costs, power efficiency, and performance scaling. By integrat- ing multiple\nsmall modular silicon units into a single package, chiplet-based architectures\noffer greater flexibility and scalability at a lower overall cost. In this\nstudy, we developed a detailed sim- ulation model of a chiplet-based system,\nincorporating multicore ARM processor clusters interconnected through a ARM\nCMN600 network-on-chip (NoC) for efficient communication [4], [7]. The\nsimulation framework in VisualSim enables the evaluation of critical system\nmetrics, including inter-chiplet communication latency, memory access\nefficiency, workload distribution, and the power-performance tradeoff under\nvarious workloads. Through simulation-driven insights, this research highlights\nkey factors influencing chiplet system performance and provides a foundation\nfor optimizing future chiplet-based semiconductor designs.", "AI": {"tldr": "\u4f7f\u7528VisualSim\u4eff\u771f\u591a\u82af\u7247\u7cfb\u7edf\u67b6\u6784\uff0c\u91cd\u70b9\u7814\u7a76chiplet\u6280\u672f\u7684\u6027\u80fd\u5206\u6790\u548c\u5efa\u6a21\uff0c\u8bc4\u4f30\u901a\u4fe1\u5ef6\u8fdf\u3001\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u7b49\u5173\u952e\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u5355\u7247\u82af\u7247\u9762\u4e34\u5236\u9020\u6210\u672c\u3001\u529f\u8017\u6548\u7387\u548c\u6027\u80fd\u6269\u5c55\u7684\u6311\u6218\uff0cchiplet\u6280\u672f\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u6a21\u5757\u5316\u7845\u5355\u5143\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u4e14\u6210\u672c\u66f4\u4f4e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86chiplet\u7cfb\u7edf\u7684\u8be6\u7ec6\u4eff\u771f\u6a21\u578b\uff0c\u5305\u542b\u591a\u6838ARM\u5904\u7406\u5668\u96c6\u7fa4\uff0c\u901a\u8fc7ARM CMN600\u7247\u4e0a\u7f51\u7edc\u4e92\u8054\uff0c\u4f7f\u7528VisualSim\u6846\u67b6\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u6307\u6807\u3002", "result": "\u4eff\u771f\u5206\u6790\u63ed\u793a\u4e86\u5f71\u54cdchiplet\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5305\u62ec\u901a\u4fe1\u5ef6\u8fdf\u3001\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u548c\u529f\u8017\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316\u672a\u6765chiplet\u57fa\u534a\u5bfc\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u4eff\u771f\u9a71\u52a8\u65b9\u6cd5\u5728\u8bc4\u4f30\u591a\u82af\u7247\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
