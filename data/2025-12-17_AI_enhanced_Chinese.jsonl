{"id": "2512.14151", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.14151", "abs": "https://arxiv.org/abs/2512.14151", "authors": ["Songze Liu", "Hongkun Du", "Shaowen Wang"], "title": "Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement", "comment": null, "summary": "Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u81ea\u9002\u5e94\u7f13\u5b58\u6c61\u67d3\u63a7\u5236\u673a\u5236\uff08ACPC\uff09\uff0c\u901a\u8fc7TCN\u9884\u6d4b\u8bbf\u95ee\u6a21\u5f0f\u548c\u4f18\u5148\u7ea7\u611f\u77e5\u66ff\u6362\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u7f13\u5b58\u6c61\u67d3\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9891\u7e41\u7684token\u5e8f\u5217\u67e5\u627e\u548c\u5d4c\u5165\u5411\u91cf\u68c0\u7d22\u4f1a\u4ea7\u751f\u9ad8\u5ea6\u4e0d\u89c4\u5219\u548c\u7a81\u53d1\u7684\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5bfc\u81f4\u4f20\u7edf\u9884\u53d6\u548c\u66ff\u6362\u7b56\u7565\u8bef\u5224\uff0c\u5f15\u53d1\u4e25\u91cd\u7684\u7f13\u5b58\u6c61\u67d3\uff0c\u4ece\u800c\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7f13\u5b58\u6c61\u67d3\u63a7\u5236\u673a\u5236\uff08ACPC\uff09\uff0c\u96c6\u6210\u57fa\u4e8e\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u7684\u8bbf\u95ee\u9884\u6d4b\u548c\u4f18\u5148\u7ea7\u611f\u77e5\u66ff\u6362\u7b56\u7565\u3002TCN\u6a21\u5757\u5b66\u4e60token\u8bbf\u95ee\u5e8f\u5217\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u4ee5\u8bc6\u522b\u6f5c\u5728\u9ad8\u91cd\u7528\u7f13\u5b58\u884c\uff0c\u66ff\u6362\u7b56\u7565\u6839\u636e\u9884\u6d4b\u7684\u91cd\u7528\u53ef\u80fd\u6027\u548c\u7f13\u5b58\u5360\u7528\u60c5\u51b5\u52a8\u6001\u8c03\u6574\u9a71\u9010\u4f18\u5148\u7ea7\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u66ff\u6362\u57fa\u51c6\u76f8\u6bd4\uff0cACPC\u51cf\u5c11\u7f13\u5b58\u6c61\u67d341.7%\uff0c\u63d0\u9ad8\u7f13\u5b58\u547d\u4e2d\u73878.9%\uff0c\u964d\u4f4eL2\u7f3a\u5931\u60e9\u7f5a60.0%\u3002\u6b64\u5916\uff0cACPC\u6846\u67b6\u63d0\u9ad8token\u751f\u6210\u541e\u5410\u91cf15.9%\uff0c\u8fbe\u5230\u6700\u4f4e\u6700\u7ec8\u635f\u59310.21\u3002", "conclusion": "ACPC\u80fd\u6709\u6548\u8bc6\u522b\u6709\u7528\u7f13\u5b58\u884c\u5e76\u5728\u52a8\u6001LLM\u8bbf\u95ee\u884c\u4e3a\u4e0b\u51cf\u5c11\u5197\u4f59\u9884\u53d6\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u670d\u52a1\u548c\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u5185\u5b58\u6548\u7387\u548c\u5ef6\u8fdf\u3002"}}
{"id": "2512.14172", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.14172", "abs": "https://arxiv.org/abs/2512.14172", "authors": ["Qijun Zhang", "Shang Liu", "Yao Lu", "Mengming Li", "Zhiyao Xie"], "title": "ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework", "comment": "Accepted by ASP-DAC'26", "summary": "Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.", "AI": {"tldr": "ReadyPower\u662f\u4e00\u4e2a\u65b0\u7684\u5206\u6790\u578b\u529f\u8017\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u67b6\u6784\u7ea7\u3001\u5b9e\u73b0\u7ea7\u548c\u6280\u672f\u7ea7\u53c2\u6570\u6765\u6539\u8fdbMcPAT\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u6790\u6a21\u578b\u7cbe\u5ea6\u4f4e\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0d\u53ef\u9760\u3001\u96be\u89e3\u91ca\u3001\u96be\u4f7f\u7528\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u5904\u7406\u5668\u8bbe\u8ba1\u4e2d\u529f\u8017\u662f\u4e3b\u8981\u76ee\u6807\uff0c\u9700\u8981\u51c6\u786e\u9ad8\u6548\u7684\u529f\u8017\u5efa\u6a21\u6280\u672f\u3002\u4f20\u7edf\u5206\u6790\u6a21\u578b\uff08\u5982McPAT\uff09\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u867d\u7136\u7cbe\u5ea6\u9ad8\u4f46\u5b58\u5728\u4e0d\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u4f7f\u7528\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5de5\u4e1a\u754c\u5e94\u7528\u3002", "method": "\u63d0\u51faReadyPower\u6846\u67b6\uff0c\u901a\u8fc7\u5411McPAT\u5206\u6790\u6a21\u578b\u4e2d\u5f15\u5165\u4e09\u4e2a\u5c42\u6b21\u7684\u53c2\u6570\u6765\u5f25\u5408\u5b9e\u9645\u5904\u7406\u5668\u5b9e\u73b0\u4e0e\u5206\u6790\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\uff1a\u67b6\u6784\u7ea7\u3001\u5b9e\u73b0\u7ea7\u548c\u6280\u672f\u7ea7\u53c2\u6570\u3002\u8fd9\u4e9b\u53c2\u6570\u901a\u8fc7\u4e0d\u540c\u65b9\u5f0f\u786e\u5b9a\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e0d\u540c\u8bad\u7ec3\u573a\u666f\u4e0b\uff0cReadyPower\u5728BOOM\u548cXiangShan CPU\u67b6\u6784\u4e0a\uff0c\u76f8\u6bd4\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\u964d\u4f4e\u8d85\u8fc720%\uff0c\u76f8\u5173\u7cfb\u6570R\u63d0\u9ad8\u8d85\u8fc70.2\u3002", "conclusion": "ReadyPower\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u529f\u8017\u5efa\u6a21\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u6790\u6a21\u578b\u7cbe\u5ea6\u4e0d\u8db3\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4e3a\u65e9\u671f\u529f\u8017\u4f18\u5316\u548c\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u3002"}}
{"id": "2512.14256", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.14256", "abs": "https://arxiv.org/abs/2512.14256", "authors": ["Huizheng Wang", "Taiquan Wei", "Zichuan Wang", "Dingcheng Jiang", "Qize Yang", "Jiaxin Liu", "Jingxiang Hou", "Chao Li", "Jinyi Deng", "Yang Hu", "Shouyi Yin"], "title": "TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips", "comment": "Accepted by HPCA 2026", "summary": "Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.\n  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.\n  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.", "AI": {"tldr": "TEMP\u6846\u67b6\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7684\u5f20\u91cf\u6d41\u5206\u533a\u3001\u6d41\u91cf\u611f\u77e5\u6620\u5c04\u548c\u53cc\u5c42\u6676\u5706\u6c42\u89e3\uff0c\u4f18\u5316\u4e86\u5728\u6676\u5706\u7ea7\u82af\u7247\u4e0a\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\uff0c\u5b9e\u73b0\u4e861.7\u500d\u7684\u5e73\u5747\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u6676\u5706\u7ea7\u82af\u7247\u867d\u7136\u63d0\u4f9b\u9ad8\u8ba1\u7b97\u80fd\u529b\u548c\u5e26\u5bbd\uff0c\u4f46\u9762\u4e34\u7247\u4e0a\u5185\u5b58\u4e0e\u8ba1\u7b97\u8d44\u6e90\u7684\u6743\u8861\uff0c\u4e14\u73b0\u6709\u5f20\u91cf\u5e76\u884c\u7b56\u7565\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5176\u901a\u4fe1\u4f18\u52bf\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5f20\u91cf\u6d41\u5206\u533a\u8303\u5f0f\uff0c\u5e76\u5f00\u53d1TEMP\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u62d3\u6251\u611f\u77e5\u7684\u5f20\u91cf\u6d41\u5206\u533a\uff0c2) \u6d41\u91cf\u611f\u77e5\u6620\u5c04\uff0c3) \u53cc\u5c42\u6676\u5706\u6c42\u89e3\uff0c\u4ee5\u514b\u670d\u786c\u4ef6\u9650\u5236\u548c\u5e76\u884c\u6311\u6218\u3002", "result": "TEMP\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7cfb\u7edf\u5b9e\u73b0\u4e861.7\u500d\u7684\u5e73\u5747\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "TEMP\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u4f18\u5316\u5185\u5b58\u6548\u7387\u548c\u541e\u5410\u91cf\uff0c\u5145\u5206\u91ca\u653e\u4e86\u5f20\u91cf\u6d41\u5206\u533a\u8303\u5f0f\u5728\u6676\u5706\u7ea7\u82af\u7247\u4e0a\u7684\u6f5c\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.14322", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.14322", "abs": "https://arxiv.org/abs/2512.14322", "authors": ["Huizheng Wang", "Hongbin Wang", "Zichuan Wang", "Zhiheng Yue", "Yang Wang", "Chao Li", "Yang Hu", "Shouyi Yin"], "title": "PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion", "comment": "Accepted by HPCA 2026", "summary": "Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.\n  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.\n  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.", "AI": {"tldr": "PADE\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u6d4b\u5668\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u7528\u4e8e\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u901a\u8fc7\u4f4d\u7ea7\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u4fdd\u62a4\u8fc7\u6ee4\u3001\u53cc\u5411\u7a00\u758f\u4e71\u5e8f\u6267\u884c\u548c\u4ea4\u9519\u7a00\u758f\u5206\u5757\u6ce8\u610f\u529b\u7b49\u6280\u672f\uff0c\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e867.43\u500d\u52a0\u901f\u548c31.1\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u867d\u7136\u9769\u547d\u6027\uff0c\u4f46\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u5e26\u6765\u4e25\u91cd\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u56e0\u9700\u8981\u989d\u5916\u7684\u7a00\u758f\u6027\u9884\u6d4b\u5668\u800c\u7f3a\u4e4f\u5b9e\u7528\u6027\uff0c\u9884\u6d4b\u5668\u4f1a\u4e25\u91cd\u964d\u4f4e\u786c\u4ef6\u6548\u7387\u3002", "method": "\u63d0\u51faPADE\uff0c\u4e00\u79cd\u65e0\u9700\u9884\u6d4b\u5668\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u4f4d\u7ea7\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u4fdd\u62a4\u8fc7\u6ee4\u7b56\u7565\uff0c\u5728\u6bcf\u4e2a\u4f4d\u8f6e\u6b21\u51c6\u786e\u8bc6\u522b\u65e0\u5173\u7d27\u8981\u7684token\uff1b2) \u57fa\u4e8e\u53cc\u5411\u7a00\u758f\u6027\u7684\u4e71\u5e8f\u6267\u884c\uff0c\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\uff1b3) \u57fa\u4e8e\u4ea4\u9519\u7684\u7a00\u758f\u5206\u5757\u6ce8\u610f\u529b\uff0c\u964d\u4f4eI/O\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u7ed3\u5408\u5b9a\u5236\u52a0\u901f\u5668\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u65e0\u9700\u7a00\u758f\u9884\u6d4b\u5668\u7684\u5b9e\u7528\u7a00\u758f\u52a0\u901f\u3002", "result": "\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPADE\u76f8\u6bd4Nvidia H100 GPU\u5b9e\u73b0\u4e867.43\u500d\u52a0\u901f\u548c31.1\u500d\u80fd\u6548\u63d0\u5347\u3002\u76f8\u6bd4SOTA\u52a0\u901f\u5668\uff0cPADE\u5206\u522b\u6bd4Sanger\u3001DOTA\u548cSOFA\u8282\u77015.1\u500d\u30014.3\u500d\u548c3.4\u500d\u80fd\u8017\u3002", "conclusion": "PADE\u901a\u8fc7\u521b\u65b0\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u989d\u5916\u9884\u6d4b\u5668\u7684\u9ad8\u6548\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u6548\u3002"}}
{"id": "2512.14661", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.14661", "abs": "https://arxiv.org/abs/2512.14661", "authors": ["Chiyue Wei", "Cong Guo", "Junyao Zhang", "Haoxuan Shan", "Yifan Xu", "Ziyue Zhang", "Yudong Liu", "Qinsi Wang", "Changchun Zhou", "Hai \"Helen\" Li", "Yiran Chen"], "title": "Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models", "comment": "HPCA 2026", "summary": "Vision-Language Models (VLMs) have demonstrated strong performance on tasks such as video captioning and visual question answering. However, their growing scale and video-level inputs lead to significant computational and memory overhead, posing challenges for real-time deployment on hardware accelerators. While prior work attempts to reduce redundancy via token pruning or merging, these methods typically operate at coarse granularity and incur high runtime overhead due to global token-level operations. In this study, we propose Focus, a Streaming Concentration Architecture that efficiently accelerates VLM inference through progressive, fine-grained redundancy elimination. Focus introduces a multilevel concentration paradigm that hierarchically compresses vision-language inputs at three levels: (1) semantic-guided token pruning based on textual prompts, (2) spatial-temporal block-level concentration using localized comparisons, and (3) vector-level redundancy removal via motion-aware matching. All concentration steps are tightly co-designed with the architecture to support streaming-friendly, on-chip execution. Focus leverages GEMM tiling, convolution-style layout, and cross-modal attention to minimize off-chip access while enabling high throughput. Implemented as a modular unit within a systolic-array accelerator, Focus achieves a 2.4x speedup and 3.3x reduction in energy, significantly outperforming state-of-the-art accelerators in both performance and energy efficiency. Full-stack implementation of Focus is open-sourced at https://github.com/dubcyfor3/Focus.", "AI": {"tldr": "Focus\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u5f0f\u538b\u7f29\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u7ea7\u5197\u4f59\u6d88\u9664\u9ad8\u6548\u52a0\u901f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u5b9e\u73b02.4\u500d\u52a0\u901f\u548c3.3\u500d\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6a21\u578b\u89c4\u6a21\u589e\u5927\u548c\u89c6\u9891\u7ea7\u8f93\u5165\u5bfc\u81f4\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u663e\u8457\uff0c\u96be\u4ee5\u5728\u786c\u4ef6\u52a0\u901f\u5668\u4e0a\u5b9e\u65f6\u90e8\u7f72\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u7c97\u7c92\u5ea6\u7684token\u526a\u679d\u6216\u5408\u5e76\uff0c\u5b58\u5728\u8fd0\u884c\u65f6\u5f00\u9500\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFocus\u6d41\u5f0f\u538b\u7f29\u67b6\u6784\uff0c\u91c7\u7528\u4e09\u7ea7\u5c42\u6b21\u5316\u538b\u7f29\uff1a1) \u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u8bed\u4e49\u5f15\u5bfctoken\u526a\u679d\uff1b2) \u4f7f\u7528\u5c40\u90e8\u6bd4\u8f83\u7684\u7a7a\u95f4-\u65f6\u95f4\u5757\u7ea7\u538b\u7f29\uff1b3) \u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u5339\u914d\u7684\u5411\u91cf\u7ea7\u5197\u4f59\u6d88\u9664\u3002\u6240\u6709\u538b\u7f29\u6b65\u9aa4\u4e0e\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u652f\u6301\u6d41\u5f0f\u53cb\u597d\u7684\u7247\u4e0a\u6267\u884c\uff0c\u5229\u7528GEMM\u5206\u5757\u3001\u5377\u79ef\u5f0f\u5e03\u5c40\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6700\u5c0f\u5316\u7247\u5916\u8bbf\u95ee\u3002", "result": "\u5728\u8109\u52a8\u9635\u5217\u52a0\u901f\u5668\u4e2d\u4f5c\u4e3a\u6a21\u5757\u5355\u5143\u5b9e\u73b0\uff0cFocus\u5b9e\u73b0\u4e862.4\u500d\u52a0\u901f\u548c3.3\u500d\u80fd\u8017\u964d\u4f4e\uff0c\u5728\u6027\u80fd\u548c\u80fd\u6548\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u52a0\u901f\u5668\u3002", "conclusion": "Focus\u901a\u8fc7\u6e10\u8fdb\u5f0f\u7ec6\u7c92\u5ea6\u5197\u4f59\u6d88\u9664\u6709\u6548\u89e3\u51b3\u4e86VLM\u63a8\u7406\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u4e3a\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u5f00\u6e90\u5b8c\u6574\u5b9e\u73b0\u3002"}}
