{"id": "2602.10218", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10218", "abs": "https://arxiv.org/abs/2602.10218", "authors": ["Chenhui Deng", "Zhongzhi Yu", "Guan-Ting Liu", "Nathaniel Pinckney", "Haoxing Ren"], "title": "ACE-RTL: When Agentic Context Evolution Meets RTL-Specialized LLMs", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest in applying them to hardware design automation, particularly for accurate RTL code generation. Prior efforts follow two largely independent paths: (i) training domain-adapted RTL models to internalize hardware semantics, (ii) developing agentic systems that leverage frontier generic LLMs guided by simulation feedback. However, these two paths exhibit complementary strengths and weaknesses. In this work, we present ACE-RTL that unifies both directions through Agentic Context Evolution (ACE). ACE-RTL integrates an RTL-specialized LLM, trained on a large-scale dataset of 1.7 million RTL samples, with a frontier reasoning LLM through three synergistic components: the generator, reflector, and coordinator. These components iteratively refine RTL code toward functional correctness. We further introduce a parallel scaling strategy that significantly reduces the number of iterations required to reach correct solutions. On the Comprehensive Verilog Design Problems (CVDP) benchmark, ACE-RTL achieves up to a 44.87% pass rate improvement over 14 competitive baselines while requiring only four iterations on average.", "AI": {"tldr": "ACE-RTL\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9886\u57df\u4e13\u7528LLM\u548c\u524d\u6cbf\u63a8\u7406LLM\u7684\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5668\u3001\u53cd\u5c04\u5668\u548c\u534f\u8c03\u5668\u534f\u540c\u8fed\u4ee3\u4f18\u5316RTL\u4ee3\u7801\uff0c\u5728CVDP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\uff0c\u9886\u57df\u9002\u5e94\u7684RTL\u6a21\u578b\u548c\u57fa\u4e8e\u6a21\u62df\u53cd\u9988\u7684\u901a\u7528LLM\u4ee3\u7406\u7cfb\u7edf\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faACE-RTL\u6846\u67b6\uff0c\u6574\u5408\u4e86\u5728170\u4e07RTL\u6837\u672c\u4e0a\u8bad\u7ec3\u7684\u4e13\u7528LLM\u548c\u524d\u6cbf\u63a8\u7406LLM\uff0c\u901a\u8fc7\u751f\u6210\u5668\u3001\u53cd\u5c04\u5668\u3001\u534f\u8c03\u5668\u4e09\u4e2a\u7ec4\u4ef6\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\uff0c\u5e76\u5f15\u5165\u5e76\u884c\u6269\u5c55\u7b56\u7565\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u3002", "result": "\u5728CVDP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACE-RTL\u76f8\u6bd414\u4e2a\u7ade\u4e89\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u9ad844.87%\u7684\u901a\u8fc7\u7387\u63d0\u5347\uff0c\u5e73\u5747\u4ec5\u97004\u6b21\u8fed\u4ee3\u5373\u53ef\u83b7\u5f97\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ACE-RTL\u6210\u529f\u7edf\u4e00\u4e86\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u4e24\u79cd\u4e3b\u8981\u8def\u5f84\uff0c\u901a\u8fc7\u667a\u80fd\u4e0a\u4e0b\u6587\u6f14\u5316\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u51c6\u786e\u7684RTL\u4ee3\u7801\u751f\u6210\u3002"}}
{"id": "2602.10254", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.10254", "abs": "https://arxiv.org/abs/2602.10254", "authors": ["Hanyuan Gao", "Xiaoxuan Yang"], "title": "Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching", "comment": "Accepted by ISCAS 2026", "summary": "Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411MoE Transformer\u7684\u9762\u79ef\u9ad8\u6548\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u4ea4\u53c9\u5f00\u5173\u590d\u7528\u3001\u4e13\u5bb6\u5206\u7ec4\u8c03\u5ea6\u548c\u95e8\u8f93\u51fa\u7f13\u5b58\u4f18\u5316\uff0c\u63d0\u5347\u9762\u79ef\u6548\u73872.2\u500d\uff0c\u751f\u6210\u6027\u80fd\u63d0\u53474.2\u500d\u3002", "motivation": "MoE\u5c42\u901a\u8fc7\u6fc0\u6d3b\u90e8\u5206\u4e13\u5bb6\u6743\u91cd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u90e8\u7f72\uff0c\u4f46\u5b58\u5185\u8ba1\u7b97\u82af\u7247\u901a\u5e38\u5b58\u5728\u8f83\u5927\u7684\u5916\u56f4\u7535\u8def\u9762\u79ef\u5f00\u9500\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u8bbe\u8ba1\u3002", "method": "1) \u63d0\u51fa\u4ea4\u53c9\u5f00\u5173\u7ea7\u590d\u7528\u7b56\u7565\uff0c\u5229\u7528MoE\u7a00\u758f\u6027\u8ba9\u591a\u4e2a\u4ea4\u53c9\u5f00\u5173\u5171\u4eab\u5916\u56f4\u7535\u8def\uff1b2) \u4e13\u5bb6\u5206\u7ec4\u548c\u7ec4\u7ea7\u8c03\u5ea6\u65b9\u6cd5\u7f13\u89e3\u5171\u4eab\u5e26\u6765\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u548c\u4e89\u7528\u5f00\u9500\uff1b3) \u95e8\u8f93\u51fa\u7f13\u5b58\u5b58\u50a8\u5fc5\u8981\u7ed3\u679c\uff0c\u907f\u514d\u751f\u6210\u9636\u6bb5\u4e13\u5bb6\u9009\u62e9\u8def\u7531\u5668\u8bbf\u95ee\u6240\u6709\u9690\u85cf\u72b6\u6001\u5e26\u6765\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "result": "MoE\u90e8\u5206\u9762\u79ef\u6548\u7387\u76f8\u6bd4SOTA\u67b6\u6784\u63d0\u53472.2\u500d\uff1b\u751f\u6210\u9636\u6bb5\u7f13\u5b58\u4f7f8\u4e2atoken\u751f\u6210\u7684\u6027\u80fd\u548c\u80fd\u6548\u5206\u522b\u63d0\u53474.2\u500d\u548c10.1\u500d\uff1b\u603b\u4f53\u6027\u80fd\u5bc6\u5ea6\u8fbe\u523015.6 GOPS/W/mm\u00b2\u3002", "conclusion": "\u63d0\u51fa\u7684\u9762\u79ef\u9ad8\u6548\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86MoE Transformer\u5728PIM\u90e8\u7f72\u4e2d\u7684\u9762\u79ef\u5f00\u9500\u95ee\u9898\uff0c\u901a\u8fc7\u4ea4\u53c9\u5f00\u5173\u590d\u7528\u3001\u4e13\u5bb6\u5206\u7ec4\u8c03\u5ea6\u548c\u95e8\u8f93\u51fa\u7f13\u5b58\u7b49\u521b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u79ef\u6548\u7387\u3001\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2602.10654", "categories": ["cs.AR", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.10654", "abs": "https://arxiv.org/abs/2602.10654", "authors": ["Derek Christ", "Thomas Zimmermann", "Philippe Barbie", "Dmitri Saberi", "Yao Yin", "Matthias Jung"], "title": "DRAMPyML: A Formal Description of DRAM Protocols with Timed Petri Nets", "comment": null, "summary": "The JEDEC committee defines various domain-specific DRAM standards. These standards feature increasingly complex and evolving protocol specifications, which are detailed in timing diagrams and command tables. Understanding these protocols is becoming progressively challenging as new features and complex device hierarchies are difficult to comprehend without an expressive model. While each JEDEC standard features a simplified state machine, this state machine fails to reflect the parallel operation of memory banks.\n  In this paper, we present an evolved modeling approach based on timed Petri nets and Python. This model provides a more accurate representation of DRAM protocols, making them easier to understand and directly executable, which enables the evaluation of interesting metrics and the verification of controller RTL models, DRAM logic and memory simulators.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u95f4Petri\u7f51\u548cPython\u7684DRAM\u534f\u8bae\u5efa\u6a21\u65b9\u6cd5\uff0c\u6539\u8fdbJEDEC\u6807\u51c6\u4e2d\u7b80\u5316\u7684\u72b6\u6001\u673a\u6a21\u578b\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u53ef\u6267\u884c\u7684\u534f\u8bae\u8868\u793a", "motivation": "JEDEC\u5b9a\u4e49\u7684DRAM\u6807\u51c6\u534f\u8bae\u65e5\u76ca\u590d\u6742\uff0c\u73b0\u6709\u7b80\u5316\u72b6\u6001\u673a\u65e0\u6cd5\u53cd\u6620\u5185\u5b58bank\u7684\u5e76\u884c\u64cd\u4f5c\uff0c\u96be\u4ee5\u7406\u89e3\u548c\u9a8c\u8bc1\u590d\u6742\u7684\u8bbe\u5907\u5c42\u6b21\u7ed3\u6784", "method": "\u4f7f\u7528\u65f6\u95f4Petri\u7f51\u548cPython\u6784\u5efa\u6f14\u5316\u5efa\u6a21\u65b9\u6cd5\uff0c\u521b\u5efa\u66f4\u51c6\u786e\u7684DRAM\u534f\u8bae\u8868\u793a\u6a21\u578b", "result": "\u6a21\u578b\u4f7fDRAM\u534f\u8bae\u66f4\u6613\u7406\u89e3\u4e14\u53ef\u76f4\u63a5\u6267\u884c\uff0c\u652f\u6301\u8bc4\u4f30\u5173\u952e\u6307\u6807\u5e76\u9a8c\u8bc1\u63a7\u5236\u5668RTL\u6a21\u578b\u3001DRAM\u903b\u8f91\u548c\u5185\u5b58\u6a21\u62df\u5668", "conclusion": "\u63d0\u51fa\u7684\u5efa\u6a21\u65b9\u6cd5\u663e\u8457\u6539\u8fdbDRAM\u534f\u8bae\u7684\u7406\u89e3\u548c\u9a8c\u8bc1\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u5185\u5b58\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6709\u6548\u5de5\u5177"}}
{"id": "2602.10790", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.10790", "abs": "https://arxiv.org/abs/2602.10790", "authors": ["Paula Carolina Lozano Duarte", "Sule Ozev", "Mehdi Tahoori"], "title": "Fault Tolerant Design of IGZO-based Binary Search ADCs", "comment": "Accepted for publication at the 27th International Symposium on Quality Electronic Design (ISQED'26), April 8-10, 2026", "summary": "Thin-film technologies such as Indium Gallium Zinc Oxide (IGZO) enable Flexible Electronics (FE) for emerging applications in wearable sensing, personal health monitoring, and large-area systems. Analog-to-digital converters (ADCs) serve as critical sensor interfaces in these systems. Yet, their vulnerability to manufacturing defects remains poorly understood despite unipolar technologies' inherently high defect densities and process variations compared to mature CMOS technologies. We present a hierarchical fault injection framework to characterize defect sensitivity in Binary Search ADCs implemented in n-type only technologies. Our methodology combines transistor-level defect characterization with system-level fault propagation analysis, enabling efficient exploration of both single and multiple fault scenarios across the conversion hierarchy. The framework identifies critical fault-sensitive circuit components and enables selective redundancy strategies targeting only the most sensitive components. The resulting defect-tolerant designs improve fault coverage from 60% to 92% under single-fault injections and from 34% to 77.6% under multi-fault injection, while incurring only 4.2% area overhead and 6% power increase. While validated on IGZO-TFTs, the methodology applies to all emerging unipolar technologies.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u6545\u969c\u6ce8\u5165\u6846\u67b6\u5206\u6790\u5355\u6781\u6280\u672f\u4e2d\u4e8c\u8fdb\u5236\u641c\u7d22ADC\u7684\u7f3a\u9677\u654f\u611f\u6027\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5197\u4f59\u7b56\u7565\u5c06\u6545\u969c\u8986\u76d6\u7387\u4ece60%\u63d0\u5347\u81f392%\uff0c\u9762\u79ef\u5f00\u9500\u4ec54.2%", "motivation": "\u67d4\u6027\u7535\u5b50\u6280\u672f\uff08\u5982IGZO\uff09\u5728\u53ef\u7a7f\u6234\u4f20\u611f\u3001\u5065\u5eb7\u76d1\u6d4b\u7b49\u65b0\u5174\u5e94\u7528\u4e2d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5355\u6781\u6280\u672f\u76f8\u6bd4\u6210\u719fCMOS\u6280\u672f\u5177\u6709\u66f4\u9ad8\u7684\u7f3a\u9677\u5bc6\u5ea6\u548c\u5de5\u827a\u53d8\u5316\u3002\u4f5c\u4e3a\u5173\u952e\u4f20\u611f\u5668\u63a5\u53e3\u7684ADC\u5bf9\u5236\u9020\u7f3a\u9677\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u7ed3\u5408\u6676\u4f53\u7ba1\u7ea7\u7f3a\u9677\u8868\u5f81\u548c\u7cfb\u7edf\u7ea7\u6545\u969c\u4f20\u64ad\u5206\u6790\uff0c\u5728\u8f6c\u6362\u5c42\u6b21\u7ed3\u6784\u4e2d\u9ad8\u6548\u63a2\u7d22\u5355\u6545\u969c\u548c\u591a\u6545\u969c\u573a\u666f\uff0c\u8bc6\u522b\u5173\u952e\u6545\u969c\u654f\u611f\u7535\u8def\u7ec4\u4ef6\uff0c\u5e76\u5b9e\u65bd\u9009\u62e9\u6027\u5197\u4f59\u7b56\u7565\u3002", "result": "\u7f3a\u9677\u5bb9\u5fcd\u8bbe\u8ba1\u5c06\u5355\u6545\u969c\u6ce8\u5165\u4e0b\u7684\u6545\u969c\u8986\u76d6\u7387\u4ece60%\u63d0\u5347\u81f392%\uff0c\u591a\u6545\u969c\u6ce8\u5165\u4e0b\u4ece34%\u63d0\u5347\u81f377.6%\uff0c\u540c\u65f6\u4ec5\u4ea7\u751f4.2%\u7684\u9762\u79ef\u5f00\u9500\u548c6%\u7684\u529f\u8017\u589e\u52a0\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u8bc6\u522b\u4e86\u4e8c\u8fdb\u5236\u641c\u7d22ADC\u4e2d\u7684\u5173\u952e\u6545\u969c\u654f\u611f\u7ec4\u4ef6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5197\u4f59\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u9677\u5bb9\u5fcd\u80fd\u529b\uff0c\u867d\u7136\u57fa\u4e8eIGZO-TFT\u9a8c\u8bc1\uff0c\u4f46\u9002\u7528\u4e8e\u6240\u6709\u65b0\u5174\u5355\u6781\u6280\u672f\u3002"}}
{"id": "2602.11016", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11016", "abs": "https://arxiv.org/abs/2602.11016", "authors": ["Jinxin Yu", "Yudong Pan", "Mengdi Wang", "Huawei Li", "Yinhe Han", "Xiaowei Li", "Ying Wang"], "title": "From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design", "comment": "Accepted to DATE 2026", "summary": "Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.", "AI": {"tldr": "3D-Flow\uff1a\u4e00\u79cd\u57fa\u4e8e3D\u5806\u53e0\u7a7a\u95f4\u52a0\u901f\u5668\u7684Transformer\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u5782\u76f4\u5206\u533aPE\u5c42\u95f4\u7684\u5bc4\u5b58\u5668\u901a\u4fe1\uff0c\u51cf\u5c11\u7247\u4e0aSRAM\u8bbf\u95ee\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u5b58\u5728\u4e8c\u6b21\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u548c\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002\u73b0\u6709\u52a0\u901f\u5668\u867d\u7136\u51cf\u5c11\u4e86\u7247\u5916\u6d41\u91cf\uff0c\u4f46\u7247\u4e0aSRAM\u8bbf\u95ee\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u5360\u80fd\u801760%\u4ee5\u4e0a\uff0c\u6210\u4e3a\u65b0\u7684\u74f6\u9888\u3002", "method": "\u63d0\u51fa3D-Flow\uff1a\u6df7\u5408\u952e\u54083D\u5806\u53e0\u7a7a\u95f4\u52a0\u901f\u5668\uff0c\u652f\u6301\u5782\u76f4\u5206\u533aPE\u5c42\u95f4\u7684\u5bc4\u5b58\u5668\u5230\u5bc4\u5b58\u5668\u901a\u4fe1\uff1b\u8bbe\u8ba13D-FlashAttention\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u65b9\u6cd5\uff0c\u5b9e\u73b0\u65e0\u6c14\u6ce1\u5782\u76f4\u6570\u636e\u6d41\uff0c\u907f\u514d\u7247\u4e0aSRAM\u5f80\u8fd4\u3002", "result": "\u5728Transformer\u5de5\u4f5c\u8d1f\u8f7d\uff08OPT\u548cQWEN\u6a21\u578b\uff09\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u76842D\u548c3D\u8bbe\u8ba1\uff0c\u80fd\u8017\u964d\u4f4e46-93%\uff0c\u901f\u5ea6\u63d0\u53471.4-7.6\u500d\u3002", "conclusion": "3D-Flow\u901a\u8fc73D\u5782\u76f4\u901a\u4fe1\u67b6\u6784\u548c\u7ec6\u7c92\u5ea6\u8c03\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86Transformer\u52a0\u901f\u4e2d\u7684\u7247\u4e0aSRAM\u8bbf\u95ee\u74f6\u9888\uff0c\u4e3a\u672a\u6765AI\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
