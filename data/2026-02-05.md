<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [SPPAM: Signature Pattern Prediction and Access-Map Prefetcher](https://arxiv.org/abs/2602.04100)
*Maccoy Merrell,Lei Wang,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: SPPAM是一种新的缓存预取方法，结合了SPP和AMPM的优点，通过在线学习构建访问模式图，使用置信度调节的推测性前瞻机制，在二级缓存上显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 处理器速度与内存系统性能之间的差距持续限制许多工作负载的性能。现有预取技术如SPP容易受到高级缓存和乱序核心的引用重排序影响，而AMPM虽然对重排序有抵抗力，但无法推测超出区域范围。需要一种结合两者优点、克服各自局限的新方法。

Method: SPPAM采用在线学习构建一组访问模式图，这些模式图用于置信度调节的推测性前瞻机制。该方法针对二级缓存设计，结合了SPP的推测能力和AMPM对重排序的抵抗力。

Result: SPPAM与最先进的预取器Berti和Bingo结合，相比无预取系统性能提升31.4%，相比Berti和Pythia基线提升6.2%。

Conclusion: SPPAM通过结合SPP和AMPM的优点，克服了各自局限性，提供了一种有效的缓存预取解决方案，显著提升了系统性能。

Abstract: The discrepancy between processor speed and memory system performance continues to limit the performance of many workloads. To address the issue, one effective and well studied technique is cache prefetching. Many prefetching designs have been proposed, with varying approaches and effectiveness. For example, SPP is a popular prefetcher that leverages confidence throttled recursion to speculate on the future path of program's references, however it is very susceptible to the reference reordering of higher-level caches and the OoO core. Orthogonally, AMPM is another popular approach to prefetching which uses reordering-resistant access maps to identify patterns within a region, but is unable to speculate beyond that region. In this paper, we propose SPPAM, a new approach to prefetching, inspired by prior works such as SPP and AMPM, while addressing their limitations. SPPAM utilizes online-learning to build a set of access-map patterns. These patterns are used in a speculative lookahead which is throttled by a confidence metric. Targeting the second-level cache, SPPAM alongside state-of-the-art prefetchers Berti and Bingo improve system performance by 31.4% over no prefetching and 6.2% over the baseline of Berti and Pythia.

</details>


### [2] [Crypto-RV: High-Efficiency FPGA-Based RISC-V Cryptographic Co-Processor for IoT Security](https://arxiv.org/abs/2602.04415)
*Anh Kiet Pham,Van Truong Vo,Vu Trung Duong Le,Tuan Hai Vu,Hoai Luan Pham,Van Tinh Nguyen,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: Crypto-RV：一个RISC-V协处理器架构，在单个64位数据路径中统一支持多种加密算法（包括后量子密码），通过创新的缓冲、执行单元和调度机制，在FPGA上实现高性能、高能效的加密处理。


<details>
  <summary>Details</summary>
Motivation: 当前RISC-V平台缺乏对全面加密算法家族和后量子密码的高效硬件支持，而物联网、边缘计算和自主系统对加密操作有重要需求。

Method: 提出Crypto-RV架构，包含三个关键创新：1）高带宽内部缓冲区（128x64位）；2）专门化的加密执行单元，具有四级流水线数据路径；3）针对大哈希优化的双缓冲机制和自适应调度。

Result: 在Xilinx ZCU102 FPGA上以160MHz频率和0.851W动态功耗实现，相比基线RISC-V核心获得165-1061倍加速，相比强大CPU实现5.8-17.4倍能效提升，仅占用34,704 LUTs、37,329 FFs和22 BRAMs。

Conclusion: Crypto-RV展示了在资源受限的物联网环境中实现高性能、高能效加密处理的可行性，为RISC-V平台提供了全面的加密硬件支持解决方案。

Abstract: Cryptographic operations are critical for securing IoT, edge computing, and autonomous systems. However, current RISC-V platforms lack efficient hardware support for comprehensive cryptographic algorithm families and post-quantum cryptography. This paper presents Crypto-RV, a RISC-V co-processor architecture that unifies support for SHA-256, SHA-512, SM3, SHA3-256, SHAKE-128, SHAKE-256 AES-128, HARAKA-256, and HARAKA-512 within a single 64-bit datapath. Crypto-RV introduces three key architectural innovations: a high-bandwidth internal buffer (128x64-bit), cryptography-specialized execution units with four-stage pipelined datapaths, and a double-buffering mechanism with adaptive scheduling optimized for large-hash. Implemented on Xilinx ZCU102 FPGA at 160 MHz with 0.851 W dynamic power, Crypto-RV achieves 165 times to 1,061 times speedup over baseline RISC-V cores, 5.8 times to 17.4 times better energy efficiency compared to powerful CPUs. The design occupies only 34,704 LUTs, 37,329 FFs, and 22 BRAMs demonstrating viability for high-performance, energy-efficient cryptographic processing in resource-constrained IoT environments.

</details>


### [3] [Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference](https://arxiv.org/abs/2602.04595)
*Xinyu Wang,Jieyu Li,Yanan Sun,Weifeng He*

Main category: cs.AR

TL;DR: Harmonia是一个算法-硬件协同设计框架，通过在所有层使用可配置的块浮点数激活，结合非对称位分配和混合离在线异常平滑技术，实现高效的LLM推理，显著提升面积效率、能效和速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大但内存和计算成本高。现有量化方法在线性层使用块浮点数激活，但无法扩展到注意力层，导致精度严重下降，限制了整体效率。

Method: 1. 系统探索BFP配置以在所有层实现精度与激活压缩的更好权衡；2. 引入非对称位分配策略结合混合离在线异常平滑技术，将KV缓存从FP16压缩到4位尾数BFP；3. 设计专用硬件组件，包括支持混合数据格式的可重构PE、实时FP16到BFP转换器和分块感知数据流。

Result: 在8个广泛使用的LLM上评估，相比先前工作，Harmonia平均实现3.84倍（最高5.05倍）面积效率提升、2.03倍（最高3.90倍）能效提升和3.08倍（最高4.62倍）加速。

Conclusion: Harmonia通过算法-硬件协同设计成功在所有层实现BFP激活，解决了注意力层BFP应用的精度问题，显著提升了LLM推理的效率，为高效LLM部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.

</details>
