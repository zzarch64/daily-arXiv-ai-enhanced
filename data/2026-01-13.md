<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [DS-CIM: Digital Stochastic Computing-In-Memory Featuring Accurate OR-Accumulation via Sample Region Remapping for Edge AI Models](https://arxiv.org/abs/2601.06724)
*Kunming Shao,Liang Zhao,Jiangnan Yu,Zhipeng Liao,Xiaomeng Wang,Yi Zou,Tim Kwang-Ting Cheng,Chi-Ying Tsui*

Main category: cs.AR

TL;DR: 本文提出了一种数字随机计算内存（DS-CIM）架构，通过修改数据表示在紧凑的无符号OR电路中实现有符号乘累加，采用共享伪随机数生成器和2D分区策略提升吞吐量，解决了OR门冲突和1饱和问题，在精度和能效方面均取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 随机计算（SC）硬件简单但吞吐量低，而高吞吐量的数字计算内存（DCIM）在矩阵向量乘法中受限于昂贵的加法器逻辑。需要解决这种权衡，实现高精度和高效率的计算内存架构。

Method: 1. 通过修改数据表示在紧凑的无符号OR电路中实现有符号乘累加（MAC）
2. 通过64次复制低成本电路提升吞吐量，面积仅增加1倍
3. 采用共享伪随机数生成器（PRNG）和2D分区策略，实现单周期互斥激活，消除OR门冲突
4. 通过随机过程分析和数据重映射解决1饱和问题

Result: 1. 高精度DS-CIM1变体：在CIFAR-10上实现INT8 ResNet18的94.45%准确率，RMSE仅0.74%
2. 高效率DS-CIM2变体：能效达3566.1 TOPS/W，面积效率达363.7 TOPS/mm²，RMSE为3.81%
3. 在更大模型上验证：INT8 ResNet50在ImageNet和FP8 LLaMA-7B模型上展示了扩展能力

Conclusion: DS-CIM架构成功解决了随机计算和数字计算内存之间的权衡问题，实现了高精度和高效率的计算内存设计，为大规模神经网络部署提供了有前景的硬件解决方案。

Abstract: Stochastic computing (SC) offers hardware simplicity but suffers from low throughput, while high-throughput Digital Computing-in-Memory (DCIM) is bottlenecked by costly adder logic for matrix-vector multiplication (MVM). To address this trade-off, this paper introduces a digital stochastic CIM (DS-CIM) architecture that achieves both high accuracy and efficiency. We implement signed multiply-accumulation (MAC) in a compact, unsigned OR-based circuit by modifying the data representation. Throughput is enhanced by replicating this low-cost circuit 64 times with only a 1x area increase. Our core strategy, a shared Pseudo Random Number Generator (PRNG) with 2D partitioning, enables single-cycle mutually exclusive activation to eliminate OR-gate collisions. We also resolve the 1s saturation issue via stochastic process analysis and data remapping, significantly improving accuracy and resilience to input sparsity. Our high-accuracy DS-CIM1 variant achieves 94.45% accuracy for INT8 ResNet18 on CIFAR-10 with a root-mean-squared error (RMSE) of just 0.74%. Meanwhile, our high-efficiency DS-CIM2 variant attains an energy efficiency of 3566.1 TOPS/W and an area efficiency of 363.7 TOPS/mm^2, while maintaining a low RMSE of 3.81%. The DS-CIM capability with larger models is further demonstrated through experiments with INT8 ResNet50 on ImageNet and the FP8 LLaMA-7B model.

</details>


### [2] [GRPO with State Mutations: Improving LLM-Based Hardware Test Plan Generation](https://arxiv.org/abs/2601.07593)
*Dimple Vijay Kochar,Nathaniel Pinckney,Guan-Ting Liu,Chia-Tung Ho,Chenhui Deng,Haoxing Ren,Brucek Khailany*

Main category: cs.AR

TL;DR: 本文首次系统研究LLM在RTL验证激励生成中的推理能力，提出两阶段框架，并通过创新的强化学习方法显著提升LLM生成测试激励的质量。


<details>
  <summary>Details</summary>
Motivation: RTL设计早期严重依赖临时测试平台创建，而LLM在硬件规范理解和针对性测试计划生成方面的能力尚未得到充分探索。现有最先进模型在生成能通过黄金RTL设计的激励方面成功率很低（15.7-21.7%），需要改进。

Method: 建立两阶段框架：测试计划生成与测试平台执行分离。开发综合训练方法：监督微调结合新颖的强化学习方法GRPO-SMu（带状态突变的GRPO），采用树状分支突变策略构建训练数据，超越线性突变方法。

Result: 7B参数模型达到33.3%的黄金测试通过率和13.9%的突变检测率，相比基线绝对提升17.6%，优于更大的通用模型。证明了专门训练方法能显著提升LLM在硬件验证任务中的推理能力。

Conclusion: 专门训练方法能显著增强LLM在硬件验证任务中的推理能力，为半导体设计流程中的自动化子单元测试奠定了基础。

Abstract: RTL design often relies heavily on ad-hoc testbench creation early in the design cycle. While large language models (LLMs) show promise for RTL code generation, their ability to reason about hardware specifications and generate targeted test plans remains largely unexplored. We present the first systematic study of LLM reasoning capabilities for RTL verification stimuli generation, establishing a two-stage framework that decomposes test plan generation from testbench execution. Our benchmark reveals that state-of-the-art models, including DeepSeek-R1 and Claude-4.0-Sonnet, achieve only 15.7-21.7% success rates on generating stimuli that pass golden RTL designs. To improve LLM generated stimuli, we develop a comprehensive training methodology combining supervised fine-tuning with a novel reinforcement learning approach, GRPO with State Mutation (GRPO-SMu), which enhances exploration by varying input mutations. Our approach leverages a tree-based branching mutation strategy to construct training data comprising equivalent and mutated trees, moving beyond linear mutation approaches to provide rich learning signals. Training on this curated dataset, our 7B parameter model achieves a 33.3% golden test pass rate and a 13.9% mutation detection rate, representing a 17.6% absolute improvement over baseline and outperforming much larger general-purpose models. These results demonstrate that specialized training methodologies can significantly enhance LLM reasoning capabilities for hardware verification tasks, establishing a foundation for automated sub-unit testing in semiconductor design workflows.

</details>
