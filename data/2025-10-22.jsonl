{"id": "2510.18525", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.18525", "abs": "https://arxiv.org/abs/2510.18525", "authors": ["Yushu Zhao", "Yubin Qin", "Yang Wang", "Xiaolong Yang", "Huiming Han", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing", "comment": null, "summary": "Large language models achieve impressive performance across diverse tasks but\nexhibit high inference latency due to their large parameter sizes. While\nquantization reduces model size, it often leads to performance degradation\ncompared to the full model. Speculative decoding remains lossless but typically\nincurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed\nspeculative decoding method that uses part of the full-model weight bits to\nform a quantized draft model, thereby eliminating additional training or\nstorage overhead. A reconfigurable processing element array enables efficient\nexecution of both the draft and verification passes. Experimental results\nacross 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,\n1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively."}
