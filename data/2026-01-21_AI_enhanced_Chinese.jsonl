{"id": "2601.11770", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.11770", "abs": "https://arxiv.org/abs/2601.11770", "authors": ["Voktho Das", "Kimia Azar", "Hadi Kamali"], "title": "NuRedact: Non-Uniform eFPGA Architecture for Low-Overhead and Secure IP Redaction", "comment": "Accepted at Design, Automation, and Test 2026", "summary": "While logic locking has been extensively studied as a countermeasure against integrated circuit (IC) supply chain threats, recent research has shifted toward reconfigurable-based redaction techniques, e.g., LUT- and eFPGA-based schemes. While these approaches raise the bar against attacks, they incur substantial overhead, much of which arises not from genuine functional reconfigurability need, but from artificial complexity intended solely to frustrate reverse engineering (RE). As a result, fabrics are often underutilized, and security is achieved at disproportionate cost. This paper introduces NuRedact, the first full-custom eFPGA redaction framework that embraces architectural non-uniformity to balance security and efficiency. Built as an extension of the widely adopted OpenFPGA infrastructure, NuRedact introduces a three-stage methodology: (i) custom fabric generation with pin-mapping irregularity, (ii) VPR-level modifications to enable non-uniform placement guided by an automated Python-based optimizer, and (iii) redaction-aware reconfiguration and mapping of target IP modules. Experimental results show up to 9x area reduction compared to conventional uniform fabrics, achieving competitive efficiency with LUT-based and even transistor-level redaction techniques while retaining strong resilience. From a security perspective, NuRedact fabrics are evaluated against state-of-the-art attack models, including SAT-based, cyclic, and sequential variants, and show enhanced resilience while maintaining practical design overheads.", "AI": {"tldr": "NuRedact\u662f\u4e00\u4e2a\u975e\u5747\u5300eFPGA\u91cd\u5220\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u67b6\u6784\u5e73\u8861\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u5747\u5300\u7ed3\u6784\u5b9e\u73b0\u9ad8\u8fbe9\u500d\u7684\u9762\u79ef\u7f29\u51cf\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u5b89\u5168\u5f39\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLUT\u548ceFPGA\u7684\u53ef\u91cd\u6784\u91cd\u5220\u6280\u672f\u867d\u7136\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u4f46\u5e26\u6765\u4e86\u5de8\u5927\u7684\u5f00\u9500\uff0c\u8fd9\u4e9b\u5f00\u9500\u4e3b\u8981\u6765\u81ea\u4e3a\u963b\u788d\u9006\u5411\u5de5\u7a0b\u800c\u5f15\u5165\u7684\u4eba\u5de5\u590d\u6742\u6027\uff0c\u800c\u975e\u771f\u6b63\u7684\u529f\u80fd\u53ef\u91cd\u6784\u9700\u6c42\uff0c\u5bfc\u81f4\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u4e14\u5b89\u5168\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u4f5c\u4e3aOpenFPGA\u57fa\u7840\u8bbe\u65bd\u7684\u6269\u5c55\uff0cNuRedact\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u751f\u6210\u5177\u6709\u5f15\u811a\u6620\u5c04\u4e0d\u89c4\u5219\u6027\u7684\u5b9a\u5236\u5316\u786c\u4ef6\u7ed3\u6784\uff1b2) \u5728VPR\u7ea7\u522b\u8fdb\u884c\u4fee\u6539\uff0c\u652f\u6301\u57fa\u4e8ePython\u81ea\u52a8\u5316\u4f18\u5316\u5668\u7684\u975e\u5747\u5300\u5e03\u5c40\uff1b3) \u5bf9\u76ee\u6807IP\u6a21\u5757\u8fdb\u884c\u91cd\u5220\u611f\u77e5\u7684\u91cd\u65b0\u914d\u7f6e\u548c\u6620\u5c04\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u5747\u5300\u7ed3\u6784\uff0cNuRedact\u5b9e\u73b0\u4e86\u9ad8\u8fbe9\u500d\u7684\u9762\u79ef\u7f29\u51cf\uff0c\u5728\u4fdd\u6301\u4e0eLUT\u57fa\u751a\u81f3\u6676\u4f53\u7ba1\u7ea7\u91cd\u5220\u6280\u672f\u7ade\u4e89\u6548\u7387\u7684\u540c\u65f6\uff0c\u7ef4\u6301\u4e86\u5f3a\u5927\u7684\u5b89\u5168\u5f39\u6027\u3002\u8be5\u6846\u67b6\u80fd\u6709\u6548\u62b5\u6297SAT\u57fa\u3001\u5faa\u73af\u548c\u65f6\u5e8f\u53d8\u4f53\u7b49\u5148\u8fdb\u653b\u51fb\u6a21\u578b\u3002", "conclusion": "NuRedact\u901a\u8fc7\u5f15\u5165\u67b6\u6784\u975e\u5747\u5300\u6027\uff0c\u5728\u5b89\u5168\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u4e3a\u96c6\u6210\u7535\u8def\u4f9b\u5e94\u94fe\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u91cd\u5220\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b89\u5168\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u653b\u51fb\u62b5\u6297\u529b\u3002"}}
{"id": "2601.12089", "categories": ["cs.AR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12089", "abs": "https://arxiv.org/abs/2601.12089", "authors": ["Erwan Tanguy-Legac", "Tommaso Belvedere", "Gianluca Corsini", "Marco Tognon", "Marcello Traiola"], "title": "Domain-specific Hardware Acceleration for Model Predictive Path Integral Control", "comment": "7 pages, 11 figures", "summary": "Accurately controlling a robotic system in real time is a challenging problem. To address this, the robotics community has adopted various algorithms, such as Model Predictive Control (MPC) and Model Predictive Path Integral (MPPI) control. The first is difficult to implement on non-linear systems such as unmanned aerial vehicles, whilst the second requires a heavy computational load. GPUs have been successfully used to accelerate MPPI implementations; however, their power consumption is often excessive for autonomous or unmanned targets, especially when battery-powered. On the other hand, custom designs, often implemented on FPGAs, have been proposed to accelerate robotic algorithms while consuming considerably less energy than their GPU (or CPU) implementation. However, no MPPI custom accelerator has been proposed so far. In this work, we present a hardware accelerator for MPPI control and simulate its execution. Results show that the MPPI custom accelerator allows more accurate trajectories than GPU-based MPPI implementations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u786c\u4ef6\u52a0\u901f\u5668\u7684MPPI\u63a7\u5236\u5668\uff0c\u76f8\u6bd4GPU\u5b9e\u73b0\u66f4\u7cbe\u786e\u4e14\u80fd\u8017\u66f4\u4f4e", "motivation": "\u673a\u5668\u4eba\u5b9e\u65f6\u63a7\u5236\u4e2d\uff0cMPC\u96be\u4ee5\u5e94\u7528\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\uff0cMPPI\u8ba1\u7b97\u8d1f\u8f7d\u5927\uff0cGPU\u52a0\u901f\u80fd\u8017\u8fc7\u9ad8\uff0c\u4e0d\u9002\u5408\u7535\u6c60\u4f9b\u7535\u7684\u81ea\u4e3b\u7cfb\u7edf", "method": "\u8bbe\u8ba1\u5e76\u6a21\u62df\u6267\u884cMPPI\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u91c7\u7528\u5b9a\u5236\u5316\u8bbe\u8ba1\uff08\u53ef\u80fd\u57fa\u4e8eFPGA\uff09\u6765\u52a0\u901fMPPI\u63a7\u5236\u7b97\u6cd5", "result": "MPPI\u5b9a\u5236\u52a0\u901f\u5668\u6bd4\u57fa\u4e8eGPU\u7684MPPI\u5b9e\u73b0\u80fd\u591f\u751f\u6210\u66f4\u7cbe\u786e\u7684\u8f68\u8ff9", "conclusion": "\u786c\u4ef6\u52a0\u901f\u5668\u4e3aMPPI\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u4f4e\u80fd\u8017\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u7535\u6c60\u4f9b\u7535\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf"}}
{"id": "2601.12156", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.12156", "abs": "https://arxiv.org/abs/2601.12156", "authors": ["Debabrata Das", "Yogeeth G. K.", "Arnav Gupta"], "title": "Biological Intuition on Digital Hardware: An RTL Implementation of Poisson-Encoded SNNs for Static Image Classification", "comment": "5 pages, 8 figures, 2 tables. Code available at: https://github.com/Yogeeth/neuromorphic-lif-rtl", "summary": "The deployment of Artificial Intelligence on edge devices (TinyML) is often constrained by the high power consumption and latency associated with traditional Artificial Neural Networks (ANNs) and their reliance on intensive Matrix-Multiply (MAC) operations. Neuromorphic computing offers a compelling alternative by mimicking biological efficiency through event-driven processing. This paper presents the design and implementation of a cycle-accurate, hardware-oriented Spiking Neural Network (SNN) core implemented in SystemVerilog. Unlike conventional accelerators, this design utilizes a Leaky Integrate-and-Fire (LIF) neuron model powered by fixed-point arithmetic and bit-wise primitives (shifts and additions) to eliminate the need for complex floating-point hardware. The architecture features an on-chip Poisson encoder for stochastic spike generation and a novel active pruning mechanism that dynamically disables neurons post-classification to minimize dynamic power consumption. We demonstrate the hardware's efficacy through a fully connected layer implementation targeting digit classification. Simulation results indicate that the design achieves rapid convergence (89% accuracy) within limited timesteps while maintaining a significantly reduced computational footprint compared to traditional dense architectures. This work serves as a foundational building block for scalable, energy-efficient neuromorphic hardware on FPGA and ASIC platforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eSystemVerilog\u7684\u7cbe\u786e\u5468\u671f\u786c\u4ef6SNN\u6838\u5fc3\uff0c\u91c7\u7528LIF\u795e\u7ecf\u5143\u6a21\u578b\u548c\u5b9a\u70b9\u8fd0\u7b97\uff0c\u901a\u8fc7\u7247\u4e0a\u6cca\u677e\u7f16\u7801\u548c\u4e3b\u52a8\u526a\u679d\u673a\u5236\uff0c\u5728FPGA/ASIC\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548\u8282\u80fd\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\uff08TinyML\uff09\u5e38\u53d7\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u9ad8\u529f\u8017\u548c\u5ef6\u8fdf\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5176\u4f9d\u8d56\u5bc6\u96c6\u7684\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\u3002\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u6a21\u62df\u751f\u7269\u6548\u7387\uff0c\u63d0\u4f9b\u4e86\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eSystemVerilog\u7684\u7cbe\u786e\u5468\u671f\u786c\u4ef6SNN\u6838\u5fc3\uff0c\u91c7\u7528LIF\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u4f7f\u7528\u5b9a\u70b9\u7b97\u672f\u548c\u4f4d\u7ea7\u539f\u8bed\uff08\u79fb\u4f4d\u548c\u52a0\u6cd5\uff09\u66ff\u4ee3\u590d\u6742\u6d6e\u70b9\u786c\u4ef6\u3002\u67b6\u6784\u5305\u542b\u7247\u4e0a\u6cca\u677e\u7f16\u7801\u5668\u7528\u4e8e\u968f\u673a\u8109\u51b2\u751f\u6210\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u4e3b\u52a8\u526a\u679d\u673a\u5236\uff0c\u5728\u5206\u7c7b\u540e\u52a8\u6001\u7981\u7528\u795e\u7ecf\u5143\u4ee5\u51cf\u5c11\u52a8\u6001\u529f\u8017\u3002", "result": "\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u5b9e\u73b0\u6570\u5b57\u5206\u7c7b\uff0c\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8bbe\u8ba1\u5728\u6709\u9650\u65f6\u95f4\u6b65\u5185\u5feb\u901f\u6536\u655b\uff0889%\u51c6\u786e\u7387\uff09\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u5bc6\u96c6\u67b6\u6784\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aFPGA\u548cASIC\u5e73\u53f0\u4e0a\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u63d0\u4f9b\u4e86\u57fa\u7840\u6784\u5efa\u6a21\u5757\uff0c\u5c55\u793a\u4e86\u786c\u4ef6SNN\u6838\u5fc3\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12298", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.12298", "abs": "https://arxiv.org/abs/2601.12298", "authors": ["Ye Lin", "Chao Fang", "Xiaoyong Song", "Qi Wu", "Anying Jiang", "Yichuan Bai", "Li Du"], "title": "CD-PIM: A High-Bandwidth and Compute-Efficient LPDDR5-Based PIM for Low-Batch LLM Acceleration on Edge-Device", "comment": "To appear in 2026 Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "Edge deployment of low-batch large language models (LLMs) faces critical memory bandwidth bottlenecks when executing memory-intensive general matrix-vector multiplications (GEMV) operations. While digital processing-in-memory (PIM) architectures promise to accelerate GEMV operations, existing PIM-equipped edge devices still suffer from three key limitations: limited bandwidth improvement, component under-utilization in mixed workloads, and low compute capacity of computing units (CUs). In this paper, we propose CD-PIM to address these challenges through three key innovations. First, we introduce a high-bandwidth compute-efficient mode (HBCEM) that enhances bandwidth by dividing each bank into four pseudo-banks through segmented global bitlines. Second, we propose a low-batch interleaving mode (LBIM) to improve component utilization by overlapping GEMV operations with GEMM operations. Third, we design a compute-efficient CU that performs enhanced GEMV operations in a pipelined manner by serially feeding weight data into the computing core. Forth, we adopt a column-wise mapping for the key-cache matrix and row-wise mapping for the value-cache matrix, which fully utilizes CU resources. Our evaluation shows that compared to a GPU-only baseline and state-of-the-art PIM designs, our CD-PIM achieves 11.42x and 4.25x speedup on average within a single batch in HBCEM mode, respectively. Moreover, for low-batch sizes, the CD-PIM achieves an average speedup of 1.12x in LBIM compared to HBCEM.", "AI": {"tldr": "CD-PIM\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8fb9\u7f18\u90e8\u7f72\u4f4e\u6279\u6b21\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u6570\u5b57\u5904\u7406\u5185\u5b58\u67b6\u6784\uff0c\u901a\u8fc7\u9ad8\u5e26\u5bbd\u8ba1\u7b97\u9ad8\u6548\u6a21\u5f0f\u3001\u4f4e\u6279\u6b21\u4ea4\u9519\u6a21\u5f0f\u548c\u8ba1\u7b97\u9ad8\u6548\u5355\u5143\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709PIM\u67b6\u6784\u7684\u5e26\u5bbd\u9650\u5236\u3001\u7ec4\u4ef6\u5229\u7528\u7387\u4f4e\u548c\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u8fb9\u7f18\u90e8\u7f72\u4f4e\u6279\u6b21\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff0c\u73b0\u6709\u6570\u5b57\u5904\u7406\u5185\u5b58\u67b6\u6784\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5e26\u5bbd\u6539\u8fdb\u6709\u9650\u3001\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7ec4\u4ef6\u5229\u7528\u7387\u4f4e\u3001\u8ba1\u7b97\u5355\u5143\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1. \u9ad8\u5e26\u5bbd\u8ba1\u7b97\u9ad8\u6548\u6a21\u5f0f\uff1a\u901a\u8fc7\u5206\u6bb5\u5168\u5c40\u4f4d\u7ebf\u5c06\u6bcf\u4e2a\u5b58\u50a8\u4f53\u5206\u4e3a\u56db\u4e2a\u4f2a\u5b58\u50a8\u4f53\uff1b2. \u4f4e\u6279\u6b21\u4ea4\u9519\u6a21\u5f0f\uff1a\u91cd\u53e0GEMV\u548cGEMM\u64cd\u4f5c\u63d0\u9ad8\u7ec4\u4ef6\u5229\u7528\u7387\uff1b3. \u8ba1\u7b97\u9ad8\u6548\u5355\u5143\uff1a\u4ee5\u6d41\u6c34\u7ebf\u65b9\u5f0f\u4e32\u884c\u8f93\u5165\u6743\u91cd\u6570\u636e\u6267\u884c\u589e\u5f3aGEMV\u64cd\u4f5c\uff1b4. \u5217\u5411\u6620\u5c04\u952e\u7f13\u5b58\u77e9\u9635\u548c\u884c\u5411\u6620\u5c04\u503c\u7f13\u5b58\u77e9\u9635\u3002", "result": "\u5728\u5355\u6279\u6b21\u9ad8\u5e26\u5bbd\u8ba1\u7b97\u9ad8\u6548\u6a21\u5f0f\u4e0b\uff0c\u76f8\u6bd4GPU\u57fa\u51c6\u548c\u5148\u8fdbPIM\u8bbe\u8ba1\uff0cCD-PIM\u5206\u522b\u5b9e\u73b0\u5e73\u574711.42\u500d\u548c4.25\u500d\u52a0\u901f\u3002\u5728\u4f4e\u6279\u6b21\u4e0b\uff0c\u4f4e\u6279\u6b21\u4ea4\u9519\u6a21\u5f0f\u76f8\u6bd4\u9ad8\u5e26\u5bbd\u8ba1\u7b97\u9ad8\u6548\u6a21\u5f0f\u5b9e\u73b0\u5e73\u57471.12\u500d\u52a0\u901f\u3002", "conclusion": "CD-PIM\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u4f4e\u6279\u6b21\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6027\u80fd\u548c\u7ec4\u4ef6\u5229\u7528\u7387\u3002"}}
{"id": "2601.12686", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.12686", "abs": "https://arxiv.org/abs/2601.12686", "authors": ["Rafi Zahedi", "Amin Zamani", "Rahul Anilkumar"], "title": "Best Practices for Large Load Interconnections: A North American Perspective on Data Centers", "comment": "Presented at CIGRE United States, and published by CIGRE", "summary": "Large loads are expanding rapidly across North America, led by data centers, cryptocurrency mining, hydrogen production facilities, and heavy-duty charging stations. Each class presents distinct electrical characteristics, but data centers are drawing particular attention as AI deployment drives unprecedented capacity growth. Their scale, duty cycles, and converter-dominated interfaces introduce new challenges for transmission interconnections, especially regarding disturbance behavior, steady-state performance, and operational visibility. This paper reviews best practices for large-load interconnections across North America, synthesizing utility and system operator guidelines into a coherent set of technical requirements. The approach combines handbook and manual analysis with cross-utility comparisons and an outlook on European directions. The review highlights requirements on power quality, telemetry, commissioning tests, and protection coordination, while noting gaps in ride-through specifications, load-variation management, and post-disturbance recovery targets. Building on these findings, the paper proposes practical guidance for developers and utilities.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u5317\u7f8e\u5927\u578b\u8d1f\u8377\uff08\u6570\u636e\u4e2d\u5fc3\u3001\u52a0\u5bc6\u8d27\u5e01\u6316\u77ff\u7b49\uff09\u5e76\u7f51\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u5206\u6790\u6280\u672f\u6311\u6218\u5e76\u63d0\u51fa\u5b9e\u7528\u6307\u5bfc", "motivation": "\u5317\u7f8e\u5927\u578b\u8d1f\u8377\uff08\u7279\u522b\u662f\u6570\u636e\u4e2d\u5fc3\uff09\u5feb\u901f\u589e\u957f\uff0c\u5176\u89c4\u6a21\u3001\u5de5\u4f5c\u5468\u671f\u548c\u53d8\u6d41\u5668\u4e3b\u5bfc\u7684\u63a5\u53e3\u7ed9\u8f93\u7535\u5e76\u7f51\u5e26\u6765\u65b0\u6311\u6218\uff0c\u5305\u62ec\u6270\u52a8\u884c\u4e3a\u3001\u7a33\u6001\u6027\u80fd\u548c\u8fd0\u884c\u53ef\u89c1\u6027\u7b49\u95ee\u9898", "method": "\u7ed3\u5408\u624b\u518c\u548c\u624b\u518c\u5206\u6790\u3001\u8de8\u7535\u529b\u516c\u53f8\u6bd4\u8f83\u4ee5\u53ca\u6b27\u6d32\u65b9\u5411\u5c55\u671b\uff0c\u7efc\u5408\u5317\u7f8e\u516c\u7528\u4e8b\u4e1a\u548c\u7cfb\u7edf\u8fd0\u8425\u5546\u6307\u5357\u5f62\u6210\u4e00\u5957\u8fde\u8d2f\u7684\u6280\u672f\u8981\u6c42", "result": "\u786e\u5b9a\u4e86\u7535\u80fd\u8d28\u91cf\u3001\u9065\u6d4b\u3001\u8c03\u8bd5\u6d4b\u8bd5\u548c\u4fdd\u62a4\u534f\u8c03\u7b49\u65b9\u9762\u7684\u8981\u6c42\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u7a7f\u8d8a\u89c4\u8303\u3001\u8d1f\u8377\u53d8\u5316\u7ba1\u7406\u548c\u6270\u52a8\u540e\u6062\u590d\u76ee\u6807\u65b9\u9762\u7684\u5dee\u8ddd", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u4e3a\u5f00\u53d1\u5546\u548c\u516c\u7528\u4e8b\u4e1a\u516c\u53f8\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u5efa\u8bae\uff0c\u4ee5\u5e94\u5bf9\u5927\u578b\u8d1f\u8377\u5e76\u7f51\u7684\u6280\u672f\u6311\u6218"}}
{"id": "2601.13628", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.13628", "abs": "https://arxiv.org/abs/2601.13628", "authors": ["Yue Jiet Chong", "Yimin Wang", "Zhen Wu", "Xuanyao Fong"], "title": "PRIMAL: Processing-In-Memory Based Low-Rank Adaptation for LLM Inference Accelerator", "comment": "Accepted to 2026 IEEE International Symposium on Circuits and Systems (ISCAS'26)", "summary": "This paper presents PRIMAL, a processing-in-memory (PIM) based large language model (LLM) inference accelerator with low-rank adaptation (LoRA). PRIMAL integrates heterogeneous PIM processing elements (PEs), interconnected by 2D-mesh inter-PE computational network (IPCN). A novel SRAM reprogramming and power gating (SRPG) scheme enables pipelined LoRA updates and sub-linear power scaling by overlapping reconfiguration with computation and gating idle resources. PRIMAL employs optimized spatial mapping and dataflow orchestration to minimize communication overhead, and achieves $1.5\\times$ throughput and $25\\times$ energy efficiency over NVIDIA H100 with LoRA rank 8 (Q,V) on Llama-13B.", "AI": {"tldr": "PRIMAL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b58\u5185\u8ba1\u7b97(PIM)\u7684LLM\u63a8\u7406\u52a0\u901f\u5668\uff0c\u96c6\u6210\u4e86\u5f02\u6784PIM\u5904\u7406\u5355\u5143\u548c2D\u7f51\u683c\u4e92\u8fde\u7f51\u7edc\uff0c\u901a\u8fc7SRAM\u91cd\u7f16\u7a0b\u548c\u7535\u6e90\u95e8\u63a7\u65b9\u6848\u5b9e\u73b0\u6d41\u6c34\u7ebfLoRA\u66f4\u65b0\u548c\u6b21\u7ebf\u6027\u529f\u8017\u6269\u5c55\uff0c\u76f8\u6bd4NVIDIA H100\u5728LoRA rank 8\u4e0a\u5b9e\u73b0\u4e861.5\u500d\u541e\u5410\u91cf\u548c25\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9762\u4e34\u8ba1\u7b97\u5bc6\u96c6\u548c\u5185\u5b58\u8bbf\u95ee\u74f6\u9888\u95ee\u9898\uff0c\u7279\u522b\u662f\u7ed3\u5408LoRA\u5fae\u8c03\u65f6\uff0c\u4f20\u7edf\u67b6\u6784\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u52a8\u6001\u53c2\u6570\u66f4\u65b0\u3002\u9700\u8981\u8bbe\u8ba1\u4e13\u95e8\u7684\u786c\u4ef6\u52a0\u901f\u5668\u6765\u4f18\u5316LLM\u63a8\u7406\u4e0eLoRA\u9002\u914d\u7684\u7ed3\u5408\u3002", "method": "1. \u96c6\u6210\u5f02\u6784PIM\u5904\u7406\u5355\u5143\u548c2D\u7f51\u683c\u4e92\u8fde\u7f51\u7edc\uff1b2. \u91c7\u7528SRAM\u91cd\u7f16\u7a0b\u548c\u7535\u6e90\u95e8\u63a7\u65b9\u6848\u5b9e\u73b0\u6d41\u6c34\u7ebfLoRA\u66f4\u65b0\uff1b3. \u4f18\u5316\u7a7a\u95f4\u6620\u5c04\u548c\u6570\u636e\u6d41\u7f16\u6392\u4ee5\u6700\u5c0f\u5316\u901a\u4fe1\u5f00\u9500\uff1b4. \u652f\u6301\u91cd\u53e0\u91cd\u914d\u7f6e\u4e0e\u8ba1\u7b97\uff0c\u95e8\u63a7\u7a7a\u95f2\u8d44\u6e90\u5b9e\u73b0\u6b21\u7ebf\u6027\u529f\u8017\u6269\u5c55\u3002", "result": "\u5728Llama-13B\u6a21\u578b\u4e0a\uff0c\u4f7f\u7528LoRA rank 8\uff08Q,V\uff09\u914d\u7f6e\u65f6\uff0c\u76f8\u6bd4NVIDIA H100\u5b9e\u73b0\u4e861.5\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c25\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "PRIMAL\u5c55\u793a\u4e86PIM\u67b6\u6784\u5728LLM\u63a8\u7406\u52a0\u901f\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u7ed3\u5408LoRA\u5fae\u8c03\u65f6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u786c\u4ef6\u8bbe\u8ba1\u548c\u4f18\u5316\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4e3a\u9ad8\u6548LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13804", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.13804", "abs": "https://arxiv.org/abs/2601.13804", "authors": ["Ioannis Constantinou", "Arthur Perais", "Yiannakis Sazeides"], "title": "The Non-Predictability of Mispredicted Branches using Timing Information", "comment": null, "summary": "Branch misprediction latency is one of the most important contributors to performance degradation and wasted energy consumption in a modern core. State-of-the-art predictors generally perform very well but occasionally suffer from high Misprediction Per Kilo Instruction due to hard-to-predict branches. In this work, we investigate if predicting branches using microarchitectural information, in addition to traditional branch history, can improve prediction accuracy. Our approach considers branch timing information (resolution cycle) both for older branches in the Reorder Buffer (ROB) and recently committed, and for younger branches relative to the branch we re-predict. We propose Speculative Branch Resolution (SBR) in which, N cycles after a branch allocates in the ROB, various timing information is collected and used to re-predict. Using the gem5 simulator we implement and perform a limit-study of SBR using a TAGE-Like predictor. Our experiments show that the post-alloc timing information we used was not able to yield performance gains over an unbounded TAGE-SC. However, we find two hard to predict branches where timing information did provide an advantage and thoroughly analysed one of them to understand why. This finding suggests that predictors may benefit from specific microarchitectural information to increase accuracy on specific hard to predict branches and that overriding predictions in the backend may yet yield performance benefits, but that further research is needed to determine such information vectors.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5fae\u67b6\u6784\u65f6\u5e8f\u4fe1\u606f\uff08\u5206\u652f\u89e3\u6790\u5468\u671f\uff09\u6765\u6539\u8fdb\u5206\u652f\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u63a8\u6d4b\u6027\u5206\u652f\u89e3\u6790\uff08SBR\uff09\u65b9\u6cd5\uff0c\u4f46\u5b9e\u9a8c\u8868\u660e\u4f20\u7edfTAGE-SC\u9884\u6d4b\u5668\u5728\u6574\u4f53\u6027\u80fd\u4e0a\u4ecd\u4f18\u4e8e\u8be5\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u5904\u7406\u5668\u4e2d\u5206\u652f\u9884\u6d4b\u9519\u8bef\u662f\u6027\u80fd\u4e0b\u964d\u548c\u80fd\u8017\u6d6a\u8d39\u7684\u4e3b\u8981\u539f\u56e0\u3002\u867d\u7136\u73b0\u6709\u9884\u6d4b\u5668\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u4e8e\u96be\u4ee5\u9884\u6d4b\u7684\u5206\u652f\u4ecd\u5b58\u5728\u9ad8\u9519\u8bef\u7387\u3002\u672c\u7814\u7a76\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5229\u7528\u5fae\u67b6\u6784\u4fe1\u606f\uff08\u9664\u4f20\u7edf\u5206\u652f\u5386\u53f2\u5916\uff09\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u63a8\u6d4b\u6027\u5206\u652f\u89e3\u6790\uff08SBR\uff09\u65b9\u6cd5\uff1a\u5728\u5206\u652f\u5206\u914d\u81f3\u91cd\u6392\u5e8f\u7f13\u51b2\uff08ROB\uff09N\u4e2a\u5468\u671f\u540e\uff0c\u6536\u96c6\u65f6\u5e8f\u4fe1\u606f\uff08\u5305\u62ecROB\u4e2d\u8f83\u8001\u5206\u652f\u3001\u5df2\u63d0\u4ea4\u5206\u652f\u4ee5\u53ca\u8f83\u65b0\u5206\u652f\u7684\u89e3\u6790\u5468\u671f\uff09\u8fdb\u884c\u91cd\u65b0\u9884\u6d4b\u3002\u4f7f\u7528gem5\u6a21\u62df\u5668\u5b9e\u73b0\u57fa\u4e8eTAGE-Like\u9884\u6d4b\u5668\u7684SBR\u8fdb\u884c\u6781\u9650\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u4f7f\u7528\u7684\u5206\u914d\u540e\u65f6\u5e8f\u4fe1\u606f\u672a\u80fd\u8d85\u8d8a\u65e0\u9650\u5236TAGE-SC\u9884\u6d4b\u5668\u7684\u6027\u80fd\u3002\u4f46\u5728\u4e24\u4e2a\u96be\u4ee5\u9884\u6d4b\u7684\u5206\u652f\u4e0a\uff0c\u65f6\u5e8f\u4fe1\u606f\u786e\u5b9e\u63d0\u4f9b\u4e86\u4f18\u52bf\u3002\u7814\u7a76\u4eba\u5458\u6df1\u5165\u5206\u6790\u4e86\u5176\u4e2d\u4e00\u4e2a\u6848\u4f8b\u4ee5\u7406\u89e3\u539f\u56e0\u3002", "conclusion": "\u7279\u5b9a\u5fae\u67b6\u6784\u4fe1\u606f\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u67d0\u4e9b\u96be\u4ee5\u9884\u6d4b\u5206\u652f\u7684\u51c6\u786e\u6027\uff0c\u540e\u7aef\u8986\u76d6\u9884\u6d4b\u53ef\u80fd\u5e26\u6765\u6027\u80fd\u6536\u76ca\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u786e\u5b9a\u6709\u6548\u7684\u4fe1\u606f\u5411\u91cf\u3002\u6574\u4f53\u800c\u8a00\uff0c\u4f20\u7edfTAGE-SC\u4ecd\u4f18\u4e8e\u57fa\u4e8e\u65f6\u5e8f\u4fe1\u606f\u7684SBR\u65b9\u6cd5\u3002"}}
{"id": "2601.13815", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.13815", "abs": "https://arxiv.org/abs/2601.13815", "authors": ["Lukas Krupp", "Matthew Venn", "Norbert Wehn"], "title": "From RTL to Prompt Coding: Empowering the Next Generation of Chip Designers through LLMs", "comment": "Accepted for presentation at the 2026 IEEE International Symposium on Circuits and Systems (ISCAS 2026). Proceedings to be included in IEEE Xplore", "summary": "This paper presents an LLM-based learning platform for chip design education, aiming to make chip design accessible to beginners without overwhelming them with technical complexity. It represents the first educational platform that assists learners holistically across both frontend and backend design. The proposed approach integrates an LLM-based chat agent into a browser-based workflow built upon the Tiny Tapeout ecosystem. The workflow guides users from an initial design idea through RTL code generation to a tapeout-ready chip. To evaluate the concept, a case study was conducted with 18 high-school students. Within a 90-minute session they developed eight functional VGA chip designs in a 130 nm technology. Despite having no prior experience in chip design, all groups successfully implemented tapeout-ready projects. The results demonstrate the feasibility and educational impact of LLM-assisted chip design, highlighting its potential to attract and inspire early learners and significantly broaden the target audience for the field.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u82af\u7247\u8bbe\u8ba1\u6559\u80b2\u5e73\u53f0\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u5de5\u4f5c\u6d41\u548c\u804a\u5929\u52a9\u624b\u5e2e\u52a9\u521d\u5b66\u8005\u5b8c\u6210\u4ece\u8bbe\u8ba1\u6784\u601d\u5230\u6d41\u7247\u7684\u5168\u8fc7\u7a0b\uff0c\u6210\u529f\u8ba9\u9ad8\u4e2d\u751f\u5728\u6ca1\u6709\u7ecf\u9a8c\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u53ef\u6d41\u7247\u82af\u7247\u8bbe\u8ba1\u3002", "motivation": "\u82af\u7247\u8bbe\u8ba1\u9886\u57df\u6280\u672f\u590d\u6742\uff0c\u5bf9\u521d\u5b66\u8005\u95e8\u69db\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u8ba9\u6ca1\u6709\u7ecf\u9a8c\u7684\u5b66\u4e60\u8005\u4e5f\u80fd\u63a5\u89e6\u548c\u638c\u63e1\u82af\u7247\u8bbe\u8ba1\u7684\u6559\u80b2\u5e73\u53f0\uff0c\u6269\u5927\u8be5\u9886\u57df\u7684\u53d7\u4f17\u7fa4\u4f53\u3002", "method": "\u5c06LLM\u804a\u5929\u4ee3\u7406\u96c6\u6210\u5230\u57fa\u4e8eTiny Tapeout\u751f\u6001\u7cfb\u7edf\u7684\u6d4f\u89c8\u5668\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5f15\u5bfc\u7528\u6237\u4ece\u8bbe\u8ba1\u6784\u601d\u3001RTL\u4ee3\u7801\u751f\u6210\u5230\u53ef\u6d41\u7247\u82af\u7247\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "result": "18\u540d\u9ad8\u4e2d\u751f\u572890\u5206\u949f\u8bfe\u7a0b\u4e2d\u5f00\u53d1\u4e868\u4e2a\u529f\u80fd\u6027\u7684VGA\u82af\u7247\u8bbe\u8ba1\uff08130nm\u5de5\u827a\uff09\u3002\u6240\u6709\u5c0f\u7ec4\u90fd\u6210\u529f\u5b9e\u73b0\u4e86\u53ef\u6d41\u7247\u9879\u76ee\uff0c\u5c3d\u7ba1\u4ed6\u4eec\u4e4b\u524d\u6ca1\u6709\u4efb\u4f55\u82af\u7247\u8bbe\u8ba1\u7ecf\u9a8c\u3002", "conclusion": "LLM\u8f85\u52a9\u7684\u82af\u7247\u8bbe\u8ba1\u5177\u6709\u53ef\u884c\u6027\u548c\u6559\u80b2\u5f71\u54cd\u529b\uff0c\u80fd\u591f\u5438\u5f15\u548c\u6fc0\u52b1\u65e9\u671f\u5b66\u4e60\u8005\uff0c\u663e\u8457\u6269\u5927\u82af\u7247\u8bbe\u8ba1\u9886\u57df\u7684\u53d7\u4f17\u8303\u56f4\u3002"}}
{"id": "2601.14087", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14087", "abs": "https://arxiv.org/abs/2601.14087", "authors": ["Ruichi Han", "Yizhi Chen", "Tong Lei", "Jordi Altayo Gonzalez", "Ahmed Hemani"], "title": "'1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators", "comment": "Accepted for oral presentation at the 2026 VLSI Symposium on Technology, Systems and Applications (VLSI TSA) on April 13-17, 2026, at the Ambassador Hotel, Hsinchu, Taiwan", "summary": "Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\\% BT reduction compared to 20.42% of precise implementation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8eCNN\u7684\u514d\u6bd4\u8f83\u8fd1\u4f3c\u6392\u5e8f\u5355\u5143\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u6876\u5206\u7ec4\u964d\u4f4e\u786c\u4ef6\u9762\u79ef\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u91cd\u6392\u5e8f\u7684\u94fe\u8def\u529f\u8017\u4f18\u52bf", "motivation": "\u4e92\u8fde\u529f\u8017\u662fDNN\u52a0\u901f\u5668\u7684\u74f6\u9888\uff0c\u57fa\u4e8e'1'\u6bd4\u7279\u8ba1\u6570\u7684\u6570\u636e\u6392\u5e8f\u53ef\u4ee5\u51cf\u5c11\u5f00\u5173\u6d3b\u52a8\uff0c\u4f46\u5b9e\u9645\u786c\u4ef6\u6392\u5e8f\u5b9e\u73b0\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u5229\u7528\u8fd1\u4f3c\u8ba1\u7b97\u5c06\u4eba\u53e3\u8ba1\u6570\u5206\u7ec4\u5230\u7c97\u7c92\u5ea6\u6876\u4e2d\uff0c\u8bbe\u8ba1\u4f18\u5316\u7684\u514d\u6bd4\u8f83\u6392\u5e8f\u5355\u5143\uff0c\u4e13\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b9a\u5236", "result": "\u8fd1\u4f3c\u6392\u5e8f\u5355\u5143\u5b9e\u73b0\u9ad8\u8fbe35.4%\u7684\u9762\u79ef\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u630119.50%\u7684BT\u51cf\u5c11\uff0c\u63a5\u8fd1\u7cbe\u786e\u5b9e\u73b0\u768420.42%", "conclusion": "\u8be5\u786c\u4ef6\u6392\u5e8f\u8bbe\u8ba1\u5728\u4fdd\u6301\u94fe\u8def\u529f\u8017\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u9762\u79ef\uff0c\u4e3aDNN\u52a0\u901f\u5668\u7684\u4e92\u8fde\u529f\u8017\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.14140", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14140", "abs": "https://arxiv.org/abs/2601.14140", "authors": ["Tong Xie", "Yijiahao Qi", "Jinqi Wen", "Zishen Wan", "Yanchi Dong", "Zihao Wang", "Shaofei Cai", "Yitao Liang", "Tianyu Jia", "Yuan Wang", "Runsheng Wang", "Meng Li"], "title": "CREATE: Cross-Layer Resilience Characterization and Optimization for Efficient yet Reliable Embodied AI Systems", "comment": "18 pages, 21 figures. Accepted by ASPLOS 2026", "summary": "Embodied Artificial Intelligence (AI) has recently attracted significant attention as it bridges AI with the physical world. Modern embodied AI systems often combine a Large Language Model (LLM)-based planner for high-level task planning and a reinforcement learning (RL)-based controller for low-level action generation, enabling embodied agents to tackle complex tasks in real-world environments. However, deploying embodied agents remains challenging due to their high computation requirements, especially for battery-powered local devices. Although techniques like lowering operating voltage can improve energy efficiency, they can introduce bit errors and result in task failures. In this work, we propose CREATE, a general design principle that leverages heterogeneous resilience at different layers for synergistic energy-reliability co-optimization. For the first time, we conduct a comprehensive error injection study on modern embodied AI systems and observe an inherent but heterogeneous fault tolerance. Building upon these insights, we develop an anomaly detection and clearance mechanism at the circuit level to eliminate outlier errors. At the model level, we propose a weight-rotation-enhanced planning algorithm to improve the fault tolerance of the LLM-based planner. Furthermore, we introduce an application-level technique, autonomy-adaptive voltage scaling, to dynamically adjust the operating voltage of the controllers. The voltage scaling circuit is co-designed to enable online voltage adjustment. Extensive experiments demonstrate that without compromising task quality, CREATE achieves 40.6% computational energy savings on average over nominal-voltage baselines and 35.0% over prior-art techniques. This further leads to 29.5% to 37.3% chip-level energy savings and approximately a 15% to 30% improvement in battery life.", "AI": {"tldr": "CREATE\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u5f39\u6027\u8bbe\u8ba1\u539f\u5219\uff0c\u901a\u8fc7\u7535\u8def\u5c42\u3001\u6a21\u578b\u5c42\u548c\u5e94\u7528\u5c42\u7684\u534f\u540c\u4f18\u5316\uff0c\u5728\u4fdd\u8bc1\u4efb\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5177\u8eabAI\u7cfb\u7edf\u7684\u80fd\u8017", "motivation": "\u5177\u8eabAI\u7cfb\u7edf\u901a\u5e38\u7ed3\u5408LLM\u89c4\u5212\u5668\u548cRL\u63a7\u5236\u5668\uff0c\u4f46\u90e8\u7f72\u65f6\u9762\u4e34\u9ad8\u8ba1\u7b97\u80fd\u8017\u6311\u6218\u3002\u964d\u4f4e\u5de5\u4f5c\u7535\u538b\u867d\u80fd\u63d0\u9ad8\u80fd\u6548\uff0c\u4f46\u4f1a\u5f15\u5165\u6bd4\u7279\u9519\u8bef\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\uff0c\u9700\u8981\u89e3\u51b3\u80fd\u6548\u4e0e\u53ef\u9760\u6027\u7684\u6743\u8861\u95ee\u9898", "method": "1) \u7535\u8def\u5c42\uff1a\u5f02\u5e38\u68c0\u6d4b\u4e0e\u6e05\u9664\u673a\u5236\u6d88\u9664\u5f02\u5e38\u9519\u8bef\uff1b2) \u6a21\u578b\u5c42\uff1a\u6743\u91cd\u65cb\u8f6c\u589e\u5f3a\u89c4\u5212\u7b97\u6cd5\u63d0\u9ad8LLM\u89c4\u5212\u5668\u7684\u5bb9\u9519\u6027\uff1b3) \u5e94\u7528\u5c42\uff1a\u81ea\u4e3b\u6027\u81ea\u9002\u5e94\u7535\u538b\u8c03\u8282\u52a8\u6001\u8c03\u6574\u63a7\u5236\u5668\u5de5\u4f5c\u7535\u538b\uff1b4) \u534f\u540c\u8bbe\u8ba1\u7535\u538b\u8c03\u8282\u7535\u8def\u652f\u6301\u5728\u7ebf\u7535\u538b\u8c03\u6574", "result": "CREATE\u5e73\u5747\u5b9e\u73b040.6%\u7684\u8ba1\u7b97\u80fd\u8017\u8282\u7701\uff08\u76f8\u6bd4\u6807\u79f0\u7535\u538b\u57fa\u51c6\uff09\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u8282\u770135.0%\u3002\u82af\u7247\u7ea7\u80fd\u8017\u8282\u770129.5%-37.3%\uff0c\u7535\u6c60\u5bff\u547d\u63d0\u5347\u7ea615%-30%\uff0c\u4e14\u4e0d\u635f\u5bb3\u4efb\u52a1\u8d28\u91cf", "conclusion": "CREATE\u901a\u8fc7\u5f02\u6784\u5f39\u6027\u534f\u540c\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5177\u8eabAI\u7cfb\u7edf\u7684\u80fd\u6548\u4e0e\u53ef\u9760\u6027\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u7535\u6c60\u4f9b\u7535\u672c\u5730\u8bbe\u5907\u7684\u5177\u8eabAI\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.14148", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14148", "abs": "https://arxiv.org/abs/2601.14148", "authors": ["Meng Li", "Tong Xie", "Zuodong Zhang", "Runsheng Wang"], "title": "The Quest for Reliable AI Accelerators: Cross-Layer Evaluation and Design Optimization", "comment": "4 pages, 9 figures. Invited paper at ASICON 2025", "summary": "As the CMOS technology pushes to the nanoscale, aging effects and process variations have become increasingly pronounced, posing significant reliability challenges for AI accelerators. Traditional guardband-based design approaches, which rely on pessimistic timing margin, sacrifice significant performance and computational efficiency, rendering them inadequate for high-performance AI computing demands. Current reliability-aware AI accelerator design faces two core challenges: (1) the lack of systematic cross-layer analysis tools to capture coupling reliability effects across device, circuit, architecture, and application layers; and (2) the fundamental trade-off between conventional reliability optimization and computational efficiency. To address these challenges, this paper systematically presents a series of reliability-aware accelerator designs, encompassing (1) aging and variation-aware dynamic timing analyzer, (2) accelerator dataflow optimization using critical input pattern reduction, and (3) resilience characterization and novel architecture design for large language models (LLMs). By tightly integrating cross-layer reliability modeling and AI workload characteristics, these co-optimization approaches effectively achieve reliable and efficient AI acceleration.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u7eb3\u7c73\u7ea7CMOS\u6280\u672f\u4e0bAI\u52a0\u901f\u5668\u7684\u53ef\u9760\u6027\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u8de8\u5c42\u53ef\u9760\u6027\u611f\u77e5\u52a0\u901f\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5305\u62ec\u52a8\u6001\u65f6\u5e8f\u5206\u6790\u3001\u6570\u636e\u6d41\u4f18\u5316\u548cLLM\u5f39\u6027\u67b6\u6784\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u534f\u540c\u4f18\u5316\u3002", "motivation": "\u968f\u7740CMOS\u6280\u672f\u8fdb\u5165\u7eb3\u7c73\u5c3a\u5ea6\uff0c\u8001\u5316\u6548\u5e94\u548c\u5de5\u827a\u53d8\u5f02\u65e5\u76ca\u663e\u8457\uff0c\u4f20\u7edf\u57fa\u4e8e\u4fdd\u62a4\u5e26\u7684\u8bbe\u8ba1\u65b9\u6cd5\u727a\u7272\u4e86\u8fc7\u591a\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u6027\u80fdAI\u8ba1\u7b97\u9700\u6c42\u3002\u5f53\u524dAI\u52a0\u901f\u5668\u8bbe\u8ba1\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u8de8\u5c42\u53ef\u9760\u6027\u5206\u6790\u5de5\u5177\uff0c\u4ee5\u53ca\u53ef\u9760\u6027\u4f18\u5316\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6839\u672c\u6027\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e09\u65b9\u9762\u7cfb\u7edf\u5316\u8bbe\u8ba1\u65b9\u6cd5\uff1a(1) \u8001\u5316\u4e0e\u53d8\u5f02\u611f\u77e5\u7684\u52a8\u6001\u65f6\u5e8f\u5206\u6790\u5668\uff1b(2) \u57fa\u4e8e\u5173\u952e\u8f93\u5165\u6a21\u5f0f\u51cf\u5c11\u7684\u52a0\u901f\u5668\u6570\u636e\u6d41\u4f18\u5316\uff1b(3) \u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f39\u6027\u7279\u6027\u8868\u5f81\u4e0e\u65b0\u9896\u67b6\u6784\u8bbe\u8ba1\u3002\u901a\u8fc7\u7d27\u5bc6\u96c6\u6210\u8de8\u5c42\u53ef\u9760\u6027\u5efa\u6a21\u4e0eAI\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\uff0c\u5b9e\u73b0\u534f\u540c\u4f18\u5316\u3002", "result": "\u8fd9\u4e9b\u534f\u540c\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8bbe\u8ba1\u4e2d\u7684\u53ef\u9760\u6027-\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684AI\u52a0\u901f\u3002\u901a\u8fc7\u8de8\u5c42\u5206\u6790\u548c\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u5728\u4fdd\u8bc1\u53ef\u9760\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8de8\u5c42\u53ef\u9760\u6027\u611f\u77e5\u52a0\u901f\u5668\u8bbe\u8ba1\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7eb3\u7c73\u7ea7CMOS\u6280\u672f\u4e0b\u7684AI\u52a0\u901f\u5668\u53ef\u9760\u6027\u6311\u6218\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u534f\u540c\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u8bbe\u5907\u3001\u7535\u8def\u3001\u67b6\u6784\u548c\u5e94\u7528\u591a\u4e2a\u5c42\u9762\u5b9e\u73b0\u4e86\u53ef\u9760\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u9ad8\u6027\u80fdAI\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
