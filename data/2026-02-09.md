<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs](https://arxiv.org/abs/2602.06252)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: D-Legion是一种新型可扩展多核架构，使用自适应精度脉动阵列核心加速量化LLM中的矩阵乘法，支持量化稀疏和密集矩阵乘法，相比现有方案显著降低延迟、提高内存节省和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的性能提升伴随着巨大的计算和内存需求，量化LLM具有显著优势，但需要专门架构来加速其工作负载。

Method: 提出D-Legion架构，由多个Legion组成，每个Legion包含一组自适应精度脉动阵列。支持多种计算模式，利用块结构化稀疏性，通过并行累加器减少部分和内存访问，通过多播优化数据重用，并进行设计空间探索确定最优配置。

Result: 在BitNet模型的注意力工作负载上，相比最先进方案实现8.2倍延迟降低、3.8倍内存节省和3倍部分和内存节省。8个Legion配置达到135.68 TOPS峰值吞吐量。32个Legion版本相比Google TPUv4i实现2.5倍延迟降低、2.3倍吞吐量提升和2.7倍内存节省。

Conclusion: D-Legion是一种高效的可扩展架构，能够显著加速量化LLM的矩阵乘法运算，在延迟、吞吐量和内存效率方面优于现有解决方案。

Abstract: The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\times$ lower latency, up to 3.8$\times$ higher memory savings, and up to 3$\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\times$ lower total latency, up to 2.3$\times$ higher total throughput, and up to 2.7$\times$ higher total memory savings.

</details>
