<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [\textit{No One-Size-Fits-All}: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory](https://arxiv.org/abs/2509.22980)
*Jingyao Zhang,Elaheh Sadredini*

Main category: cs.AR

TL;DR: 本文首次系统性地分析了PIM架构中两种基础数据布局（位并行BP和位串行BS）的性能差异，发现没有单一布局适用于所有场景，最优选择取决于工作负载特性。


<details>
  <summary>Details</summary>
Motivation: PIM社区长期以来将BP和BS两种数据布局视为可互换的，这种"一种布局适合所有"的假设缺乏系统性的工作负载驱动指导，导致架构师无法为特定应用选择最优数据布局。

Method: 开发了等面积、周期精确的BP和BS PIM架构模型，使用多样化基准测试套件进行综合评估，包括MIMDRAM的细粒度微工作负载和PIMBench套件中的大规模应用（如VGG网络）。

Result: BP在控制流密集型任务和不规则内存访问模式中表现优异，而BS在大规模并行、低精度计算（如INT4/INT8）中具有显著优势，特别是在AI应用中。

Conclusion: 挑战了PIM数据布局的通用化观点，为设计下一代工作负载感知和潜在混合PIM系统提供了原则性基础，并提炼出一套可操作的设计指南。

Abstract: Processing-in-Memory (PIM) is a promising approach to overcoming the
memory-wall bottleneck. However, the PIM community has largely treated its two
fundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they
were interchangeable. This implicit "one-layout-fits-all" assumption, often
hard-coded into existing evaluation frameworks, creates a critical gap:
architects lack systematic, workload-driven guidelines for choosing the optimal
data layout for their target applications.
  To address this gap, this paper presents the first systematic,
workload-driven characterization of BP and BS PIM architectures. We develop
iso-area, cycle-accurate BP and BS PIM architectural models and conduct a
comprehensive evaluation using a diverse set of benchmarks. Our suite includes
both fine-grained microworkloads from MIMDRAM to isolate specific operational
characteristics, and large-scale applications from the PIMBench suite, such as
the VGG network, to represent realistic end-to-end workloads.
  Our results quantitatively demonstrate that no single layout is universally
superior; the optimal choice is strongly dependent on workload characteristics.
BP excels on control-flow-intensive tasks with irregular memory access
patterns, whereas BS shows substantial advantages in massively parallel,
low-precision (e.g., INT4/INT8) computations common in AI. Based on this
characterization, we distill a set of actionable design guidelines for
architects. This work challenges the prevailing one-size-fits-all view on PIM
data layouts and provides a principled foundation for designing
next-generation, workload-aware, and potentially hybrid PIM systems.

</details>


### [2] [Enhanced Hybrid Temporal Computing Using Deterministic Summations for Ultra-Low-Power Accelerators](https://arxiv.org/abs/2509.22999)
*Sachin Sachdeva,Jincong Lu,Wantong Li,Sheldon X. -D. Tan*

Main category: cs.AR

TL;DR: 提出了E-HTC框架，通过精确的EMBA和确定性缩放的DTSA加法器，在保持超低功耗的同时显著提高了混合时间计算的精度，在FIR和DCT应用中实现了更好的精度-功耗权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的HTC架构虽然通过脉冲率和时间数据编码降低了功耗，但由于基于MUX的缩放加法导致精度损失，需要开发更精确的加法方案来平衡精度和功耗。

Method: 提出了两种位流加法方案：精确多输入二进制累加器(EMBA)和确定性阈值缩放加法器(DTSA)，集成到支持单极和双极编码的MAC单元中，并在FIR滤波器和DCT引擎中验证。

Result: 在4x4 MAC中，单极模式下E-HTC与CBSC的RMSE相当，比MUX-HTC精度提高94%，功耗和面积分别减少23%和7%；双极模式下RMSE为2.09%，比MUX-HTC提高83%。在FIR和DCT应用中均实现了显著的PSNR提升和功耗面积节省。

Conclusion: E-HTC框架成功解决了HTC架构的精度问题，在保持超低功耗优势的同时显著提高了计算精度，为超低功耗硬件加速器提供了更优的精度-功耗权衡方案。

Abstract: This paper presents an accuracy-enhanced Hybrid Temporal Computing (E-HTC)
framework for ultra-low-power hardware accelerators with deterministic
additions. Inspired by the recently proposed HTC architecture, which leverages
pulse-rate and temporal data encoding to reduce switching activity and energy
consumption but loses accuracy due to its multiplexer (MUX)-based scaled
addition, we propose two bitstream addition schemes: (1) an Exact
Multiple-input Binary Accumulator (EMBA), which performs precise binary
accumulation, and (2) a Deterministic Threshold-based Scaled Adder (DTSA),
which employs threshold logic for scaled addition. These adders are integrated
into a multiplier accumulator (MAC) unit supporting both unipolar and bipolar
encodings. To validate the framework, we implement two accelerators: a Finite
Impulse Response (FIR) filter and an 8-point Discrete Cosine Transform
(DCT)/iDCT engine. Results on a 4x4 MAC show that, in unipolar mode, E-HTC
matches the RMSE of state-of-the-art Counter-Based Stochastic Computing (CBSC)
MAC, improves accuracy by 94% over MUX-based HTC, and reduces power and area by
23% and 7% compared to MUX-based HTC and 64% and 74% compared to CBSC. In
bipolar mode, E-HTC MAC achieves 2.09% RMSE -- an 83% improvement over
MUX-based HTC -- and approaches CBSC's 1.40% RMSE with area and power savings
of 28% and 43% vs. MUX-based HTC and about 76% vs. CBSC. In FIR experiments,
both E-HTC variants yield PSNR gains of 3--5 dB (30--45% RMSE reduction) while
saving 13% power and 3% area. For DCT/iDCT, E-HTC boosts PSNR by 10--13 dB
(70--75% RMSE reduction) while saving area and power over both MUX- and
CBSC-based designs.

</details>


### [3] [A Near-Cache Architectural Framework for Cryptographic Computing](https://arxiv.org/abs/2509.23179)
*Jingyao Zhang,Elaheh Sadredini*

Main category: cs.AR

TL;DR: 提出了一种名为CNC的近缓存切片计算范式，通过将具有位线计算能力的SRAM阵列放置在缓存切片附近，来加速后量子密码算法和其他应用。


<details>
  <summary>Details</summary>
Motivation: 后量子密码算法的公钥和签名长度是前量子密码的3-9倍，导致显著的性能和能效开销。分析发现缓存带宽是关键瓶颈，这促使采用片上近缓存计算范式。

Method: 设计CNC近缓存切片计算范式，在缓存切片附近放置具有位线计算能力的SRAM阵列，实现高内部带宽和短数据移动，支持虚拟地址，并提出了ISA扩展。

Result: 通过CNC实现了高内部带宽和短数据移动，原生支持虚拟寻址，为核心/缓存数据路径提供了详细实现方案。

Conclusion: CNC能够无缝集成到现有系统中，解决后量子密码算法的性能和能效问题，为其他应用也提供了加速能力。

Abstract: Recent advancements in post-quantum cryptographic algorithms have led to
their standardization by the National Institute of Standards and Technology
(NIST) to safeguard information security in the post-quantum era. These
algorithms, however, employ public keys and signatures that are 3 to 9$\times$
longer than those used in pre-quantum cryptography, resulting in significant
performance and energy efficiency overheads. A critical bottleneck identified
in our analysis is the cache bandwidth. This limitation motivates the adoption
of on-chip in-/near-cache computing, a computing paradigm that offers
high-performance, exceptional energy efficiency, and flexibility to accelerate
post-quantum cryptographic algorithms. Our analysis of existing works reveals
challenges in integrating in-/near-cache computing into modern computer systems
and performance limitations due to external bandwidth limitation, highlighting
the need for innovative solutions that can seamlessly integrate into existing
systems without performance and energy efficiency issues. In this paper, we
introduce a near-cache-slice computing paradigm with support of customization
and virtual address, named Crypto-Near-Cache (CNC), designed to accelerate
post-quantum cryptographic algorithms and other applications. By placing SRAM
arrays with bitline computing capability near cache slices, high internal
bandwidth and short data movement are achieved with native support of virtual
addressing. An ISA extension to facilitate CNC is also proposed, with detailed
discussion on the implementation aspects of the core/cache datapath.

</details>


### [4] [AssertGen: Enhancement of LLM-aided Assertion Generation through Cross-Layer Signal Bridging](https://arxiv.org/abs/2509.23674)
*Hongqin Lyu,Yonghao Wang,Yunlin Du,Mingyu Shi,Zhiteng Chao,Wenxing Li,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertGen是一个基于LLM的断言生成框架，通过链式推理提取验证目标并构建跨层信号链，显著提升了SystemVerilog断言生成的质量和效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助的断言生成方法无法有效识别设计规范与RTL设计之间的关系，导致生成的断言不充分。

Method: 首先使用链式推理策略从规范中提取验证目标，然后在目标与RTL代码之间桥接相应信号构建跨层信号链，最后基于LLM生成SystemVerilog断言。

Result: 实验结果表明AssertGen在形式属性验证通过率、影响锥、证明核心和变异测试覆盖率等关键指标上优于现有最先进方法。

Conclusion: AssertGen框架通过有效连接设计规范与RTL代码，显著提升了断言生成的质量和验证效果。

Abstract: Assertion-based verification (ABV) serves as a crucial technique for ensuring
that register-transfer level (RTL) designs adhere to their specifications.
While Large Language Model (LLM) aided assertion generation approaches have
recently achieved remarkable progress, existing methods are still unable to
effectively identify the relationship between design specifications and RTL
designs, which leads to the insufficiency of the generated assertions. To
address this issue, we propose AssertGen, an assertion generation framework
that automatically generates SystemVerilog assertions (SVA). AssertGen first
extracts verification objectives from specifications using a chain-of-thought
(CoT) reasoning strategy, then bridges corresponding signals between these
objectives and the RTL code to construct a cross-layer signal chain, and
finally generates SVAs based on the LLM. Experimental results demonstrate that
AssertGen outperforms the existing state-of-the-art methods across several key
metrics, such as pass rate of formal property verification (FPV), cone of
influence (COI), proof core and mutation testing coverage.

</details>


### [5] [ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights](https://arxiv.org/abs/2509.23693)
*Tao Lu,Jiapin Wang,Yelin Shan,Xiangping Zhang,Xiang Chen*

Main category: cs.AR

TL;DR: 本文评估了ASIC存储内压缩解压处理单元(CDPU)的设计，并与Intel QAT系列加速器进行对比，分析了三种CDPU部署位置对性能的影响。


<details>
  <summary>Details</summary>
Motivation: CPU执行无损压缩在数据中心带来显著计算开销，硬件CDPU可以缓解此问题，但最优算法选择、微架构设计和系统级部署位置尚未得到充分理解。

Method: 设计了ASIC存储内CDPU，并与Intel QAT 8970和QAT 4xxx进行端到端评估，涵盖三种部署模式：外设、片上芯片和存储内。

Result: 发现吞吐量和延迟对CDPU部署位置和互连高度敏感；压缩效率与数据模式/布局强相关；微基准测试增益与实际应用加速存在差异；模块级与系统级能效存在差异；各种CDPU存在可扩展性和多租户干扰问题。

Conclusion: 这些发现促使需要重新思考超大规模存储基础设施中硬件压缩解压的部署位置感知和跨层设计方法。

Abstract: Lossless compression imposes significant computational over head on
datacenters when performed on CPUs. Hardware compression and decompression
processing units (CDPUs) can alleviate this overhead, but optimal algorithm
selection, microarchitectural design, and system-level placement of CDPUs are
still not well understood. We present the design of an ASIC-based in-storage
CDPU and provide a comprehensive end-to-end evaluation against two leading ASIC
accelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant
CDPU placement regimes: peripheral, on-chip, and in-storage. Our results
reveal: (i) acute sensitivity of throughput and latency to CDPU placement and
interconnection, (ii) strong correlation between compression efficiency and
data patterns/layouts, (iii) placement-driven divergences between
microbenchmark gains and real-application speedups, (iv) discrepancies between
module and system-level power efficiency, and (v) scalability and multi-tenant
interference is sues of various CDPUs. These findings motivate a
placement-aware, cross-layer rethinking of hardware (de)compression for
hyperscale storage infrastructures.

</details>


### [6] [AssertFix: Empowering Automated Assertion Fix via Large Language Models](https://arxiv.org/abs/2509.23972)
*Hongqin Lyu,Yunlin Du,Yonghao Wang,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: 提出基于LLM的自动断言修复框架AssertFix，通过定位RTL代码、识别错误根源、分类错误类型并应用专用修复策略，自动修正生成的断言错误。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的断言生成方法将断言生成视为最终步骤，需要人工修复错误断言，限制了这些方法的应用。

Method: AssertFix框架：1）准确定位与错误断言相关的RTL代码；2）系统识别断言错误的根本原因；3）分类错误类型；4）应用专用修复策略自动修正错误。

Result: 在Opencore基准测试中，AssertFix在修复率和验证覆盖率方面均取得显著提升。

Conclusion: AssertFix能够自动修复生成的断言错误，提高断言质量，解决了现有LLM断言生成方法的局限性。

Abstract: Assertion-based verification (ABV) is critical in ensuring that
register-transfer level (RTL) designs conform to their functional
specifications. SystemVerilog Assertions (SVA) effectively specify design
properties, but writing and maintaining them manually is challenging and
error-prone. Although recent progress of assertion generation methods
leveraging large language models (LLMs) have shown great potential in improving
assertion quality, they typically treat assertion generation as a final step,
leaving the burden of fixing of the incorrect assertions to human effects,
which may significantly limits the application of these methods. To address the
above limitation, we propose an automatic assertion fix framework based on
LLMs, named AssertFix. AsserFix accurately locates the RTL code related to the
incorrect assertion, systematically identifies the root causes of the assertion
errors, classifies the error type and finally applies dedicated fix strategies
to automatically correct these errors, improving the overall quality of the
generated assertions. Experimental results show that AssertFix achieves
noticeable improvements in both fix rate and verification coverage across the
Opencore benchmarks.

</details>


### [7] [Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI](https://arxiv.org/abs/2509.24929)
*Hongwei Zhao,Vianney Lapotre,Guy Gogniat*

Main category: cs.AR

TL;DR: 该论文通过仿真驱动的故障注入方法，系统分析了Wishbone、AXI Lite和AXI三种主流总线协议在故障攻击下的脆弱性，揭示了总线协议的一致行为模式。


<details>
  <summary>Details</summary>
Motivation: 随着SoC架构复杂度的增加，片上通信结构（特别是总线）的脆弱性日益突出，总线作为IP核间的互连结构可能成为故障攻击的潜在载体。

Method: 采用仿真驱动的故障注入方法，对Wishbone、AXI Lite和AXI三种总线协议进行系统性的故障注入分析，考察故障成功率、空间脆弱性分布和时间依赖性。

Result: 研究结果揭示了总线协议在故障攻击下的一致行为模式，为攻击建模提供了实用见解。

Conclusion: 该研究为攻击建模和弹性SoC设计开发提供了实用见解，有助于理解故障如何与总线级事务交互。

Abstract: Fault injection attacks exploit physical disturbances to compromise the
functionality and security of integrated circuits. As System on Chip (SoC)
architectures grow in complexity, the vulnerability of on chip communication
fabrics has become increasingly prominent. Buses, serving as interconnects
among various IP cores, represent potential vectors for fault-based
exploitation. In this study, we perform simulation-driven fault injection
across three mainstream bus protocols Wishbone, AXI Lite, and AXI. We
systematically examine fault success rates, spatial vulnerability
distributions, and timing dependencies to characterize how faults interact with
bus-level transactions. The results uncover consistent behavioral patterns
across protocols, offering practical insights for both attack modeling and the
development of resilient SoC designs.

</details>
