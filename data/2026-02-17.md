<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory](https://arxiv.org/abs/2602.13434)
*Maccoy Merrell,Daniel Puckett,Gino Chacon,Jeffrey Stuecheli,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: Rowhammer缓解机制与硬件预取器存在交叉干扰，显著降低了预取性能收益。本文提出ORAP预取器，通过缓存DRAM行缓冲区内容减少激活次数，在RFM缓解系统中提升性能4.6%，在PRAC缓解系统中降低能耗11.8%。


<details>
  <summary>Details</summary>
Motivation: 现有Rowhammer缓解方案评估通常忽略硬件预取器的影响。硬件预取器会增加DRAM激活率，而Rowhammer缓解的性能开销与内存访问模式直接相关，两者存在交叉干扰，导致预取器的性能收益严重受损。

Method: 提出优化行访问预取器(ORAP)，利用最后一级缓存(LLC)空间缓存DRAM行缓冲区的大部分内容，减少未来的行激活需求。与最先进的Berti预取器协同工作，降低DRAM激活率。

Result: 在RFM缓解内存系统中，ORAP将DRAM激活率降低51.3%，相比Berti和SPP-PPF预取器配置实现4.6%的性能提升。在PRAC缓解系统中，ORAP降低能耗开销11.8%。

Conclusion: Rowhammer缓解与硬件预取器之间存在显著的交叉干扰问题。ORAP通过缓存行缓冲区内容有效减少DRAM激活，既保持了预取性能收益，又降低了Rowhammer缓解的开销，为缓解机制与预取器协同设计提供了有效解决方案。

Abstract: Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%.

</details>


### [2] [Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits](https://arxiv.org/abs/2602.13825)
*Paras Tiwari,Narendra Singh Dhakad,Shalu Rani,Sanjay Kumar,Themis Prodromakis*

Main category: cs.AR

TL;DR: 该论文实现了基于忆阻器的基本逻辑门和时序逻辑电路设计，采用90nm CMOS工艺，相比现有技术显著降低了面积、功耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 探索忆阻器在逻辑电路设计中的应用潜力，开发更高效、低功耗、紧凑的电路解决方案，以应对传统CMOS技术的限制。

Method: 在Cadence Virtuoso的SPECTRE中设计和优化忆阻器驱动的逻辑门（NOT、AND、NAND、OR、NOR、XOR）和时序逻辑电路（D、T、JK、SR触发器），采用90nm CMOS工艺集成，并使用Y2O3基忆阻器件实验数据进行验证。

Result: 忆阻器基设计相比现有时序电路技术，面积减少约24%，功耗降低60%，延迟减少58%，且忆阻器在D2D和C2C操作中表现出低变异性。

Conclusion: 忆阻器基设计显著提升了逻辑电路的性能，展现了忆阻器在实现低功耗、低成本、超快速、紧凑电路方面的巨大潜力。

Abstract: In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits.

</details>


### [3] [ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute](https://arxiv.org/abs/2602.14262)
*Siddhartha Raman Sundara Raman,Jaydeep P. Kulkarni*

Main category: cs.AR

TL;DR: 提出了一种紧密集成的近内存GPU架构，在多种工作负载上相比MIAOW GPU实现了6-16倍加速和6-13倍能效提升，支持可重构计算和动态分辨率更新。


<details>
  <summary>Details</summary>
Motivation: 传统GPU架构在处理多样化工作负载（如CNN、GCN、线性规划、大语言模型、Ising模型）时存在能效和性能瓶颈，需要更紧密集成的近内存架构来提升计算效率和能效。

Method: 设计了紧密集成的统一近内存GPU架构，包含定制化的稀疏感知近内存电路和轻量级softmax电路，支持INT16可重构计算和动态分辨率更新，能够高效扩展到不同问题规模。

Result: 相比MIAOW GPU实现了6-16倍加速和6-13倍能效提升；稀疏感知电路提供约1.5倍能效提升，softmax电路提供约1.6倍能效提升；在ABI-enabled MI300和Blackwell系统上相比基线实现了约4.5倍加速。

Conclusion: 该紧密集成的近内存GPU架构能够显著提升多种工作负载的性能和能效，支持可重构计算和动态分辨率更新，为多样化计算需求提供了高效的硬件解决方案。

Abstract: We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.

</details>


### [4] [Scope: A Scalable Merged Pipeline Framework for Multi-Chip-Module NN Accelerators](https://arxiv.org/abs/2602.14393)
*Zongle Huang,Hongyang Jia,Kaiwei Zou,Yongpan Liu*

Main category: cs.AR

TL;DR: Scope：一种用于多芯片模块神经网络加速器的合并流水线框架，通过联合优化多层维度，在保持相似能耗的同时实现最高1.73倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 多芯片模块神经网络加速器面临计算资源利用不足和片外通信开销的挑战。传统的并行化方案（如层内并行和层间流水线）无法同时解决这两个问题，限制了MCM架构的可扩展性。现有工作通常单独部署各层而非联合考虑，导致系统计算和通信之间的妥协。

Method: 提出Scope框架，引入被忽视的多层维度进行合并流水线优化。开发了一系列搜索算法，将设计空间探索的复杂度从指数级降低到线性级，同时能找到性能排名前0.05%的解决方案。

Result: 实验显示，Scope在ResNet-152推理中相比最先进方法，在保持相似能耗的同时实现了最高1.73倍的吞吐量提升。

Conclusion: 通过联合考虑多层维度并开发高效的搜索算法，Scope框架能够显著提升多芯片模块神经网络加速器的吞吐量和可扩展性，有效缓解计算、通信和内存成本之间的权衡。

Abstract: Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.
  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches.

</details>
