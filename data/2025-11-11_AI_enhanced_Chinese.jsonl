{"id": "2511.05502", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05502", "abs": "https://arxiv.org/abs/2511.05502", "authors": ["Varun Rajesh", "Om Jodhpurkar", "Pooja Anbuselvan", "Mantinder Singh", "Ashok Jallepali", "Shantanu Godbole", "Pradeep Kumar Sharma", "Hritvik Shrivastava"], "title": "Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS", "comment": null, "summary": "We present a systematic, empirical evaluation of five local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp, Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with an M2 Ultra processor and 192 GB of unified memory. Using the Qwen-2.5 model family across prompts ranging from a few hundred to 100,000 tokens, we measure time-to-first-token (TTFT), steady-state throughput, latency percentiles, long-context behavior (key-value and prompt caching), quantization support, streaming performance, batching and concurrency behavior, and deployment complexity.\n  Under our settings, MLX achieves the highest sustained generation throughput, while MLC-LLM delivers consistently lower TTFT for moderate prompt sizes and offers stronger out-of-the-box inference features. llama.cpp is highly efficient for lightweight single-stream use, Ollama emphasizes developer ergonomics but lags in throughput and TTFT, and PyTorch MPS remains limited by memory constraints on large models and long contexts.\n  All frameworks execute fully on-device with no telemetry, ensuring strong privacy guarantees. We release scripts, logs, and plots to reproduce all results. Our analysis clarifies the design trade-offs in Apple-centric LLM deployments and provides evidence-based recommendations for interactive and long-context processing. Although Apple Silicon inference frameworks still trail NVIDIA GPU-based systems such as vLLM in absolute performance, they are rapidly maturing into viable, production-grade solutions for private, on-device LLM inference.", "AI": {"tldr": "\u5bf9\u82f9\u679c\u82af\u7247\u4e0a5\u4e2a\u672c\u5730LLM\u8fd0\u884c\u65f6\u7684\u7cfb\u7edf\u8bc4\u4f30\uff1aMLX\u3001MLC-LLM\u3001llama.cpp\u3001Ollama\u548cPyTorch MPS\u3002MLX\u83b7\u5f97\u6700\u9ad8\u6301\u7eed\u751f\u6210\u541e\u5410\u91cf\uff0cMLC-LLM\u5728\u4e2d\u7b49\u63d0\u793a\u5927\u5c0f\u4e0bTTFT\u6700\u4f4e\uff0cllama.cpp\u8f7b\u91cf\u7ea7\u5355\u6d41\u6548\u7387\u9ad8\uff0cOllama\u5f00\u53d1\u8005\u4f53\u9a8c\u597d\u4f46\u6027\u80fd\u8f83\u5dee\uff0cPyTorch MPS\u53d7\u5185\u5b58\u9650\u5236\u3002", "motivation": "\u7cfb\u7edf\u8bc4\u4f30\u82f9\u679c\u82af\u7247\u4e0a\u672c\u5730LLM\u8fd0\u884c\u65f6\u7684\u6027\u80fd\uff0c\u4e3a\u79c1\u6709\u3001\u8bbe\u5907\u7aefLLM\u63a8\u7406\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u8350\uff0c\u660e\u786e\u82f9\u679c\u4e2d\u5fc3\u5316LLM\u90e8\u7f72\u7684\u8bbe\u8ba1\u6743\u8861\u3002", "method": "\u5728\u914d\u5907M2 Ultra\u5904\u7406\u5668\u548c192GB\u7edf\u4e00\u5185\u5b58\u7684Mac Studio\u4e0a\uff0c\u4f7f\u7528Qwen-2.5\u6a21\u578b\u5bb6\u65cf\uff0c\u6d4b\u8bd5\u4ece\u51e0\u767e\u523010\u4e07token\u7684\u63d0\u793a\uff0c\u6d4b\u91cfTTFT\u3001\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u767e\u5206\u4f4d\u6570\u3001\u957f\u4e0a\u4e0b\u6587\u884c\u4e3a\u3001\u91cf\u5316\u652f\u6301\u3001\u6d41\u6027\u80fd\u3001\u6279\u5904\u7406\u548c\u5e76\u53d1\u884c\u4e3a\u4ee5\u53ca\u90e8\u7f72\u590d\u6742\u6027\u3002", "result": "MLX\u5b9e\u73b0\u6700\u9ad8\u6301\u7eed\u751f\u6210\u541e\u5410\u91cf\uff1bMLC-LLM\u5728\u4e2d\u7b49\u63d0\u793a\u5927\u5c0f\u4e0b\u63d0\u4f9b\u4e00\u81f4\u66f4\u4f4e\u7684TTFT\u548c\u66f4\u5f3a\u7684\u5f00\u7bb1\u5373\u7528\u63a8\u7406\u529f\u80fd\uff1bllama.cpp\u8f7b\u91cf\u7ea7\u5355\u6d41\u4f7f\u7528\u9ad8\u6548\uff1bOllama\u5f00\u53d1\u8005\u4f53\u9a8c\u597d\u4f46\u541e\u5410\u91cf\u548cTTFT\u843d\u540e\uff1bPyTorch MPS\u5728\u5927\u6a21\u578b\u548c\u957f\u4e0a\u4e0b\u6587\u4e0a\u53d7\u5185\u5b58\u9650\u5236\u3002", "conclusion": "\u82f9\u679c\u82af\u7247\u63a8\u7406\u6846\u67b6\u867d\u7136\u7edd\u5bf9\u6027\u80fd\u4ecd\u843d\u540e\u4e8eNVIDIA GPU\u7cfb\u7edf\uff0c\u4f46\u6b63\u5feb\u901f\u6210\u719f\u4e3a\u79c1\u6709\u3001\u8bbe\u5907\u7aefLLM\u63a8\u7406\u7684\u53ef\u884c\u751f\u4ea7\u7ea7\u89e3\u51b3\u65b9\u6848\u3002\u6240\u6709\u6846\u67b6\u5b8c\u5168\u5728\u8bbe\u5907\u4e0a\u6267\u884c\uff0c\u65e0\u9065\u6d4b\uff0c\u786e\u4fdd\u5f3a\u9690\u79c1\u4fdd\u8bc1\u3002"}}
{"id": "2511.06313", "categories": ["cs.AR", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.06313", "abs": "https://arxiv.org/abs/2511.06313", "authors": ["Stef Cuyckens", "Xiaoling Yi", "Robin Geens", "Joren Dumoulin", "Martin Wiesner", "Chao Fang", "Marian Verhelst"], "title": "Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration", "comment": "To appear in the 31st Asia and South Pacific Design Automation Conference (ASP-DAC 2026, Invited Paper)", "summary": "Emerging continual learning applications necessitate next-generation neural processing unit (NPU) platforms to support both training and inference operations. The promising Microscaling (MX) standard enables narrow bit-widths for inference and large dynamic ranges for training. However, existing MX multiply-accumulate (MAC) designs face a critical trade-off: integer accumulation requires expensive conversions from narrow floating-point products, while FP32 accumulation suffers from quantization losses and costly normalization. To address these limitations, we propose a hybrid precision-scalable reduction tree for MX MACs that combines the benefits of both approaches, enabling efficient mixed-precision accumulation with controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to provide efficient control and data transfer to our optimized precision-scalable MX datapath. We evaluate our design both on MAC and system level and compare it to the SotA. Our integrated system achieves an energy efficiency of 657, 1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with a throughput of 64, 256, and 512 GOPS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55\u7684\u5f52\u7ea6\u6811\u8bbe\u8ba1\uff0c\u7528\u4e8eMX MAC\u5355\u5143\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MX MAC\u8bbe\u8ba1\u4e2d\u6574\u6570\u7d2f\u52a0\u548c\u6d6e\u70b9\u7d2f\u52a0\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5e76\u5728SNAX NPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u7684\u6df7\u5408\u7cbe\u5ea6\u8ba1\u7b97\u3002", "motivation": "\u65b0\u5174\u7684\u6301\u7eed\u5b66\u4e60\u5e94\u7528\u9700\u8981NPU\u5e73\u53f0\u540c\u65f6\u652f\u6301\u8bad\u7ec3\u548c\u63a8\u7406\u64cd\u4f5c\uff0c\u800c\u73b0\u6709\u7684MX MAC\u8bbe\u8ba1\u9762\u4e34\u5173\u952e\u6743\u8861\uff1a\u6574\u6570\u7d2f\u52a0\u9700\u8981\u6602\u8d35\u7684\u7a84\u6d6e\u70b9\u4e58\u79ef\u8f6c\u6362\uff0c\u800cFP32\u7d2f\u52a0\u5219\u5b58\u5728\u91cf\u5316\u635f\u5931\u548c\u6602\u8d35\u7684\u5f52\u4e00\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55\u5f52\u7ea6\u6811\u7528\u4e8eMX MAC\uff0c\u7ed3\u5408\u4e86\u6574\u6570\u548c\u6d6e\u70b9\u7d2f\u52a0\u7684\u4f18\u52bf\uff1b\u5c068x8 MAC\u9635\u5217\u96c6\u6210\u5230SNAX NPU\u5e73\u53f0\uff0c\u4e3a\u4f18\u5316\u7684\u7cbe\u5ea6\u53ef\u6269\u5c55MX\u6570\u636e\u8def\u5f84\u63d0\u4f9b\u9ad8\u6548\u63a7\u5236\u548c\u6570\u636e\u4f20\u8f93\u3002", "result": "\u5728MAC\u548c\u7cfb\u7edf\u7ea7\u522b\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u4e0eSotA\u76f8\u6bd4\uff0c\u96c6\u6210\u7cfb\u7edf\u5728MXINT8\u3001MXFP8/6\u548cMXFP4\u4e0b\u5206\u522b\u5b9e\u73b0\u4e86657\u30011438-1675\u548c4065 GOPS/W\u7684\u80fd\u6548\uff0c\u541e\u5410\u91cf\u5206\u522b\u4e3a64\u3001256\u548c512 GOPS\u3002", "conclusion": "\u8be5\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55\u5f52\u7ea6\u6811\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e86MX MAC\u4e2d\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u5728NPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u7684\u6df7\u5408\u7cbe\u5ea6\u8ba1\u7b97\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u786c\u4ef6\u652f\u6301\u3002"}}
{"id": "2511.06679", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06679", "abs": "https://arxiv.org/abs/2511.06679", "authors": ["Sangun Choi", "Yunho Oh"], "title": "EONSim: An NPU Simulator for On-Chip Memory and Embedding Vector Operations", "comment": null, "summary": "Embedding vector operations are a key component of modern deep neural network workloads. Unlike matrix operations with deterministic access patterns, embedding vector operations exhibit input data-dependent and non-deterministic memory accesses. Existing neural processing unit (NPU) simulators focus on matrix computations with simple double-buffered on-chip memory systems, lacking the modeling capability for realistic embedding behavior. Next-generation NPUs, however, call for more flexible on-chip memory architectures that can support diverse access and management schemes required by embedding workloads. To enable flexible exploration and design of emerging NPU architectures, we present EONSim, an NPU simulator that holistically models both matrix and embedding vector operations. EONSim integrates a validated performance model for matrix computations with detailed memory simulation for embedding accesses, supporting various on-chip memory management policies. Validated against TPUv6e, EONSim achieves an average inference time error of 1.4\\% and an average on-chip memory access count error of 2.2\\%.", "AI": {"tldr": "EONSim\u662f\u4e00\u4e2aNPU\u6a21\u62df\u5668\uff0c\u80fd\u591f\u6574\u4f53\u5efa\u6a21\u77e9\u9635\u548c\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\uff0c\u652f\u6301\u7075\u6d3b\u7684\u7247\u4e0a\u5185\u5b58\u67b6\u6784\u63a2\u7d22\uff0c\u9a8c\u8bc1\u663e\u793a\u5176\u63a8\u7406\u65f6\u95f4\u8bef\u5dee\u4e3a1.4%\uff0c\u5185\u5b58\u8bbf\u95ee\u8ba1\u6570\u8bef\u5dee\u4e3a2.2%\u3002", "motivation": "\u73b0\u6709NPU\u6a21\u62df\u5668\u4e3b\u8981\u5173\u6ce8\u5177\u6709\u786e\u5b9a\u6027\u8bbf\u95ee\u6a21\u5f0f\u7684\u77e9\u9635\u8ba1\u7b97\uff0c\u7f3a\u4e4f\u5bf9\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\u4e2d\u6570\u636e\u4f9d\u8d56\u6027\u548c\u975e\u786e\u5b9a\u6027\u5185\u5b58\u8bbf\u95ee\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u800c\u4e0b\u4e00\u4ee3NPU\u9700\u8981\u652f\u6301\u5d4c\u5165\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7075\u6d3b\u7247\u4e0a\u5185\u5b58\u67b6\u6784\u3002", "method": "\u5f00\u53d1EONSim\u6a21\u62df\u5668\uff0c\u96c6\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u77e9\u9635\u8ba1\u7b97\u6027\u80fd\u6a21\u578b\u548c\u8be6\u7ec6\u7684\u5d4c\u5165\u8bbf\u95ee\u5185\u5b58\u6a21\u62df\uff0c\u652f\u6301\u591a\u79cd\u7247\u4e0a\u5185\u5b58\u7ba1\u7406\u7b56\u7565\u3002", "result": "\u4e0eTPUv6e\u9a8c\u8bc1\u5bf9\u6bd4\uff0cEONSim\u5b9e\u73b0\u4e86\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u8bef\u5dee1.4%\u548c\u5e73\u5747\u7247\u4e0a\u5185\u5b58\u8bbf\u95ee\u8ba1\u6570\u8bef\u5dee2.2%\u3002", "conclusion": "EONSim\u4e3a\u65b0\u5174NPU\u67b6\u6784\u7684\u7075\u6d3b\u63a2\u7d22\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6a21\u62df\u5e73\u53f0\uff0c\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\u7684\u5185\u5b58\u8bbf\u95ee\u884c\u4e3a\u3002"}}
