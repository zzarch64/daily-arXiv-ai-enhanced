{"id": "2510.24112", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.24112", "abs": "https://arxiv.org/abs/2510.24112", "authors": ["Junchi Wu", "Xinfei Wan", "Zhuoran Li", "Yuyang Jin", "Guangyu Sun", "Yun Liang", "Diyu Zhou", "Youwei Zhuo"], "title": "SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems", "comment": "15 pages, 15 figures", "summary": "Many-core architectures are essential for high-performance computing, but\ntheir performance is undermined by widespread fail-slow failures. Detecting\nsuch failures on-chip is challenging, as prior methods from distributed systems\nare unsuitable due to strict memory limits and their inability to track\nfailures across the hardware topology. This paper introduces SlowPoke, a\nlightweight, hardware-aware framework for practical on-chip fail-slow\ndetection. SlowPoke combines compiler-based instrumentation for low-overhead\nmonitoring, on-the-fly trace compression to operate within kilobytes of memory,\nand a novel topology-aware ranking algorithm to pinpoint a failure's root\ncause. We evaluate SlowPoke on a wide range of representative many-core\nworkloads, and the results demonstrate that SlowPoke reduces the storage\noverhead of detection traces by an average of 115.9$\\times$, while achieving an\naverage fail-slow detection accuracy of 86.77% and a false positive rate (FPR)\nof 12.11%. More importantly, SlowPoke scales effectively across different\nmany-core architectures, making it practical for large-scale deployments.", "AI": {"tldr": "SlowPoke\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u786c\u4ef6\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u6838\u67b6\u6784\u4e0a\u8fdb\u884c\u7247\u4e0a\u6545\u969c\u6162\u901f\u68c0\u6d4b\uff0c\u901a\u8fc7\u7f16\u8bd1\u5668\u63d2\u6869\u3001\u5b9e\u65f6\u8ddf\u8e2a\u538b\u7f29\u548c\u62d3\u6251\u611f\u77e5\u6392\u540d\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u5f00\u9500\u5e76\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u591a\u6838\u67b6\u6784\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u5230\u5e7f\u6cdb\u5b58\u5728\u7684\u6545\u969c\u6162\u901f\u95ee\u9898\u7684\u4e25\u91cd\u5f71\u54cd\u3002\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u65b9\u6cd5\u7531\u4e8e\u4e25\u683c\u7684\u5185\u5b58\u9650\u5236\u548c\u65e0\u6cd5\u8ddf\u8e2a\u786c\u4ef6\u62d3\u6251\u4e2d\u7684\u6545\u969c\u800c\u4e0d\u9002\u7528\u3002", "method": "\u7ed3\u5408\u7f16\u8bd1\u5668\u63d2\u6869\u8fdb\u884c\u4f4e\u5f00\u9500\u76d1\u63a7\u3001\u5b9e\u65f6\u8ddf\u8e2a\u538b\u7f29\u4ee5\u5728\u5343\u5b57\u8282\u5185\u5b58\u5185\u8fd0\u884c\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u62d3\u6251\u611f\u77e5\u6392\u540d\u7b97\u6cd5\u6765\u5b9a\u4f4d\u6545\u969c\u7684\u6839\u672c\u539f\u56e0\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u6838\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cSlowPoke\u5c06\u68c0\u6d4b\u8ddf\u8e2a\u7684\u5b58\u50a8\u5f00\u9500\u5e73\u5747\u964d\u4f4e115.9\u500d\uff0c\u5b9e\u73b0\u5e73\u574786.77%\u7684\u6545\u969c\u6162\u901f\u68c0\u6d4b\u51c6\u786e\u7387\u548c12.11%\u7684\u8bef\u62a5\u7387\u3002", "conclusion": "SlowPoke\u5728\u4e0d\u540c\u591a\u6838\u67b6\u6784\u4e0a\u90fd\u80fd\u6709\u6548\u6269\u5c55\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u4e3a\u7247\u4e0a\u6545\u969c\u6162\u901f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24113", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24113", "abs": "https://arxiv.org/abs/2510.24113", "authors": ["Arnav Shukla", "Harsh Sharma", "Srikant Bharadwaj", "Vinayak Abrol", "Sujay Deb"], "title": "Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on Chiplet-Based Accelerators", "comment": null, "summary": "Heterogeneous chiplet-based systems improve scaling by disag-gregating\nCPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package\ndisaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe\nthat in modern large-modelinference, parameters and activations routinely move\nbackand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.\nThese memory-driven transfers inflate tail latency andviolate Service Level\nAgreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this\ngap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown\nunder contention.We then formulate NoI synthesis as a multi-objective\noptimization(MOO) problem. We develop PARL (Partition-Aware\nReinforcementLearner), a topology generator that balances throughput,\nlatency,and power. PARL-generated topologies reduce contention at the memory\ncut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining\ncompetitive mean throughput relative to link-rich meshes. Overall, this\nreframes NoI design for heterogeneouschiplet accelerators with workload-aware\nobjectives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPARL\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u751f\u6210\u5f02\u6784chiplet\u7cfb\u7edf\u4e2d\u7f51\u7edc\u4e92\u8fde\u62d3\u6251\uff0c\u89e3\u51b3\u5185\u5b58\u9a71\u52a8\u4f20\u8f93\u5bfc\u81f4\u7684\u5c3e\u90e8\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u5f02\u6784chiplet\u7cfb\u7edf\u901a\u8fc7\u89e3\u8026CPU/GPU\u548c\u65b0\u5174\u6280\u672f(HBM/DRAM)\u6765\u63d0\u5347\u6269\u5c55\u6027\uff0c\u4f46\u5c01\u88c5\u5185\u89e3\u8026\u5f15\u5165\u4e86\u7f51\u7edc\u4e92\u8fde\u5ef6\u8fdf\u3002\u5728\u5927\u6a21\u578b\u63a8\u7406\u4e2d\uff0c\u53c2\u6570\u548c\u6fc0\u6d3b\u503c\u5728HBM/DRAM\u95f4\u9891\u7e41\u79fb\u52a8\uff0c\u4ea7\u751f\u7a81\u53d1\u6d41\u91cf\uff0c\u5bfc\u81f4\u5c3e\u90e8\u5ef6\u8fdf\u589e\u52a0\u5e76\u8fdd\u53cdSLA\u3002", "method": "\u5f15\u5165\u5e72\u6270\u8bc4\u5206\u91cf\u5316\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5c06\u7f51\u7edc\u4e92\u8fde\u5408\u6210\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1PARL\uff08\u5206\u533a\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff09\u62d3\u6251\u751f\u6210\u5668\uff0c\u5e73\u8861\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u3002", "result": "PARL\u751f\u6210\u7684\u62d3\u6251\u51cf\u5c11\u5185\u5b58\u5207\u5206\u5904\u7684\u4e89\u7528\uff0c\u6ee1\u8db3SLA\u8981\u6c42\uff0c\u5c06\u6700\u574f\u60c5\u51b5\u6027\u80fd\u4e0b\u964d\u964d\u81f31.2\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u94fe\u8def\u4e30\u5bcc\u7f51\u683c\u76f8\u5f53\u7684\u5747\u503c\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u91cd\u65b0\u5b9a\u4e49\u4e86\u5f02\u6784chiplet\u52a0\u901f\u5668\u7684\u7f51\u7edc\u4e92\u8fde\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u76ee\u6807\u4f18\u5316\u3002"}}
