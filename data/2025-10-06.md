<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference](https://arxiv.org/abs/2510.02675)
*Shubham Negi,Kaushik Roy*

Main category: cs.AR

TL;DR: HALO是一个异构内存中心加速器，针对LLM推理中的prefill和decode两个阶段的不同计算特性进行优化，通过结合HBM CiD和片上CiM，在低批次长上下文场景下实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: LLM推理包含prefill和decode两个计算特性完全不同的阶段，现有工作主要针对高批次推理或短上下文优化，而交互式应用需要的低批次长上下文场景研究不足。

Method: 提出HALO异构加速器，集成HBM CiD和片上CiM，采用阶段感知映射策略：prefill阶段计算密集型操作映射到CiM，decode阶段内存密集型操作映射到CiD。

Result: 在LLaMA-2 7B和Qwen3 8B模型上，HALO相比AttAcc获得18倍几何平均加速，相比全CiD设计CENT获得2.5倍加速。

Conclusion: 异构内存架构能有效应对LLM推理中不同阶段的多样化计算需求，在低批次长上下文场景下具有显著优势。

Abstract: The rapid adoption of Large Language Models (LLMs) has driven a growing
demand for efficient inference, particularly in latency-sensitive applications
such as chatbots and personalized assistants. Unlike traditional deep neural
networks, LLM inference proceeds in two distinct phases: the prefill phase,
which processes the full input sequence in parallel, and the decode phase,
which generates tokens sequentially. These phases exhibit highly diverse
compute and memory requirements, which makes accelerator design particularly
challenging. Prior works have primarily been optimized for high-batch inference
or evaluated only short input context lengths, leaving the low-batch and long
context regime, which is critical for interactive applications, largely
underexplored.
  We propose HALO, a heterogeneous memory centric accelerator designed for
these unique challenges of prefill and decode phases in low-batch LLM
inference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip
analog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further
improve the hardware utilization, we introduce a phase-aware mapping strategy
that adapts to the distinct demands of the prefill and decode phases. Compute
bound operations in the prefill phase are mapped to CiM to exploit its high
throughput matrix multiplication capability, while memory-bound operations in
the decode phase are executed on CiD to benefit from reduced data movement
within DRAM. Additionally, we present an analysis of the performance tradeoffs
of LLMs under two architectural extremes: a fully CiD and a fully on-chip
analog CiM design to highlight the need for a heterogeneous design. We evaluate
HALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs
mapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an
attention-optimized mapping and 2.5x over CENT, a fully CiD based mapping.

</details>


### [2] [A Hardware Accelerator for the Goemans-Williamson Algorithm](https://arxiv.org/abs/2510.02863)
*D. A. Herrera-Martí,E. Guthmuller,J. Fereyre*

Main category: cs.AR

TL;DR: 该论文研究了在凸优化中引入扩展浮点精度来加速大规模Max-Cut问题的求解，特别是在使用共轭梯度等间接矩阵求逆方法时，扩展精度能显著减少求解时间。


<details>
  <summary>Details</summary>
Motivation: Max-Cut问题已成为量子与经典优化器局部搜索启发式算法的基准测试。与仅提供平均性能保证的局部搜索不同，Goemans和Williamson的凸半定松弛方法提供最坏情况保证，适用于基准构建和性能关键场景。

Method: 在凸优化中引入扩展浮点精度，特别应用于内点法中使用的间接矩阵求逆方法（如共轭梯度法），这些方法在超大规模问题中比直接方法复杂度更低。

Result: 使用扩展精度时，内部工作精度的提高能减少求解时间，且加速因子随系统规模增大而增加。

Conclusion: 扩展浮点精度可有效加速大规模Max-Cut问题的凸优化求解，特别是在使用间接矩阵求逆方法时，这种加速效果随问题规模扩大而更加显著。

Abstract: The combinatorial problem Max-Cut has become a benchmark in the evaluation of
local search heuristics for both quantum and classical optimisers. In contrast
to local search, which only provides average-case performance guarantees, the
convex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides
worst-case guarantees and is therefore suited to both the construction of
benchmarks and in applications to performance-critic scenarios.
  We show how extended floating point precision can be incorporated in
algebraic subroutines in convex optimisation, namely in indirect matrix
inversion methods like Conjugate Gradient, which are used in Interior Point
Methods in the case of very large problem sizes. Also, an estimate is provided
of the expected acceleration of the time to solution for a hardware
architecture that runs natively on extended precision. Specifically, when using
indirect matrix inversion methods like Conjugate Gradient, which have lower
complexity than direct methods and are therefore used in very large problems,
we see that increasing the internal working precision reduces the time to
solution by a factor that increases with the system size.

</details>


### [3] [A Resource-Driven Approach for Implementing CNNs on FPGAs Using Adaptive IPs](https://arxiv.org/abs/2510.02990)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: 开发了一个资源高效的卷积IP库，能自动适应FPGA可用资源，使用VHDL实现，支持参数化配置和定点算术，在Zynq UltraScale+ FPGA上验证了性能与资源使用的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着对实时、低延迟AI应用需求的增长，FPGA因其可重构性、能效和性能优势，在CNN实现中比GPU更适合边缘设备和嵌入式系统。

Method: 开发了四个参数化的卷积IP核，使用VHDL实现和定点算术，每个IP针对特定资源约束设计，在DSP使用、逻辑消耗和精度方面提供灵活性。

Result: 在Zynq UltraScale+ FPGA上的实验结果表明了性能与资源使用之间的权衡，与现有FPGA加速技术相比，该方法具有更好的通用性和架构独立性。

Conclusion: 该方法展示了资源高效卷积IP的可行性，未来工作将扩展库以包含池化和激活函数，提升在CNN框架中的适用性和集成能力。

Abstract: The increasing demand for real-time, low-latency artificial intelligence
applications has propelled the use of Field-Programmable Gate Arrays (FPGAs)
for Convolutional Neural Network (CNN) implementations. FPGAs offer
reconfigurability, energy efficiency, and performance advantages over GPUs,
making them suitable for edge devices and embedded systems. This work presents
a novel library of resource-efficient convolution IPs designed to automatically
adapt to the available FPGA resources. Developed in VHDL, these IPs are
parameterizable and utilize fixed-point arithmetic for optimal performance.
Four IPs are introduced, each tailored to specific resource constraints,
offering flexibility in DSP usage, logic consumption, and precision.
Experimental results on a Zynq UltraScale+ FPGA highlight the trade-offs
between performance and resource usage. The comparison with recent FPGA-based
CNN acceleration techniques emphasizes the versatility and independence of this
approach from specific FPGA architectures or technological advancements. Future
work will expand the library to include pooling and activation functions,
enabling broader applicability and integration into CNN frameworks.

</details>
