<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Optimized Memory Tagging on AmpereOne Processors](https://arxiv.org/abs/2511.17773)
*Shiv Kaushik,Mahesh Madhav,Nagi Aboulenein,Jason Bessette,Sandeep Brahmadathan,Ben Chaffin,Matthew Erler,Stephan Jourdan,Thomas Maciukenas,Ramya Masti,Jon Perry,Massimo Sutera,Scott Tetrick,Bret Toll,David Turley,Carl Worth,Atiq Bajwa*

Main category: cs.AR

TL;DR: AmpereOne处理器是首个支持ARM MTE的数据中心处理器，其优化的MTE实现无内存容量开销，同步标签检查对数据中心工作负载性能影响仅为个位数百分比，为生产云环境部署提供了高效硬件基础。


<details>
  <summary>Details</summary>
Motivation: 解决基于C/C++等指针语言软件中的内存安全问题，这些安全漏洞构成了广泛安全攻击的基础，现有编译器扩展和ISA扩展在开销和适用性方面存在局限。

Method: 采用ARM AArch64指令集架构中的内存标签扩展(MTE)，在AmpereOne处理器中实现优化的MTE硬件支持，包括无内存容量开销的标签存储和同步标签检查机制。

Result: AmpereOne处理器的MTE实现能够确定性检测和预防顺序缓冲区溢出攻击，概率性检测和预防使用时序释放后使用指针编程错误，性能影响仅为个位数百分比。

Conclusion: AmpereOne处理器的高效MTE实现与明确的软件优化路径相结合，使其在生产云环境部署中极具吸引力，应用程序内存管理是剩余的主要开销来源，存在明确的软件优化机会。

Abstract: Memory-safety escapes continue to form the launching pad for a wide range of security attacks, especially for the substantial base of deployed software that is coded in pointer-based languages such as C/C++. Although compiler and Instruction Set Architecture (ISA) extensions have been introduced to address elements of this issue, the overhead and/or comprehensive applicability have limited broad production deployment. The Memory Tagging Extension (MTE) to the ARM AArch64 Instruction Set Architecture is a valuable tool to address memory-safety escapes; when used in synchronous tag-checking mode, MTE provides deterministic detection and prevention of sequential buffer overflow attacks, and probabilistic detection and prevention of exploits resulting from temporal use-after-free pointer programming bugs. The AmpereOne processor, launched in 2024, is the first datacenter processor to support MTE. Its optimized MTE implementation uniquely incurs no memory capacity overhead for tag storage and provides synchronous tag-checking with single-digit performance impact across a broad range of datacenter class workloads. Furthermore, this paper analyzes the complete hardware-software stack, identifying application memory management as the primary remaining source of overhead and highlighting clear opportunities for software optimization. The combination of an efficient hardware foundation and a clear path for software improvement makes the MTE implementation of the AmpereOne processor highly attractive for deployment in production cloud environments.

</details>


### [2] [Comprehensive Design Space Exploration for Tensorized Neural Network Hardware Accelerators](https://arxiv.org/abs/2511.17971)
*Jinsong Zhang,Minghe Li,Jiayi Tian,Jinming Lu,Zheng Zhang*

Main category: cs.AR

TL;DR: 提出一个统一的协同探索框架，将张量分解模型的收缩路径、硬件架构和数据流映射联合优化，以最大化边缘设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有高阶张量分解研究主要关注算法优势而忽视硬件部署效率，硬件无关设计往往掩盖了张量化模型的潜在延迟和能耗优势。

Method: 提出协同探索框架，在统一设计空间中联合优化收缩路径、硬件架构和数据流映射，通过全局延迟驱动探索实现端到端模型效率。

Result: 在可配置FPGA内核上实现优化配置，相比密集基线模型，推理和训练延迟分别降低4倍和3.85倍。

Conclusion: 收缩路径、硬件架构和数据流映射需要在一个统一设计空间中联合优化，才能最大化张量化神经网络在边缘平台上的部署效率。

Abstract: High-order tensor decomposition has been widely adopted to obtain compact deep neural networks for edge deployment. However, existing studies focus primarily on its algorithmic advantages such as accuracy and compression ratio-while overlooking the hardware deployment efficiency. Such hardware-unaware designs often obscure the potential latency and energy benefits of tensorized models. Although several works attempt to reduce computational cost by optimizing the contraction sequence based on the number of multiply-accumulate operations, they typically neglect the underlying hardware characteristics, resulting in suboptimal real-world performance. We observe that the contraction path, hardware architecture, and dataflow mapping are tightly coupled and must be optimized jointly within a unified design space to maximize deployment efficiency on real devices. To this end, we propose a co-exploration framework that unifies these dimensions within a unified design space for efficient training and inference of tensorized neural networks on edge platforms. The framework formulates a latency oriented search objective and solves it via a global latency-driven exploration across the unified design space to achieve end-to-end model efficiency. The optimized configurations are implemented on a configurable FPGA kernel, achieving up to 4 and 3.85 lower inference and training latency compared with the dense baseline.

</details>


### [3] [HDDB: Efficient In-Storage SQL Database Search Using Hyperdimensional Computing on Ferroelectric NAND Flash](https://arxiv.org/abs/2511.18234)
*Quanling Zhao,Yanru Chen,Runyang Tian,Sumukh Pinge,Weihong Xu,Augusto Vega,Steven Holmes,Saransh Gupta,Tajana Rosing*

Main category: cs.AR

TL;DR: HDDB是一个硬件-软件协同设计，将超维计算与铁电NAND存储器结合，在存储内执行SQL谓词评估和分析，实现大规模并行和最小数据移动。


<details>
  <summary>Details</summary>
Motivation: 超维计算的噪声容错特性与新兴铁电NAND存储器的高密度和存储内计算能力天然匹配，但后者存在较高的原始比特错误率。

Method: 提出新颖的HDC编码技术用于标准SQL数据表，将基于谓词的过滤和聚合制定为高效的HDC操作，利用HDC固有冗余在设备噪声下保持正确结果。

Result: 在TPC-DS事实表上的实验显示，HDDB相比传统CPU/GPU SQL数据库引擎实现了80.6倍延迟降低和12,636倍能耗降低。

Conclusion: HDDB为噪声鲁棒、内存中心的数据处理提供了实用基础。

Abstract: Hyperdimensional Computing (HDC) encodes information and data into high-dimensional distributed vectors that can be manipulated using simple bitwise operations and similarity searches, offering parallelism, low-precision hardware friendliness, and strong robustness to noise. These properties are a natural fit for SQL database workloads dominated by predicate evaluation and scans, which demand low energy and low latency over large fact tables. Notably, HDC's noise-tolerance maps well onto emerging ferroelectric NAND (FeNAND) memories, which provide ultra-high density and in-storage compute capability but suffer from elevated raw bit-error rates. In this work, we propose HDDB, a hardware-software co-design that combines HDC with FeNAND multi-level cells (MLC) to perform in-storage SQL predicate evaluation and analytics with massive parallelism and minimal data movement. Particularly, we introduce novel HDC encoding techniques for standard SQL data tables and formulate predicate-based filtering and aggregation as highly efficient HDC operations that can happen in-storage. By exploiting the intrinsic redundancy of HDC, HDDB maintains correct predicate and decode outcomes under substantial device noise (up to 10% randomly corrupted TLC cells) without explicit error-correction overheads. Experiments on TPC-DS fact tables show that HDDB achieves up to 80.6x lower latency and 12,636x lower energy consumption compared to conventional CPU/GPU SQL database engines, suggesting that HDDB provides a practical substrate for noise-robust, memory-centric database processing.

</details>


### [4] [Evaluation of NVENC Split-Frame Encoding (SFE) for UHD Video Transcoding](https://arxiv.org/abs/2511.18687)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: NVIDIA的Split-Frame Encoding (SFE)技术通过将UHD帧分割到多个NVENC芯片并行编码，显著提升编码吞吐量，在实时应用中能以可忽略的RD性能损失实现近乎翻倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 随着消费设备能够拍摄4K/8K超高清视频，需要高性能视频转码器进行互联网传输。NVIDIA GPU在数据中心广泛部署，NVENC将在UHD视频转码中发挥关键作用。

Method: 利用高端GPU中的多个NVENC芯片，将单个UHD帧分割进行并行编码，然后拼接结果，评估SFE对RD性能、编码吞吐量、功耗和端到端延迟的影响。

Result: SFE在实时应用中使编码吞吐量近乎翻倍，RD性能损失可忽略，支持4K使用更高质量预设，使实时8K编码可行，4K无延迟增加，8K可降低延迟。

Conclusion: SFE是实现高吞吐量、实时UHD转码的关键技术，在吞吐量和效率之间提供了良好的权衡。

Abstract: NVIDIA Encoder (NVENC) features in modern NVIDIA GPUs, offer significant advantages over software encoders by providing comparable Rate-Distortion (RD) performance while consuming considerably less power. The increasing capability of consumer devices to capture footage in Ultra High-Definition (UHD) at 4K and 8K resolutions necessitates high-performance video transcoders for internet-based delivery. To address this demand, NVIDIA introduced Split-Frame Encoding (SFE), a technique that leverages multiple on-die NVENC chips available in high-end GPUs. SFE splits a single UHD frame for parallel encoding across these physical encoders and subsequently stitches the results, which significantly improves encoding throughput. However, this approach is known to incur an RD performance penalty. The widespread adoption of NVIDIA GPUs in data centers, driven by the rise of Generative AI, means NVENC is poised to play a critical role in transcoding UHD video. To better understand the performance-efficiency tradeoff of SFE, this paper evaluates SFE's impact on RD performance, encoding throughput, power consumption, and end-to-end latency using standardized test sequences. The results show that for real-time applications, SFE nearly doubles encoding throughput with a negligible RD performance penalty, which enables the use of higher-quality presets for 4K and makes real-time 8K encoding feasible, effectively offsetting the minor RD penalty. Moreover, SFE adds no latency at 4K and can reduce it at 8K, positioning it as a key enabler for high-throughput, real-time UHD transcoding.

</details>


### [5] [Evaluation of GPU Video Encoder for Low-Latency Real-Time 4K UHD Encoding](https://arxiv.org/abs/2511.18688)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: 评估NVIDIA、Intel和AMD GPU上的低延迟编码模式，比较硬件编码器与软件编码器的性能表现，发现硬件编码器在保持低延迟的同时提供更好的率失真性能。


<details>
  <summary>Details</summary>
Motivation: 随着4K超高清视频流需求的增长，需要了解GPU硬件编码器在低延迟模式下的性能表现，为6G时代的高质量实时视频流提供参考。

Method: 从率失真性能和延迟两个角度评估NVIDIA、Intel和AMD GPU的低延迟编码模式，并与硬件编码器的正常延迟模式和领先软件编码器进行对比。

Result: 硬件编码器相比软件解决方案实现了显著更低的端到端延迟和略好的率失真性能。超低延迟模式可将E2E延迟降至83毫秒（5帧）且不影响率失真性能。

Conclusion: 硬件编码器在保持高质量的同时能够实现极低延迟，为实时视频流应用提供了理想的解决方案。

Abstract: The demand for high-quality, real-time video streaming has grown exponentially, with 4K Ultra High Definition (UHD) becoming the new standard for many applications such as live broadcasting, TV services, and interactive cloud gaming. This trend has driven the integration of dedicated hardware encoders into modern Graphics Processing Units (GPUs). Nowadays, these encoders support advanced codecs like HEVC and AV1 and feature specialized Low-Latency and Ultra Low-Latency tuning, targeting end-to-end latencies of < 2 seconds and < 500 ms, respectively. As the demand for such capabilities grows toward the 6G era, a clear understanding of their performance implications is essential. In this work, we evaluate the low-latency encoding modes on GPUs from NVIDIA, Intel, and AMD from both Rate-Distortion (RD) performance and latency perspectives. The results are then compared against both the normal-latency tuning of hardware encoders and leading software encoders. Results show hardware encoders achieve significantly lower E2E latency than software solutions with slightly better RD performance. While standard Low-Latency tuning yields a poor quality-latency trade-off, the Ultra Low-Latency mode reduces E2E latency to 83 ms (5 frames) without additional RD impact. Furthermore, hardware encoder latency is largely insensitive to quality presets, enabling high-quality, low-latency streams without compromise.

</details>


### [6] [Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing](https://arxiv.org/abs/2511.18755)
*Xiaotong Huang,He Zhu,Tianrui Ma,Yuxiang Xiong,Fangxin Liu,Zhezhi He,Yiming Gan,Zihan Liu,Jingwen Leng,Yu Feng,Minyi Guo*

Main category: cs.AR

TL;DR: Splatonic是一个稀疏高效的3DGS-SLAM软硬件协同设计，通过自适应稀疏像素采样和新型像素渲染流水线，在移动设备上实现实时性能，相比移动GPU获得274.9倍加速和4738.5倍节能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)在SLAM中展现出高保真重建和快速收敛的优势，但其高计算成本使其在移动平台上不实用，特别是跟踪过程。

Method: 提出自适应稀疏像素采样算法减少渲染像素数量达256倍；设计新型基于像素的渲染流水线，通过高斯并行渲染和抢占式α检查提高硬件利用率；提出流水线架构简化设计并解决投影和聚合中的新瓶颈。

Result: 在瓶颈阶段实现121.7倍加速，端到端加速14.6倍；相比移动GPU实现274.9倍加速和4738.5倍节能；相比最先进加速器实现25.2倍加速和241.1倍节能，同时保持可比较的精度。

Conclusion: Splatonic通过软硬件协同设计成功解决了3DGS-SLAM在移动设备上的计算瓶颈，实现了实时性能和高能效，为资源受限设备上的3D重建应用提供了可行解决方案。

Abstract: 3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.
  This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $α$-checking. Together, these optimizations yield up to 121.7$\times$ speedup on the bottleneck stages and 14.6$\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\times$ speedup and 4738.5$\times$ energy savings over mobile GPUs and up to 25.2$\times$ speedup and 241.1$\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.

</details>


### [7] [HeLEx: A Heterogeneous Layout Explorer for Spatial Elastic Coarse-Grained Reconfigurable Arrays](https://arxiv.org/abs/2511.19366)
*Alan Jia Bao Du,Tarek S. Abdelrahman*

Main category: cs.AR

TL;DR: HeLEx是一个用于确定异构空间配置弹性粗粒度可重构阵列(CGRA)功能布局的框架，通过分支定界搜索优化PE操作集，显著减少CGRA面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 传统CGRA设计中PE支持所有操作导致资源浪费，需要优化功能布局以减少面积和功耗。

Method: 使用分支定界搜索算法，从全功能布局开始逐步消除PE中不必要的操作，确保输入数据流图仍能成功映射。

Result: 平均减少68.7%的操作数量，CGRA面积减少近70%，功耗降低超过51%，与理论最小值仅差6.2%。

Conclusion: HeLEx框架能有效优化异构CGRA功能布局，在操作减少方面优于现有方法达2.6倍。

Abstract: We present HeLEx, a framework for determining the functional layout of heterogeneous spatially-configured elastic Coarse-Grained Reconfigurable Arrays (CGRAs). Given a collection of input data flow graphs (DFGs) and a target CGRA, the framework starts with a full layout in which every processing element (PE) supports every operation in the DFGs. It then employs a branch-and-bound (BB) search to eliminate operations out of PEs, ensuring that the input DFGs successfully map onto the resulting CGRAs, eventually returning an optimized heterogeneous CGRA. Experimental evaluation with 12 DFGs and 9 target CGRA sizes reveals that the framework reduces the number of operations by 68.7% on average, resulting in a reduction of CGRA area by almost 70% and of power by over 51%, all compared to the initial full layout. HeLEx generates CGRAs that are on average only within 6.2% of theoretically minimum CGRAs that support exactly the number of operations needed by the input DFGs. A comparison with functional layouts produced by two state-of-the-art frameworks indicates that HeLEx achieves better reduction in the number of operations, by up to 2.6X.

</details>
