{"id": "2601.09773", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09773", "abs": "https://arxiv.org/abs/2601.09773", "authors": ["Binglei Lou", "Ruilin Wu", "Philip Leong"], "title": "Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.12829, arXiv:2406.04910", "summary": "Deploying deep neural networks (DNNs) on resource-constrained edge devices such as FPGAs requires a careful balance among latency, power, and hardware resource usage, while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs -- such as LogicNets, PolyLUT, and NeuraLUT -- face two critical challenges: the exponential growth of LUT size and inefficient random sparse connectivity. This paper presents SparseLUT, a comprehensive framework that addresses these challenges through two orthogonal optimizations. First, we propose an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, significantly reducing LUT consumption by 2.0x-13.9x and lowering inference latency by 1.2x-1.6x, all while maintaining comparable accuracy. Building upon this foundation, we further introduce a non-greedy training algorithm that optimizes neuron connectivity by selectively pruning less significant inputs and strategically regrowing more effective ones. This training optimization, which incurs no additional area and latency overhead, delivers consistent accuracy improvements across benchmarks -- achieving up to a 2.13% gain on MNIST and 0.94% on Jet Substructure Classification compared to existing LUT-DNN approaches.", "AI": {"tldr": "SparseLUT\u6846\u67b6\u901a\u8fc7\u67b6\u6784\u589e\u5f3a\u548c\u8bad\u7ec3\u7b97\u6cd5\u4f18\u5316\uff0c\u89e3\u51b3\u4e86LUT-based DNNs\u4e2dLUT\u5927\u5c0f\u6307\u6570\u589e\u957f\u548c\u7a00\u758f\u8fde\u63a5\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u786c\u4ef6\u8d44\u6e90\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5728FPGA\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72DNN\u9700\u8981\u5e73\u8861\u5ef6\u8fdf\u3001\u529f\u8017\u548c\u786c\u4ef6\u8d44\u6e90\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u73b0\u6709\u57fa\u4e8e\u67e5\u627e\u8868(LUT)\u7684DNN\uff08\u5982LogicNets\u3001PolyLUT\u3001NeuraLUT\uff09\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1aLUT\u5927\u5c0f\u7684\u6307\u6570\u589e\u957f\u548c\u4f4e\u6548\u7684\u968f\u673a\u7a00\u758f\u8fde\u63a5\u3002", "method": "\u63d0\u51faSparseLUT\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6b63\u4ea4\u4f18\u5316\uff1a1) \u67b6\u6784\u589e\u5f3a\uff1a\u901a\u8fc7\u52a0\u6cd5\u5668\u805a\u5408\u591a\u4e2aPolyLUT\u5b50\u795e\u7ecf\u5143\uff0c\u663e\u8457\u51cf\u5c11LUT\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\uff1b2) \u975e\u8d2a\u5a6a\u8bad\u7ec3\u7b97\u6cd5\uff1a\u901a\u8fc7\u9009\u62e9\u6027\u526a\u679d\u4e0d\u91cd\u8981\u7684\u8f93\u5165\u5e76\u7b56\u7565\u6027\u5730\u91cd\u65b0\u751f\u957f\u66f4\u6709\u6548\u7684\u8fde\u63a5\u6765\u4f18\u5316\u795e\u7ecf\u5143\u8fde\u63a5\u3002", "result": "\u67b6\u6784\u4f18\u5316\u5c06LUT\u6d88\u8017\u964d\u4f4e2.0x-13.9x\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e1.2x-1.6x\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u7cbe\u5ea6\u3002\u8bad\u7ec3\u4f18\u5316\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e26\u6765\u4e00\u81f4\u7684\u7cbe\u5ea6\u63d0\u5347\uff1a\u5728MNIST\u4e0a\u8fbe\u52302.13%\u589e\u76ca\uff0c\u5728Jet Substructure Classification\u4e0a\u8fbe\u52300.94%\u589e\u76ca\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u9762\u79ef\u548c\u5ef6\u8fdf\u5f00\u9500\u3002", "conclusion": "SparseLUT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LUT-based DNNs\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u67b6\u6784\u589e\u5f3a\u548c\u8bad\u7ec3\u4f18\u5316\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u7cbe\u5ea6\u6743\u8861\uff0c\u4e3aFPGA\u4e0a\u7684\u9ad8\u6548DNN\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.10463", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.10463", "abs": "https://arxiv.org/abs/2601.10463", "authors": ["Xinyu Shi", "Simei Yang", "Francky Catthoor"], "title": "Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications", "comment": null, "summary": "Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5c42\u65b9\u6cd5\u5bf9XR\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u67b6\u6784\u5206\u7c7b\uff0c\u8bc6\u522b\u51fa\u5bb9\u91cf\u53d7\u9650\u548c\u5f00\u9500\u654f\u611f\u7b49\u6838\u5fc3\u5de5\u4f5c\u8d1f\u8f7d\u539f\u578b\uff0c\u4e3a\u4e0b\u4e00\u4ee3XR SoC\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\uff0c\u5f3a\u8c03\u9700\u8981\u4ece\u901a\u7528\u8d44\u6e90\u6269\u5c55\u8f6c\u5411\u9636\u6bb5\u611f\u77e5\u8c03\u5ea6\u548c\u5f39\u6027\u8d44\u6e90\u5206\u914d\u3002", "motivation": "XR\u5e73\u53f0\u9700\u8981\u5728\u4e25\u683c\u7684\u529f\u8017\u548c\u9762\u79ef\u7ea6\u675f\u4e0b\u63d0\u4f9b\u786e\u5b9a\u6027\u8d85\u4f4e\u5ef6\u8fdf\u6027\u80fd\uff0c\u4f46XR\u5de5\u4f5c\u8d1f\u8f7d\u591a\u6837\u6027\u5feb\u901f\u589e\u52a0\uff0c\u5177\u6709\u5f02\u6784\u7b97\u5b50\u7c7b\u578b\u548c\u590d\u6742\u6570\u636e\u6d41\u7ed3\u6784\uff0c\u8fd9\u5bf9\u4f20\u7edf\u4ee5CNN\u4e3a\u4e2d\u5fc3\u7684\u52a0\u901f\u5668\u67b6\u6784\u6784\u6210\u6311\u6218\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5b8c\u6574XR\u7ba1\u9053\u7684\u7cfb\u7edf\u6027\u67b6\u6784\u7406\u89e3\u3002", "method": "\u91c7\u7528\u8de8\u5c42\u65b9\u6cd5\u6574\u5408\u6a21\u578b\u9a71\u52a8\u7684\u9ad8\u5c42\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u5546\u7528GPU/CPU\u786c\u4ef6\u4e0a\u7684\u7ecf\u9a8c\u6027\u6027\u80fd\u5206\u6790\uff0c\u5bf912\u4e2a\u4e0d\u540cXR\u5185\u6838\u7684\u4ee3\u8868\u6027\u5de5\u4f5c\u8d1f\u8f7d\u96c6\u8fdb\u884c\u5206\u6790\uff0c\u5c06\u590d\u6742\u67b6\u6784\u7279\u5f81\u63d0\u70bc\u4e3a\u5c11\u91cf\u8de8\u5c42\u5de5\u4f5c\u8d1f\u8f7d\u539f\u578b\u3002", "result": "\u8bc6\u522b\u51fa\u5bb9\u91cf\u53d7\u9650\u3001\u5f00\u9500\u654f\u611f\u7b49\u5173\u952e\u5de5\u4f5c\u8d1f\u8f7d\u539f\u578b\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u539f\u578b\u63d0\u53d6\u4e86\u91cd\u8981\u7684\u67b6\u6784\u6d1e\u5bdf\uff0c\u4e3a\u4e0b\u4e00\u4ee3XR SoC\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u6307\u5357\u3002", "conclusion": "XR\u67b6\u6784\u8bbe\u8ba1\u5fc5\u987b\u4ece\u901a\u7528\u8d44\u6e90\u6269\u5c55\u8f6c\u5411\u9636\u6bb5\u611f\u77e5\u8c03\u5ea6\u548c\u5f39\u6027\u8d44\u6e90\u5206\u914d\uff0c\u4ee5\u5b9e\u73b0\u672a\u6765XR\u7cfb\u7edf\u66f4\u9ad8\u7684\u80fd\u6548\u548c\u6027\u80fd\u3002"}}
