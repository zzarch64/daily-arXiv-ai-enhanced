{"id": "2601.19900", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.19900", "abs": "https://arxiv.org/abs/2601.19900", "authors": ["William Oswald", "Mario Renteria-Pinon", "Md. Sajjad Hossain", "Kyle Mooney", "Md. Bipul Hossain", "Destinie Diggs", "Yiwen Xu", "Mohamed Shaban", "Jinhui Wang", "Na Gong"], "title": "Flexible Bit-Truncation Memory for Approximate Applications on the Edge", "comment": null, "summary": "Bit truncation has demonstrated great potential to enable run-time quality-power adaptive data storage, thereby optimizing the power/energy efficiency of approximate applications and supporting their deployment in edge environments. However, existing bit-truncation memories require custom designs for a specific application. In this paper, we present a novel bit-truncation memory with full adaptation flexibility, which can truncate any number of data bits at run time to meet different quality and power trade-off requirements for various approximate applications. The developed bit-truncation memory has been applied to two representative data-intensive approximate applications: video processing and deep learning. Our experiments show that the proposed memory can support three different video applications (including luminance-aware, content-aware, and region-of-interest-aware) with enhanced power efficiency (up to 47.02% power savings) as compared to state-of-the-art. In addition, the proposed memory achieves significant (up to 51.69%) power savings for both baseline and pruned lightweight deep learning models, respectively, with a low implementation cost (2.89% silicon area overhead).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u5b8c\u5168\u81ea\u9002\u5e94\u7075\u6d3b\u6027\u7684\u4f4d\u622a\u65ad\u5b58\u50a8\u5668\uff0c\u53ef\u8fd0\u884c\u65f6\u622a\u65ad\u4efb\u610f\u6570\u636e\u4f4d\u4ee5\u6ee1\u8db3\u4e0d\u540c\u8fd1\u4f3c\u5e94\u7528\u7684\u8d28\u91cf-\u529f\u8017\u6743\u8861\u9700\u6c42\uff0c\u5e94\u7528\u4e8e\u89c6\u9891\u5904\u7406\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u4f4d\u622a\u65ad\u5b58\u50a8\u5668\u9700\u8981\u4e3a\u7279\u5b9a\u5e94\u7528\u5b9a\u5236\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u591a\u79cd\u8fd1\u4f3c\u5e94\u7528\u3001\u5728\u8fd0\u884c\u65f6\u7075\u6d3b\u8c03\u6574\u8d28\u91cf-\u529f\u8017\u6743\u8861\u7684\u901a\u7528\u4f4d\u622a\u65ad\u5b58\u50a8\u5668\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u6709\u5b8c\u5168\u81ea\u9002\u5e94\u7075\u6d3b\u6027\u7684\u65b0\u578b\u4f4d\u622a\u65ad\u5b58\u50a8\u5668\u67b6\u6784\uff0c\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u622a\u65ad\u4efb\u610f\u6570\u91cf\u7684\u6570\u636e\u4f4d\u3002\u8be5\u5b58\u50a8\u5668\u5e94\u7528\u4e8e\u4e24\u79cd\u6570\u636e\u5bc6\u96c6\u578b\u8fd1\u4f3c\u5e94\u7528\uff1a\u89c6\u9891\u5904\u7406\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u652f\u6301\u4e09\u79cd\u4e0d\u540c\u7684\u89c6\u9891\u5e94\u7528\u6a21\u5f0f\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0c\u5728\u89c6\u9891\u5e94\u7528\u4e2d\u5b9e\u73b0\u9ad8\u8fbe47.02%\u7684\u529f\u8017\u8282\u7701\uff1b\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u8fbe51.69%\u7684\u529f\u8017\u8282\u7701\uff0c\u540c\u65f6\u4ec5\u5e26\u67652.89%\u7684\u7845\u9762\u79ef\u5f00\u9500\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f4d\u622a\u65ad\u5b58\u50a8\u5668\u5177\u6709\u5b8c\u5168\u81ea\u9002\u5e94\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u591a\u79cd\u8fd1\u4f3c\u5e94\u7528\u7684\u8d28\u91cf-\u529f\u8017\u6743\u8861\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\uff0c\u4e14\u5b9e\u73b0\u6210\u672c\u4f4e\uff0c\u9002\u5408\u8fb9\u7f18\u73af\u5883\u90e8\u7f72\u3002"}}
{"id": "2601.19902", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.19902", "abs": "https://arxiv.org/abs/2601.19902", "authors": ["Elizabeth Shen", "Huiyang Zhou"], "title": "A Flower-Inspired Solution for Computer Memory Wear-Leveling", "comment": "6 pages, 6 figures, and 2 tables", "summary": "Lengthening a computer memory's lifespan is important for e-waste and sustainability. Uneven wear of memory is a major barrier. The problem is becoming even more urgent as emerging memory such as phase-change memory is subject to even shorter lifespan. Various solutions have been proposed, but they either require complicated hardware extensions or apply only to certain program constructs such as loops. This research proposes a new method, dual-ring wear leveling. It takes inspiration from the natural law known as the ``golden ratio\" and how it helps flower petals evenly receive sun lights. By modeling memory as two rings and combines the idea with existing memory management, garbage collection, the new solution offers an effective way to reduce memory wear and hence lengthen memory lifespan. It is deterministic, able to automatically adapt to memory size, requiring no hardware changes, and adding no slowdown to program executions.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u73af\u78e8\u635f\u5747\u8861\u65b9\u6cd5\uff0c\u5229\u7528\u9ec4\u91d1\u6bd4\u4f8b\u539f\u7406\uff0c\u901a\u8fc7\u4e24\u4e2a\u73af\u5f62\u5185\u5b58\u6a21\u578b\u7ed3\u5408\u5783\u573e\u56de\u6536\u673a\u5236\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u5373\u53ef\u5ef6\u957f\u5185\u5b58\u5bff\u547d", "motivation": "\u5ef6\u957f\u8ba1\u7b97\u673a\u5185\u5b58\u5bff\u547d\u5bf9\u7535\u5b50\u5783\u573e\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u5185\u5b58\u78e8\u635f\u4e0d\u5747\u8861\u662f\u4e3b\u8981\u969c\u788d\uff0c\u800c\u76f8\u53d8\u5b58\u50a8\u5668\u7b49\u65b0\u5174\u5185\u5b58\u5bff\u547d\u66f4\u77ed\uff0c\u95ee\u9898\u66f4\u52a0\u7d27\u8feb\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u9700\u8981\u590d\u6742\u786c\u4ef6\u6269\u5c55\uff0c\u8981\u4e48\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u7a0b\u5e8f\u7ed3\u6784\u5982\u5faa\u73af\u3002", "method": "\u63d0\u51fa\u53cc\u73af\u78e8\u635f\u5747\u8861\u65b9\u6cd5\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u9ec4\u91d1\u6bd4\u4f8b\u7684\u81ea\u7136\u6cd5\u5219\u53ca\u5176\u5e2e\u52a9\u82b1\u74e3\u5747\u5300\u63a5\u6536\u9633\u5149\u7684\u539f\u7406\u3002\u5c06\u5185\u5b58\u5efa\u6a21\u4e3a\u4e24\u4e2a\u73af\u5f62\u7ed3\u6784\uff0c\u7ed3\u5408\u73b0\u6709\u7684\u5185\u5b58\u7ba1\u7406\u548c\u5783\u573e\u56de\u6536\u673a\u5236\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u78e8\u635f\u5747\u8861\u3002", "result": "\u8be5\u65b9\u6cd5\u5177\u6709\u786e\u5b9a\u6027\uff0c\u80fd\u81ea\u52a8\u9002\u5e94\u5185\u5b58\u5927\u5c0f\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\uff0c\u4e14\u4e0d\u4f1a\u7ed9\u7a0b\u5e8f\u6267\u884c\u5e26\u6765\u989d\u5916\u5ef6\u8fdf\uff0c\u6709\u6548\u51cf\u5c11\u5185\u5b58\u78e8\u635f\u4ece\u800c\u5ef6\u957f\u5185\u5b58\u5bff\u547d\u3002", "conclusion": "\u53cc\u73af\u78e8\u635f\u5747\u8861\u65b9\u6cd5\u4e3a\u89e3\u51b3\u5185\u5b58\u78e8\u635f\u4e0d\u5747\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u501f\u9274\u81ea\u7136\u754c\u7684\u9ec4\u91d1\u6bd4\u4f8b\u539f\u7406\uff0c\u7ed3\u5408\u73b0\u6709\u5185\u5b58\u7ba1\u7406\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u3001\u65e0\u6027\u80fd\u5f00\u9500\u7684\u5185\u5b58\u5bff\u547d\u5ef6\u957f\u65b9\u6848\u3002"}}
{"id": "2601.19903", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19903", "abs": "https://arxiv.org/abs/2601.19903", "authors": ["Saeid Rajabi", "Chengmo Yang", "Satwik Patnaik"], "title": "STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification", "comment": "7 pages, 6 figures", "summary": "Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.", "AI": {"tldr": "STELLAR\uff1a\u9996\u4e2a\u57fa\u4e8e\u7ed3\u6784\u76f8\u4f3c\u6027\u5f15\u5bfcLLM\u751f\u6210SystemVerilog\u65ad\u8a00\u7684\u6846\u67b6\uff0c\u901a\u8fc7RTL\u5757\u7684AST\u7ed3\u6784\u6307\u7eb9\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u65ad\u8a00\u751f\u6210\u7684\u8bed\u6cd5\u6b63\u786e\u6027\u3001\u98ce\u683c\u5bf9\u9f50\u548c\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u624b\u52a8\u7f16\u5199SystemVerilog\u65ad\u8a00(SVA)\u8fc7\u7a0b\u7f13\u6162\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u8981\u4e48\u4ece\u5934\u751f\u6210\u65ad\u8a00\uff0c\u8981\u4e48\u5ffd\u7565\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u7ed3\u6784\u6a21\u5f0f\u548c\u4e13\u5bb6\u7f16\u5199\u7684\u65ad\u8a00\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "STELLAR\u5c06RTL\u5757\u8868\u793a\u4e3aAST\u7ed3\u6784\u6307\u7eb9\uff0c\u4ece\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u7ed3\u6784\u76f8\u5173\u7684(RTL, SVA)\u5bf9\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7ed3\u6784\u4fe1\u606f\u6574\u5408\u5230\u5f15\u5bfc\u63d0\u793a\u4e2d\uff0c\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u7684LLM\u65ad\u8a00\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSTELLAR\u5728\u8bed\u6cd5\u6b63\u786e\u6027\u3001\u98ce\u683c\u5bf9\u9f50\u548c\u529f\u80fd\u6b63\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u5728\u5de5\u4e1a\u5f62\u5f0f\u9a8c\u8bc1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u662f\u63d0\u5347LLM\u751f\u6210SystemVerilog\u65ad\u8a00\u8d28\u91cf\u7684\u6709\u6548\u65b9\u5411\uff0cSTELLAR\u6846\u67b6\u4e3a\u5de5\u4e1a\u5f62\u5f0f\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.19904", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.19904", "abs": "https://arxiv.org/abs/2601.19904", "authors": ["Ziyu Hu", "Zhiqing Zhong", "Weijian Zheng", "Zhijing Ye", "Xuwei Tan", "Xueru Zhang", "Zheng Xie", "Rajkumar Kettimuthu", "Xiaodong Yu"], "title": "DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs", "comment": null, "summary": "The exponential growth of large language models has outpaced the capabilities of traditional CPU and GPU architectures due to the slowdown of Moore's Law. Dataflow AI accelerators present a promising alternative; however, there remains a lack of in-depth performance analysis and standardized benchmarking methodologies for LLM training. We introduce DABench-LLM, the first benchmarking framework designed for evaluating LLM workloads on dataflow-based accelerators. By combining intra-chip performance profiling and inter-chip scalability analysis, DABench-LLM enables comprehensive evaluation across key metrics such as resource allocation, load balance, and resource efficiency. The framework helps researchers rapidly gain insights into underlying hardware and system behaviors, and provides guidance for performance optimizations. We validate DABench-LLM on three commodity dataflow accelerators, Cerebras WSE-2, SambaNova RDU, and Graphcore IPU. Our framework reveals performance bottlenecks and provides specific optimization strategies, demonstrating its generality and effectiveness across a diverse range of dataflow-based AI hardware platforms.", "AI": {"tldr": "DABench-LLM\u662f\u9996\u4e2a\u9488\u5bf9\u6570\u636e\u6d41AI\u52a0\u901f\u5668\u7684LLM\u8bad\u7ec3\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u82af\u7247\u5185\u6027\u80fd\u5206\u6790\u548c\u82af\u7247\u95f4\u53ef\u6269\u5c55\u6027\u5206\u6790\uff0c\u5168\u9762\u8bc4\u4f30\u8d44\u6e90\u5206\u914d\u3001\u8d1f\u8f7d\u5e73\u8861\u548c\u8d44\u6e90\u6548\u7387\u7b49\u5173\u952e\u6307\u6807\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4f20\u7edfCPU/GPU\u67b6\u6784\u53d7\u6469\u5c14\u5b9a\u5f8b\u653e\u7f13\u9650\u5236\uff0c\u6570\u636e\u6d41AI\u52a0\u901f\u5668\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u6df1\u5165\u7684\u6027\u80fd\u5206\u6790\u548c\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1DABench-LLM\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7ed3\u5408\u82af\u7247\u5185\u6027\u80fd\u5206\u6790\u548c\u82af\u7247\u95f4\u53ef\u6269\u5c55\u6027\u5206\u6790\uff0c\u652f\u6301\u5bf9Cerebras WSE-2\u3001SambaNova RDU\u548cGraphcore IPU\u7b49\u6570\u636e\u6d41\u52a0\u901f\u5668\u7684\u8bc4\u4f30\u3002", "result": "\u5728\u4e09\u79cd\u5546\u7528\u6570\u636e\u6d41\u52a0\u901f\u5668\u4e0a\u9a8c\u8bc1\u4e86DABench-LLM\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u74f6\u9888\u5e76\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u5728\u4e0d\u540c\u6570\u636e\u6d41AI\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "DABench-LLM\u4e3a\u6570\u636e\u6d41\u52a0\u901f\u5668\u7684LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9996\u4e2a\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5feb\u901f\u7406\u89e3\u5e95\u5c42\u786c\u4ef6\u548c\u7cfb\u7edf\u884c\u4e3a\uff0c\u5e76\u4e3a\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2601.19905", "categories": ["cs.AR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.19905", "abs": "https://arxiv.org/abs/2601.19905", "authors": ["Giulio Filippeschi", "Mirko Brazzini", "Cristhopher Mosquera", "Marco Lanuzza", "Alessandro Catania", "Sebastiano Strangio", "Giuseppe Iannaccone"], "title": "Hardware-Aware Model Design and Training of Silicon-based Analog Neural Networks", "comment": "11 pages, 5 figures", "summary": "Silicon-based analog neural networks physically embody the ideal neural network model in an approximate way. We show that by retraining the neural network using a physics-informed hardware-aware model one can fully recover the inference accuracy of the ideal network model even in the presence of significant non-idealities. This is way more promising for scalability and integration density than the default option of improving the fidelity of the analog neural network at the cost of significant energy, area, and design overhead, through extensive calibration and conservative analog design.\n  We first present a physics-informed hardware-aware model for a time-domain vector-matrix multiplier implemented with single-transistor floating-gate memory cells that explicitly accounts for two dominant non-idealities of the physical implementation - capacitive crosstalk and bit-line voltage drop - and integrates seamlessly with modern deep-learning workflows. The model discretizes each operation into adaptive time slots, processes activation patterns in parallel, and accumulates their contributions to predict effective multiplier outputs. Using measurements from a 16x16 silicon array, we calibrate the model, show that crosstalk is layout-dependent and often dominant, and introduce an improved weight-extraction procedure that doubles signal-to-error ratio versus an ideal vector-matrix multiplier model. Finally, we show that by training silicon-based analog neural networks using an hardware-aware model in the forward pass we can recover the accuracy of the ideal software networks across three architectures -- custom MLP on low-resolution MNIST, LeNet-5 on MNIST, and a VGG-style CNN on CIFAR-10 - establishing a complete design-to-deployment workflow for time-domain analog neuromorphic chips.", "AI": {"tldr": "\u901a\u8fc7\u7269\u7406\u611f\u77e5\u7684\u786c\u4ef6\u611f\u77e5\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u5728\u5b58\u5728\u663e\u8457\u975e\u7406\u60f3\u6027\u7684\u7845\u57fa\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u4e2d\u5b8c\u5168\u6062\u590d\u7406\u60f3\u7f51\u7edc\u6a21\u578b\u7684\u63a8\u7406\u7cbe\u5ea6\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u901a\u8fc7\u6821\u51c6\u548c\u4fdd\u5b88\u8bbe\u8ba1\u5e26\u6765\u7684\u80fd\u8017\u548c\u9762\u79ef\u5f00\u9500\u3002", "motivation": "\u7845\u57fa\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u5728\u7269\u7406\u5b9e\u73b0\u4e2d\u5b58\u5728\u975e\u7406\u60f3\u6027\uff08\u5982\u7535\u5bb9\u4e32\u6270\u548c\u4f4d\u7ebf\u7535\u538b\u964d\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u63d0\u9ad8\u6a21\u62df\u4fdd\u771f\u5ea6\u6765\u5e94\u5bf9\u8fd9\u4e9b\u975e\u7406\u60f3\u6027\uff0c\u4f46\u8fd9\u4f1a\u5e26\u6765\u663e\u8457\u7684\u80fd\u8017\u3001\u9762\u79ef\u548c\u8bbe\u8ba1\u5f00\u9500\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6062\u590d\u63a8\u7406\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7269\u7406\u611f\u77e5\u7684\u786c\u4ef6\u611f\u77e5\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u4f7f\u7528\u5355\u6676\u4f53\u7ba1\u6d6e\u6805\u5b58\u50a8\u5355\u5143\u5b9e\u73b0\u7684\u65f6\u57df\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u5668\u3002\u8be5\u6a21\u578b\u660e\u786e\u8003\u8651\u4e86\u4e24\u79cd\u4e3b\u8981\u7684\u7269\u7406\u975e\u7406\u60f3\u6027\uff1a\u7535\u5bb9\u4e32\u6270\u548c\u4f4d\u7ebf\u7535\u538b\u964d\u3002\u6a21\u578b\u5c06\u6bcf\u4e2a\u64cd\u4f5c\u79bb\u6563\u5316\u4e3a\u81ea\u9002\u5e94\u65f6\u9699\uff0c\u5e76\u884c\u5904\u7406\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5e76\u7d2f\u79ef\u5b83\u4eec\u7684\u8d21\u732e\u6765\u9884\u6d4b\u6709\u6548\u7684\u4e58\u6cd5\u5668\u8f93\u51fa\u3002", "result": "\u4f7f\u752816x16\u7845\u9635\u5217\u7684\u6d4b\u91cf\u6570\u636e\u6821\u51c6\u6a21\u578b\uff0c\u53d1\u73b0\u4e32\u6270\u662f\u5e03\u5c40\u4f9d\u8d56\u7684\u4e14\u901a\u5e38\u662f\u4e3b\u5bfc\u56e0\u7d20\u3002\u6539\u8fdb\u7684\u6743\u91cd\u63d0\u53d6\u7a0b\u5e8f\u5c06\u4fe1\u566a\u6bd4\u76f8\u5bf9\u4e8e\u7406\u60f3\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u5668\u6a21\u578b\u63d0\u9ad8\u4e86\u4e00\u500d\u3002\u901a\u8fc7\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u4f7f\u7528\u786c\u4ef6\u611f\u77e5\u6a21\u578b\u8bad\u7ec3\u7845\u57fa\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u4e09\u79cd\u67b6\u6784\u4e0a\u6062\u590d\u4e86\u7406\u60f3\u8f6f\u4ef6\u7f51\u7edc\u7684\u7cbe\u5ea6\uff1a\u4f4e\u5206\u8fa8\u7387MNIST\u4e0a\u7684\u81ea\u5b9a\u4e49MLP\u3001MNIST\u4e0a\u7684LeNet-5\u4ee5\u53caCIFAR-10\u4e0a\u7684VGG\u98ce\u683cCNN\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u65f6\u57df\u6a21\u62df\u795e\u7ecf\u5f62\u6001\u82af\u7247\u4ece\u8bbe\u8ba1\u5230\u90e8\u7f72\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u5730\u8865\u507f\u7269\u7406\u975e\u7406\u60f3\u6027\uff0c\u4e3a\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u7684\u6269\u5c55\u6027\u548c\u96c6\u6210\u5bc6\u5ea6\u63d0\u4f9b\u4e86\u66f4\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2601.19906", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19906", "abs": "https://arxiv.org/abs/2601.19906", "authors": ["Jingxin Wang", "Shitong Guo", "Ruicheng Dai", "Wenhui Liang", "Ruogu Ding", "Xin Ning", "Weikang Qian"], "title": "GTAC: A Generative Transformer for Approximate Circuits", "comment": null, "summary": "Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce GTAC, a novel generative Transformer-based model for producing approximate circuits. By leveraging principles of approximate computing and AI-driven EDA, our model innovatively integrates error thresholds into the design process. Experimental results show that compared with a state-of-the-art method, GTAC further reduces 6.4% area under the error rate constraint, while being 4.3x faster.", "AI": {"tldr": "GTAC\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fTransformer\u7684\u8fd1\u4f3c\u7535\u8def\u8bbe\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u8bef\u5dee\u9608\u503c\u7ea6\u675f\uff0c\u5728\u8bef\u5dee\u5bb9\u5fcd\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u7535\u8def\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u9762\u79ef\u51cf\u5c116.4%\uff0c\u901f\u5ea6\u63d0\u53474.3\u500d\u3002", "motivation": "\u9488\u5bf9\u8bef\u5dee\u5bb9\u5fcd\u5e94\u7528\uff0c\u8fd1\u4f3c\u7535\u8def\u901a\u8fc7\u5f15\u5165\u53ef\u63a7\u8bef\u5dee\u6765\u663e\u8457\u6539\u5584\u7535\u8def\u7684\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5e73\u8861\u8bef\u5dee\u7ea6\u675f\u4e0ePPA\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u8bbe\u8ba1\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGTAC\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fTransformer\u7684\u8fd1\u4f3c\u7535\u8def\u8bbe\u8ba1\u65b9\u6cd5\u3002\u521b\u65b0\u6027\u5730\u5c06\u8bef\u5dee\u9608\u503c\u96c6\u6210\u5230\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\uff0c\u7ed3\u5408\u8fd1\u4f3c\u8ba1\u7b97\u548cAI\u9a71\u52a8\u7684EDA\u6280\u672f\uff0c\u5b9e\u73b0\u667a\u80fd\u5316\u7684\u7535\u8def\u8fd1\u4f3c\u4f18\u5316\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cGTAC\u5728\u6ee1\u8db3\u8bef\u5dee\u7387\u7ea6\u675f\u7684\u6761\u4ef6\u4e0b\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e866.4%\u7684\u9762\u79ef\uff0c\u540c\u65f6\u8bbe\u8ba1\u901f\u5ea6\u63d0\u5347\u4e864.3\u500d\u3002", "conclusion": "GTAC\u8bc1\u660e\u4e86\u751f\u6210\u5f0fTransformer\u6a21\u578b\u5728\u8fd1\u4f3c\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3aAI\u9a71\u52a8\u7684EDA\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u5728\u8bef\u5dee\u7ea6\u675f\u4e0b\u4f18\u5316\u7535\u8defPPA\u6307\u6807\u3002"}}
{"id": "2601.19907", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19907", "abs": "https://arxiv.org/abs/2601.19907", "authors": ["Yanru Chen", "Zheyu Li", "Keming Fan", "Runyang Tian", "John Hsu", "Weihong Xu", "Minxuan Zhou", "Tajana Rosing"], "title": "RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs", "comment": null, "summary": "All-pairs shortest paths (APSP) remains a major bottleneck for large-scale graph analytics, as data movement with cubic complexity overwhelms the bandwidth of conventional memory hierarchies. In this work, we propose RAPID-Graph to address this challenge through a co-designed processing-in-memory (PIM) system that integrates algorithm, architecture, and device-level optimizations. At the algorithm level, we introduce a recursion-aware partitioner that enables an exact APSP computation by decomposing graphs into vertex tiles to reduce data dependency, such that both Floyd-Warshall and Min-Plus kernels execute fully in-place within digital PIM arrays. At the architecture and device levels, we design a 2.5D PIM stack integrating two phase-change memory compute dies, a logic die, and high-bandwidth scratchpad memory within a unified advanced package. An external non-volatile storage stack stores large APSP results persistently. The design achieves both tile-level and unit-level parallel processing to sustain high throughput. On the 2.45M-node OGBN-Products dataset, RAPID-Graph is 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters, while exceeding prior PIM accelerators by 8.3x in speed and 104x in efficiency. It further delivers up to 42.8x speedup and 392x energy savings over an NVIDIA H100 GPU.", "AI": {"tldr": "RAPID-Graph\uff1a\u4e00\u79cd\u9488\u5bf9\u5168\u5bf9\u6700\u77ed\u8def\u5f84\uff08APSP\uff09\u95ee\u9898\u7684\u5b58\u5185\u8ba1\u7b97\uff08PIM\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u7b97\u6cd5\u3001\u67b6\u6784\u548c\u5668\u4ef6\u7ea7\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u5927\u578b\u56fe\u5206\u6790\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u548c\u80fd\u6548\u6539\u8fdb\u3002", "motivation": "\u5168\u5bf9\u6700\u77ed\u8def\u5f84\uff08APSP\uff09\u5728\u5927\u89c4\u6a21\u56fe\u5206\u6790\u4e2d\u9762\u4e34\u6570\u636e\u79fb\u52a8\u7684\u7acb\u65b9\u590d\u6742\u5ea6\u74f6\u9888\uff0c\u4f20\u7edf\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u7684\u5e26\u5bbd\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "1. \u7b97\u6cd5\u5c42\u9762\uff1a\u5f15\u5165\u9012\u5f52\u611f\u77e5\u5206\u533a\u5668\uff0c\u5c06\u56fe\u5206\u89e3\u4e3a\u9876\u70b9\u74e6\u7247\u4ee5\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\u6027\uff0c\u4f7fFloyd-Warshall\u548cMin-Plus\u5185\u6838\u5b8c\u5168\u5728\u6570\u5b57PIM\u9635\u5217\u4e2d\u539f\u5730\u6267\u884c\n2. \u67b6\u6784\u5c42\u9762\uff1a\u8bbe\u8ba12.5D PIM\u5806\u6808\uff0c\u96c6\u6210\u4e24\u4e2a\u76f8\u53d8\u5185\u5b58\u8ba1\u7b97\u82af\u7247\u3001\u4e00\u4e2a\u903b\u8f91\u82af\u7247\u548c\u9ad8\u5e26\u5bbd\u6682\u5b58\u5185\u5b58\n3. \u5668\u4ef6\u5c42\u9762\uff1a\u5916\u90e8\u975e\u6613\u5931\u6027\u5b58\u50a8\u5806\u6808\u6301\u4e45\u5b58\u50a8\u5927\u578bAPSP\u7ed3\u679c\uff0c\u652f\u6301\u74e6\u7247\u7ea7\u548c\u5355\u5143\u7ea7\u5e76\u884c\u5904\u7406", "result": "\u57282.45M\u8282\u70b9\u7684OGBN-Products\u6570\u636e\u96c6\u4e0a\uff1a\u6bd4\u6700\u5148\u8fdb\u7684GPU\u96c6\u7fa4\u5feb5.8\u500d\u3001\u80fd\u6548\u9ad81,186\u500d\uff1b\u6bd4\u73b0\u6709PIM\u52a0\u901f\u5668\u5feb8.3\u500d\u3001\u80fd\u6548\u9ad8104\u500d\uff1b\u6bd4NVIDIA H100 GPU\u5feb42.8\u500d\u3001\u8282\u80fd392\u500d", "conclusion": "RAPID-Graph\u901a\u8fc7\u7b97\u6cd5\u3001\u67b6\u6784\u548c\u5668\u4ef6\u7ea7\u7684\u534f\u540c\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86APSP\u5728\u5927\u89c4\u6a21\u56fe\u5206\u6790\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u5b58\u5185\u8ba1\u7b97\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u80fd\u6548\u3002"}}
{"id": "2601.19908", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19908", "abs": "https://arxiv.org/abs/2601.19908", "authors": ["Yanru Chen", "Runyang Tian", "Yue Pan", "Zheyu Li", "Weihong Xu", "Tajana Rosing"], "title": "CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference", "comment": null, "summary": "The proliferation of large language models (LLMs) is accelerating the integration of multimodal assistants into edge devices, where inference is executed under stringent latency and energy constraints, often exacerbated by intermittent connectivity. These challenges become particularly acute in the context of multimodal LLMs (MLLMs), as high-dimensional visual inputs are transformed into extensive token sequences, thereby inflating the key-value (KV) cache and imposing substantial data movement overheads to the LLM backbone. To address these issues, we present CHIME, a chiplet-based heterogeneous near-memory acceleration for edge MLLMs inference. CHIME leverages the complementary strengths of integrated monolithic 3D (M3D) DRAM and RRAM chiplets: DRAM supplies low-latency bandwidth for attention, while RRAM offers dense, non-volatile storage for weights. This heterogeneous hardware is orchestrated by a co-designed mapping framework that executes fused kernels near data, minimizing cross-chiplet traffic to maximize effective bandwidth. On FastVLM (0.6B/1.7B) and MobileVLM (1.7B/3B), CHIME achieves up to 54x speedup and up to 246x better energy efficiency per inference as compared to the edge GPU NVIDIA Jetson Orin NX. It sustains 116.5-266.5 token/J compared to Jetson's 0.7-1.1 token/J. Furthermore, it delivers up to 69.2x higher throughput than the state-of-the-art PIM accelerator FACIL. Compared to the M3D DRAM-only design, CHIME's heterogeneous memory further improves energy efficiency by 7% and performance by 2.4x.", "AI": {"tldr": "CHIME\uff1a\u57fa\u4e8eChiplet\u7684\u5f02\u6784\u8fd1\u5185\u5b58\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u8fb9\u7f18\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u901a\u8fc7\u7ed3\u5408M3D DRAM\u548cRRAM\u82af\u7247\uff0c\u5b9e\u73b0\u9ad8\u8fbe54\u500d\u52a0\u901f\u548c246\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9762\u4e34\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u8fde\u63a5\u4e0d\u7a33\u5b9a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u89c6\u89c9\u8f93\u5165\u8f6c\u5316\u4e3a\u5927\u91cftoken\u5e8f\u5217\u4f1a\u663e\u8457\u589e\u52a0KV\u7f13\u5b58\u548c\u6570\u636e\u79fb\u52a8\u5f00\u9500\u3002", "method": "\u91c7\u7528\u57fa\u4e8eChiplet\u7684\u5f02\u6784\u8fd1\u5185\u5b58\u52a0\u901f\u67b6\u6784\uff0c\u7ed3\u5408M3D DRAM\uff08\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u5e26\u5bbd\u7528\u4e8e\u6ce8\u610f\u529b\u8ba1\u7b97\uff09\u548cRRAM\uff08\u63d0\u4f9b\u9ad8\u5bc6\u5ea6\u975e\u6613\u5931\u6027\u6743\u91cd\u5b58\u50a8\uff09\uff0c\u5e76\u8bbe\u8ba1\u534f\u540c\u6620\u5c04\u6846\u67b6\u6267\u884c\u8fd1\u6570\u636e\u878d\u5408\u5185\u6838\u3002", "result": "\u5728FastVLM\u548cMobileVLM\u6a21\u578b\u4e0a\uff0c\u76f8\u6bd4NVIDIA Jetson Orin NX\u5b9e\u73b0\u9ad8\u8fbe54\u500d\u52a0\u901f\u548c246\u500d\u80fd\u6548\u63d0\u5347\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684PIM\u52a0\u901f\u5668FACIL\u5b9e\u73b069.2\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "CHIME\u901a\u8fc7\u5f02\u6784\u5185\u5b58\u67b6\u6784\u548c\u8fd1\u6570\u636e\u8ba1\u7b97\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18MLLM\u63a8\u7406\u7684\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4e3a\u8fb9\u7f18AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.19910", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19910", "abs": "https://arxiv.org/abs/2601.19910", "authors": ["William Meng", "Benjamin Lee", "Hong Wang"], "title": "Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading", "comment": "Submitted to MLSys 2026", "summary": "KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks. In this paper, we develops an analytical framework that derives $\u03ba_{\\text{crit}}$, the critical cached-to-prefill token ratio where execution becomes memory-bound and show typical workloads exceed this threshold by orders of magnitude. Empirical characterization reveals 99\\% of latency spent on transfers and serving offloaded requests results in GPU's consuming only 28\\% of their rated TDP, motivating our proposed optimizations for hardware interconnects, model architectures, and scheduling algorithms.", "AI": {"tldr": "KV\u7f13\u5b58\u5378\u8f7d\u901a\u8fc7\u5c06\u7f13\u5b58\u5b58\u50a8\u5728CPU DRAM\u4e2d\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\uff0c\u4f46PCIe\u5e26\u5bbd\u9650\u5236\u9020\u6210\u4e25\u91cd\u74f6\u9888\u3002\u672c\u6587\u5f00\u53d1\u5206\u6790\u6846\u67b6\u63a8\u5bfc\u4e34\u754c\u7f13\u5b58-\u9884\u586b\u5145\u4ee4\u724c\u6bd4\u03ba_crit\uff0c\u53d1\u73b0\u5178\u578b\u5de5\u4f5c\u8d1f\u8f7d\u8fdc\u8d85\u6b64\u9608\u503c\uff0c99%\u5ef6\u8fdf\u7528\u4e8e\u6570\u636e\u4f20\u8f93\uff0cGPU\u4ec5\u4f7f\u752828%\u989d\u5b9aTDP\uff0c\u63d0\u51fa\u786c\u4ef6\u4e92\u8fde\u3001\u6a21\u578b\u67b6\u6784\u548c\u8c03\u5ea6\u7b97\u6cd5\u4f18\u5316\u3002", "motivation": "KV\u7f13\u5b58\u5378\u8f7d\u6280\u672f\u867d\u7136\u80fd\u652f\u6301\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\uff0c\u4f46\u53d7\u9650\u4e8ePCIe\u5e26\u5bbd\u74f6\u9888\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6027\u80fd\u95ee\u9898\u3002\u5f53\u524d\u7cfb\u7edf\u5728\u6570\u636e\u4f20\u8f93\u4e0a\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\uff0cGPU\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u4f18\u5316\u3002", "method": "\u5f00\u53d1\u5206\u6790\u6846\u67b6\u63a8\u5bfc\u4e34\u754c\u7f13\u5b58-\u9884\u586b\u5145\u4ee4\u724c\u6bd4\u03ba_crit\uff0c\u901a\u8fc7\u7ecf\u9a8c\u6027\u8868\u5f81\u5206\u6790\u7cfb\u7edf\u6027\u80fd\u74f6\u9888\uff0c\u6d4b\u91cf\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u548cGPU\u529f\u8017\u5229\u7528\u7387\u3002", "result": "\u53d1\u73b0\u5178\u578b\u5de5\u4f5c\u8d1f\u8f7d\u8fdc\u8d85\u4e34\u754c\u9608\u503c\uff0c99%\u7684\u5ef6\u8fdf\u7528\u4e8ePCIe\u6570\u636e\u4f20\u8f93\uff0cGPU\u4ec5\u6d88\u801728%\u989d\u5b9aTDP\uff0c\u8868\u660e\u7cfb\u7edf\u4e25\u91cd\u53d7\u9650\u4e8e\u5185\u5b58\u5e26\u5bbd\u800c\u975e\u8ba1\u7b97\u80fd\u529b\u3002", "conclusion": "\u9700\u8981\u4ece\u786c\u4ef6\u4e92\u8fde\uff08\u5982\u66f4\u9ad8\u5e26\u5bbd\u8fde\u63a5\uff09\u3001\u6a21\u578b\u67b6\u6784\uff08\u51cf\u5c11\u7f13\u5b58\u9700\u6c42\uff09\u548c\u8c03\u5ea6\u7b97\u6cd5\uff08\u4f18\u5316\u6570\u636e\u4f20\u8f93\uff09\u4e09\u4e2a\u5c42\u9762\u8fdb\u884c\u7cfb\u7edf\u6027\u4f18\u5316\uff0c\u4ee5\u89e3\u51b3KV\u7f13\u5b58\u5378\u8f7d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2601.19911", "categories": ["cs.AR", "cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19911", "abs": "https://arxiv.org/abs/2601.19911", "authors": ["Ilsun Chang"], "title": "GPU-Augmented OLAP Execution Engine: GPU Offloading", "comment": "4 pages, figures included. PostgreSQL microbenchmarks and GPU proxy measurements (RTX 4060 Laptop GPU). Extends arXiv:2512.19750 to execution-layer OLAP primitives", "summary": "Modern OLAP systems have mitigated I/O bottlenecks via storage-compute separation and columnar layouts, but CPU costs in the execution layer (especially Top-K selection and join probe) are emerging as new bottlenecks at scale. This paper proposes a hybrid architecture that augments existing vectorized execution by selectively offloading only high-impact primitives to the GPU. To reduce data movement, we use key-only transfer (keys and pointers) with late materialization. We further introduce a Risky Gate (risk-aware gating) that triggers offloading only in gain/risk intervals based on input size, transfer, kernel and post-processing costs, and candidate-set complexity (K, M). Using PostgreSQL microbenchmarks and GPU proxy measurements, we observe improved tail latency (P95/P99) under gated offloading compared to always-on GPU offloading. This work extends the risk-aware gating principle used for optimizer-stage GPU-assisted measurement (arXiv:2512.19750) to execution-layer OLAP primitives.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u67b6\u6784\uff0c\u5728\u5411\u91cf\u5316\u6267\u884c\u57fa\u7840\u4e0a\u9009\u62e9\u6027\u5c06\u9ad8\u5f71\u54cd\u539f\u8bed\u5378\u8f7d\u5230GPU\uff0c\u901a\u8fc7\u952e\u503c\u4f20\u8f93\u548c\u98ce\u9669\u611f\u77e5\u95e8\u63a7\u673a\u5236\u4f18\u5316OLAP\u7cfb\u7edf\u6027\u80fd", "motivation": "\u73b0\u4ee3OLAP\u7cfb\u7edf\u901a\u8fc7\u5b58\u50a8\u8ba1\u7b97\u5206\u79bb\u548c\u5217\u5f0f\u5e03\u5c40\u7f13\u89e3\u4e86I/O\u74f6\u9888\uff0c\u4f46\u6267\u884c\u5c42\u7684CPU\u6210\u672c\uff08\u7279\u522b\u662fTop-K\u9009\u62e9\u548c\u8fde\u63a5\u63a2\u6d4b\uff09\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u6210\u4e3a\u65b0\u7684\u74f6\u9888", "method": "1) \u6df7\u5408\u67b6\u6784\uff1a\u5728\u73b0\u6709\u5411\u91cf\u5316\u6267\u884c\u57fa\u7840\u4e0a\u9009\u62e9\u6027\u5378\u8f7d\u9ad8\u5f71\u54cd\u539f\u8bed\u5230GPU\uff1b2) \u952e\u503c\u4f20\u8f93\uff1a\u4ec5\u4f20\u8f93\u952e\u548c\u6307\u9488\uff0c\u5ef6\u8fdf\u7269\u5316\u51cf\u5c11\u6570\u636e\u79fb\u52a8\uff1b3) \u98ce\u9669\u611f\u77e5\u95e8\u63a7\uff1a\u57fa\u4e8e\u8f93\u5165\u5927\u5c0f\u3001\u4f20\u8f93\u6210\u672c\u3001\u5185\u6838\u6210\u672c\u3001\u540e\u5904\u7406\u6210\u672c\u548c\u5019\u9009\u96c6\u590d\u6742\u5ea6(K,M)\u52a8\u6001\u89e6\u53d1\u5378\u8f7d", "result": "\u4f7f\u7528PostgreSQL\u5fae\u57fa\u51c6\u6d4b\u8bd5\u548cGPU\u4ee3\u7406\u6d4b\u91cf\uff0c\u76f8\u6bd4\u59cb\u7ec8\u5f00\u542f\u7684GPU\u5378\u8f7d\uff0c\u95e8\u63a7\u5378\u8f7d\u5728\u5c3e\u90e8\u5ef6\u8fdf(P95/P99)\u65b9\u9762\u8868\u73b0\u66f4\u4f18", "conclusion": "\u5c06\u98ce\u9669\u611f\u77e5\u95e8\u63a7\u539f\u5219\u4ece\u4f18\u5316\u5668\u9636\u6bb5\u7684GPU\u8f85\u52a9\u6d4b\u91cf\u6269\u5c55\u5230\u6267\u884c\u5c42\u7684OLAP\u539f\u8bed\uff0c\u4e3a\u5927\u89c4\u6a21OLAP\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u7684CPU-GPU\u6df7\u5408\u6267\u884c\u65b9\u6848"}}
{"id": "2601.19912", "categories": ["cs.AR", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.19912", "abs": "https://arxiv.org/abs/2601.19912", "authors": ["Duo Chai", "Zizhen Liu", "Shuhuai Wang", "Songwei Pei", "Cheng Liu", "Huawei Li", "Shangguang Wang"], "title": "Analysis of LLM Vulnerability to GPU Soft Errors: An Instruction-Level Fault Injection Study", "comment": "14 pages, 13 figures", "summary": "Large language models (LLMs) are highly compute- and memory-intensive, posing significant demands on high-performance GPUs. At the same time, advances in GPU technology driven by shrinking transistor sizes and lower operating voltages have made these devices increasingly susceptible to soft errors. While prior work has examined GPU reliability, most studies have focused on general-purpose applications or conventional neural networks mostly used for vision tasks such as classification and detection. In contrast, systematic analysis of modern large-scale LLMs remains limited, despite their rapid adoption in diverse application scenarios. Given the unique characteristics of LLMs, their resilience to soft errors may differ substantially from earlier models. To bridge this gap, we conduct the first instruction-level fault injection study of LLM inference. Our approach reveals reliability characteristics from multiple perspectives, highlighting the effects of model architecture, parameter scale, and task complexity. These findings provide new insights into LLM reliability and inform the design of more effective fault tolerance mechanisms.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fdb\u884c\u6307\u4ee4\u7ea7\u6545\u969c\u6ce8\u5165\u7814\u7a76\uff0c\u5206\u6790\u6a21\u578b\u67b6\u6784\u3001\u53c2\u6570\u89c4\u6a21\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u5bf9\u53ef\u9760\u6027\u7684\u5f71\u54cd", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9GPU\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\uff0c\u800cGPU\u6280\u672f\u53d1\u5c55\u4f7f\u5176\u66f4\u6613\u53d7\u8f6f\u9519\u8bef\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u5e94\u7528\u6216\u4f20\u7edf\u89c6\u89c9\u795e\u7ecf\u7f51\u7edc\uff0c\u7f3a\u4e4f\u5bf9\u73b0\u4ee3\u5927\u89c4\u6a21LLM\u7684\u7cfb\u7edf\u6027\u53ef\u9760\u6027\u5206\u6790", "method": "\u91c7\u7528\u6307\u4ee4\u7ea7\u6545\u969c\u6ce8\u5165\u7814\u7a76\u65b9\u6cd5\uff0c\u4ece\u591a\u4e2a\u89d2\u5ea6\u5206\u6790LLM\u63a8\u7406\u7684\u53ef\u9760\u6027\u7279\u5f81", "result": "\u63ed\u793a\u4e86\u6a21\u578b\u67b6\u6784\u3001\u53c2\u6570\u89c4\u6a21\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u5bf9LLM\u53ef\u9760\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u7406\u89e3LLM\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3", "conclusion": "LLM\u5bf9\u8f6f\u9519\u8bef\u7684\u6062\u590d\u80fd\u529b\u53ef\u80fd\u4e0e\u65e9\u671f\u6a21\u578b\u6709\u663e\u8457\u5dee\u5f02\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u5bb9\u9519\u673a\u5236\u63d0\u4f9b\u4e86\u4f9d\u636e"}}
{"id": "2601.19920", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19920", "abs": "https://arxiv.org/abs/2601.19920", "authors": ["Yuval Harary", "Almog Sharoni", "Esteban Garz\u00f3n", "Marco Lanuzza", "Adam Teman", "Leonid Yavits"], "title": "PiC-BNN: A 128-kbit 65 nm Processing-in-CAM-Based End-to-End Binary Neural Network Accelerator", "comment": "7 pages, 6 figures. Accepted to IEEE CCMCC 2025", "summary": "Binary Neural Networks (BNNs), where weights and activations are constrained to binary values (+1, -1), are a highly efficient alternative to traditional neural networks. Unfortunately, typical BNNs, while binarizing linear layers (matrix-vector multiplication), still implement other network layers (batch normalization, softmax, output layer, and sometimes the input layer of a convolutional neural network) in full precision. This limits the area and energy benefits and requires architectural support for full precision operations. We propose PiC-BNN, a true end-to-end binary in-approximate search (Hamming distance tolerant) Content Addressable Memory based BNN accelerator. PiC-BNN is designed and manufactured in a commercial 65nm process. PiC-BNN uses Hamming distance tolerance to apply the law of large numbers to enable accurate classification without implementing full precision operations. PiC-BNN achieves baseline software accuracy (95.2%) on the MNIST dataset and 93.5% on the Hand Gesture (HG) dataset, a throughput of 560K inferences/s, and presents a power efficiency of 703M inferences/s/W when implementing a binary MLP model for MNIST/HG dataset classification.", "AI": {"tldr": "PiC-BNN\u662f\u4e00\u4e2a\u771f\u6b63\u7684\u7aef\u5230\u7aef\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\uff0c\u4f7f\u7528\u6c49\u660e\u8ddd\u79bb\u5bb9\u9519\u7684\u5185\u5bb9\u5bfb\u5740\u5b58\u50a8\u5668\uff0c\u65e0\u9700\u5168\u7cbe\u5ea6\u8fd0\u7b97\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edfBNN\u867d\u7136\u5bf9\u7ebf\u6027\u5c42\u8fdb\u884c\u4e8c\u503c\u5316\uff0c\u4f46\u4ecd\u9700\u5728\u6279\u5f52\u4e00\u5316\u3001softmax\u7b49\u5c42\u4f7f\u7528\u5168\u7cbe\u5ea6\u8fd0\u7b97\uff0c\u9650\u5236\u4e86\u9762\u79ef\u548c\u80fd\u6548\u4f18\u52bf\u3002\u9700\u8981\u8bbe\u8ba1\u771f\u6b63\u7684\u7aef\u5230\u7aef\u4e8c\u8fdb\u5236\u52a0\u901f\u5668\u3002", "method": "\u63d0\u51faPiC-BNN\u52a0\u901f\u5668\uff0c\u57fa\u4e8e\u6c49\u660e\u8ddd\u79bb\u5bb9\u9519\u7684\u5185\u5bb9\u5bfb\u5740\u5b58\u50a8\u5668\uff0c\u5229\u7528\u5927\u6570\u5b9a\u5f8b\u5b9e\u73b0\u51c6\u786e\u5206\u7c7b\uff0c\u65e0\u9700\u5168\u7cbe\u5ea6\u8fd0\u7b97\u3002\u91c7\u752865nm\u5de5\u827a\u5236\u9020\u3002", "result": "\u5728MNIST\u6570\u636e\u96c6\u4e0a\u8fbe\u523095.2%\u51c6\u786e\u7387\uff0c\u5728\u624b\u52bf\u6570\u636e\u96c6\u4e0a\u8fbe\u523093.5%\u51c6\u786e\u7387\uff0c\u541e\u5410\u91cf560K\u63a8\u7406/\u79d2\uff0c\u80fd\u6548703M\u63a8\u7406/\u79d2/\u74e6\u3002", "conclusion": "PiC-BNN\u8bc1\u660e\u4e86\u771f\u6b63\u7684\u7aef\u5230\u7aef\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6c49\u660e\u8ddd\u79bb\u5bb9\u9519\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u548c\u9ad8\u541e\u5410\u91cf\uff0c\u65e0\u9700\u5168\u7cbe\u5ea6\u8fd0\u7b97\u652f\u6301\u3002"}}
{"id": "2601.19941", "categories": ["cs.AR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19941", "abs": "https://arxiv.org/abs/2601.19941", "authors": ["M Zafir Sadik Khan", "Kimia Azar", "Hadi Kamali"], "title": "Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation", "comment": "Accepted to the Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "In last two years, large language models (LLMs) have shown strong capabilities in code generation, including hardware design at register-transfer level (RTL). While their use in high-level synthesis (HLS) remains comparatively less mature, the ratio of HLS- to RTL-focused studies has shifted from 1:10 to 2:10 in the past six months, indicating growing interest in leveraging LLMs for high-level design entry while relying on downstream synthesis for optimization. This growing trend highlights the need for a comprehensive benchmarking and evaluation framework dedicated to LLM-based HLS. To address this, We present Bench4HLS for evaluating LLM-generated HLS designs. Bench4HLS comprises 170 manually drafted and validated case studies, spanning small kernels to complex accelerators, curated from widely used public repositories. The framework supports fully automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility/optimization. Crucially, Bench4HLS integrates a pluggable API for power, performance, and area (PPA) analysis across various HLS toolchains and architectures, demonstrated here with Xilinx Vitis HLS and validated on Catapult HLS. By providing a structured, extensible, and plug-and-play testbed, Bench4HLS establishes a foundational methodology for benchmarking LLMs in HLS workflows.", "AI": {"tldr": "Bench4HLS\uff1a\u9996\u4e2a\u9488\u5bf9LLM\u751f\u6210HLS\u8bbe\u8ba1\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b170\u4e2a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u652f\u6301\u81ea\u52a8\u5316\u7f16\u8bd1\u3001\u529f\u80fd\u9a8c\u8bc1\u548cPPA\u5206\u6790\u3002", "motivation": "\u968f\u7740LLM\u5728RTL\u8bbe\u8ba1\u4e2d\u7684\u6210\u529f\u5e94\u7528\uff0cHLS\u9886\u57df\u7684LLM\u7814\u7a76\u6b63\u5728\u5feb\u901f\u589e\u957f\uff08HLS:RTL\u7814\u7a76\u6bd4\u4f8b\u4ece1:10\u63d0\u5347\u52302:10\uff09\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u7cfb\u7edf\u8bc4\u4f30LLM\u751f\u6210\u7684HLS\u8bbe\u8ba1\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u5305\u542b170\u4e2a\u624b\u52a8\u7f16\u5199\u548c\u9a8c\u8bc1\u7684\u6d4b\u8bd5\u6848\u4f8b\u96c6\uff0c\u6db5\u76d6\u4ece\u7b80\u5355\u5185\u6838\u5230\u590d\u6742\u52a0\u901f\u5668\u7684\u5404\u79cd\u8bbe\u8ba1\u3002\u63d0\u4f9b\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5305\u62ec\u7f16\u8bd1\u6210\u529f\u7387\u3001\u529f\u80fd\u6b63\u786e\u6027\u9a8c\u8bc1\u548c\u7efc\u5408\u53ef\u884c\u6027\u5206\u6790\u3002\u5173\u952e\u521b\u65b0\u662f\u96c6\u6210\u4e86\u53ef\u63d2\u62d4API\uff0c\u652f\u6301\u8de8\u4e0d\u540cHLS\u5de5\u5177\u94fe\u548c\u67b6\u6784\u7684PPA\u5206\u6790\u3002", "result": "\u5f00\u53d1\u4e86Bench4HLS\u6846\u67b6\uff0c\u5df2\u5728Xilinx Vitis HLS\u548cCatapult HLS\u4e0a\u9a8c\u8bc1\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u6269\u5c55\u7684\u5373\u63d2\u5373\u7528\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e3aLLM\u5728HLS\u5de5\u4f5c\u6d41\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\u5efa\u7acb\u4e86\u57fa\u7840\u65b9\u6cd5\u8bba\u3002", "conclusion": "Bench4HLS\u586b\u8865\u4e86LLM\u5728HLS\u9886\u57df\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5c06\u4fc3\u8fdbLLM\u5728\u9ad8\u7ea7\u7efc\u5408\u4e2d\u7684\u7814\u7a76\u548c\u5e94\u7528\u53d1\u5c55\uff0c\u652f\u6301\u8de8\u5de5\u5177\u94fe\u7684\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef\u5206\u6790\u3002"}}
{"id": "2601.20061", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20061", "abs": "https://arxiv.org/abs/2601.20061", "authors": ["Dhruv Parikh", "Jebacyril Arockiaraj", "Viktor Prasanna"], "title": "Primitive-Driven Acceleration of Hyperdimensional Computing for Real-Time Image Classification", "comment": null, "summary": "Hyperdimensional Computing (HDC) represents data using extremely high-dimensional, low-precision vectors, termed hypervectors (HVs), and performs learning and inference through lightweight, noise-tolerant operations. However, the high dimensionality, sparsity, and repeated data movement involved in HDC make these computations difficult to accelerate efficiently on conventional processors. As a result, executing core HDC operations: binding, permutation, bundling, and similarity search: on CPUs or GPUs often leads to suboptimal utilization, memory bottlenecks, and limits on real-time performance. In this paper, our contributions are two-fold. First, we develop an image-encoding algorithm that, similar in spirit to convolutional neural networks, maps local image patches to hypervectors enriched with spatial information. These patch-level hypervectors are then merged into a global representation using the fundamental HDC operations, enabling spatially sensitive and robust image encoding. This encoder achieves 95.67% accuracy on MNIST and 85.14% on Fashion-MNIST, outperforming prior HDC-based image encoders. Second, we design an end-to-end accelerator that implements these compute operations on an FPGA through a pipelined architecture that exploits parallelism both across the hypervector dimensionality and across the set of image patches. Our Alveo U280 implementation delivers 0.09ms inference latency, achieving up to 1300x and 60x speedup over state-of-the-art CPU and GPU baselines, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8d85\u7ef4\u8ba1\u7b97\uff08HDC\uff09\u7684\u65b0\u578b\u56fe\u50cf\u7f16\u7801\u7b97\u6cd5\u548cFPGA\u52a0\u901f\u5668\uff0c\u5728MNIST\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u8d85\u7ef4\u8ba1\u7b97\u4f7f\u7528\u9ad8\u7ef4\u4f4e\u7cbe\u5ea6\u5411\u91cf\u8868\u793a\u6570\u636e\uff0c\u5177\u6709\u8f7b\u91cf\u7ea7\u548c\u566a\u58f0\u5bb9\u5fcd\u6027\uff0c\u4f46\u5728\u4f20\u7edfCPU/GPU\u4e0a\u6267\u884c\u6838\u5fc3\u64cd\u4f5c\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u5185\u5b58\u74f6\u9888\u548c\u5b9e\u65f6\u6027\u80fd\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "1. \u5f00\u53d1\u7c7b\u4f3c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u50cf\u7f16\u7801\u7b97\u6cd5\uff0c\u5c06\u5c40\u90e8\u56fe\u50cf\u5757\u6620\u5c04\u5230\u5bcc\u542b\u7a7a\u95f4\u4fe1\u606f\u7684\u8d85\u5411\u91cf\uff0c\u7136\u540e\u901a\u8fc7HDC\u57fa\u672c\u64cd\u4f5c\u5408\u5e76\u4e3a\u5168\u5c40\u8868\u793a\uff1b2. \u8bbe\u8ba1\u7aef\u5230\u7aefFPGA\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u6d41\u6c34\u7ebf\u67b6\u6784\u5728\u8d85\u5411\u91cf\u7ef4\u5ea6\u548c\u56fe\u50cf\u5757\u96c6\u5408\u4e0a\u5e76\u884c\u6267\u884c\u8ba1\u7b97\u64cd\u4f5c\u3002", "result": "\u7f16\u7801\u5668\u5728MNIST\u4e0a\u8fbe\u523095.67%\u51c6\u786e\u7387\uff0c\u5728Fashion-MNIST\u4e0a\u8fbe\u523085.14%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709HDC\u56fe\u50cf\u7f16\u7801\u5668\u3002Alveo U280 FPGA\u5b9e\u73b00.09ms\u63a8\u7406\u5ef6\u8fdf\uff0c\u6bd4\u6700\u4f18CPU\u548cGPU\u57fa\u7ebf\u5206\u522b\u52a0\u901f1300\u500d\u548c60\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684HDC\u56fe\u50cf\u7f16\u7801\u7b97\u6cd5\u548cFPGA\u52a0\u901f\u5668\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5904\u7406\u5668\u4e0a\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u8d85\u7ef4\u8ba1\u7b97\u5728\u56fe\u50cf\u5904\u7406\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.20067", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.20067", "abs": "https://arxiv.org/abs/2601.20067", "authors": ["Maxwell Phillips", "Firas Hassan", "Ahmed Ammar"], "title": "A Paradigm for Generalized Multi-Level Priority Encoders", "comment": "17 pages, 21 figures", "summary": "Priority encoders are typically considered expensive hardware components in terms of complexity, especially at high bit precisions or input lengths (e.g., above 512 bits). However, if the complexity can be reduced, priority encoders can feasibly accelerate a variety of key applications, such as high-precision integer arithmetic and content-addressable memory. We propose a new paradigm for constructing priority encoders by generalizing the previously proposed two-level priority encoder structure. We extend this concept to three and four levels using two techniques -- cascading and composition -- and discuss further generalization. We then analyze the complexity and delay of new and existing priority encoder designs as a function of input length, for both FPGA and ASIC implementation technologies. In particular, we compare the multi-level structure to the traditional single-level priority encoder structure, a tree-based design, a recursive design, and the two-level structure. We find that the two-level architecture provides balanced performance -- reducing complexity by around half, but at the cost of a corresponding increase in delay. Additional levels have diminishing returns, highlighting a tradeoff between complexity and delay. Meanwhile, the tree and recursive designs are generally faster, but are more complex than the two-level and multi-level structures. We explore several characteristics and patterns of the designs across a wide range of input lengths. We then provide recommendations on which architecture to use for a given input length and implementation technology, based on which design factors -- such as complexity or delay -- are most important to the hardware designer. With this overview and analysis of various priority encoder architectures, we provide a priority encoder toolkit to assist hardware designers in creating the most optimal design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7ea7\u4f18\u5148\u7f16\u7801\u5668\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u7ea7\u8054\u548c\u7ec4\u5408\u6280\u672f\u5c06\u4e24\u7ea7\u7ed3\u6784\u6269\u5c55\u5230\u4e09\u3001\u56db\u7ea7\uff0c\u5206\u6790\u4e86\u5404\u79cd\u67b6\u6784\u5728\u590d\u6742\u5ea6\u548c\u5ef6\u8fdf\u4e0a\u7684\u6743\u8861\uff0c\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u9009\u62e9\u6307\u5357\u3002", "motivation": "\u4f20\u7edf\u4f18\u5148\u7f16\u7801\u5668\u5728\u9ad8\u6bd4\u7279\u7cbe\u5ea6\uff08\u5982512\u4f4d\u4ee5\u4e0a\uff09\u65f6\u786c\u4ef6\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u5e94\u7528\uff08\u5982\u9ad8\u7cbe\u5ea6\u6574\u6570\u8fd0\u7b97\u548c\u5185\u5bb9\u53ef\u5bfb\u5740\u5b58\u50a8\u5668\uff09\u4e2d\u7684\u4f7f\u7528\u3002\u9700\u8981\u964d\u4f4e\u590d\u6742\u5ea6\u4ee5\u52a0\u901f\u8fd9\u4e9b\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u591a\u7ea7\u4f18\u5148\u7f16\u7801\u5668\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5c06\u5df2\u6709\u7684\u4e24\u7ea7\u7ed3\u6784\u901a\u8fc7\u7ea7\u8054\u548c\u7ec4\u5408\u6280\u672f\u6269\u5c55\u5230\u4e09\u3001\u56db\u7ea7\uff0c\u5e76\u4e0e\u4f20\u7edf\u5355\u7ea7\u3001\u6811\u5f62\u3001\u9012\u5f52\u7b49\u8bbe\u8ba1\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u4e24\u7ea7\u67b6\u6784\u5728\u590d\u6742\u5ea6\u548c\u5ef6\u8fdf\u95f4\u63d0\u4f9b\u6700\u4f73\u5e73\u8861\uff08\u590d\u6742\u5ea6\u51cf\u534a\u4f46\u5ef6\u8fdf\u76f8\u5e94\u589e\u52a0\uff09\uff0c\u66f4\u591a\u5c42\u7ea7\u6536\u76ca\u9012\u51cf\u3002\u6811\u5f62\u548c\u9012\u5f52\u8bbe\u8ba1\u66f4\u5feb\u4f46\u66f4\u590d\u6742\u3002\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8f93\u5165\u957f\u5ea6\u548c\u5b9e\u73b0\u6280\u672f\u7684\u8bbe\u8ba1\u9009\u62e9\u5efa\u8bae\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u5404\u79cd\u4f18\u5148\u7f16\u7801\u5668\u67b6\u6784\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u4f18\u5148\u7f16\u7801\u5668\u5de5\u5177\u5305\uff0c\u5e2e\u52a9\u6839\u636e\u5177\u4f53\u9700\u6c42\uff08\u590d\u6742\u5ea6\u6216\u5ef6\u8fdf\u4f18\u5148\uff09\u9009\u62e9\u6700\u4f18\u8bbe\u8ba1\u3002"}}
{"id": "2601.20115", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20115", "abs": "https://arxiv.org/abs/2601.20115", "authors": ["Emanuele Del Sozzo", "Martin Fleming", "Kenneth Flamm", "Neil Thompson"], "title": "How Much Progress Has There Been in NVIDIA Datacenter GPUs?", "comment": null, "summary": "Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performance demands, analyzing past and current progress becomes paramount in determining future constraints on scientific research. This is particularly compelling in the AI domain, where rapid technological advancements and fierce global competition have led the United States to recently implement export control regulations limiting international access to advanced AI chips. For this reason, this paper studies technical progress in NVIDIA datacenter GPUs released from the mid-2000s until today. Specifically, we compile a comprehensive dataset of datacenter NVIDIA GPUs comprising several features, ranging from computational performance to release price. Then, we examine trends in main GPU features and estimate progress indicators for per-memory bandwidth, per-dollar, and per-watt increase rates. Our main results identify doubling times of 1.44 and 1.69 years for FP16 and FP32 operations (without accounting for sparsity benefits), while FP64 doubling times range from 2.06 to 3.79 years. Off-chip memory size and bandwidth grew at slower rates than computing performance, doubling every 3.32 to 3.53 years. The release prices of datacenter GPUs have roughly doubled every 5.1 years, while their power consumption has approximately doubled every 16 years. Finally, we quantify the potential implications of current U.S. export control regulations in terms of the potential performance gaps that would result if implementation were assumed to be complete and successful. We find that recently proposed changes to export controls would shrink the potential performance gap from 23.6x to 3.54x.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e862000\u5e74\u4ee3\u4e2d\u671f\u81f3\u4ecaNVIDIA\u6570\u636e\u4e2d\u5fc3GPU\u7684\u6280\u672f\u8fdb\u6b65\u8d8b\u52bf\uff0c\u91cf\u5316\u4e86\u5404\u9879\u6027\u80fd\u6307\u6807\u7684\u500d\u589e\u65f6\u95f4\uff0c\u5e76\u8bc4\u4f30\u4e86\u7f8e\u56fd\u51fa\u53e3\u7ba1\u5236\u5bf9\u6f5c\u5728\u6027\u80fd\u5dee\u8ddd\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740GPU\u5728AI\u7b49\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4e86\u89e3\u5176\u6280\u672f\u8fdb\u6b65\u8d8b\u52bf\u5bf9\u4e8e\u9884\u6d4b\u672a\u6765\u79d1\u5b66\u7814\u7a76\u7684\u7ea6\u675f\u6761\u4ef6\u81f3\u5173\u91cd\u8981\u3002\u7279\u522b\u662f\u5728\u7f8e\u56fd\u5b9e\u65bdAI\u82af\u7247\u51fa\u53e3\u7ba1\u5236\u7684\u80cc\u666f\u4e0b\uff0c\u5206\u6790GPU\u6280\u672f\u53d1\u5c55\u8f68\u8ff9\u5177\u6709\u91cd\u8981\u73b0\u5b9e\u610f\u4e49\u3002", "method": "\u6536\u96c6\u4e86\u4ece2000\u5e74\u4ee3\u4e2d\u671f\u81f3\u4eca\u7684NVIDIA\u6570\u636e\u4e2d\u5fc3GPU\u5168\u9762\u6570\u636e\u96c6\uff0c\u5305\u62ec\u8ba1\u7b97\u6027\u80fd\u3001\u4ef7\u683c\u7b49\u591a\u9879\u7279\u5f81\u3002\u5206\u6790\u4e3b\u8981GPU\u7279\u5f81\u8d8b\u52bf\uff0c\u5e76\u4f30\u7b97\u6bcf\u5185\u5b58\u5e26\u5bbd\u3001\u6bcf\u7f8e\u5143\u3001\u6bcf\u74e6\u7279\u7684\u8fdb\u6b65\u6307\u6807\u3002", "result": "FP16\u548cFP32\u64cd\u4f5c\u7684\u500d\u589e\u65f6\u95f4\u4e3a1.44-1.69\u5e74\uff0cFP64\u4e3a2.06-3.79\u5e74\u3002\u5185\u5b58\u5927\u5c0f\u548c\u5e26\u5bbd\u589e\u957f\u8f83\u6162\uff083.32-3.53\u5e74\uff09\u3002GPU\u4ef7\u683c\u6bcf5.1\u5e74\u7ffb\u500d\uff0c\u529f\u8017\u6bcf16\u5e74\u7ffb\u500d\u3002\u51fa\u53e3\u7ba1\u5236\u82e5\u5b8c\u5168\u5b9e\u65bd\uff0c\u6f5c\u5728\u6027\u80fd\u5dee\u8ddd\u5c06\u4ece23.6\u500d\u7f29\u5c0f\u52303.54\u500d\u3002", "conclusion": "GPU\u8ba1\u7b97\u6027\u80fd\u589e\u957f\u8fdc\u5feb\u4e8e\u5185\u5b58\u5e26\u5bbd\u548c\u4ef7\u683c\u589e\u957f\uff0c\u51fa\u53e3\u7ba1\u5236\u63aa\u65bd\u53ef\u80fd\u663e\u8457\u7f29\u5c0f\u56fd\u9645\u95f4\u7684\u6280\u672f\u6027\u80fd\u5dee\u8ddd\uff0c\u8fd9\u5bf9\u5168\u7403AI\u7ade\u4e89\u683c\u5c40\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2601.20267", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.20267", "abs": "https://arxiv.org/abs/2601.20267", "authors": ["Zhenkun Fan", "Zishen Wan", "Che-Kai Liu", "Ashwin Sanjay Lele", "Win-San Khwa", "Bo Zhang", "Meng-Fan Chang", "Arijit Raychowdhury"], "title": "SATA: Sparsity-Aware Scheduling for Selective Token Attention", "comment": "This paper has been accepted to DATE 2026", "summary": "Transformers have become the foundation of numerous state-of-the-art AI models across diverse domains, thanks to their powerful attention mechanism for modeling long-range dependencies. However, the quadratic scaling complexity of attention poses significant challenges for efficient hardware implementation. While techniques such as quantization and pruning help mitigate this issue, selective token attention offers a promising alternative by narrowing the attention scope to only the most relevant tokens, reducing computation and filtering out noise.\n  In this work, we propose SATA, a locality-centric dynamic scheduling scheme that proactively manages sparsely distributed access patterns from selective Query-Key operations. By reordering operand flow and exploiting data locality, our approach enables early fetch and retirement of intermediate Query/Key vectors, improving system utilization. We implement and evaluate our token management strategy in a control and compute system, using runtime traces from selective-attention-based models. Experimental results show that our method improves system throughput by up to 1.76x and boosts energy efficiency by 2.94x, while incurring minimal scheduling overhead.", "AI": {"tldr": "SATA\uff1a\u4e00\u79cd\u9762\u5411\u7a00\u758f\u67e5\u8be2-\u952e\u64cd\u4f5c\u7684\u52a8\u6001\u8c03\u5ea6\u65b9\u6848\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u64cd\u4f5c\u6570\u6d41\u548c\u5229\u7528\u6570\u636e\u5c40\u90e8\u6027\uff0c\u63d0\u9ad8\u9009\u62e9\u6027\u6ce8\u610f\u529b\u6a21\u578b\u7684\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u80fd\u6548", "motivation": "Transformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\u7ed9\u786c\u4ef6\u5b9e\u73b0\u5e26\u6765\u6311\u6218\uff0c\u9009\u62e9\u6027\u4ee4\u724c\u6ce8\u610f\u529b\u901a\u8fc7\u805a\u7126\u76f8\u5173\u4ee4\u724c\u6765\u51cf\u5c11\u8ba1\u7b97\uff0c\u4f46\u4f1a\u4ea7\u751f\u7a00\u758f\u8bbf\u95ee\u6a21\u5f0f\uff0c\u9700\u8981\u9ad8\u6548\u8c03\u5ea6", "method": "\u63d0\u51faSATA\uff08\u5c40\u90e8\u6027\u4e2d\u5fc3\u7684\u52a8\u6001\u8c03\u5ea6\u65b9\u6848\uff09\uff0c\u4e3b\u52a8\u7ba1\u7406\u7a00\u758f\u5206\u5e03\u7684\u67e5\u8be2-\u952e\u64cd\u4f5c\u8bbf\u95ee\u6a21\u5f0f\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u64cd\u4f5c\u6570\u6d41\u3001\u5229\u7528\u6570\u636e\u5c40\u90e8\u6027\uff0c\u5b9e\u73b0\u4e2d\u95f4\u67e5\u8be2/\u952e\u5411\u91cf\u7684\u63d0\u524d\u83b7\u53d6\u548c\u91ca\u653e", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5c06\u7cfb\u7edf\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad81.76\u500d\uff0c\u80fd\u6548\u63d0\u53472.94\u500d\uff0c\u540c\u65f6\u8c03\u5ea6\u5f00\u9500\u6700\u5c0f", "conclusion": "SATA\u901a\u8fc7\u9ad8\u6548\u7684\u52a8\u6001\u8c03\u5ea6\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u9009\u62e9\u6027\u6ce8\u610f\u529b\u6a21\u578b\u7684\u7a00\u758f\u8bbf\u95ee\u6a21\u5f0f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u786c\u4ef6\u5b9e\u73b0\u6548\u7387\u548c\u80fd\u6548"}}
{"id": "2601.20317", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.20317", "abs": "https://arxiv.org/abs/2601.20317", "authors": ["Yipu Zhang", "Jintao Cheng", "Xingyu Liu", "Zeyu Li", "Carol Jingyi Li", "Jin Wu", "Lin Jiang", "Yuan Xie", "Jiang Xu", "Wei Zhang"], "title": "VersaQ-3D: A Reconfigurable Accelerator Enabling Feed-Forward and Generalizable 3D Reconstruction via Versatile Quantization", "comment": null, "summary": "The Visual Geometry Grounded Transformer (VGGT) enables strong feed-forward 3D reconstruction without per-scene optimization. However, its billion-parameter scale creates high memory and compute demands, hindering on-device deployment. Existing LLM quantization methods fail on VGGT due to saturated activation channels and diverse 3D semantics, which cause unreliable calibration. Furthermore, VGGT presents hardware challenges regarding precision-sensitive nonlinear operators and memory-intensive global attention. To address this, we propose VersaQ-3D, an algorithm-architecture co-design framework. Algorithmically, we introduce the first calibration-free, scene-agnostic quantization for VGGT down to 4-bit, leveraging orthogonal transforms to decorrelate features and suppress outliers. Architecturally, we design a reconfigurable accelerator supporting BF16, INT8, and INT4. A unified systolic datapath handles both linear and nonlinear operators, reducing latency by 60%, while two-stage recomputation-based tiling alleviates memory pressure for long-sequence attention. Evaluations show VersaQ-3D preserves 98-99% accuracy at W4A8. At W4A4, it outperforms prior methods by 1.61x-2.39x across diverse scenes. The accelerator delivers 5.2x-10.8x speedup over edge GPUs with low power, enabling efficient instant 3D reconstruction.", "AI": {"tldr": "VersaQ-3D\u662f\u4e00\u4e2a\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u6821\u51c6\u76844\u4f4d\u91cf\u5316\u6280\u672f\u89e3\u51b3VGGT\u6a21\u578b\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u5b9e\u73b0\u9ad8\u6548\u5373\u65f63D\u91cd\u5efa\u3002", "motivation": "VGGT\u6a21\u578b\u867d\u7136\u80fd\u5b9e\u73b0\u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u7684\u524d\u9988\u5f0f3D\u91cd\u5efa\uff0c\u4f46\u5176\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u5bfc\u81f4\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u963b\u788d\u4e86\u5728\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u73b0\u6709LLM\u91cf\u5316\u65b9\u6cd5\u56e0\u9971\u548c\u6fc0\u6d3b\u901a\u9053\u548c\u591a\u68373D\u8bed\u4e49\u800c\u5931\u6548\uff0c\u4e14\u786c\u4ef6\u4e0a\u5b58\u5728\u7cbe\u5ea6\u654f\u611f\u975e\u7ebf\u6027\u7b97\u5b50\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u5168\u5c40\u6ce8\u610f\u529b\u95ee\u9898\u3002", "method": "\u63d0\u51faVersaQ-3D\u6846\u67b6\uff1a\u7b97\u6cd5\u4e0a\u91c7\u7528\u9996\u4e2a\u65e0\u6821\u51c6\u3001\u573a\u666f\u65e0\u5173\u76844\u4f4d\u91cf\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u6b63\u4ea4\u53d8\u6362\u89e3\u76f8\u5173\u7279\u5f81\u5e76\u6291\u5236\u5f02\u5e38\u503c\uff1b\u67b6\u6784\u4e0a\u8bbe\u8ba1\u53ef\u91cd\u6784\u52a0\u901f\u5668\uff0c\u652f\u6301BF16\u3001INT8\u548cINT4\u7cbe\u5ea6\uff0c\u7edf\u4e00\u8109\u52a8\u6570\u636e\u901a\u8def\u5904\u7406\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7b97\u5b50\uff0c\u4e24\u9636\u6bb5\u91cd\u8ba1\u7b97\u5206\u5757\u7f13\u89e3\u957f\u5e8f\u5217\u6ce8\u610f\u529b\u5185\u5b58\u538b\u529b\u3002", "result": "\u5728W4A8\u7cbe\u5ea6\u4e0b\u4fdd\u630198-99%\u51c6\u786e\u7387\uff1b\u5728W4A4\u7cbe\u5ea6\u4e0b\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5728\u591a\u6837\u573a\u666f\u4e2d\u63d0\u53471.61x-2.39x\uff1b\u52a0\u901f\u5668\u76f8\u6bd4\u8fb9\u7f18GPU\u5b9e\u73b05.2x-10.8x\u52a0\u901f\uff0c\u529f\u8017\u4f4e\uff0c\u652f\u6301\u9ad8\u6548\u5373\u65f63D\u91cd\u5efa\u3002", "conclusion": "VersaQ-3D\u6210\u529f\u89e3\u51b3\u4e86VGGT\u6a21\u578b\u7684\u90e8\u7f72\u74f6\u9888\uff0c\u901a\u8fc7\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u4f4e\u7cbe\u5ea6\u76843D\u91cd\u5efa\uff0c\u4e3a\u8bbe\u5907\u7aef\u5373\u65f63D\u91cd\u5efa\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.20706", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.20706", "abs": "https://arxiv.org/abs/2601.20706", "authors": ["Binglei Lou", "Haoran Wu", "Yao Lai", "Jiayi Nie", "Can Xiao", "Xuan Guo", "Rika Antonova", "Robert Mullins", "Aaron Zhao"], "title": "Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) introduce iterative denoising to enable parallel token generation, but their sampling phase displays fundamentally different characteristics compared to GEMM-centric transformer layers. Profiling on modern GPUs reveals that sampling can account for up to 70% of total model inference latency-primarily due to substantial memory loads and writes from vocabulary-wide logits, reduction-based token selection, and iterative masked updates. These processes demand large on-chip SRAM and involve irregular memory accesses that conventional NPUs struggle to handle efficiently. To address this, we identify a set of critical instructions that an NPU architecture must specifically optimize for dLLM sampling. Our design employs lightweight non-GEMM vector primitives, in-place memory reuse strategies, and a decoupled mixed-precision memory hierarchy. Together, these optimizations deliver up to a 2.53x speedup over the NVIDIA RTX A6000 GPU under an equivalent nm technology node. We also open-source our cycle-accurate simulation and post-synthesis RTL verification code, confirming functional equivalence with current dLLM PyTorch implementations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u91c7\u6837\u9636\u6bb5\u7684NPU\u67b6\u6784\u4f18\u5316\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5411\u91cf\u539f\u8bed\u3001\u5185\u5b58\u91cd\u7528\u7b56\u7565\u548c\u6df7\u5408\u7cbe\u5ea6\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\uff0c\u76f8\u6bd4GPU\u5b9e\u73b02.53\u500d\u52a0\u901f\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91c7\u6837\u9636\u6bb5\u5360\u63a8\u7406\u5ef6\u8fdf\u768470%\uff0c\u4e3b\u8981\u7531\u4e8e\u8bcd\u6c47\u7ea7logits\u7684\u5927\u91cf\u5185\u5b58\u8bfb\u5199\u3001\u57fa\u4e8e\u89c4\u7ea6\u7684token\u9009\u62e9\u548c\u8fed\u4ee3\u63a9\u7801\u66f4\u65b0\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u9700\u8981\u5927\u5bb9\u91cf\u7247\u4e0aSRAM\u4e14\u6d89\u53ca\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\uff0c\u4f20\u7edfNPU\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u3002", "method": "\u8bc6\u522bNPU\u67b6\u6784\u5fc5\u987b\u4f18\u5316\u7684\u5173\u952e\u6307\u4ee4\u96c6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u975eGEMM\u5411\u91cf\u539f\u8bed\u3001\u539f\u5730\u5185\u5b58\u91cd\u7528\u7b56\u7565\u548c\u89e3\u8026\u7684\u6df7\u5408\u7cbe\u5ea6\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u8bbe\u8ba1\u3002", "result": "\u5728\u7b49\u6548\u7eb3\u7c73\u6280\u672f\u8282\u70b9\u4e0b\uff0c\u76f8\u6bd4NVIDIA RTX A6000 GPU\u5b9e\u73b0\u6700\u9ad82.53\u500d\u52a0\u901f\uff0c\u5e76\u5f00\u6e90\u4e86\u5468\u671f\u7cbe\u786e\u6a21\u62df\u548c\u540e\u7efc\u5408RTL\u9a8c\u8bc1\u4ee3\u7801\u3002", "conclusion": "\u9488\u5bf9dLLM\u91c7\u6837\u9636\u6bb5\u7684\u7279\u5b9a\u67b6\u6784\u4f18\u5316\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u63d0\u51fa\u7684NPU\u8bbe\u8ba1\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
