{"id": "2510.01730", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.01730", "abs": "https://arxiv.org/abs/2510.01730", "authors": ["Ashiyana Abdul Majeed", "Mahmoud Meribout", "Safa Mohammed Sali"], "title": "Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI Reconstruction and Analysis", "comment": "11 pages. 14 figures. This work has been submitted to IEEE for\n  possible publication", "summary": "Advancements in AI have greatly enhanced the medical imaging process, making\nit quicker to diagnose patients. However, very few have investigated the\noptimization of a multi-model system with hardware acceleration. As specialized\nedge devices emerge, the efficient use of their accelerators is becoming\nincreasingly crucial. This paper proposes a hardware-accelerated method for\nsimultaneous reconstruction and diagnosis of \\ac{MRI} from \\ac{CT} images.\nReal-time performance of achieving a throughput of nearly 150 frames per second\nwas achieved by leveraging hardware engines available in modern NVIDIA edge\nGPU, along with scheduling techniques. This includes the GPU and the \\ac{DLA}\navailable in both Jetson AGX Xavier and Jetson AGX Orin, which were considered\nin this paper. The hardware allocation of different layers of the multiple AI\nmodels was done in such a way that the ideal time between the hardware engines\nis reduced. In addition, the AI models corresponding to the \\ac{GAN} model were\nfine-tuned in such a way that no fallback execution into the GPU engine is\nrequired without compromising accuracy. Indeed, the accuracy corresponding to\nthe fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of\n5\\%. A further hardware allocation of two fine-tuned GPU-aware GAN models\nproves they can double the performance over the original model, leveraging\nadequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The\nresults prove the effectiveness of employing hardware-aware models in parallel\nfor medical image analysis and diagnosis."}
{"id": "2510.02099", "categories": ["cs.AR", "cs.NE", "B.3; B.7; I.4"], "pdf": "https://arxiv.org/pdf/2510.02099", "abs": "https://arxiv.org/abs/2510.02099", "authors": ["Felix Zeller", "John Reuben", "Dietmar Fey"], "title": "Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic", "comment": "9 pages, 10 figures", "summary": "Vector-Matrix Multiplication (VMM) is the fundamental and frequently required\ncomputation in inference of Neural Networks (NN). Due to the large data\nmovement required during inference, VMM can benefit greatly from in-memory\ncomputing. However, ADC/DACs required for in-memory VMM consume significant\npower and area. `Distributed Arithmetic (DA)', a technique in computer\narchitecture prevalent in 1980s was used to achieve inner product or dot\nproduct of two vectors without using a hard-wired multiplier when one of the\nvectors is a constant. In this work, we extend the DA technique to multiply an\ninput vector with a constant matrix. By storing the sum of the weights in\nmemory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM\nmemory. We verify functional and also estimate non-functional properties\n(latency, energy, area) by performing transistor-level simulations. Using\nenergy-efficient sensing and fine grained pipelining, our approach achieves 4.5\nx less latency and 12 x less energy than VMM performed in memory conventionally\nby bit slicing. Furthermore, DA completely eliminated the need for power-hungry\nADCs which are the main source of area and energy consumption in the current\nVMM implementations in memory."}
