<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference](https://arxiv.org/abs/2512.18152)
*Rui Xie,Yunhua Fang,Asad Ul Haq,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: REACH提出控制器管理的ECC设计，通过两级Reed-Solomon编码方案，在不改变HBM物理接口和32B传输大小的情况下，将可容忍的原始误码率提高约三个数量级，同时保持LLM推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前HBM中的片上ECC设计限制了芯片筛选，提高了成本，并将可靠性策略固化在设备内部。HBM成本占系统成本主导地位，而LLM推理日益受内存限制。需要探索系统是否能容忍更高的原始HBM误码率，同时保持端到端正确性和吞吐量。

Method: REACH采用两级Reed-Solomon编码方案：1) 内层码对每个32B块进行本地检错和纠错；2) 外层码跨越KB级范围，仅以擦除模式运行，修复标记为擦除的块，避免昂贵的定位步骤。针对小随机写入，使用差分奇偶校验更新外层奇偶校验，避免重新计算整个跨度的奇偶校验。可选的重要性自适应位平面策略可仅保护关键字段（如BF16指数）以减少ECC工作和流量。

Result: 在8K上下文长度的三个LLM上，REACH在零误码率下保持约79%的片上ECC吞吐量，在原始误码率高达1e-3时仍保持合格，将可容忍设备误码率扩展约三个数量级，同时保持每秒令牌数几乎不变。在ASAP7工艺中，完整REACH控制器占用15.2 mm²，在3.56 TB/s下功耗17.5 W，与朴素的长Reed-Solomon基线相比，ECC面积减少11.6倍，功耗降低约60%。

Conclusion: 通过将强ECC移至控制器，REACH将长码可靠性转变为系统选择，可在相同标准接口下实现更低成本的HBM。这为在不改变HBM物理接口的情况下提高系统可靠性和降低成本提供了可行方案。

Abstract: LLM inference is increasingly memory bound, and HBM cost per GB dominates system cost. Current HBM stacks include short on-die ECC that tightens binning, raises price, and fixes reliability policy inside the device. This paper asks whether a system can tolerate a much higher raw HBM bit error rate and still keep end-to-end correctness and throughput, without changing the HBM PHY or the fixed 32 B transaction size. We propose REACH, a controller managed ECC design that keeps the HBM link and 32 B transfers unchanged. REACH uses a two level Reed-Solomon scheme: each 32 B chunk uses an inner code to check and correct most faults locally, while chunks that cannot be fixed are marked as erasures. An outer code spans kilobytes and runs in erasure only mode, repairing only flagged chunks and avoiding the expensive locator step. For small random writes, REACH updates outer parity with differential parity to avoid recomputing parity over the whole span, and an optional importance adaptive bit plane policy can protect only critical fields such as BF16 exponents to reduce ECC work and traffic. On three LLMs at 8K context, REACH keeps about 79 percent of on-die ECC throughput at zero BER and remains qualified up to a raw BER of 1e-3, extending tolerable device error rates by about three orders of magnitude while keeping tokens per second nearly flat. In ASAP7, a full REACH controller occupies 15.2 mm2 and consumes 17.5 W at 3.56 TB/s, and it reduces ECC area by 11.6x and power by about 60 percent compared to a naive long Reed-Solomon baseline. By moving strong ECC into the controller, REACH turns long code reliability into a system choice that can enable lower cost HBM under the same standard interface.

</details>


### [2] [PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM](https://arxiv.org/abs/2512.18158)
*Tsung-Han Lu,Zheyu Li,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: PIM-FW：一种基于HBM3内存处理架构的Floyd-Warshall算法加速方案，通过内存内计算消除数据移动瓶颈，相比GPU方案实现18.7倍加速和3200倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 传统Floyd-Warshall算法在CPU/GPU上存在立方时间复杂度和大量数据移动问题，限制了其在路由、物流和网络分析等应用中的可扩展性。

Method: 提出PIM-FW硬件架构与数据流协同设计：1）基于HBM3内存栈的处理近内存架构；2）大规模并行比特串行bank PE和channel PE加速min-plus操作；3）交错映射策略实现负载均衡；4）混合内存内和近内存计算模型；5）在内存bank内完成所有距离更新和存储。

Result: 在8192×8192图上，PIM-FW相比最先进的GPU-only Floyd-Warshall实现：端到端执行速度提升18.7倍，DRAM能耗降低3200倍。

Conclusion: PIM-FW通过内存内计算有效解决了Floyd-Warshall算法的数据移动瓶颈，显著提升了性能和能效，为大规模图处理提供了新的硬件加速方案。

Abstract: All-pairs shortest paths (APSP) is a fundamental algorithm used for routing, logistics, and network analysis, but the cubic time complexity and heavy data movement of the canonical Floyd-Warshall (FW) algorithm severely limits its scalability on conventional CPUs or GPUs. In this paper, we propose PIM-FW, a novel co-designed hardware architecture and dataflow that leverages processing in and near memory architecture designed to accelerate blocked FW algorithm on an HBM3 stack. To enable fine-grained parallelism, we propose a massively parallel array of specialized bit-serial bank PE and channel PE designed to accelerate the core min-plus operations. Our novel dataflow complements this hardware, employing an interleaved mapping policy for superior load balancing and hybrid in and near memory computing model for efficient computation and reduction. The novel in-bank computing approach allows all distance updates to be performed and stored in memory bank, a key contribution is that eliminates the data movement bottleneck inherent in GPU-based approaches. We implement a full software and hardware co-design using a cycle-accurate simulator to simulate an 8-channel, 4-Hi HBM3 PIM stack on real road-network traces. Experimental results show that, for a 8192 x 8192 graph, PIM-FW achieves a 18.7x speedup in end-to-end execution, and consumes 3200x less DRAM energy compared to a state-of-the-art GPU-only Floyd-Warshall.

</details>


### [3] [BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism](https://arxiv.org/abs/2512.18300)
*Suhas Vittal,Moinuddin Qureshi*

Main category: cs.AR

TL;DR: 提出BARD（Bank-Aware Replacement Decisions）缓存替换策略，通过考虑DDR5 DRAM写入延迟的差异性，优化写入流以提升系统性能。


<details>
  <summary>Details</summary>
Motivation: DDR5 DRAM中，由于片上ECC的存在，写入操作时间差异显著（1x到24x不等）。当DRAM执行写入时无法服务读取请求，导致读取延迟增加和性能下降。当前系统的写入流由缓存替换策略决定，但该策略不了解DRAM写入的可变延迟特性。

Method: 提出BARD策略，修改缓存替换策略以优先处理没有待处理写入的bank中的脏数据行。包括两种变体：BARD-E（基于驱逐）和BARD-C（基于清理），以及混合策略BARD-H。BARD仅需每个LLC切片8字节SRAM开销。

Result: 在SPEC2017、LIGRA、STREAM和Google服务器跟踪工作负载上的评估显示，BARD-H平均提升性能4.3%，最高可达8.5%。

Conclusion: 通过修改缓存替换策略以考虑DRAM写入延迟的差异性，可以有效提升DDR5系统性能，BARD-H混合策略在性能和实现复杂度之间取得了良好平衡。

Abstract: This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.
  Our paper proposes {\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\% on average and up-to 8.5\%. BARD requires only 8 bytes of SRAM per LLC slice.

</details>


### [4] [Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework](https://arxiv.org/abs/2512.18459)
*Akul Malhotra,Sumeet Kumar Gupta*

Main category: cs.AR

TL;DR: 提出两种无需训练的权重变换技术（sign-flip和bit-flip），增强多比特DNN在比特切片交叉阵列中的粘滞故障容忍度，与CVM协同工作，无需重训练或额外内存。


<details>
  <summary>Details</summary>
Motivation: 计算内存（CiM）加速器部署DNN能显著节能加速，但内存单元的粘滞故障（SAFs）会损坏存储权重导致精度下降。现有的最近值映射（CVM）方法在高SAF率或复杂任务下容错能力不足。

Method: 提出两种权重变换技术：1) sign-flip在权重列级别选择权重或其负值；2) bit-flip通过选择性反转单个比特切片提供更细粒度控制。两种方法都与CVM协同工作，无需重训练。引入基于查找表（LUT）的框架加速最优变换计算。

Result: 在ResNet-18、ResNet-50和ViT模型上使用CIFAR-100和ImageNet进行实验，显示提出的技术能恢复SAF注入下的大部分精度损失。硬件分析表明sign-flip带来可忽略的开销，bit-flip以适度开销提供更高容错能力。

Conclusion: sign-flip和bit-flip是实用且可扩展的SAF缓解策略，适用于基于CiM的DNN加速器，能有效应对内存粘滞故障问题。

Abstract: The deployment of deep neural networks (DNNs) on compute-in-memory (CiM) accelerators offers significant energy savings and speed-up by reducing data movement during inference. However, the reliability of CiM-based systems is challenged by stuck-at faults (SAFs) in memory cells, which corrupt stored weights and lead to accuracy degradation. While closest value mapping (CVM) has been shown to partially mitigate these effects for multibit DNNs deployed on bit-sliced crossbars, its fault tolerance is often insufficient under high SAF rates or for complex tasks. In this work, we propose two training-free weight transformation techniques, sign-flip and bit-flip, that enhance SAF tolerance in multi-bit DNNs deployed on bit-sliced crossbar arrays. Sign-flip operates at the weight-column level by selecting between a weight and its negation, whereas bit-flip provides finer granularity by selectively inverting individual bit slices. Both methods expand the search space for fault-aware mappings, operate synergistically with CVM, and require no retraining or additional memory. To enable scalability, we introduce a look-up-table (LUT)-based framework that accelerates the computation of optimal transformations and supports rapid evaluation across models and fault rates. Extensive experiments on ResNet-18, ResNet-50, and ViT models with CIFAR-100 and ImageNet demonstrate that the proposed techniques recover most of the accuracy lost under SAF injection. Hardware analysis shows that these methods incur negligible overhead, with sign-flip leading to negligible energy, latency, and area cost, and bit-flip providing higher fault resilience with modest overheads. These results establish sign-flip and bit-flip as practical and scalable SAF-mitigation strategies for CiM-based DNN accelerators.

</details>


### [5] [Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA](https://arxiv.org/abs/2512.19304)
*Emir Devlet Ertörer,Cem Ünsalan*

Main category: cs.AR

TL;DR: 提出一个完全定制的BNN推理加速器，用于手写数字识别，在Xilinx Artix-7 FPGA上实现实时分类，达到84%的MNIST测试准确率。


<details>
  <summary>Details</summary>
Motivation: 二进制神经网络（BNN）通过用位运算替代浮点运算，为资源受限平台（如FPGA）提供了低功耗、高速推理的解决方案。本研究旨在开发一个完全定制的BNN加速器，无需高级综合工具，实现透明、高效、灵活的嵌入式部署。

Method: 完全使用Verilog手动设计BNN推理加速器，不使用高级综合工具。针对Xilinx Artix-7 FPGA进行优化，实现80MHz的实时分类。设计包括完整的训练脚本和Verilog源代码。

Result: 在MNIST测试集上达到84%的准确率，在Xilinx Artix-7 FPGA上实现80MHz的实时分类，具有低功耗和可预测的时序特性。完整项目已在GitHub开源。

Conclusion: 手动HDL设计为BNN在嵌入式系统中的部署提供了透明、高效、灵活的解决方案，特别适合资源受限平台。开源代码促进了可重复性和未来发展。

Abstract: Binary neural networks provide a promising solution for low-power, high-speed inference by replacing expensive floating-point operations with bitwise logic. This makes them well-suited for deployment on resource-constrained platforms such as FPGAs. In this study, we present a fully custom BNN inference accelerator for handwritten digit recognition, implemented entirely in Verilog without the use of high-level synthesis tools. The design targets the Xilinx Artix-7 FPGA and achieves real-time classification at 80\,MHz with low power consumption and predictable timing. Simulation results demonstrate 84\% accuracy on the MNIST test set and highlight the advantages of manual HDL design for transparent, efficient, and flexible BNN deployment in embedded systems. The complete project including training scripts and Verilog source code are available at GitHub repo for reproducibility and future development.

</details>


### [6] [Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory](https://arxiv.org/abs/2512.19445)
*Guan-Cheng Chen,Chieh-Lin Tsai,Pei-Hsuan Tsai,Yuan-Hao Chang*

Main category: cs.AR

TL;DR: 提出一种结合敏感度分析和混合精度策略的结构化量化方法，用于优化ReRAM存内计算系统的权重存储和计算性能，在保持高精度的同时显著降低功耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统量化压缩技术在ReRAM存内计算系统中无法充分优化性能和效率，需要专门的方法来提升ReRAM交叉阵列利用率并降低功耗。

Method: 结合敏感度分析和混合精度策略的结构化量化方法，优化权重存储和计算性能，提高ReRAM交叉阵列利用率。

Result: 在70%压缩率下达到86.33%准确率，功耗降低40%，显著减少延迟和计算负载。

Conclusion: 该方法能有效提升ReRAM存内计算系统的性能和能效，适用于功耗受限的应用场景。

Abstract: Compute-In-Memory (CIM) systems, particularly those utilizing ReRAM and memristive technologies, offer a promising path toward energy-efficient neural network computation. However, conventional quantization and compression techniques often fail to fully optimize performance and efficiency in these architectures. In this work, we present a structured quantization method that combines sensitivity analysis with mixed-precision strategies to enhance weight storage and computational performance on ReRAM-based CIM systems. Our approach improves ReRAM Crossbar utilization, significantly reducing power consumption, latency, and computational load, while maintaining high accuracy. Experimental results show 86.33% accuracy at 70% compression, alongside a 40% reduction in power consumption, demonstrating the method's effectiveness for power-constrained applications.

</details>
