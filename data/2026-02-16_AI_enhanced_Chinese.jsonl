{"id": "2602.12295", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.12295", "abs": "https://arxiv.org/abs/2602.12295", "authors": ["R. Kanda", "N. Onizawa", "M. Leonardon", "V. Gripon", "T. Hanyu"], "title": "Design Environment of Quantization-Aware Edge AI Hardware for Few-Shot Learning", "comment": null, "summary": "This study aims to ensure consistency in accuracy throughout the entire design flow in the implementation of edge AI hardware for few-shot learning, by implementing fixed-point data processing in the pre-training and evaluation phases. Specifically, the quantization module, called Brevitas, is applied to implement fixed-point data processing, which allows for arbitrary specification of the bit widths for the integer and fractional parts. Two methods of fixed-point data quantization, quantization-aware training (QAT) and post-training quantization (PTQ), are utilized in Brevitas. With Tensil, which is used in the current design flow, the bit widths of the integer and fractional parts need to be 8 bits each or 16 bits each when implemented in hardware, but performance validation has shown that accuracy comparable to floating-point operations can be maintained even with 6 bits or 5 bits each, indicating potential for further reduction in computational resources. These results clearly contribute to the creation of a versatile design and evaluation environment for edge AI hardware for few-shot learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7Brevitas\u91cf\u5316\u6a21\u5757\u5b9e\u73b0\u5b9a\u70b9\u6570\u636e\u5904\u7406\uff0c\u5728\u8fb9\u7f18AI\u786c\u4ef6\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u8bbe\u8ba1\u4e2d\u786e\u4fdd\u5168\u6d41\u7a0b\u7cbe\u5ea6\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e865-6\u4f4d\u5b9a\u70b9\u6570\u5373\u53ef\u8fbe\u5230\u6d6e\u70b9\u8fd0\u7b97\u7cbe\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u5728\u8fb9\u7f18AI\u786c\u4ef6\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u5b9e\u73b0\u4e2d\uff0c\u786e\u4fdd\u4ece\u9884\u8bad\u7ec3\u5230\u8bc4\u4f30\u7684\u6574\u4e2a\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7cbe\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u9700\u8981\u5b9e\u73b0\u5b9a\u70b9\u6570\u636e\u5904\u7406\u6765\u4f18\u5316\u786c\u4ef6\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528Brevitas\u91cf\u5316\u6a21\u5757\u5b9e\u73b0\u5b9a\u70b9\u6570\u636e\u5904\u7406\uff0c\u652f\u6301\u4efb\u610f\u6307\u5b9a\u6574\u6570\u548c\u5c0f\u6570\u90e8\u5206\u7684\u4f4d\u5bbd\u3002\u91c7\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u548c\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5f53\u524d\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7684Tensil\u5de5\u5177\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6027\u80fd\u9a8c\u8bc1\u8868\u660e\uff0c\u5373\u4f7f\u4f7f\u75286\u4f4d\u62165\u4f4d\u7684\u6574\u6570\u548c\u5c0f\u6570\u90e8\u5206\uff0c\u4e5f\u80fd\u4fdd\u6301\u4e0e\u6d6e\u70b9\u8fd0\u7b97\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u800cTensil\u8981\u6c428\u4f4d\u621616\u4f4d\u3002\u8fd9\u8868\u660e\u6709\u8fdb\u4e00\u6b65\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u8fb9\u7f18AI\u786c\u4ef6\u521b\u5efa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u73af\u5883\uff0c\u901a\u8fc7\u5b9a\u70b9\u91cf\u5316\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u4fdd\u6301\u548c\u8d44\u6e90\u4f18\u5316\u7684\u5e73\u8861\u3002"}}
{"id": "2602.12422", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12422", "abs": "https://arxiv.org/abs/2602.12422", "authors": ["Kaushal Mhapsekar", "Azam Ghanbari", "Bita Aslrousta", "Samira Mirbagher-Ajorpaz"], "title": "CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement", "comment": "16 pages, 13 figures, ASPLOS 2026", "summary": "Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, \"Why is the memory access associated with PC X causing more evictions?\", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface.", "AI": {"tldr": "CacheMind\uff1a\u9996\u4e2a\u57fa\u4e8eRAG\u548cLLM\u7684\u5bf9\u8bdd\u5f0f\u7f13\u5b58\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7f13\u5b58\u8ddf\u8e2a\u6570\u636e\uff0c\u63d0\u4f9b\u8bed\u4e49\u5316\u7b54\u6848\uff0c\u663e\u8457\u63d0\u5347\u7f13\u5b58\u6027\u80fd\u5206\u6790\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7f13\u5b58\u66ff\u6362\u7b56\u7565\u4f9d\u8d56\u624b\u5de5\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6027\u80fd\u6709\u9650\uff1b\u7f13\u5b58\u6570\u636e\u5206\u6790\u9700\u8981\u89e3\u6790\u6570\u767e\u4e07\u6761\u8ddf\u8e2a\u8bb0\u5f55\u5e76\u8fdb\u884c\u624b\u52a8\u8fc7\u6ee4\uff0c\u8fc7\u7a0b\u7f13\u6162\u4e14\u975e\u4ea4\u4e92\u5f0f\u3002\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u3001\u4ea4\u4e92\u6027\u5f3a\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u63d0\u51faCacheMind\u7cfb\u7edf\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5b9e\u73b0\u7f13\u5b58\u8ddf\u8e2a\u6570\u636e\u7684\u8bed\u4e49\u63a8\u7406\u3002\u4f7f\u7528SIEVE\u548cRANGER\u68c0\u7d22\u5668\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u63d0\u4f9b\u57fa\u4e8e\u8ddf\u8e2a\u6570\u636e\u7684\u53ef\u8bfb\u7b54\u6848\u3002", "result": "\u5728CacheMindBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1aSIEVE\u572875\u4e2a\u672a\u89c1\u8ddf\u8e2a\u95ee\u9898\u4e2d\u8fbe\u523066.67%\u51c6\u786e\u7387\uff0c\u572825\u4e2a\u7b56\u7565\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u523084.80%\uff1bRANGER\u5206\u522b\u8fbe\u523089.33%\u548c64.80%\u3002RANGER\u57286\u4e2a\u7c7b\u522b\u4e2d\u76844\u4e2a\u5b9e\u73b0100%\u51c6\u786e\u7387\u3002\u5b9e\u9645\u5e94\u7528\u663e\u793a\uff1a\u65c1\u8def\u7528\u4f8b\u63d0\u5347\u7f13\u5b58\u547d\u4e2d\u73877.66%\u3001\u52a0\u901f2.04%\uff1b\u8f6f\u4ef6\u4fee\u590d\u7528\u4f8b\u52a0\u901f76%\uff1bMockingjay\u66ff\u6362\u7b56\u7565\u7528\u4f8b\u52a0\u901f0.7%\u3002", "conclusion": "CacheMind\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u7f13\u5b58\u8ddf\u8e2a\u8bed\u4e49\u5206\u6790\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709RAG\u65b9\u6cd5\uff08LlamaIndex\u4ec510%\u68c0\u7d22\u6210\u529f\u7387\uff09\u3002\u8be5\u7cfb\u7edf\u4e3a\u5fae\u67b6\u6784\u5e08\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u4ece\u590d\u6742\u7f13\u5b58\u6570\u636e\u4e2d\u63d0\u53d6\u53ef\u64cd\u4f5c\u7684\u6027\u80fd\u6d1e\u5bdf\u3002"}}
{"id": "2602.12480", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.12480", "abs": "https://arxiv.org/abs/2602.12480", "authors": ["George Karfakis", "Samyak Chakrabarty", "Vinod Kurian Jacob", "Siyun Qiao", "Subramanian S. Iyer", "Sudhakar Pamarti", "Puneet Gupta"], "title": "MXFormer: A Microscaling Floating-Point Charge-Trap Transistor Compute-in-Memory Transformer Accelerator", "comment": null, "summary": "The proliferation of Transformer models is often constrained by the significant computational and memory bandwidth demands of deployment. To address this, we present MXFormer, a novel, hybrid, weight-stationary Compute-in-Memory (CIM) accelerator that provides high throughput and efficiency for fixed-model inference on large short-sequence Transformers. Our architecture's foundation is the use of ultra-dense Charge-Trap Transistors (CTTs) in Microscaling MXFP4 CIM arrays, uniquely enabling the on-chip storage of up to hundreds of millions of parameters in Fully Weight Stationary (FWS) fashion.\n  We introduce a statically partitioned design with 12 Transformer blocks connected by a deeply pipelined dataflow. Static-weight layers (MLPs and linear projections) execute on highly parallel analog CTT arrays using an MXFP4-native flow with per-block exponent alignment and a 10-bit SAR ADC. Dynamic computations are handled in fully accurate digital blocks that utilize MXFP-enabled systolic arrays for scaled dot-product attention and vector units for LayerNorm and FlashAttention-style Softmax.\n  By eliminating all weight movement, the deeply pipelined MXFormer architecture yields very high single-stream throughput and efficiency, processing 58275 FPS on ViT-L/32 (dual-chip) or 41269 FPS on ViT-B/16 (single chip). MXFormer outperforms comparable state-of-the-art non-FWS digital, hybrid and photonic Transformer accelerators ~3.3x-60.5x in compute density and ~1.7x-2.5x in energy efficiency. Against FWS accelerators, MXFormer improves compute density by ~20.9x and resident weight storage density by ~2x, while preserving near-digital accuracy (drop of <1%) without any model retraining.", "AI": {"tldr": "MXFormer\u662f\u4e00\u79cd\u57fa\u4e8e\u7535\u8377\u6355\u83b7\u6676\u4f53\u7ba1\u8ba1\u7b97\u5185\u5b58\u9635\u5217\u7684\u6df7\u5408\u6743\u91cd\u56fa\u5b9aTransformer\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u6d88\u9664\u6743\u91cd\u79fb\u52a8\u5b9e\u73b0\u9ad8\u541e\u5410\u548c\u9ad8\u80fd\u6548\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u6570\u5b57\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u5bc6\u5ea6\u3002", "motivation": "Transformer\u6a21\u578b\u90e8\u7f72\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u5e26\u5bbd\u7684\u5de8\u5927\u9700\u6c42\u9650\u5236\uff0c\u9700\u8981\u9ad8\u6548\u52a0\u901f\u5668\u6765\u652f\u6301\u5927\u89c4\u6a21\u56fa\u5b9a\u6a21\u578b\u7684\u77ed\u5e8f\u5217\u63a8\u7406\u3002", "method": "\u91c7\u7528\u8d85\u5bc6\u96c6\u7535\u8377\u6355\u83b7\u6676\u4f53\u7ba1\u6784\u5efaMXFP4\u8ba1\u7b97\u5185\u5b58\u9635\u5217\uff0c\u5b9e\u73b0\u5168\u6743\u91cd\u56fa\u5b9a\u5b58\u50a8\uff1b\u8bbe\u8ba1\u9759\u6001\u5206\u533a\u67b6\u6784\uff0c12\u4e2aTransformer\u5757\u901a\u8fc7\u6df1\u5ea6\u6d41\u6c34\u7ebf\u6570\u636e\u6d41\u8fde\u63a5\uff1b\u9759\u6001\u6743\u91cd\u5c42\u5728\u6a21\u62dfCTT\u9635\u5217\u6267\u884c\uff0c\u52a8\u6001\u8ba1\u7b97\u5728\u6570\u5b57\u5757\u4e2d\u5904\u7406\u3002", "result": "MXFormer\u5728ViT-L/32\u4e0a\u8fbe\u523058275 FPS\uff08\u53cc\u82af\u7247\uff09\uff0cViT-B/16\u4e0a\u8fbe\u523041269 FPS\uff08\u5355\u82af\u7247\uff09\uff1b\u76f8\u6bd4\u975eFWS\u52a0\u901f\u5668\u8ba1\u7b97\u5bc6\u5ea6\u63d0\u53473.3-60.5\u500d\uff0c\u80fd\u6548\u63d0\u53471.7-2.5\u500d\uff1b\u76f8\u6bd4FWS\u52a0\u901f\u5668\u8ba1\u7b97\u5bc6\u5ea6\u63d0\u534720.9\u500d\uff0c\u6743\u91cd\u5b58\u50a8\u5bc6\u5ea6\u63d0\u53472\u500d\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e1%\u3002", "conclusion": "MXFormer\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u6743\u91cd\u56fa\u5b9a\u8ba1\u7b97\u5185\u5b58\u67b6\u6784\uff0c\u4e3a\u5927\u89c4\u6a21Transformer\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u541e\u5410\u3001\u9ad8\u80fd\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u786c\u4ef6\u6548\u7387\u3002"}}
{"id": "2602.12596", "categories": ["cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.12596", "abs": "https://arxiv.org/abs/2602.12596", "authors": ["Johnson Umeike", "Pongstorn Maidee", "Bahar Asgari"], "title": "Arcalis: Accelerating Remote Procedure Calls Using a Lightweight Near-Cache Solution", "comment": null, "summary": "Modern microservices increasingly depend on high-performance remote procedure calls (RPCs) to coordinate fine-grained, distributed computation. As network bandwidths continue to scale, the CPU overhead associated with RPC processing, particularly serialization, deserialization, and protocol handling, has become a critical bottleneck. This challenge is exacerbated by fast user-space networking stacks such as DPDK, which expose RPC processing as the dominant performance limiter. While prior work has explored software optimizations and FPGA-based offload engines, these approaches remain physically distant from the CPU's memory hierarchy, incurring unnecessary data movement and cache pollution. We present Arcalis, a near-cache RPC accelerator that positions a lightweight hardware engine adjacent to the last-level cache (LLC). Arcalis offloads RPC processing to dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. By decoupling RPC processing logic, enabling microservice-specific execution, and positioning itself near the LLC to immediately consume data injected by network cards, Arcalis achieves 1.79-4.16$\\times$ end-to-end speedup compared to the CPU baseline, while significantly reducing microarchitectural overhead by up to 88%, and achieves up to a 1.62$\\times$ higher throughput than prior solutions. These results highlight the potential of near-cache RPC acceleration as a practical solution for high-performance microservice deployment.", "AI": {"tldr": "Arcalis\u662f\u4e00\u79cd\u8fd1\u7f13\u5b58RPC\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5728\u672b\u7ea7\u7f13\u5b58\u65c1\u653e\u7f6e\u8f7b\u91cf\u7ea7\u786c\u4ef6\u5f15\u64ce\uff0c\u663e\u8457\u63d0\u5347\u5fae\u670d\u52a1RPC\u6027\u80fd\uff0c\u76f8\u6bd4CPU\u57fa\u7ebf\u83b7\u5f971.79-4.16\u500d\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u5e26\u5bbd\u6269\u5c55\uff0cRPC\u5904\u7406\u4e2d\u7684CPU\u5f00\u9500\uff08\u7279\u522b\u662f\u5e8f\u5217\u5316\u3001\u53cd\u5e8f\u5217\u5316\u548c\u534f\u8bae\u5904\u7406\uff09\u5df2\u6210\u4e3a\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u8f6f\u4ef6\u4f18\u5316\u548cFPGA\u5378\u8f7d\u65b9\u6848\u8ddd\u79bbCPU\u5185\u5b58\u5c42\u6b21\u8f83\u8fdc\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u6570\u636e\u79fb\u52a8\u548c\u7f13\u5b58\u6c61\u67d3\u3002", "method": "\u8bbe\u8ba1Arcalis\u8fd1\u7f13\u5b58RPC\u52a0\u901f\u5668\uff0c\u5c06\u8f7b\u91cf\u7ea7\u786c\u4ef6\u5f15\u64ce\u653e\u7f6e\u5728\u672b\u7ea7\u7f13\u5b58\u65c1\uff0c\u4f7f\u7528\u4e13\u7528\u5fae\u5f15\u64ce\u5728\u63a5\u6536\u548c\u53d1\u9001\u8def\u5f84\u4e0a\u5378\u8f7dRPC\u5904\u7406\uff0c\u4ee5\u7f13\u5b58\u884c\u5ef6\u8fdf\u8fd0\u884c\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u7f16\u7a0b\u6027\u3002", "result": "\u76f8\u6bd4CPU\u57fa\u7ebf\u5b9e\u73b01.79-4.16\u500d\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u5fae\u67b6\u6784\u5f00\u9500\u964d\u4f4e\u9ad8\u8fbe88%\uff0c\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u9ad8\u8fbe1.62\u500d\u3002", "conclusion": "\u8fd1\u7f13\u5b58RPC\u52a0\u901f\u662f\u9ad8\u6027\u80fd\u5fae\u670d\u52a1\u90e8\u7f72\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c06RPC\u5904\u7406\u903b\u8f91\u89e3\u8026\u3001\u652f\u6301\u5fae\u670d\u52a1\u7279\u5b9a\u6267\u884c\uff0c\u5e76\u9760\u8fd1LLC\u4ee5\u7acb\u5373\u6d88\u8d39\u7f51\u5361\u6ce8\u5165\u7684\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.12847", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.12847", "abs": "https://arxiv.org/abs/2602.12847", "authors": ["Alexandros Patras", "Spyros Lalis", "Christos D. Antonopoulos", "Nikolaos Bellas"], "title": "DPUConfig: Optimizing ML Inference in FPGAs Using Reinforcement Learning", "comment": "8 pages, 6 figures, to appear in the proceedings of DATE 2026", "summary": "Heterogeneous embedded systems, with diverse computing elements and accelerators such as FPGAs, offer a promising platform for fast and flexible ML inference, which is crucial for services such as autonomous driving and augmented reality, where delays can be costly. However, efficiently allocating computational resources for deep learning applications in FPGA-based systems is a challenging task. A Deep Learning Processor Unit (DPU) is a parameterizable FPGA-based accelerator module optimized for ML inference. It supports a wide range of ML models and can be instantiated multiple times within a single FPGA to enable concurrent execution. This paper introduces DPUConfig, a novel runtime management framework, based on a custom Reinforcement Learning (RL) agent, that dynamically selects optimal DPU configurations by leveraging real-time telemetry data monitoring, system utilization, power consumption, and application performance to inform its configuration selection decisions. The experimental evaluation demonstrates that the RL agent achieves energy efficiency 95% (on average) of the optimal attainable energy efficiency for several CNN models on the Xilinx Zynq UltraScale+ MPSoC ZCU102.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDPUConfig\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u52a8\u6001\u9009\u62e9FPGA\u4e2d\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u5668\u5355\u5143(DPU)\u7684\u6700\u4f18\u914d\u7f6e\uff0c\u4ee5\u63d0\u5347\u80fd\u6548\u3002", "motivation": "\u5f02\u6784\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff08\u5982FPGA\uff09\u4e3aML\u63a8\u7406\u63d0\u4f9b\u4e86\u5feb\u901f\u7075\u6d3b\u7684\u5e73\u53f0\uff0c\u4f46\u5728FPGA\u7cfb\u7edf\u4e2d\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u9ad8\u6548\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u5177\u6709\u6311\u6218\u6027\u3002DPU\u4f5c\u4e3a\u53ef\u53c2\u6570\u5316\u7684FPGA\u52a0\u901f\u6a21\u5757\uff0c\u652f\u6301\u591a\u79cdML\u6a21\u578b\u5e76\u80fd\u591a\u6b21\u5b9e\u4f8b\u5316\u5b9e\u73b0\u5e76\u53d1\u6267\u884c\uff0c\u4f46\u9700\u8981\u667a\u80fd\u7684\u8fd0\u884c\u65f6\u7ba1\u7406\u6765\u4f18\u5316\u914d\u7f6e\u3002", "method": "\u63d0\u51faDPUConfig\u8fd0\u884c\u65f6\u7ba1\u7406\u6846\u67b6\uff0c\u57fa\u4e8e\u81ea\u5b9a\u4e49\u5f3a\u5316\u5b66\u4e60(RL)\u4ee3\u7406\uff0c\u901a\u8fc7\u76d1\u63a7\u5b9e\u65f6\u9065\u6d4b\u6570\u636e\u3001\u7cfb\u7edf\u5229\u7528\u7387\u3001\u529f\u8017\u548c\u5e94\u7528\u6027\u80fd\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18DPU\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u5728Xilinx Zynq UltraScale+ MPSoC ZCU102\u5e73\u53f0\u4e0a\uff0c\u5bf9\u4e8e\u591a\u4e2aCNN\u6a21\u578b\uff0cRL\u4ee3\u7406\u5b9e\u73b0\u7684\u80fd\u6548\u5e73\u5747\u8fbe\u5230\u53ef\u8fbe\u6700\u4f18\u80fd\u6548\u768495%\u3002", "conclusion": "DPUConfig\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u52a8\u6001\u914d\u7f6e\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86FPGA\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u6027\u80fd\u3002"}}
{"id": "2602.12962", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12962", "abs": "https://arxiv.org/abs/2602.12962", "authors": ["Jonghun Lee", "Junghoon Lee", "Hyeonjin Kim", "Seoho Jeon", "Jisup Yoon", "Hyunbin Park", "Meejeong Park", "Heonjae Ha"], "title": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design", "comment": "13 pages, 14 figures", "summary": "Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of parameter reuse compared to conventional CNNs, making end-to-end execution on resource-limited devices extremely challenging. To address these challenges, we propose TriGen, a novel NPU architecture tailored for resource-constrained environments through software-hardware co-design. Firstly, TriGen adopts low-precision computation using microscaling (MX) to enable additional optimization opportunities while preserving accuracy, and resolves the issues that arise by employing such precision. Secondly, to jointly optimize both nonlinear and linear operations, TriGen eliminates the need for specialized hardware for essential nonlinear operations by using fast and accurate LUT, thereby maximizing performance gains and reducing hardware-cost in on-device environments, and finally, by taking practical hardware constraints into account, further employs scheduling techniques to maximize computational utilization even under limited on-chip memory capacity. We evaluate the performance of TriGen on various LLMs and show that TriGen achieves an average 2.73x performance speedup and 52% less memory transfer over the baseline NPU design with negligible accuracy loss.", "AI": {"tldr": "TriGen\u662f\u4e00\u79cd\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u5b9a\u5236\u7684\u65b0\u578bNPU\u67b6\u6784\uff0c\u9488\u5bf9LLM\u63a8\u7406\u4f18\u5316\uff0c\u91c7\u7528\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u3001LUT\u66ff\u4ee3\u975e\u7ebf\u6027\u64cd\u4f5c\u4e13\u7528\u786c\u4ef6\uff0c\u4ee5\u53ca\u8c03\u5ea6\u6280\u672f\u6700\u5927\u5316\u8ba1\u7b97\u5229\u7528\u7387\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7aef\u4fa7\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u6311\u6218\uff1aLLM\u53c2\u6570\u91cf\u5927\u4f46\u53c2\u6570\u590d\u7528\u7387\u4f4e\uff0c\u4f20\u7edfNPU\u67b6\u6784\u96be\u4ee5\u9ad8\u6548\u6267\u884c\u7aef\u5230\u7aef\u63a8\u7406\u3002", "method": "1) \u91c7\u7528\u5fae\u7f29\u653e(MX)\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u540c\u65f6\u63d0\u4f9b\u4f18\u5316\u673a\u4f1a\uff1b2) \u4f7f\u7528\u5feb\u901f\u51c6\u786e\u7684\u67e5\u627e\u8868(LUT)\u66ff\u4ee3\u975e\u7ebf\u6027\u64cd\u4f5c\u4e13\u7528\u786c\u4ef6\uff0c\u8054\u5408\u4f18\u5316\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u64cd\u4f5c\uff1b3) \u8003\u8651\u5b9e\u9645\u786c\u4ef6\u7ea6\u675f\uff0c\u91c7\u7528\u8c03\u5ea6\u6280\u672f\u6700\u5927\u5316\u6709\u9650\u7247\u4e0a\u5185\u5b58\u4e0b\u7684\u8ba1\u7b97\u5229\u7528\u7387\u3002", "result": "\u5728\u5404\u79cdLLM\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cTriGen\u76f8\u6bd4\u57fa\u7ebfNPU\u8bbe\u8ba1\u5e73\u5747\u5b9e\u73b02.73\u500d\u6027\u80fd\u52a0\u901f\uff0c\u5185\u5b58\u4f20\u8f93\u51cf\u5c1152%\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "TriGen\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0bLLM\u63a8\u7406\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u5185\u5b58\u5f00\u9500\uff0c\u4e3a\u7aef\u4fa7AI\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
