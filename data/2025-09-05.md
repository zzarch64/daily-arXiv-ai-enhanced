<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs](https://arxiv.org/abs/2509.03846)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 通过预先规划计算和通信的流式执行框架，利用消息传递机制减少I/O和主机干预，实现高利用率和高速计算速度


<details>
  <summary>Details</summary>
Motivation: 解决深度学习推理中常见的I/O瓶颈、主机干预过多和离芯内存占用高的问题

Method: 开发统一的指令和数据流框架，采用精细消息传递、静态权重重用、数组内多播和分段缩减等技术

Result: 在VGG-19上实现88-92%利用率，97%消息内部生成，89%时间芯内传输，计算速度超1TFLOP/s，数据量减少100MB/层

Conclusion: 流式计算模式高效可行，通过紧密协调数据和指令流可以实现高性能硬件执行

Abstract: We introduce a mapping framework for deep learning inference that takes
advantage of predictable neural network behavior to plan both computation and
communication ahead of time. The framework generates a unified stream of
instructions and data, enabling the hardware to execute operations and route
information on its own, without frequent involvement from the host and with
minimal off-chip memory use. This naturally reduces reliance on I/O, off-chip
memory, and host control. By leveraging fine-grained message passing on a
programmable, message-based compute architecture, the framework keeps data
movement local and coordinates computation across the array using techniques
such as stationary-weight reuse, in-array multicasting, and staged reductions.
Applied to VGG-19, the framework sustains high utilization (88 to 92 percent),
with over 97 percent of messages generated internally and nearly 89 percent of
time consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s
on larger arrays, while traffic reductions from reuse and local aggregation
reach up to 100 MB per layer. Overall, the results highlight the effectiveness
of streaming-based computation and show how our mapper enables this execution
style by tightly coordinating data and instruction flow across the hardware.

</details>


### [2] [Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations](https://arxiv.org/abs/2509.04153)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 本文综述了FPGA上CNN目标检测、分类和跟踪的最新进展，重点分析了硬件加速技术、优化策略和开发工具，为边缘计算提供高效视觉系统解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶、机器人和监控等实时计算机视觉应用需求的增长，FPGA因其可重构性、低功耗和确定性延迟而成为GPU和ASIC的有力替代方案。

Method: 通过批判性分析最先进的FPGA实现，涵盖算法创新、硬件加速技术，以及剪枝、量化和稀疏感知等优化策略，同时考察现代FPGA平台和软件开发工具。

Result: 提供了对FPGA部署CNN的全面技术评估，包括混合架构、软硬件协同设计实践、数据流优化和流水线处理技术，为实时推理提供有效解决方案。

Conclusion: 本研究为研究人员和工程师提供了开发下一代高能效、高性能视觉系统的关键见解，特别适用于边缘和嵌入式应用中的FPGA部署。

Abstract: This paper presents a comprehensive review of recent advances in deploying
convolutional neural networks (CNNs) for object detection, classification, and
tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand
for real-time computer vision applications in domains such as autonomous
vehicles, robotics, and surveillance, FPGAs have emerged as a powerful
alternative to GPUs and ASICs due to their reconfigurability, low power
consumption, and deterministic latency. We critically examine state-of-the-art
FPGA implementations of CNN-based vision tasks, covering algorithmic
innovations, hardware acceleration techniques, and the integration of
optimization strategies like pruning, quantization, and sparsity-aware methods
to maximize performance within hardware constraints. This survey also explores
the landscape of modern FPGA platforms, including classical LUT-DSP based
architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration
Platforms (ACAPs), comparing their capabilities in handling deep learning
workloads. Furthermore, we review available software development tools such as
Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the
design and deployment of AI models on FPGAs. The paper uniquely discusses
hybrid architecture that combine GPUs and FPGAs for collaborative acceleration
of AI inference, addressing challenges related to energy efficiency and
throughput. Additionally, we highlight hardware-software co-design practices,
dataflow optimizations, and pipelined processing techniques essential for
real-time inference on resource-constrained devices. Through this survey,
researchers and engineers are equipped with insights to develop
next-generation, power-efficient, and high-performance vision systems optimized
for FPGA deployment in edge and embedded applications.

</details>


### [3] [Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations](https://arxiv.org/abs/2509.04162)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 本文全面综述了基于FPGA的Transformer和视觉语言模型推理的设计权衡、优化策略和实现挑战，包括设备选择、内存约束、数据流编排、量化策略等关键技术问题，并讨论了硬件算法协同设计的新趋势。


<details>
  <summary>Details</summary>
Motivation: Transformer和视觉语言模型在计算机视觉和多模态AI中表现出色，但其高计算复杂度、大内存占用和不规则数据访问模式在延迟和功耗受限环境中部署面临重大挑战，FPGA因其可重构性、细粒度并行性和能效优势成为理想硬件平台。

Method: 通过系统性综述方法，分析FPGA推理的关键设计因素：设备类别选择、内存子系统约束、数据流编排、量化策略、稀疏性利用、工具链选择，以及VLMs特有的模态特定问题（异构计算平衡和交叉注意力内存管理）。

Result: 提出了针对FPGA部署的全面技术框架，识别了关键性能瓶颈和优化机会，总结了硬件算法协同设计中的创新方法（注意力机制改进、压缩技术和模块化覆盖），并指出了实际部署中的运行灵活性、验证开销和标准化基准缺失等问题。

Conclusion: FPGA为Transformer和VLMs提供了高效部署的有前景的解决方案，未来需要向可扩展、可移植和可重构的FPGA解决方案发展，以适配不断演进的模型架构，同时保持高利用率和可预测性能，弥合先进多模态AI模型与高效FPGA部署之间的差距。

Abstract: Transformers and vision-language models (VLMs) have emerged as dominant
architectures in computer vision and multimodal AI, offering state-of-the-art
performance in tasks such as image classification, object detection, visual
question answering, and caption generation. However, their high computational
complexity, large memory footprints, and irregular data access patterns present
significant challenges for deployment in latency- and power-constrained
environments. Field-programmable gate arrays (FPGAs) provide an attractive
hardware platform for such workloads due to their reconfigurability,
fine-grained parallelism, and potential for energy-efficient acceleration. This
paper presents a comprehensive review of design trade-offs, optimization
strategies, and implementation challenges for FPGA-based inference of
transformers and VLMs. We examine critical factors such as device-class
selection, memory subsystem constraints, dataflow orchestration, quantization
strategies, sparsity exploitation, and toolchain choices, alongside
modality-specific issues unique to VLMs, including heterogeneous compute
balancing and cross-attention memory management. Additionally, we discuss
emerging trends in hardware-algorithm co-design, highlighting innovations in
attention mechanisms, compression, and modular overlays to improve efficiency
and adaptability. Practical issues such as runtime flexibility, verification
overhead, and the absence of standardized FPGA multimodal benchmarks are also
considered. Finally, we outline future directions toward scalable, portable,
and reconfigurable FPGA solutions that adapt to evolving model architectures
while sustaining high utilization and predictable performance. This synthesis
offers both a technical foundation and a forward-looking perspective to help
bridge the gap between advanced multimodal AI models and efficient FPGA
deployment.

</details>


### [4] [Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review](https://arxiv.org/abs/2509.04173)
*Safa Sali,Anis Meribout,Ashiyana Majeed,Mahmoud Meribout,Juan Pablo,Varun Tiwari,Asma Baobaid*

Main category: cs.AR

TL;DR: 本文综述了自动驾驶汽车实时目标检测算法及其硬件加速器的发展现状，分析了AI算法特别是CNN如何取代传统方法，并讨论了商业AV系统与学术研究之间的技术差距。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车需要实时目标检测来确保安全，但商业AV系统与学术研究之间存在技术信息不透明的鸿沟，需要弥合这一差距为未来全自动驾驶车辆设计提供参考。

Method: 通过全面调查最新的实时目标检测算法，特别是基于CNN的AI方法，以及GPU和ASIC等硬件加速器的应用，分析其性能表现和技术挑战。

Result: AI算法已完全取代传统统计图像处理方法，CNN的并行特性使其能够在边缘硬件上部署，达到数百fps的处理速度，但仍需进一步硬件和算法改进来处理多摄像头需求。

Conclusion: 商业AV系统的技术保密限制了研究进展，本文旨在弥合学术研究与商业应用之间的差距，为未来全自动驾驶车辆的设计提供实用参考，这是此前综述未涉及的重要方面。

Abstract: The efficiency of object detectors depends on factors like detection
accuracy, processing time, and computational resources. Processing time is
crucial for real-time applications, particularly for autonomous vehicles (AVs),
where instantaneous responses are vital for safety. This review paper provides
a concise yet comprehensive survey of real-time object detection (OD)
algorithms for autonomous cars delving into their hardware accelerators (HAs).
Non-neural network-based algorithms, which use statistical image processing,
have been entirely substituted by AI algorithms, such as different models of
convolutional neural networks (CNNs). Their intrinsically parallel features led
them to be deployable into edge-based HAs of various types, where GPUs and, to
a lesser extent, ASIC (application-specific integrated circuit) remain the most
widely used. Throughputs of hundreds of frames/s (fps) could be reached;
however, handling object detection for all the cameras available in a typical
AV requires further hardware and algorithmic improvements. The intensive
competition between AV providers has limited the disclosure of algorithms,
firmware, and even hardware platform details. This remains a hurdle for
researchers, as commercial systems provide valuable insights while academics
undergo lengthy training and testing on restricted datasets and road scenarios.
Consequently, many AV research papers may not be reflected in end products,
being developed under limited conditions. This paper surveys state-of-the-art
OD algorithms and aims to bridge the gap with technologies in commercial AVs.
To our knowledge, this aspect has not been addressed in earlier surveys. Hence,
the paper serves as a tangible reference for researchers designing future
generations of vehicles, expected to be fully autonomous for comfort and
safety.

</details>
