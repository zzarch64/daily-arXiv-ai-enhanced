{"id": "2510.15872", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15872", "abs": "https://arxiv.org/abs/2510.15872", "authors": ["Yun-Da Tsai", "Chang-Yu Chao", "Liang-Yeh Shen", "Tsung-Han Lin", "Haoyu Yang", "Mark Ho", "Yi-Chen Lu", "Wen-Hao Liu", "Shou-De Lin", "Haoxing Ren"], "title": "Multimodal Chip Physical Design Engineer Assistant", "comment": null, "summary": "Modern chip physical design relies heavily on Electronic Design Automation\n(EDA) tools, which often struggle to provide interpretable feedback or\nactionable guidance for improving routing congestion. In this work, we\nintroduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this\ngap by not only predicting congestion but also delivering human-interpretable\ndesign suggestions. Our method combines automated feature generation through\nMLLM-guided genetic prompting with an interpretable preference learning\nframework that models congestion-relevant tradeoffs across visual, tabular, and\ntextual inputs. We compile these insights into a \"Design Suggestion Deck\" that\nsurfaces the most influential layout features and proposes targeted\noptimizations. Experiments on the CircuitNet benchmark demonstrate that our\napproach outperforms existing models on both accuracy and explainability.\nAdditionally, our design suggestion guidance case study and qualitative\nanalyses confirm that the learned preferences align with real-world design\nprinciples and are actionable for engineers. This work highlights the potential\nof MLLMs as interactive assistants for interpretable and context-aware physical\ndesign optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u52a9\u624b\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u3001\u8868\u683c\u548c\u6587\u672c\u8f93\u5165\u6765\u9884\u6d4b\u5e03\u7ebf\u62e5\u585e\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3\u82af\u7247\u7269\u7406\u8bbe\u8ba1\u4f9d\u8d56EDA\u5de5\u5177\uff0c\u4f46\u8fd9\u4e9b\u5de5\u5177\u96be\u4ee5\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u53cd\u9988\u6216\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u6539\u8fdb\u6307\u5bfc\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9884\u6d4b\u62e5\u585e\u5e76\u63d0\u4f9b\u4eba\u7c7b\u53ef\u7406\u89e3\u5efa\u8bae\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408MLLM\u5f15\u5bfc\u7684\u9057\u4f20\u63d0\u793a\u8fdb\u884c\u81ea\u52a8\u7279\u5f81\u751f\u6210\uff0c\u4ee5\u53ca\u53ef\u89e3\u91ca\u7684\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u5efa\u6a21\u8de8\u89c6\u89c9\u3001\u8868\u683c\u548c\u6587\u672c\u8f93\u5165\u7684\u62e5\u585e\u76f8\u5173\u6743\u8861\uff0c\u751f\u6210\u5305\u542b\u6700\u6709\u5f71\u54cd\u529b\u5e03\u5c40\u7279\u5f81\u548c\u9488\u5bf9\u6027\u4f18\u5316\u7684\u8bbe\u8ba1\u5efa\u8bae\u5361\u3002", "result": "\u5728CircuitNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u8bbe\u8ba1\u5efa\u8bae\u6307\u5bfc\u6848\u4f8b\u7814\u7a76\u548c\u5b9a\u6027\u5206\u6790\u8bc1\u5b9e\u5b66\u4e60\u5230\u7684\u504f\u597d\u4e0e\u73b0\u5b9e\u8bbe\u8ba1\u539f\u5219\u4e00\u81f4\u4e14\u5bf9\u5de5\u7a0b\u5e08\u5177\u6709\u53ef\u64cd\u4f5c\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u7a81\u663e\u4e86MLLM\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u52a9\u624b\u5728\u53ef\u89e3\u91ca\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7269\u7406\u8bbe\u8ba1\u4f18\u5316\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.15878", "categories": ["cs.AR", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15878", "abs": "https://arxiv.org/abs/2510.15878", "authors": ["David A. Roberts"], "title": "Putting the Context back into Memory", "comment": null, "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5185\u5b58\u5730\u5740\u6d41\u4e2d\u7f16\u7801\u7528\u6237\u53ef\u89c1\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u4f7f\u5185\u5b58\u8bbe\u5907\u80fd\u591f\u83b7\u53d6\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u6539\u5584\u5185\u5b58\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u786c\u4ef6\u7f13\u5b58\u9884\u53d6\u3001\u5185\u5b58\u8bf7\u6c42\u8c03\u5ea6\u548c\u4ea4\u9519\u5bfc\u81f4\u7684\u53ef\u89c2\u6d4b\u6027\u4e27\u5931\u95ee\u9898\uff0c\u4f7f\u5185\u5b58\u8bbe\u5907\u80fd\u591f\u83b7\u53d6\u6709\u4ef7\u503c\u7684\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u5728\u5185\u5b58\u8bfb\u53d6\u5730\u5740\u6d41\u4e2d\u4ee5\u975e\u7834\u574f\u6027\u65b9\u5f0f\u7f16\u7801\u7528\u6237\u53ef\u89c1\u72b6\u6001\u4f5c\u4e3a\u53ef\u68c0\u6d4b\u7684\u6570\u636e\u5305\uff0c\u6784\u5efa\u4e86\u5305\u542b\u5143\u6570\u636e\u6ce8\u5165\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u539f\u578b\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u5185\u5b58\u5730\u5740\u8ddf\u8e2a\u4e2d\u53ef\u9760\u68c0\u6d4b\u548c\u89e3\u7801\u5143\u6570\u636e\uff0c\u5c55\u793a\u4e86\u7cbe\u786e\u4ee3\u7801\u6267\u884c\u6807\u8bb0\u548c\u5bf9\u8c61\u5730\u5740\u8303\u56f4\u8ddf\u8e2a\u7684\u7528\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u5b9e\u65f6\u5143\u6570\u636e\u89e3\u7801\u548c\u8fd1\u5185\u5b58\u8ba1\u7b97\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u53ef\u5b9e\u73b0\u5b9a\u5236\u5316\u9065\u6d4b\u3001\u7edf\u8ba1\u548c\u57fa\u4e8e\u5e94\u7528\u63d0\u793a\u7684\u5185\u5b58\u4f18\u5316\u529f\u80fd\u3002"}}
{"id": "2510.15880", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.15880", "abs": "https://arxiv.org/abs/2510.15880", "authors": ["Philip Emma", "Eren Kurshan"], "title": "Opportunities and Challenges for 3D Systems and Their Design", "comment": "IEEE Design and Computers", "summary": "Although it is not a new concept, 3D integration increasingly receives\nwidespread interest and focus as lithographic scaling becomes more challenging,\nand as the ability to make miniature vias greatly improves. Like Moores law, 3D\nintegration improves density. With improvements in packaging density, however,\ncome the challenges associated with its inherently higher power density. And\nthough it acts somewhat as a scaling accelerator, the vertical integration also\nposes new challenges to design and manufacturing technologies. The placement of\ncircuits, vias, and macros in the planes of a 3D stack must be co-designed\nacross layers (or must conform to new standards) so that, when assembled, they\nhave correct spatial correspondence. Each layer, although perhaps being a mere\nfunctional slice through a system (and we can slice the system in many\ndifferent ways), must be independently testable so that we can systematically\ntest and diagnose subsystems before and after final assembly. When those layers\nare assembled, they must come together in a way that enables a sensible yield\nand facilitates testing the finished product. To make the most of 3D\nintegration, we should articulate the leverages of 3D systems (other\nresearchers offer a more complete treatment elsewhere). Then we can enumerate\nand elucidate many of the new challenges posed by the design, assembly, and\ntest of 3D systems.", "AI": {"tldr": "3D\u96c6\u6210\u6280\u672f\u867d\u7136\u4e0d\u662f\u4e00\u4e2a\u65b0\u6982\u5ff5\uff0c\u4f46\u968f\u7740\u5149\u523b\u7f29\u653e\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\u4ee5\u53ca\u5fae\u578b\u901a\u5b54\u5236\u9020\u80fd\u529b\u7684\u63d0\u5347\uff0c\u6b63\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u5b83\u50cf\u6469\u5c14\u5b9a\u5f8b\u4e00\u6837\u63d0\u9ad8\u5bc6\u5ea6\uff0c\u4f46\u5e26\u6765\u4e86\u66f4\u9ad8\u7684\u529f\u7387\u5bc6\u5ea6\u6311\u6218\uff0c\u5e76\u5bf9\u8bbe\u8ba1\u548c\u5236\u9020\u6280\u672f\u63d0\u51fa\u4e86\u65b0\u8981\u6c42\u3002", "motivation": "\u968f\u7740\u5149\u523b\u7f29\u653e\u9762\u4e34\u6311\u6218\uff0c3D\u96c6\u6210\u6280\u672f\u56e0\u5176\u80fd\u591f\u63d0\u9ad8\u5bc6\u5ea6\u800c\u53d7\u5230\u91cd\u89c6\u3002\u5fae\u578b\u901a\u5b54\u5236\u9020\u80fd\u529b\u7684\u6539\u8fdb\u4f7f\u5f973D\u96c6\u6210\u66f4\u5177\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u7531\u6b64\u5e26\u6765\u7684\u8bbe\u8ba1\u548c\u5236\u9020\u65b0\u95ee\u9898\u3002", "method": "\u8bba\u6587\u8ba8\u8bba\u4e863D\u96c6\u6210\u7cfb\u7edf\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5305\u62ec\u8de8\u5c42\u534f\u540c\u8bbe\u8ba1\u7535\u8def\u3001\u901a\u5b54\u548c\u5b8f\u5355\u5143\u7684\u5e03\u5c40\uff0c\u786e\u4fdd\u7ec4\u88c5\u540e\u7684\u7a7a\u95f4\u5bf9\u5e94\u5173\u7cfb\u6b63\u786e\u3002\u6bcf\u4e2a\u529f\u80fd\u5c42\u9700\u8981\u72ec\u7acb\u53ef\u6d4b\u8bd5\uff0c\u5e76\u5728\u7ec4\u88c5\u540e\u652f\u6301\u7cfb\u7edf\u7ea7\u6d4b\u8bd5\u3002", "result": "\u5206\u6790\u8868\u660e3D\u96c6\u6210\u786e\u5b9e\u80fd\u591f\u63d0\u9ad8\u7cfb\u7edf\u5bc6\u5ea6\uff0c\u4f46\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u529f\u7387\u5bc6\u5ea6\u589e\u52a0\u3001\u8bbe\u8ba1\u548c\u5236\u9020\u590d\u6742\u5ea6\u63d0\u5347\u7b49\u6311\u6218\u3002\u9700\u8981\u5efa\u7acb\u65b0\u7684\u8bbe\u8ba1\u6807\u51c6\u548c\u6d4b\u8bd5\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "conclusion": "3D\u96c6\u6210\u6280\u672f\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u8981\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\uff0c\u5fc5\u987b\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u8bbe\u8ba1\u3001\u7ec4\u88c5\u548c\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u65b0\u6311\u6218\uff0c\u5305\u62ec\u8de8\u5c42\u534f\u540c\u8bbe\u8ba1\u3001\u72ec\u7acb\u6d4b\u8bd5\u80fd\u529b\u548c\u6210\u54c1\u7387\u7ba1\u7406\u7b49\u65b9\u9762\u3002"}}
{"id": "2510.15882", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15882", "abs": "https://arxiv.org/abs/2510.15882", "authors": ["Ao Shen", "Rui Zhang", "Junping Zhao"], "title": "FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern", "comment": null, "summary": "As large language models (LLMs) continue to scale, multi-node deployment has\nbecome a necessity. Consequently, communication has become a critical\nperformance bottleneck. Current intra-node communication libraries, like NCCL,\ntypically make use of a single interconnect such as NVLink. This approach\ncreates performance ceilings, especially on hardware like the H800 GPU where\nthe primary interconnect's bandwidth can become a bottleneck, and leaves other\nhardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable\nNetwork Interface Cards (NICs) largely idle during intensive workloads. We\npropose FlexLink, the first collective communication framework to the best of\nour knowledge designed to systematically address this by aggregating these\nheterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance\ncommunication fabric. FlexLink employs an effective two-stage adaptive load\nbalancing strategy that dynamically partitions communication traffic across all\navailable links, ensuring that faster interconnects are not throttled by slower\nones. On an 8-GPU H800 server, our design improves the bandwidth of collective\noperators such as AllReduce and AllGather by up to 26% and 27% over the NCCL\nbaseline, respectively. This gain is achieved by offloading 2-22% of the total\ncommunication traffic to the previously underutilized PCIe and RDMA NICs.\nFlexLink provides these improvements as a lossless, drop-in replacement\ncompatible with the NCCL API, ensuring easy adoption.", "AI": {"tldr": "FlexLink\u662f\u4e00\u4e2a\u96c6\u4f53\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u5408NVLink\u3001PCIe\u548cRDMA NIC\u7b49\u5f02\u6784\u94fe\u8def\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u591a\u8282\u70b9\u90e8\u7f72\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u591a\u8282\u70b9\u90e8\u7f72\u6210\u4e3a\u5fc5\u9700\uff0c\u901a\u4fe1\u6210\u4e3a\u5173\u952e\u6027\u80fd\u74f6\u9888\u3002\u73b0\u6709\u901a\u4fe1\u5e93\u5982NCCL\u4ec5\u4f7f\u7528\u5355\u4e00\u4e92\u8fde\uff08\u5982NVLink\uff09\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0a\u9650\uff0c\u800c\u5176\u4ed6\u786c\u4ef6\u8d44\u6e90\u5982PCIe\u548cRDMA NIC\u5728\u5bc6\u96c6\u5de5\u4f5c\u8d1f\u8f7d\u671f\u95f4\u57fa\u672c\u95f2\u7f6e\u3002", "method": "FlexLink\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff0c\u52a8\u6001\u5730\u5c06\u901a\u4fe1\u6d41\u91cf\u5206\u533a\u5230\u6240\u6709\u53ef\u7528\u94fe\u8def\u4e0a\uff0c\u786e\u4fdd\u5feb\u901f\u4e92\u8fde\u4e0d\u4f1a\u88ab\u8f83\u6162\u7684\u4e92\u8fde\u6240\u9650\u5236\u3002", "result": "\u57288-GPU H800\u670d\u52a1\u5668\u4e0a\uff0c\u4e0eNCCL\u57fa\u7ebf\u76f8\u6bd4\uff0cAllReduce\u548cAllGather\u7b49\u96c6\u4f53\u64cd\u4f5c\u7b26\u7684\u5e26\u5bbd\u5206\u522b\u63d0\u9ad8\u4e8626%\u548c27%\uff0c\u901a\u8fc7\u5c062-22%\u7684\u603b\u901a\u4fe1\u6d41\u91cf\u5378\u8f7d\u5230\u4e4b\u524d\u672a\u5145\u5206\u5229\u7528\u7684PCIe\u548cRDMA NIC\u4e0a\u5b9e\u73b0\u3002", "conclusion": "FlexLink\u4f5c\u4e3a\u65e0\u635f\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u4e0eNCCL API\u517c\u5bb9\uff0c\u786e\u4fdd\u6613\u4e8e\u91c7\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u94fe\u8def\u5229\u7528\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2510.15884", "categories": ["cs.AR", "cs.MS"], "pdf": "https://arxiv.org/pdf/2510.15884", "abs": "https://arxiv.org/abs/2510.15884", "authors": ["Faizan A Khattak", "Mantas Mikaitis"], "title": "Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I", "comment": "Accepted for IEEE HPEC 2025", "summary": "Numerical features of matrix multiplier hardware units in NVIDIA and AMD data\ncentre GPUs have recently been studied. Features such as rounding,\nnormalisation, and internal precision of the accumulators are of interest. In\nthis paper, we extend the methodology for analysing those features, to\nconsumer-grade NVIDIA GPUs by implementing an architecture-independent test\nscheme for various input and output precision formats. Unlike current\napproaches, the proposed test vector generation method neither performs an\nexhaustive search nor relies on hard-coded {constants that are device-specific,\nyet remains applicable to a wide range of mixed-precision formats. We have\napplied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada\nLovelace architecture) graphics cards and determined numerical features of\nmatrix multipliers for binary16, TensorFloat32, and bfloat16 input floating\npoint formats and binary16 and binary32 IEEE 754 output formats. Our\nmethodology allowed us to determine that} the numerical features of RTX-3060, a\nconsumer-grade GPU, are identical to those of the A100, a data centre GPU. We\ndo not expect our code to require any changes for performing analysis of matrix\nmultipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future\nsuccessors, and any input/output format combination, including the latest 8-bit\nfloating-point formats.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\u65e0\u5173\u7684\u6d4b\u8bd5\u65b9\u6848\uff0c\u7528\u4e8e\u5206\u6790\u6d88\u8d39\u7ea7NVIDIA GPU\u4e2d\u77e9\u9635\u4e58\u6cd5\u5668\u7684\u6570\u503c\u7279\u6027\uff0c\u5305\u62ec\u820d\u5165\u3001\u5f52\u4e00\u5316\u548c\u7d2f\u52a0\u5668\u5185\u90e8\u7cbe\u5ea6\u7b49\u7279\u5f81\u3002", "motivation": "\u6269\u5c55\u6570\u636e\u4e2d\u5fc3GPU\u6570\u503c\u7279\u6027\u5206\u6790\u65b9\u6cd5\u5230\u6d88\u8d39\u7ea7GPU\uff0c\u5f00\u53d1\u4e0d\u4f9d\u8d56\u8bbe\u5907\u7279\u5b9a\u5e38\u6570\u7684\u901a\u7528\u6d4b\u8bd5\u5411\u91cf\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5b9e\u73b0\u67b6\u6784\u65e0\u5173\u7684\u6d4b\u8bd5\u65b9\u6848\uff0c\u4f7f\u7528\u975e\u7a77\u4e3e\u641c\u7d22\u4e14\u4e0d\u4f9d\u8d56\u786c\u7f16\u7801\u5e38\u6570\u7684\u6d4b\u8bd5\u5411\u91cf\u751f\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u6df7\u5408\u7cbe\u5ea6\u683c\u5f0f\u3002", "result": "\u5728RTX-3060\u548cAda RTX-1000\u4e0a\u6210\u529f\u5206\u6790\u4e86binary16\u3001TensorFloat32\u3001bfloat16\u8f93\u5165\u683c\u5f0f\u548cbinary16\u3001binary32\u8f93\u51fa\u683c\u5f0f\u7684\u6570\u503c\u7279\u6027\uff0c\u53d1\u73b0RTX-3060\u4e0e\u6570\u636e\u4e2d\u5fc3GPU A100\u5177\u6709\u76f8\u540c\u7684\u6570\u503c\u7279\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\uff0c\u9884\u8ba1\u65e0\u9700\u4fee\u6539\u5373\u53ef\u7528\u4e8e\u5206\u6790\u65b0\u4e00\u4ee3NVIDIA GPU\uff08\u5982Hopper\u3001Blackwell\uff09\u53ca\u5176\u672a\u6765\u7ee7\u4efb\u8005\uff0c\u4ee5\u53ca\u5305\u62ec\u6700\u65b08\u4f4d\u6d6e\u70b9\u683c\u5f0f\u5728\u5185\u7684\u5404\u79cd\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u7ec4\u5408\u3002"}}
{"id": "2510.15885", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.15885", "abs": "https://arxiv.org/abs/2510.15885", "authors": ["Dingcui Yu", "Zonghuan Yan", "Jialin Liu", "Yumiao Zhao", "Yanyun Wang", "Xinghui Duan", "Yina Lv", "Liang Shi"], "title": "ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices", "comment": null, "summary": "To facilitate the understanding and efficient enhancement of software and\nhardware design for consumer-grade zoned flash storage, ConZone is proposed as\nthe first emulator designed to model the resource constraints and architectural\nfeatures typical of such systems. It incorporates essential components commonly\ndeployed in consumer-grade devices, including limited logical to physical\nmapping caches, constrained write buffers, and hybrid flash media management.\nHowever, ConZone cannot be mounted with the file system due to the lack of\nin-place update capability, which is required by the metadata area of F2FS. To\nimprove the usability of the emulator, ConZone+ extends ConZone with support\nfor a block interface. We also provide a script to help the deployment and\nintroduces several enhancements over the original version. Users can explore\nthe internal architecture of consumer-grade zoned flash storage and integrate\ntheir optimizations with system software using ConZone+. We validate the\naccuracy of ConZone+ by comparing a hardware architecture representative of\nconsumer-grade zoned flash storage and comparing it with the state-of-the-art.\nIn addition, we conduct several case studies using ConZone+ to investigate the\ndesign of zoned storage and explore the inadequacies of the current file\nsystem.", "AI": {"tldr": "ConZone+\u662f\u4e00\u4e2a\u6a21\u62df\u6d88\u8d39\u7ea7\u5206\u533a\u95ea\u5b58\u5b58\u50a8\u7684\u4eff\u771f\u5668\uff0c\u901a\u8fc7\u6dfb\u52a0\u5757\u63a5\u53e3\u652f\u6301\u89e3\u51b3\u4e86\u539f\u7248ConZone\u65e0\u6cd5\u6302\u8f7d\u6587\u4ef6\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u90e8\u7f72\u811a\u672c\u548c\u591a\u9879\u589e\u5f3a\u529f\u80fd\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u548c\u9ad8\u6548\u6539\u8fdb\u6d88\u8d39\u7ea7\u5206\u533a\u95ea\u5b58\u5b58\u50a8\u7684\u8f6f\u786c\u4ef6\u8bbe\u8ba1\uff0c\u9700\u8981\u80fd\u591f\u6a21\u62df\u5176\u8d44\u6e90\u7ea6\u675f\u548c\u67b6\u6784\u7279\u5f81\u7684\u4eff\u771f\u5de5\u5177\u3002", "method": "ConZone+\u5728ConZone\u57fa\u7840\u4e0a\u6269\u5c55\u4e86\u5757\u63a5\u53e3\u652f\u6301\uff0c\u63d0\u4f9b\u4e86\u90e8\u7f72\u811a\u672c\uff0c\u5e76\u5305\u542b\u903b\u8f91\u5230\u7269\u7406\u6620\u5c04\u7f13\u5b58\u3001\u53d7\u9650\u5199\u7f13\u51b2\u5668\u548c\u6df7\u5408\u95ea\u5b58\u5a92\u4f53\u7ba1\u7406\u7b49\u6d88\u8d39\u7ea7\u8bbe\u5907\u5178\u578b\u7ec4\u4ef6\u3002", "result": "\u901a\u8fc7\u4e0e\u4ee3\u8868\u6027\u786c\u4ef6\u67b6\u6784\u548c\u6700\u5148\u8fdb\u6280\u672f\u6bd4\u8f83\u9a8c\u8bc1\u4e86ConZone+\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63a2\u7d22\u4e86\u5206\u533a\u5b58\u50a8\u8bbe\u8ba1\u548c\u5f53\u524d\u6587\u4ef6\u7cfb\u7edf\u7684\u4e0d\u8db3\u3002", "conclusion": "ConZone+\u4f7f\u7528\u6237\u80fd\u591f\u63a2\u7d22\u6d88\u8d39\u7ea7\u5206\u533a\u95ea\u5b58\u5b58\u50a8\u7684\u5185\u90e8\u67b6\u6784\uff0c\u5e76\u5c06\u4f18\u5316\u4e0e\u7cfb\u7edf\u8f6f\u4ef6\u96c6\u6210\uff0c\u63d0\u9ad8\u4e86\u4eff\u771f\u5668\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2510.15887", "categories": ["cs.AR", "C.1.0; B.7.1"], "pdf": "https://arxiv.org/pdf/2510.15887", "abs": "https://arxiv.org/abs/2510.15887", "authors": ["Hyun Woo Kang", "Ji Woong Choi"], "title": "basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I", "comment": "2 pages, 3 figures. Accepted to ISOCC 2025 (submitted 14 Jul. 2025;\n  accepted 8 Aug. 2025). To appear in the Proceedings of ISOCC 2025; oral\n  presentation on 17 Oct. 2025 (conference opens 15 Oct 2025). Camera-ready\n  version. Project repository: https://github.com/RISC-KC/basic_rv32s", "summary": "This paper introduces BASIC_RV32s, an open-source framework providing a\npractical microarchitectural roadmap for the RISC-V RV32I architecture,\naddressing the gap between theoretical knowledge and hardware implementation.\nFollowing the classic Patterson and Hennessy methodology, the design evolves\nfrom a basic single-cycle core to a 5-stage pipelined core design with full\nhazard forwarding, dynamic branch prediction, and exception handling. For\nverification, the final core design is integrated into a System-on-Chip (SoC)\nwith Universal Asynchronous Receiver-Transmitter (UART) communication\nimplemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving\n1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50\nMHz. By releasing all Register-Transfer Level (RTL) source code, signal-level\nlogic block diagrams, and development logs under MIT license on GitHub,\nBASIC_RV32s offers a reproducible instructional pathway for the open-source\nhardware ecosystem.", "AI": {"tldr": "BASIC_RV32s\u662f\u4e00\u4e2a\u5f00\u6e90\u7684RISC-V RV32I\u67b6\u6784\u6846\u67b6\uff0c\u4ece\u5355\u5468\u671f\u6838\u5fc3\u6f14\u8fdb\u52305\u7ea7\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u5305\u542b\u5b8c\u6574\u7684\u524d\u5411\u8f6c\u53d1\u3001\u52a8\u6001\u5206\u652f\u9884\u6d4b\u548c\u5f02\u5e38\u5904\u7406\uff0c\u5728FPGA\u4e0a\u5b9e\u73b01.09 DMIPS/MHz\u6027\u80fd\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684RTL\u6e90\u7801\u548c\u5f00\u53d1\u6587\u6863\u3002", "motivation": "\u89e3\u51b3RISC-V\u67b6\u6784\u7406\u8bba\u77e5\u8bc6\u4e0e\u786c\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5f00\u6e90\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u6559\u5b66\u8def\u5f84\u3002", "method": "\u91c7\u7528Patterson\u548cHennessy\u7ecf\u5178\u65b9\u6cd5\u5b66\uff0c\u4ece\u5355\u5468\u671f\u6838\u5fc3\u9010\u6b65\u6f14\u8fdb\u52305\u7ea7\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u96c6\u6210\u5b8c\u6574\u7684\u524d\u5411\u8f6c\u53d1\u3001\u52a8\u6001\u5206\u652f\u9884\u6d4b\u548c\u5f02\u5e38\u5904\u7406\u673a\u5236\uff0c\u5728FPGA\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728Xilinx Artix-7 FPGA\u4e0a\u5b9e\u73b0SoC\u8bbe\u8ba1\uff0c\u96c6\u6210UART\u901a\u4fe1\uff0c\u572850MHz\u9891\u7387\u4e0b\u8fbe\u52301.09 DMIPS/MHz\u7684\u6027\u80fd\u6307\u6807\u3002", "conclusion": "BASIC_RV32s\u6846\u67b6\u6210\u529f\u586b\u8865\u4e86RISC-V\u7406\u8bba\u6559\u5b66\u4e0e\u5b9e\u9645\u786c\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u5f00\u6e90RTL\u6e90\u7801\u3001\u903b\u8f91\u6846\u56fe\u548c\u53d1\u5c55\u65e5\u5fd7\uff0c\u4e3a\u5f00\u6e90\u786c\u4ef6\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6559\u5b66\u8d44\u6e90\u3002"}}
{"id": "2510.15888", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15888", "abs": "https://arxiv.org/abs/2510.15888", "authors": ["Konstantinos Kafousis"], "title": "Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol", "comment": null, "summary": "Hardware Transactional Memory (HTM) allows lock-free programming as easy as\nwith traditional coarse-grain locks or similar, while benefiting from the\nperformance advantages of fine-grained locking. Many HTM implementations have\nbeen proposed, but they have not received widespread adoption because of their\nhigh hardware complexity, their need for additions to the Instruction Set\nArchitecture (ISA), and often for modifications to the cache coherence\nprotocol.\n  We show that HTM can be implemented without adding new instructions -- merely\nby extending the semantics of two existing, Load-Linked and Store-Conditional.\nAlso, our proposed design does not modify or extend standard coherence\nprotocols. We further propose to drastically simplify the implementation of HTM\n-- confined to modifications in the L1 Data Cache only -- by restricting it to\napplications where the write set plus the read set of each transaction do not\nexceed a small number of cache lines. We also propose two alternative\nmechanisms to guarantee forward progress, both based on detecting retrial\nattempts.\n  We simulated our proposed design in Gem5, and we used it to implement several\npopular concurrent data structures, showing that a maximum of eight (8) words\n(cache lines) suffice for the write plus read sets. We provide a detailed\nexplanation of selected implementations, clarifying the intended usage of our\nHTM from a programmer's perspective. We evaluated our HTM under varying\ncontention levels to explore its scalability limits. The results indicate that\nour HTM provides good performance in concurrent data structures when contention\nis spread across multiple nodes: in such cases, the percentage of aborts\nrelative to successful commits is very low. In the atomic fetch-and-increment\nbenchmark for multiple shared counters, the results show that, under\nlow-congestion, our HTM improves performance relative to the TTS lock.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73b0\u6709Load-Linked\u548cStore-Conditional\u6307\u4ee4\u7684\u786c\u4ef6\u4e8b\u52a1\u5185\u5b58\u5b9e\u73b0\uff0c\u65e0\u9700\u4fee\u6539\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\uff0c\u4ec5\u9700\u5728L1\u6570\u636e\u7f13\u5b58\u4e2d\u8fdb\u884c\u4fee\u6539\uff0c\u9002\u7528\u4e8e\u8bfb\u5199\u96c6\u4e0d\u8d85\u8fc78\u4e2a\u7f13\u5b58\u884c\u7684\u5c0f\u578b\u4e8b\u52a1\u3002", "motivation": "\u4f20\u7edfHTM\u5b9e\u73b0\u786c\u4ef6\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u8981\u4fee\u6539ISA\u548c\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\uff0c\u5bfc\u81f4\u91c7\u7528\u7387\u4f4e\u3002\u672c\u6587\u65e8\u5728\u7b80\u5316HTM\u5b9e\u73b0\uff0c\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u6269\u5c55Load-Linked\u548cStore-Conditional\u6307\u4ee4\u8bed\u4e49\u5b9e\u73b0HTM\uff0c\u9650\u5236\u4e8b\u52a1\u8bfb\u5199\u96c6\u4e0d\u8d85\u8fc78\u4e2a\u7f13\u5b58\u884c\uff0c\u4ec5\u9700\u4fee\u6539L1\u6570\u636e\u7f13\u5b58\uff0c\u63d0\u4f9b\u4e24\u79cd\u57fa\u4e8e\u91cd\u8bd5\u68c0\u6d4b\u7684\u524d\u8fdb\u4fdd\u8bc1\u673a\u5236\u3002", "result": "\u5728Gem5\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\uff0c\u6210\u529f\u5b9e\u73b0\u591a\u4e2a\u6d41\u884c\u5e76\u53d1\u6570\u636e\u7ed3\u6784\uff0c\u5728\u4f4e\u7ade\u4e89\u60c5\u51b5\u4e0b\u6027\u80fd\u4f18\u4e8eTTS\u9501\uff0c\u8de8\u8282\u70b9\u7ade\u4e89\u65f6\u4e2d\u6b62\u7387\u5f88\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5316HTM\u8bbe\u8ba1\u5728\u4fdd\u6301\u826f\u597d\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u5b9e\u73b0\u590d\u6742\u5ea6\uff0c\u7279\u522b\u9002\u5408\u5c0f\u578b\u4e8b\u52a1\u7684\u5e76\u53d1\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.15893", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG", "68M10, 68M14", "B.4.3; C.2.4; C.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.15893", "abs": "https://arxiv.org/abs/2510.15893", "authors": ["Mikhail Bernadskiy", "Peter Carson", "Thomas Graham", "Taylor Groves", "Ho John Lee", "Eric Yeh"], "title": "Accelerating Frontier MoE Training with 3D Integrated Optics", "comment": "12 pages, 11 figures. To be published in Hot Interconnects 2025", "summary": "The unabated growth in AI workload demands is driving the need for concerted\nadvances in compute, memory, and interconnect performance. As traditional\nsemiconductor scaling slows, high-speed interconnects have emerged as the new\nscaling engine, enabling the creation of larger logical GPUs by linking many\nGPUs into a single, low-latency, high-bandwidth compute domain. While initial\nscale-up fabrics leveraged copper interconnects for their power and cost\nadvantages, the maximum reach of passive electrical interconnects\n(approximately 1 meter) effectively limits the scale-up domain to within a\nsingle rack. The advent of 3D-stacked optics and logic offers a transformative,\npower-efficient scale-up solution for connecting hundreds of GPU packages\n(thousands of GPUs) across multiple data center racks. This work explores the\ndesign tradeoffs of scale-up technologies and demonstrates how frontier LLMs\nnecessitate novel photonic solutions to achieve aggressive power and\nperformance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and\nswitches within the scale-up domain when training Frontier Mixture of Experts\n(MoE) models exceeding one trillion parameters. Our results show that the\nsubstantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X\nincrease in scale-up capability. This affords new opportunities for\nmulti-dimensional parallelism within the scale-up domain and results in a 2.7X\nreduction in time-to-train, unlocking unprecedented model scaling.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e863D\u96c6\u6210\u5149\u4e92\u8fde\u6280\u672f\u5982\u4f55\u89e3\u51b3AI\u5de5\u4f5c\u8d1f\u8f7d\u589e\u957f\u4e2d\u7684\u4e92\u8fde\u74f6\u9888\uff0c\u901a\u8fc7\u5149\u4e92\u8fde\u5b9e\u73b0\u8de8\u673a\u67b6\u7684\u5927\u89c4\u6a21GPU\u96c6\u7fa4\uff0c\u63d0\u5347\u8bad\u7ec3\u4e07\u4ebf\u53c2\u6570MoE\u6a21\u578b\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7535\u4e92\u8fde\u53d7\u9650\u4e8e1\u7c73\u4f20\u8f93\u8ddd\u79bb\uff0c\u53ea\u80fd\u5728\u540c\u4e00\u673a\u67b6\u5185\u6269\u5c55\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21AI\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\u30023D\u5806\u53e0\u5149\u4e92\u8fde\u6280\u672f\u4e3a\u89e3\u51b3\u8de8\u673a\u67b6\u5927\u89c4\u6a21GPU\u96c6\u7fa4\u4e92\u8fde\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u75283D\u5171\u5c01\u88c5\u5149\u5b66(CPO)\u6280\u672f\uff0c\u5c06\u5149\u4e92\u8fde\u4e0eGPU\u82af\u7247\u5806\u53e0\u96c6\u6210\uff0c\u5b9e\u73b0\u9ad8\u5e26\u5bbd\u3001\u4f4e\u5ef6\u8fdf\u7684\u8de8\u673a\u67b6\u4e92\u8fde\u3002\u901a\u8fc7\u5efa\u6a21\u5206\u67903D CPO\u5728\u8bad\u7ec3\u4e07\u4ebf\u53c2\u6570MoE\u6a21\u578b\u65f6\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "3D CPO\u6280\u672f\u4f7f\u6269\u5c55\u80fd\u529b\u63d0\u53478\u500d\uff0c\u652f\u6301\u65b0\u7684\u591a\u7ef4\u5e76\u884c\u7b56\u7565\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c112.7\u500d\uff0c\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u6a21\u578b\u6269\u5c55\u80fd\u529b\u3002", "conclusion": "3D\u96c6\u6210\u5149\u4e92\u8fde\u6280\u672f\u662f\u89e3\u51b3AI\u5de5\u4f5c\u8d1f\u8f7d\u6269\u5c55\u74f6\u9888\u7684\u5173\u952e\uff0c\u4e3a\u8bad\u7ec3\u8d85\u5927\u89c4\u6a21\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.15897", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15897", "abs": "https://arxiv.org/abs/2510.15897", "authors": ["Kien Le Trung", "Truong-Son Hy"], "title": "DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms", "comment": null, "summary": "Chip placement, the task of determining optimal positions of circuit modules\non a chip canvas, is a critical step in the VLSI design flow that directly\nimpacts performance, power consumption, and routability. Traditional methods\nrely on analytical optimization or reinforcement learning, which struggle with\nhard placement constraints or require expensive online training for each new\ncircuit design. To address these limitations, we introduce DiffPlace, a\nframework that formulates chip placement as a conditional denoising diffusion\nprocess, enabling transferable placement policies that generalize to unseen\ncircuit netlists without retraining. DiffPlace leverages the generative\ncapabilities of diffusion models to efficiently explore the vast space of\nplacement while conditioning on circuit connectivity and relative quality\nmetrics to identify optimal solutions globally. Our approach combines\nenergy-guided sampling with constrained manifold diffusion to ensure placement\nlegality, achieving extremely low overlap across all experimental scenarios.\nOur method bridges the gap between optimization-based and learning-based\napproaches, offering a practical path toward automated, high-quality chip\nplacement for modern VLSI design. Our source code is publicly available at:\nhttps://github.com/HySonLab/DiffPlace/", "AI": {"tldr": "DiffPlace\u662f\u4e00\u4e2a\u57fa\u4e8e\u6761\u4ef6\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u7684\u82af\u7247\u5e03\u5c40\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7535\u8def\u7f51\u8868\uff0c\u5b9e\u73b0\u53ef\u8f6c\u79fb\u7684\u5e03\u5c40\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u82af\u7247\u5e03\u5c40\u65b9\u6cd5\u4f9d\u8d56\u5206\u6790\u4f18\u5316\u6216\u5f3a\u5316\u5b66\u4e60\uff0c\u96be\u4ee5\u5904\u7406\u786c\u6027\u5e03\u5c40\u7ea6\u675f\u6216\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u7535\u8def\u8bbe\u8ba1\u8fdb\u884c\u6602\u8d35\u7684\u5728\u7ebf\u8bad\u7ec3\u3002DiffPlace\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5c06\u82af\u7247\u5e03\u5c40\u5236\u5b9a\u4e3a\u6761\u4ef6\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u63a2\u7d22\u5e03\u5c40\u7a7a\u95f4\uff0c\u540c\u65f6\u57fa\u4e8e\u7535\u8def\u8fde\u63a5\u6027\u548c\u76f8\u5bf9\u8d28\u91cf\u6307\u6807\u8fdb\u884c\u6761\u4ef6\u7ea6\u675f\uff0c\u7ed3\u5408\u80fd\u91cf\u5f15\u5bfc\u91c7\u6837\u548c\u7ea6\u675f\u6d41\u5f62\u6269\u6563\u786e\u4fdd\u5e03\u5c40\u5408\u6cd5\u6027\u3002", "result": "\u5728\u6240\u6709\u5b9e\u9a8c\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6781\u4f4e\u7684\u91cd\u53e0\u7387\uff0c\u5f25\u5408\u4e86\u57fa\u4e8e\u4f18\u5316\u548c\u57fa\u4e8e\u5b66\u4e60\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "DiffPlace\u4e3a\u73b0\u4ee3VLSI\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u9ad8\u8d28\u91cf\u82af\u7247\u5e03\u5c40\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2510.15899", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15899", "abs": "https://arxiv.org/abs/2510.15899", "authors": ["Kiran Thorat", "Jiahui Zhao", "Yaotian Liu", "Amit Hasan", "Hongwu Peng", "Xi Xie", "Bin Lei", "Caiwen Ding"], "title": "LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are gaining prominence in various fields, thanks\nto their ability to generate high- quality content from human instructions.\nThis paper delves into the field of chip design using LLMs, specifically in\nPower- Performance-Area (PPA) optimization and the generation of accurate\nVerilog codes for circuit designs. We introduce a novel framework VeriPPA\ndesigned to optimize PPA and generate Verilog code using LLMs. Our method\nincludes a two-stage process where the first stage focuses on improving the\nfunctional and syntactic correctness of the generated Verilog codes, while the\nsecond stage focuses on optimizing the Verilog codes to meet PPA constraints of\ncircuit designs, a crucial element of chip design. Our framework achieves an\n81.37% success rate in syntactic correctness and 62.06% in functional\ncorrectness for code genera- tion, outperforming current state-of-the-art\n(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework\nachieves 99.56% syntactic correctness and 43.79% functional correctness, also\nsurpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%\nfor functional correctness. Furthermore, Our framework able to optimize the PPA\nof the designs. These results highlight the potential of LLMs in handling\ncomplex technical areas and indicate an encouraging development in the\nautomation of chip design processes.", "AI": {"tldr": "VeriPPA\u6846\u67b6\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u82af\u7247\u8bbe\u8ba1\u7684PPA\uff08\u529f\u8017-\u6027\u80fd-\u9762\u79ef\uff09\u5e76\u751f\u6210\u51c6\u786e\u7684Verilog\u4ee3\u7801\uff0c\u5728\u8bed\u6cd5\u548c\u529f\u80fd\u6b63\u786e\u6027\u65b9\u9762\u5747\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u82af\u7247\u8bbe\u8ba1\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662fPPA\u4f18\u5316\u548cVerilog\u4ee3\u7801\u751f\u6210\u8fd9\u4e24\u4e2a\u5173\u952e\u73af\u8282\uff0c\u4ee5\u63a8\u52a8\u82af\u7247\u8bbe\u8ba1\u81ea\u52a8\u5316\u8fdb\u7a0b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u6539\u8fdb\u751f\u6210Verilog\u4ee3\u7801\u7684\u529f\u80fd\u548c\u8bed\u6cd5\u6b63\u786e\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f18\u5316Verilog\u4ee3\u7801\u4ee5\u6ee1\u8db3\u7535\u8def\u8bbe\u8ba1\u7684PPA\u7ea6\u675f\u3002", "result": "\u5728RTLLM\u6570\u636e\u96c6\u4e0a\u8fbe\u523081.37%\u8bed\u6cd5\u6b63\u786e\u7387\u548c62.06%\u529f\u80fd\u6b63\u786e\u7387\uff1b\u5728VerilogEval\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.56%\u8bed\u6cd5\u6b63\u786e\u7387\u548c43.79%\u529f\u80fd\u6b63\u786e\u7387\uff0c\u5747\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6280\u672f\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u82af\u7247\u8bbe\u8ba1\u81ea\u52a8\u5316\u5e26\u6765\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u53d1\u5c55\u524d\u666f\u3002"}}
{"id": "2510.15902", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.15902", "abs": "https://arxiv.org/abs/2510.15902", "authors": ["Shuhang Zhang", "Jelena Radulovic", "Thorsten Dworzak"], "title": "Fully Automated Verification Framework for Configurable IPs: From Requirements to Results", "comment": "DVCon Europe 2025", "summary": "The increasing competition in the semiconductor industry has created\nsignificant pressure to reduce chip prices while maintaining quality and\nreliability. Functional verification, particularly for configurable IPs, is a\nmajor contributor to development costs due to its complexity and\nresource-intensive nature. To address this, we propose a fully automated\nframework for requirements driven functional verification. The framework\nautomates key processes, including vPlan generation, testbench creation,\nregression execution, and reporting in a requirements management tool,\ndrastically reducing verification effort. This approach accelerates development\ncycles, minimizes human error, and enhances coverage, offering a scalable and\nefficient solution to the challenges of verifying configurable IPs.", "AI": {"tldr": "\u63d0\u51fa\u5168\u81ea\u52a8\u6846\u67b6\u7528\u4e8e\u9700\u6c42\u9a71\u52a8\u7684\u529f\u80fd\u9a8c\u8bc1\uff0c\u81ea\u52a8\u5316vPlan\u751f\u6210\u3001\u6d4b\u8bd5\u5e73\u53f0\u521b\u5efa\u3001\u56de\u5f52\u6267\u884c\u548c\u62a5\u544a\u7b49\u5173\u952e\u6d41\u7a0b\uff0c\u5927\u5e45\u964d\u4f4e\u9a8c\u8bc1\u5de5\u4f5c\u91cf", "motivation": "\u534a\u5bfc\u4f53\u884c\u4e1a\u7ade\u4e89\u52a0\u5267\u5bfc\u81f4\u82af\u7247\u4ef7\u683c\u538b\u529b\u589e\u5927\uff0c\u800c\u529f\u80fd\u9a8c\u8bc1\u7279\u522b\u662f\u53ef\u914d\u7f6eIP\u7684\u9a8c\u8bc1\u662f\u5f00\u53d1\u6210\u672c\u7684\u4e3b\u8981\u8d21\u732e\u8005\uff0c\u56e0\u5176\u590d\u6742\u6027\u548c\u8d44\u6e90\u5bc6\u96c6\u578b\u7279\u6027", "method": "\u5f00\u53d1\u5168\u81ea\u52a8\u9700\u6c42\u9a71\u52a8\u529f\u80fd\u9a8c\u8bc1\u6846\u67b6\uff0c\u81ea\u52a8\u5316vPlan\u751f\u6210\u3001\u6d4b\u8bd5\u5e73\u53f0\u521b\u5efa\u3001\u56de\u5f52\u6267\u884c\u548c\u9700\u6c42\u7ba1\u7406\u5de5\u5177\u4e2d\u7684\u62a5\u544a\u7b49\u5173\u952e\u6d41\u7a0b", "result": "\u8be5\u6846\u67b6\u5927\u5e45\u51cf\u5c11\u9a8c\u8bc1\u5de5\u4f5c\u91cf\uff0c\u52a0\u901f\u5f00\u53d1\u5468\u671f\uff0c\u6700\u5c0f\u5316\u4eba\u4e3a\u9519\u8bef\uff0c\u63d0\u9ad8\u8986\u76d6\u7387", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9a8c\u8bc1\u53ef\u914d\u7f6eIP\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.15904", "categories": ["cs.AR", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15904", "abs": "https://arxiv.org/abs/2510.15904", "authors": ["Subhradip Chakraborty", "Ankur Singh", "Xuming Chen", "Gourav Datta", "Akhilesh R. Jaiswal"], "title": "NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme", "comment": "11 pages", "summary": "The rapid growth of deep neural network (DNN) workloads has significantly\nincreased the demand for large-capacity on-chip SRAM in machine learning (ML)\napplications, with SRAM arrays now occupying a substantial fraction of the\ntotal die area. To address the dual challenges of storage density and\ncomputation efficiency, this paper proposes an NVM-in-Cache architecture that\nintegrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell,\nforming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory\n(PIM) mode, which performs massively parallel multiply-and-accumulate (MAC)\noperations directly on cache power lines while preserving stored cache data. By\nexploiting the intrinsic properties of the 6T-2R structure, the architecture\nachieves additional storage capability, high computational throughput without\nany bit-cell area overhead. Circuit- and array-level simulations in\nGlobalFoundries 22nm FDSOI technology demonstrate that the proposed design\nachieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel\noperations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18\nneural network, achieving an accuracy of 91.27%. These results highlight the\npotential of the NVM-in-Cache approach to serve as a scalable, energy-efficient\ncomputing method by re-purposing existing 6T SRAM cache architecture for\nnext-generation AI accelerators and general purpose processors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06RRAM\u96c6\u6210\u5230\u4f20\u7edf6T-SRAM\u4e2d\u7684NVM-in-Cache\u67b6\u6784\uff0c\u5f62\u62106T-2R\u6df7\u5408\u5355\u5143\uff0c\u652f\u6301\u5b58\u5185\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u7f13\u5b58\u6570\u636e\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u80fd\u6548\u7684\u5e76\u884cMAC\u64cd\u4f5c\u3002", "motivation": "DNN\u5de5\u4f5c\u8d1f\u8f7d\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u5bf9\u7247\u4e0aSRAM\u5bb9\u91cf\u9700\u6c42\u6fc0\u589e\uff0cSRAM\u9635\u5217\u5360\u7528\u5927\u91cf\u82af\u7247\u9762\u79ef\uff0c\u9700\u8981\u89e3\u51b3\u5b58\u50a8\u5bc6\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "\u5728\u4f20\u7edf6T-SRAM\u5355\u5143\u4e2d\u96c6\u6210RRAM\u5668\u4ef6\u5f62\u62106T-2R\u6df7\u5408\u5355\u5143\uff0c\u5229\u7528\u5176\u56fa\u6709\u7279\u6027\u5b9e\u73b0\u5b58\u5185\u8ba1\u7b97\u6a21\u5f0f\uff0c\u901a\u8fc7\u7f13\u5b58\u7535\u6e90\u7ebf\u76f4\u63a5\u6267\u884c\u5e76\u884c\u4e58\u7d2f\u52a0\u64cd\u4f5c\u3002", "result": "\u572822nm FDSOI\u6280\u672f\u4e2d\u5b9e\u73b00.4 TOPS\u541e\u5410\u91cf\u548c491.78 TOPS/W\u80fd\u6548\uff0c\u4f7f\u7528Resnet-18\u7f51\u7edc\u5728CIFAR-10\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u523091.27%\u51c6\u786e\u7387\u3002", "conclusion": "NVM-in-Cache\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u73b0\u67096T SRAM\u7f13\u5b58\u67b6\u6784\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u52a0\u901f\u5668\u548c\u901a\u7528\u5904\u7406\u5668\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u8ba1\u7b97\u65b9\u6848\u3002"}}
{"id": "2510.15906", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15906", "abs": "https://arxiv.org/abs/2510.15906", "authors": ["Yunsheng Bai", "Ghaith Bany Hamad", "Chia-Tung Ho", "Syed Suhaib", "Haoxing Ren"], "title": "FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures", "comment": null, "summary": "Debugging formal verification (FV) failures represents one of the most\ntime-consuming bottlenecks in modern hardware design workflows. When properties\nfail, engineers must manually trace through complex counter-examples spanning\nmultiple cycles, analyze waveforms, and cross-reference design specifications\nto identify root causes - a process that can consume hours or days per bug.\nExisting solutions are largely limited to manual waveform viewers or simple\nautomated tools that cannot reason about the complex interplay between design\nintent and implementation logic. We present FVDebug, an intelligent system that\nautomates root-cause analysis by combining multiple data sources - waveforms,\nRTL code, design specifications - to transform failure traces into actionable\ninsights. Our approach features a novel pipeline: (1) Causal Graph Synthesis\nthat structures failure traces into directed acyclic graphs, (2) Graph Scanner\nusing batched Large Language Model (LLM) analysis with for-and-against\nprompting to identify suspicious nodes, and (3) Insight Rover leveraging\nagentic narrative exploration to generate high-level causal explanations.\nFVDebug further provides concrete RTL fixes through its Fix Generator.\nEvaluated on open benchmarks, FVDebug attains high hypothesis quality and\nstrong Pass@k fix rates. We further report results on two proprietary,\nproduction-scale FV counterexamples. These results demonstrate FVDebug's\napplicability from academic benchmarks to industrial designs.", "AI": {"tldr": "FVDebug\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u786c\u4ef6\u8bbe\u8ba1\u5f62\u5f0f\u9a8c\u8bc1\u5931\u8d25\u8c03\u8bd5\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u6ce2\u5f62\u3001RTL\u4ee3\u7801\u548c\u8bbe\u8ba1\u89c4\u8303\uff0c\u5c06\u5931\u8d25\u8ddf\u8e2a\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5e76\u63d0\u4f9b\u5177\u4f53\u7684RTL\u4fee\u590d\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u786c\u4ef6\u8bbe\u8ba1\u5f62\u5f0f\u9a8c\u8bc1\u5931\u8d25\u8c03\u8bd5\u7684\u74f6\u9888\u95ee\u9898\uff0c\u8be5\u8fc7\u7a0b\u901a\u5e38\u9700\u8981\u5de5\u7a0b\u5e08\u624b\u52a8\u5206\u6790\u590d\u6742\u53cd\u4f8b\u3001\u6ce2\u5f62\u548c\u8bbe\u8ba1\u89c4\u8303\uff0c\u8017\u65f6\u6570\u5c0f\u65f6\u751a\u81f3\u6570\u5929\u3002\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u5904\u7406\u8bbe\u8ba1\u610f\u56fe\u4e0e\u5b9e\u73b0\u903b\u8f91\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a1) \u56e0\u679c\u56fe\u5408\u6210\uff0c\u5c06\u5931\u8d25\u8ddf\u8e2a\u7ed3\u6784\u5316\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff1b2) \u56fe\u626b\u63cf\u5668\uff0c\u4f7f\u7528\u6279\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u548c\u6b63\u53cd\u63d0\u793a\u6765\u8bc6\u522b\u53ef\u7591\u8282\u70b9\uff1b3) \u6d1e\u5bdf\u6f2b\u6e38\u5668\uff0c\u5229\u7528\u4ee3\u7406\u53d9\u4e8b\u63a2\u7d22\u751f\u6210\u9ad8\u7ea7\u56e0\u679c\u89e3\u91ca\u3002\u7cfb\u7edf\u8fd8\u5305\u542b\u4fee\u590d\u751f\u6210\u5668\u63d0\u4f9b\u5177\u4f53RTL\u4fee\u590d\u3002", "result": "\u5728\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFVDebug\u5b9e\u73b0\u4e86\u9ad8\u5047\u8bbe\u8d28\u91cf\u548c\u5f3aPass@k\u4fee\u590d\u7387\u3002\u5728\u4e24\u4e2a\u4e13\u6709\u7684\u751f\u4ea7\u7ea7\u5f62\u5f0f\u9a8c\u8bc1\u53cd\u4f8b\u4e0a\u7684\u7ed3\u679c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u7cfb\u7edf\u4ece\u5b66\u672f\u57fa\u51c6\u5230\u5de5\u4e1a\u8bbe\u8ba1\u7684\u9002\u7528\u6027\u3002", "conclusion": "FVDebug\u6210\u529f\u5c55\u793a\u4e86\u5c06\u5f62\u5f0f\u9a8c\u8bc1\u5931\u8d25\u8ddf\u8e2a\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u89c1\u89e3\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u667a\u80fd\u5206\u6790\u591a\u6570\u636e\u6e90\u5b9e\u73b0\u4e86\u4ece\u5b66\u672f\u57fa\u51c6\u5230\u5de5\u4e1a\u8bbe\u8ba1\u7684\u6709\u6548\u8c03\u8bd5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15907", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15907", "abs": "https://arxiv.org/abs/2510.15907", "authors": ["Era Thaqi", "Dennis Eigner", "Arman Ferdowsi", "Ulrich Schmid"], "title": "Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions", "comment": null, "summary": "We propose a novel approach to symbolic timing analysis for digital\nintegrated circuits based on recently developed analytic delay formulas for\n2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a\nfixed order of the transitions of all input and internal signals of a circuit,\nour framework computes closed-form analytic delay expressions for all the\ninternal signal transition times that depend on (i) the symbolic transition\ntimes of the relevant input signals and (ii) the model parameters of the\nrelevant gates. The resulting formulas facilitate per-transition timing\nanalysis without any simulation, by instantiating the symbolic input transition\ntimes and the gate parameters. More importantly, however, they also enable an\n\\emph{analytic} study of the dependencies of certain timing properties on input\nsignals and gate parameters. For instance, differentiating a symbolic delay\nexpression with respect to a gate parameter or input transition time enables\nsensitivity analysis. As a proof of concept, we implement our approach using\nthe computer algebra system SageMath and apply it to the NOR-gate version of\nthe c17 slack benchmark circuit.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u6790\u5ef6\u8fdf\u516c\u5f0f\u7684\u7b26\u53f7\u65f6\u5e8f\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u5185\u90e8\u4fe1\u53f7\u8f6c\u6362\u65f6\u95f4\u7684\u95ed\u5f0f\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5b9e\u73b0\u65e0\u9700\u4eff\u771f\u7684\u65f6\u5e8f\u5206\u6790\u548c\u7075\u654f\u5ea6\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u65f6\u5e8f\u5206\u6790\u901a\u5e38\u4f9d\u8d56\u4eff\u771f\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5bf9\u65f6\u5e8f\u7279\u6027\u4e0e\u8f93\u5165\u4fe1\u53f7\u53ca\u95e8\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u7684\u89e3\u6790\u7406\u89e3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u7b26\u53f7\u5316\u5206\u6790\u7684\u65b9\u6cd5\u6765\u6df1\u5165\u7406\u89e3\u7535\u8def\u65f6\u5e8f\u884c\u4e3a\u3002", "method": "\u57fa\u4e8eFerdowsi\u7b49\u4eba\u5f00\u53d1\u76842\u8f93\u5165NOR\u3001NAND\u548cMuller-C\u95e8\u7684\u89e3\u6790\u5ef6\u8fdf\u516c\u5f0f\uff0c\u5728\u56fa\u5b9a\u4fe1\u53f7\u8f6c\u6362\u987a\u5e8f\u4e0b\uff0c\u8ba1\u7b97\u5185\u90e8\u4fe1\u53f7\u8f6c\u6362\u65f6\u95f4\u7684\u95ed\u5f0f\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u8fd9\u4e9b\u8868\u8fbe\u5f0f\u4f9d\u8d56\u4e8e\u8f93\u5165\u4fe1\u53f7\u7684\u7b26\u53f7\u8f6c\u6362\u65f6\u95f4\u548c\u76f8\u5173\u95e8\u7684\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5b9e\u73b0\u4e86\u57fa\u4e8eSageMath\u8ba1\u7b97\u673a\u4ee3\u6570\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u5e76\u5728c17 slack\u57fa\u51c6\u7535\u8def\u7684NOR\u95e8\u7248\u672c\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u80fd\u591f\u8fdb\u884c\u65e0\u4eff\u771f\u7684\u65f6\u5e8f\u5206\u6790\u548c\u7075\u654f\u5ea6\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u8fdb\u884c\u4f20\u7edf\u7684\u65f6\u5e8f\u5206\u6790\uff0c\u66f4\u91cd\u8981\u7684\u662f\u80fd\u591f\u89e3\u6790\u5730\u7814\u7a76\u65f6\u5e8f\u7279\u6027\u5bf9\u8f93\u5165\u4fe1\u53f7\u548c\u95e8\u53c2\u6570\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3a\u7535\u8def\u8bbe\u8ba1\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2510.15908", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15908", "abs": "https://arxiv.org/abs/2510.15908", "authors": ["Hana Chitsaz", "Johnson Umeike", "Amirmahdi Namjoo", "Babak N. Safa", "Bahar Asgari"], "title": "Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations", "comment": null, "summary": "Finite element simulations are essential in biomechanics, enabling detailed\nmodeling of tissues and organs. However, architectural inefficiencies in\ncurrent hardware and software stacks limit performance and scalability,\nespecially for iterative tasks like material parameter identification. As a\nresult, workflows often sacrifice fidelity for tractability. Reconfigurable\nhardware, such as FPGAs, offers a promising path to domain-specific\nacceleration without the cost of ASICs, but its potential in biomechanics\nremains underexplored. This paper presents Belenos, a comprehensive workload\ncharacterization of finite element biomechanics using FEBio, a widely adopted\nsimulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal\nthat smaller workloads experience moderate front-end stalls, typically around\n13.1%, whereas larger workloads are dominated by significant back-end\nbottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.\nComplementary gem5 sensitivity studies identify optimal hardware configurations\nfor Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,\nmemory, or branch predictor settings can degrade performance by up to 37.1%.\nThese findings underscore the need for architecture-aware co-design to\nefficiently support biomechanical simulation workloads.", "AI": {"tldr": "Belenos\u5bf9\u751f\u7269\u529b\u5b66\u6709\u9650\u5143\u6a21\u62df\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5c0f\u5de5\u4f5c\u8d1f\u8f7d\u5b58\u5728\u524d\u7aef\u74f6\u9888\uff08\u7ea613.1%\uff09\uff0c\u5927\u5de5\u4f5c\u8d1f\u8f7d\u5b58\u5728\u663e\u8457\u540e\u7aef\u74f6\u9888\uff0859.9%-82.2%\uff09\uff0c\u5e76\u901a\u8fc7gem5\u654f\u611f\u6027\u7814\u7a76\u786e\u5b9a\u4e86DSAs\u7684\u6700\u4f73\u786c\u4ef6\u914d\u7f6e\u3002", "motivation": "\u5f53\u524d\u751f\u7269\u529b\u5b66\u6709\u9650\u5143\u6a21\u62df\u5728\u786c\u4ef6\u548c\u8f6f\u4ef6\u67b6\u6784\u4e0a\u5b58\u5728\u6548\u7387\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u6750\u6599\u53c2\u6570\u8bc6\u522b\u7b49\u8fed\u4ee3\u4efb\u52a1\u4e2d\uff0c\u5f80\u5f80\u9700\u8981\u5728\u7cbe\u5ea6\u548c\u53ef\u5904\u7406\u6027\u4e4b\u95f4\u6743\u8861\u3002\u53ef\u91cd\u6784\u786c\u4ef6\u5982FPGA\u63d0\u4f9b\u4e86\u9886\u57df\u7279\u5b9a\u52a0\u901f\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u751f\u7269\u529b\u5b66\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528FEBio\u6a21\u62df\u5668\u8fdb\u884c\u751f\u7269\u529b\u5b66\u6709\u9650\u5143\u5de5\u4f5c\u8d1f\u8f7d\u8868\u5f81\uff0c\u7ed3\u5408gem5\u654f\u611f\u6027\u7814\u7a76\u548cVTune\u6027\u80fd\u5206\u6790\uff0c\u8bc6\u522b\u786c\u4ef6\u74f6\u9888\u548c\u4f18\u5316\u914d\u7f6e\u3002", "result": "VTune\u5206\u6790\u663e\u793a\u5c0f\u5de5\u4f5c\u8d1f\u8f7d\u524d\u7aef\u505c\u6ede\u7ea613.1%\uff0c\u5927\u5de5\u4f5c\u8d1f\u8f7d\u540e\u7aef\u74f6\u9888\u8fbe59.9%-82.2%\u3002gem5\u7814\u7a76\u8868\u660e\u6b21\u4f18\u7684\u6d41\u6c34\u7ebf\u3001\u5185\u5b58\u6216\u5206\u652f\u9884\u6d4b\u5668\u8bbe\u7f6e\u53ef\u4f7f\u6027\u80fd\u4e0b\u964d\u8fbe37.1%\u3002", "conclusion": "\u9700\u8981\u67b6\u6784\u611f\u77e5\u7684\u534f\u540c\u8bbe\u8ba1\u6765\u6709\u6548\u652f\u6301\u751f\u7269\u529b\u5b66\u6a21\u62df\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u9886\u57df\u7279\u5b9a\u52a0\u901f\u5668\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u8fdb\u884c\u4f18\u5316\u914d\u7f6e\u3002"}}
{"id": "2510.15910", "categories": ["cs.AR", "hep-ex"], "pdf": "https://arxiv.org/pdf/2510.15910", "abs": "https://arxiv.org/abs/2510.15910", "authors": ["Marvin Fuchs", "Lukas Scheller", "Timo Muscheid", "Oliver Sander", "Luis E. Ardila-Perez"], "title": "SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs", "comment": "26 pages, single-column, 13 figures, 2 tables", "summary": "Modern heterogeneous System-on-Chip (SoC) devices integrate advanced\ncomponents into a single package, offering powerful capabilities while also\nintroducing significant complexity. To manage these sophisticated devices,\nfirmware and software developers need powerful development tools. However, as\nthese tools become increasingly complex, they often lack adequate support,\nresulting in a steep learning curve and challenging troubleshooting. To address\nthis, this work introduces System-on-Chip blocks (SoCks), a flexible and\nexpandable build framework that reduces complexity by partitioning the SoC\nimage into high-level units called blocks. SoCks builds each firmware and\nsoftware block in an encapsulated way, independently from other components of\nthe image, thereby reducing dependencies to a minimum. While some information\nexchange between the blocks is unavoidable to ensure seamless runtime\nintegration, this interaction is standardized via interfaces. A small number of\ndependencies and well-defined interfaces simplify the reuse of existing block\nimplementations and facilitate seamless substitution between versions-for\ninstance, when choosing root file systems for the embedded Linux operating\nsystem. Additionally, this approach facilitates the establishment of a\ndecentralized and partially automated development flow through Continuous\nIntegration and Continuous Delivery (CI/CD). Measurement results demonstrate\nthat SoCks can build a complete SoC image up to three times faster than\nestablished tools.", "AI": {"tldr": "SoCks\u662f\u4e00\u4e2a\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5c06SoC\u955c\u50cf\u5206\u533a\u4e3a\u9ad8\u7ea7\u5355\u5143\uff08\u5757\uff09\u6765\u964d\u4f4e\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u72ec\u7acb\u6784\u5efa\u548c\u6700\u5c0f\u5316\u4f9d\u8d56\uff0c\u4f7f\u6784\u5efa\u901f\u5ea6\u6bd4\u73b0\u6709\u5de5\u5177\u5feb3\u500d\u3002", "motivation": "\u73b0\u4ee3\u5f02\u6784SoC\u8bbe\u5907\u96c6\u6210\u590d\u6742\u7ec4\u4ef6\uff0c\u5f00\u53d1\u5de5\u5177\u65e5\u76ca\u590d\u6742\u4f46\u7f3a\u4e4f\u8db3\u591f\u652f\u6301\uff0c\u5bfc\u81f4\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u548c\u6545\u969c\u6392\u9664\u56f0\u96be\u3002", "method": "\u5c06SoC\u955c\u50cf\u5206\u533a\u4e3a\u9ad8\u5c42\u6b21\u7684\u5757\u5355\u5143\uff0c\u6bcf\u4e2a\u56fa\u4ef6\u548c\u8f6f\u4ef6\u5757\u4ee5\u5c01\u88c5\u65b9\u5f0f\u72ec\u7acb\u6784\u5efa\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u63a5\u53e3\u5b9e\u73b0\u5757\u95f4\u4fe1\u606f\u4ea4\u6362\uff0c\u6700\u5c0f\u5316\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "SoCks\u80fd\u591f\u6bd4\u73b0\u6709\u5de5\u5177\u5feb3\u500d\u6784\u5efa\u5b8c\u6574\u7684SoC\u955c\u50cf\uff0c\u7b80\u5316\u73b0\u6709\u5757\u5b9e\u73b0\u7684\u590d\u7528\uff0c\u4fc3\u8fdb\u7248\u672c\u95f4\u65e0\u7f1d\u66ff\u6362\u3002", "conclusion": "SoCks\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86SoC\u5f00\u53d1\u7684\u590d\u6742\u6027\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6807\u51c6\u5316\u63a5\u53e3\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u548c\u90e8\u5206\u81ea\u52a8\u5316\u7684\u5f00\u53d1\u6d41\u7a0b\u3002"}}
{"id": "2510.15914", "categories": ["cs.AR", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.15914", "abs": "https://arxiv.org/abs/2510.15914", "authors": ["Jiayu Zhao", "Song Chen"], "title": "VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts", "comment": "9 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ngenerating Verilog code from natural language descriptions. However, Verilog\ncode inherently encodes structural information of hardware circuits.\nEffectively leveraging this structural information to enhance the functional\nand syntactic correctness of LLM-generated Verilog code remains a significant\nchallenge. To address this challenge, we propose VeriGRAG , a novel framework\nthat extracts structural graph embeddings from Verilog code using graph neural\nnetworks (GNNs). A multimodal retriever then selects the graph embeddings most\nrelevant to the given generation task, which are aligned with the code modality\nthrough the VeriFormer module to generate structure-aware soft prompts. Our\nexperiments demonstrate that VeriGRAG substantially improves the correctness of\nVerilog code generation, achieving state-of-the-art or superior performance\nacross both VerilogEval and RTLLM benchmarks.", "AI": {"tldr": "VeriGRAG\u6846\u67b6\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6Verilog\u4ee3\u7801\u7684\u7ed3\u6784\u56fe\u5d4c\u5165\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u68c0\u7d22\u5668\u9009\u62e9\u76f8\u5173\u5d4c\u5165\uff0c\u751f\u6210\u7ed3\u6784\u611f\u77e5\u7684\u8f6f\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347LLM\u751f\u6210Verilog\u4ee3\u7801\u7684\u6b63\u786e\u6027\u3002", "motivation": "Verilog\u4ee3\u7801\u7f16\u7801\u4e86\u786c\u4ef6\u7535\u8def\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u7ed3\u6784\u4fe1\u606f\u6765\u63d0\u5347LLM\u751f\u6210\u4ee3\u7801\u7684\u529f\u80fd\u548c\u8bed\u6cd5\u6b63\u786e\u6027\u3002", "method": "\u4f7f\u7528GNN\u63d0\u53d6Verilog\u4ee3\u7801\u7684\u7ed3\u6784\u56fe\u5d4c\u5165\uff0c\u591a\u6a21\u6001\u68c0\u7d22\u5668\u9009\u62e9\u76f8\u5173\u5d4c\u5165\uff0c\u901a\u8fc7VeriFormer\u6a21\u5757\u5bf9\u9f50\u4ee3\u7801\u6a21\u6001\u751f\u6210\u7ed3\u6784\u611f\u77e5\u8f6f\u63d0\u793a\u3002", "result": "\u5728VerilogEval\u548cRTLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6216\u66f4\u4f18\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Verilog\u4ee3\u7801\u751f\u6210\u7684\u6b63\u786e\u6027\u3002", "conclusion": "VeriGRAG\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528Verilog\u4ee3\u7801\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u6210\u529f\u63d0\u5347\u4e86LLM\u751f\u6210Verilog\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u6b63\u786e\u6027\u3002"}}
{"id": "2510.15917", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15917", "abs": "https://arxiv.org/abs/2510.15917", "authors": ["Shai Bergman", "Won Wook Song", "Lukas Cavigelli", "Konstantin Berestizshevsky", "Ke Zhou", "Ji Zhang"], "title": "Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding", "comment": null, "summary": "Existing storage systems lack visibility into workload intent, limiting their\nability to adapt to the semantics of modern, large-scale data-intensive\napplications. This disconnect leads to brittle heuristics and fragmented,\nsiloed optimizations. To address these limitations, we propose Intent-Driven\nStorage Systems (IDSS), a vision for a new paradigm where large language models\n(LLMs) infer workload and system intent from unstructured signals to guide\nadaptive and cross-layer parameter reconfiguration. IDSS provides holistic\nreasoning for competing demands, synthesizing safe and efficient decisions\nwithin policy guardrails. We present four design principles for integrating\nLLMs into storage control loops and propose a corresponding system\narchitecture. Initial results on FileBench workloads show that IDSS can improve\nIOPS by up to 2.45X by interpreting intent and generating actionable\nconfigurations for storage components such as caching and prefetching. These\nfindings suggest that, when constrained by guardrails and embedded within\nstructured workflows, LLMs can function as high-level semantic optimizers,\nbridging the gap between application goals and low-level system control. IDSS\npoints toward a future in which storage systems are increasingly adaptive,\nautonomous, and aligned with dynamic workload demands.", "AI": {"tldr": "\u63d0\u51fa\u610f\u56fe\u9a71\u52a8\u5b58\u50a8\u7cfb\u7edf(IDSS)\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u975e\u7ed3\u6784\u5316\u4fe1\u53f7\u4e2d\u63a8\u65ad\u5de5\u4f5c\u8d1f\u8f7d\u610f\u56fe\uff0c\u6307\u5bfc\u5b58\u50a8\u7cfb\u7edf\u81ea\u9002\u5e94\u548c\u8de8\u5c42\u53c2\u6570\u91cd\u914d\u7f6e\uff0c\u5728FileBench\u6d4b\u8bd5\u4e2d\u53ef\u5c06IOPS\u63d0\u5347\u81f3\u591a2.45\u500d\u3002", "motivation": "\u73b0\u6709\u5b58\u50a8\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u610f\u56fe\u7684\u53ef\u89c1\u6027\uff0c\u5bfc\u81f4\u542f\u53d1\u5f0f\u65b9\u6cd5\u8106\u5f31\u4e14\u4f18\u5316\u96f6\u6563\uff0c\u65e0\u6cd5\u9002\u5e94\u73b0\u4ee3\u5927\u89c4\u6a21\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u7684\u8bed\u4e49\u9700\u6c42\u3002", "method": "\u63d0\u51faIDSS\u67b6\u6784\uff0c\u5c06LLM\u96c6\u6210\u5230\u5b58\u50a8\u63a7\u5236\u56de\u8def\u4e2d\uff0c\u901a\u8fc7\u56db\u4e2a\u8bbe\u8ba1\u539f\u5219\uff1a\u610f\u56fe\u63a8\u65ad\u3001\u8de8\u5c42\u91cd\u914d\u7f6e\u3001\u7b56\u7565\u62a4\u680f\u548c\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u8bed\u4e49\u9a71\u52a8\u7684\u5b58\u50a8\u4f18\u5316\u3002", "result": "\u5728FileBench\u5de5\u4f5c\u8d1f\u8f7d\u6d4b\u8bd5\u4e2d\uff0cIDSS\u901a\u8fc7\u89e3\u91ca\u610f\u56fe\u5e76\u4e3a\u7f13\u5b58\u548c\u9884\u53d6\u7b49\u5b58\u50a8\u7ec4\u4ef6\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u914d\u7f6e\uff0c\u5c06IOPS\u63d0\u5347\u81f3\u591a2.45\u500d\u3002", "conclusion": "\u5728\u7b56\u7565\u62a4\u680f\u7ea6\u675f\u4e0b\uff0cLLM\u53ef\u4f5c\u4e3a\u9ad8\u5c42\u6b21\u8bed\u4e49\u4f18\u5316\u5668\uff0c\u5f25\u5408\u5e94\u7528\u76ee\u6807\u4e0e\u5e95\u5c42\u7cfb\u7edf\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u52a8\u5b58\u50a8\u7cfb\u7edf\u5411\u66f4\u81ea\u9002\u5e94\u3001\u81ea\u4e3b\u548c\u52a8\u6001\u5bf9\u9f50\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2510.15926", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15926", "abs": "https://arxiv.org/abs/2510.15926", "authors": ["Ye Qiao", "Zhiheng Chen", "Yifan Zhang", "Yian Wang", "Sitao Huang"], "title": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs", "comment": null, "summary": "With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs.", "AI": {"tldr": "TeLLMe\u662f\u4e00\u4e2a\u57fa\u4e8e\u67e5\u8868\u76841.58\u4f4d\u4e09\u5143LLM\u52a0\u901f\u5668\uff0c\u4e13\u4e3a\u4f4e\u529f\u8017\u8fb9\u7f18FPGA\u8bbe\u8ba1\uff0c\u652f\u6301\u9884\u586b\u5145\u548c\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u57285W\u529f\u8017\u4e0b\u5b9e\u73b0\u9ad8\u8fbe25 tokens/s\u7684\u89e3\u7801\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u5728\u8fb9\u7f18\u5e73\u53f0\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6210\u4e3a\u8feb\u5207\u9700\u6c42\uff0c\u4f46\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u3001\u5185\u5b58\u9700\u6c42\u548c\u529f\u8017\u9884\u7b97\uff0c\u7279\u522b\u662f\u9884\u586b\u5145\u9636\u6bb5\u7684\u957f\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8868\u67e5\u627e\u4e09\u5143\u77e9\u9635\u4e58\u6cd5\u5f15\u64ce\u3001\u7ec6\u7c92\u5ea6URAM\u6743\u91cd\u7f13\u51b2\u7ba1\u7406\u3001\u6d41\u5f0f\u6570\u636e\u6d41\u67b6\u6784\u3001\u53cd\u5411\u91cd\u6392\u5e8f\u9884\u586b\u5145\u6ce8\u610f\u529b\u4ee5\u53ca\u8d44\u6e90\u9ad8\u6548\u89e3\u7801\u6ce8\u610f\u529b\u7b49\u521b\u65b0\u6280\u672f\u3002", "result": "\u57285W\u529f\u8017\u9884\u7b97\u4e0b\uff0c\u5b9e\u73b025 tokens/s\u7684\u89e3\u7801\u541e\u5410\u91cf\uff0c\u5bf9\u4e8e64-128\u4e2atoken\u7684\u63d0\u793a\uff0c\u9996token\u5ef6\u8fdf\u4e3a0.45-0.96\u79d2\u3002", "conclusion": "TeLLMe\u5728\u8fb9\u7f18FPGA\u4e0a\u7684LLM\u63a8\u7406\u80fd\u6548\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684LLM\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15927", "categories": ["cs.AR", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15927", "abs": "https://arxiv.org/abs/2510.15927", "authors": ["Krystian Chmielewski", "Jaros\u0142aw \u0141awnicki", "Uladzislau Lukyanau", "Tadeusz Kobus", "Maciej Maciejewski"], "title": "UPMEM Unleashed: Software Secrets for Speed", "comment": null, "summary": "Developing kernels for Processing-In-Memory (PIM) platforms poses unique\nchallenges in data management and parallel programming on limited processing\nunits. Although software development kits (SDKs) for PIM, such as the UPMEM\nSDK, provide essential tools, these emerging platforms still leave significant\nroom for performance optimization. In this paper, we reveal surprising\ninefficiencies in UPMEM software stack and play with non-standard programming\ntechniques. By making simple modifications to the assembly generated by the\nUPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x\nin integer multiplication, depending on the data type. We also demonstrate that\nbit-serial processing of low precision data is a viable option for UPMEM: in\nINT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup\nover the baseline. Minor API extensions for PIM allocation that account for the\nnon-uniform memory access (NUMA) architecture of the server further improve the\nconsistency and throughput of host-PIM data transfers by up to 2.9x. Finally,\nwe show that, when the matrix is preloaded into PIM, our optimized kernels\noutperform a dual-socket CPU server by over 3x for INT8 generalized\nmatrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized\nINT8 GEMV kernel outperforms the baseline 3.5x.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86UPMEM PIM\u5e73\u53f0\u8f6f\u4ef6\u6808\u4e2d\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u901a\u8fc7\u6c47\u7f16\u4f18\u5316\u3001\u4f4d\u4e32\u884c\u5904\u7406\u7b49\u6280\u672f\uff0c\u5728\u6574\u6570\u8fd0\u7b97\u548c\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "PIM\u5e73\u53f0\u5728\u6570\u636e\u7ba1\u7406\u548c\u5e76\u884c\u7f16\u7a0b\u65b9\u9762\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709SDK\u4ecd\u6709\u8f83\u5927\u6027\u80fd\u4f18\u5316\u7a7a\u95f4\uff0c\u9700\u8981\u63a2\u7d22\u975e\u6807\u51c6\u7f16\u7a0b\u6280\u672f\u6765\u63d0\u5347\u6548\u7387\u3002", "method": "\u4fee\u6539UPMEM\u7f16\u8bd1\u5668\u751f\u6210\u7684\u6c47\u7f16\u4ee3\u7801\uff0c\u91c7\u7528\u4f4d\u4e32\u884c\u5904\u7406\u4f4e\u7cbe\u5ea6\u6570\u636e\uff0c\u6269\u5c55API\u4ee5\u8003\u8651NUMA\u67b6\u6784\uff0c\u4f18\u5316\u4e3b\u673a-PIM\u6570\u636e\u4f20\u8f93\u3002", "result": "\u6574\u6570\u52a0\u6cd5\u901f\u5ea6\u63d0\u53471.6-2\u500d\uff0c\u4e58\u6cd5\u63d0\u53471.4-5.9\u500d\uff1bINT4\u4f4d\u4e32\u884c\u70b9\u79ef\u8ba1\u7b97\u901f\u5ea6\u63d0\u53472.7\u500d\uff1b\u4e3b\u673a-PIM\u6570\u636e\u4f20\u8f93\u541e\u5410\u91cf\u63d0\u53472.9\u500d\uff1bINT8 GEMV\u6bd4CPU\u5feb3\u500d\uff0cINT4 GEMV\u5feb10\u500d\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u7684\u8f6f\u4ef6\u6808\u4f18\u5316\u548c\u7f16\u7a0b\u6280\u672f\u6539\u8fdb\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347PIM\u5e73\u53f0\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u7cbe\u5ea6\u77e9\u9635\u8fd0\u7b97\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2510.15930", "categories": ["cs.AR", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.15930", "abs": "https://arxiv.org/abs/2510.15930", "authors": ["Philippe Magalh\u00e3es", "Virginie Fresse", "Beno\u00eet Suffran", "Olivier Alata"], "title": "Impl\u00e9mentation Efficiente de Fonctions de Convolution sur FPGA \u00e0 l'Aide de Blocs Param\u00e9trables et d'Approximations Polynomiales", "comment": "in French language, XXXe Colloque Francophone de Traitement du Signal\n  et des Images (GRETSI), Aug 2025, Strabourg, France", "summary": "Implementing convolutional neural networks (CNNs) on field-programmable gate\narrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower\nlatency, greater power efficiency and greater flexibility. However, this\ndevelopment remains complex due to the hardware knowledge required and the long\nsynthesis, placement and routing stages, which slow down design cycles and\nprevent rapid exploration of network configurations, making resource\noptimisation under severe constraints particularly challenging. This paper\nproposes a library of configurable convolution Blocks designed to optimize FPGA\nimplementation and adapt to available resources. It also presents a\nmethodological framework for developing mathematical models that predict FPGA\nresources utilization. The approach is validated by analyzing the correlation\nbetween the parameters, followed by error metrics. The results show that the\ndesigned blocks enable adaptation of convolution layers to hardware\nconstraints, and that the models accurately predict resource consumption,\nproviding a useful tool for FPGA selection and optimized CNN deployment.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u914d\u7f6e\u5377\u79ef\u5757\u5e93\u548cFPGA\u8d44\u6e90\u9884\u6d4b\u6a21\u578b\uff0c\u4f18\u5316CNN\u5728FPGA\u4e0a\u7684\u90e8\u7f72\uff0c\u89e3\u51b3\u786c\u4ef6\u77e5\u8bc6\u9700\u6c42\u548c\u957f\u8bbe\u8ba1\u5468\u671f\u95ee\u9898\u3002", "motivation": "FPGA\u5b9e\u73b0CNN\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u80fd\u6548\u548c\u7075\u6d3b\u6027\u4f18\u52bf\uff0c\u4f46\u5f00\u53d1\u590d\u6742\uff0c\u9700\u8981\u786c\u4ef6\u77e5\u8bc6\u4e14\u8bbe\u8ba1\u5468\u671f\u957f\uff0c\u96be\u4ee5\u5feb\u901f\u63a2\u7d22\u7f51\u7edc\u914d\u7f6e\u548c\u8d44\u6e90\u4f18\u5316\u3002", "method": "\u8bbe\u8ba1\u53ef\u914d\u7f6e\u5377\u79ef\u5757\u5e93\u4ee5\u9002\u5e94\u786c\u4ef6\u8d44\u6e90\u7ea6\u675f\uff0c\u5e76\u5f00\u53d1\u6570\u5b66\u6a21\u578b\u9884\u6d4bFPGA\u8d44\u6e90\u5229\u7528\u7387\uff0c\u901a\u8fc7\u53c2\u6570\u76f8\u5173\u6027\u5206\u6790\u548c\u8bef\u5dee\u6307\u6807\u9a8c\u8bc1\u3002", "result": "\u8bbe\u8ba1\u7684\u5377\u79ef\u5757\u80fd\u591f\u6839\u636e\u786c\u4ef6\u7ea6\u675f\u8c03\u6574\u5377\u79ef\u5c42\uff0c\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u8d44\u6e90\u6d88\u8017\uff0c\u4e3aFPGA\u9009\u62e9\u548cCNN\u4f18\u5316\u90e8\u7f72\u63d0\u4f9b\u6709\u7528\u5de5\u5177\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86FPGA\u4e0aCNN\u5b9e\u73b0\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u90e8\u7f72\u548c\u5feb\u901f\u8bbe\u8ba1\u8fed\u4ee3\u3002"}}
{"id": "2510.16040", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16040", "abs": "https://arxiv.org/abs/2510.16040", "authors": ["Tianhua Xia", "Sai Qian Zhang"], "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing", "comment": null, "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions.", "AI": {"tldr": "\u63d0\u51faKelle\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f7f\u7528eDRAM\u5b58\u50a8LLM\u7684KV\u7f13\u5b58\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u5185\u5b58\u7ba1\u7406\u7b97\u6cd5\uff0c\u5b9e\u73b03.9\u500d\u52a0\u901f\u548c4.5\u500d\u80fd\u8017\u8282\u7701", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884cLLM\u53ef\u964d\u4f4e\u5ef6\u8fdf\u3001\u63d0\u5347\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46KV\u7f13\u5b58\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u5bfc\u81f4\u5185\u5b58\u5360\u7528\u5927\uff0c\u800c\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u6709\u9650\u96be\u4ee5\u9ad8\u6548\u5b58\u50a8\u548c\u8bbf\u95ee\u5927\u7f13\u5b58", "method": "\u4f7f\u7528eDRAM\u4f5c\u4e3a\u8fb9\u7f18\u8bbe\u5907LLM\u670d\u52a1\u7684\u4e3b\u8981\u5b58\u50a8\uff0c\u63d0\u51faKelle\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u5185\u5b58\u9a71\u9010\u3001\u91cd\u8ba1\u7b97\u548c\u5237\u65b0\u63a7\u5236\u7b97\u6cd5", "result": "Kelle\u52a0\u901f\u5668\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6848\u5b9e\u73b03.9\u500d\u52a0\u901f\u548c4.5\u500d\u80fd\u8017\u8282\u7701", "conclusion": "Kelle\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72LLM\u65f6\u7684KV\u7f13\u5b58\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7eDRAM\u548c\u4f18\u5316\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548"}}
{"id": "2510.16487", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.16487", "abs": "https://arxiv.org/abs/2510.16487", "authors": ["Giovanni Agosta", "Stefano Cherubin", "Derek Christ", "Francesco Conti", "Asbj\u00f8rn Djupdal", "Matthias Jung", "Georgios Keramidas", "Roberto Passerone", "Paolo Rech", "Elisa Ricci", "Philippe Velha", "Flavio Vella", "Kasim Sinan Yildirim", "Nils Wilbert"], "title": "Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project", "comment": null, "summary": "ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,\nin particular, optoelectronic, volatile and non-volatile processing-in-memory,\nand neuromorphic, to tackle the power, efficiency, and scalability bottlenecks\nof AI with an emphasis on defense use cases (e.g., autonomous vehicles,\nsurveillance drones, maritime and space platforms). In this paper, we present\nthe system architecture and software stack that ARCHYTAS will develop to\nintegrate and support those accelerators, as well as the simulation software\nneeded for early prototyping of the full system and its components.", "AI": {"tldr": "ARCHYTAS\u9879\u76ee\u65e8\u5728\u8bbe\u8ba1\u548c\u8bc4\u4f30\u975e\u4f20\u7edf\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5305\u62ec\u5149\u7535\u3001\u6613\u5931\u6027\u548c\u975e\u6613\u5931\u6027\u5185\u5b58\u8ba1\u7b97\u4ee5\u53ca\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\uff0c\u4ee5\u89e3\u51b3AI\u7684\u529f\u8017\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u91cd\u70b9\u5173\u6ce8\u56fd\u9632\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3AI\u5728\u56fd\u9632\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001\u76d1\u63a7\u65e0\u4eba\u673a\u3001\u6d77\u4e0a\u548c\u592a\u7a7a\u5e73\u53f0\uff09\u4e2d\u7684\u529f\u8017\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u74f6\u9888\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u7cfb\u7edf\u67b6\u6784\u548c\u8f6f\u4ef6\u6808\u6765\u96c6\u6210\u548c\u652f\u6301\u8fd9\u4e9b\u52a0\u901f\u5668\uff0c\u5e76\u5f00\u53d1\u7528\u4e8e\u5168\u7cfb\u7edf\u53ca\u5176\u7ec4\u4ef6\u65e9\u671f\u539f\u578b\u8bbe\u8ba1\u7684\u4eff\u771f\u8f6f\u4ef6\u3002", "result": "\u63d0\u51fa\u4e86ARCHYTAS\u9879\u76ee\u7684\u6574\u4f53\u67b6\u6784\u548c\u8f6f\u4ef6\u65b9\u6848\uff0c\u5305\u62ec\u7cfb\u7edf\u96c6\u6210\u6846\u67b6\u548c\u4eff\u771f\u5de5\u5177\u94fe\u3002", "conclusion": "ARCHYTAS\u9879\u76ee\u901a\u8fc7\u5f00\u53d1\u975e\u4f20\u7edf\u786c\u4ef6\u52a0\u901f\u5668\u548c\u76f8\u5e94\u7684\u8f6f\u4ef6\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u56fd\u9632AI\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16622", "categories": ["cs.AR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16622", "abs": "https://arxiv.org/abs/2510.16622", "authors": ["Kazi Ababil Azam", "Hasan Masum", "Masfiqur Rahaman", "A. B. M. Alim Al Islam"], "title": "Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization", "comment": "10 pages, Submitted to IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS)", "summary": "The vehicular density in urbanizing cities of developing countries such as\nDhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road\nexperiences. Traffic signaling is a key component in effective traffic\nmanagement for such situations, but the advancements in intelligent traffic\nsignaling have been exclusive to developed countries with structured traffic.\nThe non-lane-based, heterogeneous traffic of Dhaka City requires a contextual\napproach. This study focuses on the development of an intelligent traffic\nsignaling system feasible in the context of developing countries such as\nBangladesh. We propose a pipeline leveraging Real Time Streaming Protocol\n(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of\nthe art YOLO-based object detection model trained on the Non-lane-based and\nHeterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous\ntraffic. A multi-objective optimization algorithm, NSGA-II, then generates\noptimized signal timings, minimizing waiting time while maximizing vehicle\nthroughput. We test our implementation in a five-road intersection at Palashi,\nDhaka, demonstrating the potential to significantly improve traffic management\nin similar situations. The developed testbed paves the way for more contextual\nand effective Intelligent Traffic Signaling (ITS) solutions for developing\nareas with complicated traffic dynamics such as Dhaka City.", "AI": {"tldr": "\u5f00\u53d1\u9002\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u7b49\u53d1\u5c55\u4e2d\u56fd\u5bb6\u975e\u8f66\u9053\u5316\u3001\u5f02\u8d28\u4ea4\u901a\u7684\u667a\u80fd\u4ea4\u901a\u4fe1\u53f7\u7cfb\u7edf\uff0c\u4f7f\u7528RTSP\u89c6\u9891\u6d41\u3001\u6811\u8393\u6d3e4B\u5904\u7406\u548cYOLO\u76ee\u6807\u68c0\u6d4b\uff0c\u7ed3\u5408NSGA-II\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\u4fe1\u53f7\u914d\u65f6\u3002", "motivation": "\u89e3\u51b3\u8fbe\u5361\u7b49\u53d1\u5c55\u4e2d\u56fd\u5bb6\u57ce\u5e02\u56e0\u8f66\u8f86\u5bc6\u5ea6\u9ad8\u5bfc\u81f4\u7684\u4e25\u91cd\u4ea4\u901a\u62e5\u5835\u95ee\u9898\uff0c\u73b0\u6709\u667a\u80fd\u4ea4\u901a\u4fe1\u53f7\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u53d1\u8fbe\u56fd\u5bb6\u7684\u7ed3\u6784\u5316\u4ea4\u901a\uff0c\u4e0d\u9002\u7528\u4e8e\u975e\u8f66\u9053\u5316\u3001\u5f02\u8d28\u4ea4\u901a\u73af\u5883\u3002", "method": "\u5229\u7528RTSP\u89c6\u9891\u6d41\u91c7\u96c6\u4ea4\u901a\u6570\u636e\uff0c\u5728\u6811\u8393\u6d3e4B\u4e0a\u8fd0\u884c\u57fa\u4e8eYOLO\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08\u4f7f\u7528NHT-1071\u6570\u636e\u96c6\u8bad\u7ec3\uff09\uff0c\u7ed3\u5408NSGA-II\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u751f\u6210\u4f18\u5316\u7684\u4fe1\u53f7\u914d\u65f6\u65b9\u6848\u3002", "result": "\u5728\u8fbe\u5361Palashi\u7684\u4e94\u8def\u4ea4\u53c9\u53e3\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8bc1\u660e\u8be5\u7cfb\u7edf\u80fd\u663e\u8457\u6539\u5584\u4ea4\u901a\u7ba1\u7406\uff0c\u51cf\u5c11\u7b49\u5f85\u65f6\u95f4\u5e76\u63d0\u9ad8\u8f66\u8f86\u901a\u884c\u80fd\u529b\u3002", "conclusion": "\u8be5\u6d4b\u8bd5\u5e73\u53f0\u4e3a\u5177\u6709\u590d\u6742\u4ea4\u901a\u52a8\u6001\u7684\u53d1\u5c55\u4e2d\u56fd\u5bb6\u5730\u533a\u5f00\u53d1\u66f4\u7b26\u5408\u60c5\u5883\u7684\u667a\u80fd\u4ea4\u901a\u4fe1\u53f7\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.17251", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.17251", "abs": "https://arxiv.org/abs/2510.17251", "authors": ["Chengxi Li", "Yang Sun", "Lei Chen", "Yiwen Wang", "Mingxuan Yuan", "Evangeline F. Y. Young"], "title": "SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding", "comment": null, "summary": "This paper proposes smaRTLy: a new optimization technique for multiplexers in\nRegister-Transfer Level (RTL) logic synthesis. Multiplexer trees are very\ncommon in RTL designs, and traditional tools like Yosys optimize them by\ntraversing the tree and monitoring control port values. However, this method\ndoes not fully exploit the intrinsic logical relationships among signals or the\npotential for structural optimization. To address these limitations, we develop\ninnovative strategies to remove redundant multiplexer trees and restructure the\nremaining ones, significantly reducing the overall gate count. We evaluate\nsmaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%\nreduction in AIG area compared to Yosys. We also evaluate smaRTLy on an\nindustrial benchmark in the scale of millions of gates, results show that\nsmaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate\nthe effectiveness of our logic inferencing and structural rebuilding techniques\nin enhancing the RTL optimization process, leading to more efficient hardware\ndesigns.", "AI": {"tldr": "smaRTLy\u662f\u4e00\u79cd\u9488\u5bf9RTL\u903b\u8f91\u7efc\u5408\u4e2d\u591a\u8def\u590d\u7528\u5668\u4f18\u5316\u7684\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u79fb\u9664\u5197\u4f59\u591a\u8def\u590d\u7528\u5668\u6811\u5e76\u91cd\u6784\u5269\u4f59\u7ed3\u6784\uff0c\u663e\u8457\u51cf\u5c11\u95e8\u7535\u8def\u6570\u91cf\u3002", "motivation": "\u4f20\u7edf\u5de5\u5177\u5982Yosys\u901a\u8fc7\u904d\u5386\u591a\u8def\u590d\u7528\u5668\u6811\u548c\u76d1\u63a7\u63a7\u5236\u7aef\u53e3\u503c\u6765\u4f18\u5316\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u4fe1\u53f7\u95f4\u7684\u5185\u5728\u903b\u8f91\u5173\u7cfb\u6216\u7ed3\u6784\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u521b\u65b0\u7b56\u7565\u6765\u79fb\u9664\u5197\u4f59\u591a\u8def\u590d\u7528\u5668\u6811\u5e76\u91cd\u6784\u5269\u4f59\u7ed3\u6784\uff0c\u91c7\u7528\u903b\u8f91\u63a8\u7406\u548c\u7ed3\u6784\u91cd\u5efa\u6280\u672f\u3002", "result": "\u5728IWLS-2005\u548cRISC-V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4Yosys\u989d\u5916\u51cf\u5c118.95%\u7684AIG\u9762\u79ef\uff1b\u5728\u767e\u4e07\u95e8\u7ea7\u5de5\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4Yosys\u591a\u79fb\u966447.2%\u7684AIG\u9762\u79ef\u3002", "conclusion": "smaRTLy\u7684\u903b\u8f91\u63a8\u7406\u548c\u7ed3\u6784\u91cd\u5efa\u6280\u672f\u80fd\u6709\u6548\u589e\u5f3aRTL\u4f18\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u8bbe\u8ba1\u3002"}}
