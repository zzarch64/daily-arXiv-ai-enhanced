<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [SwiftKV: An Edge-Oriented Attention Algorithm and Multi-Head Accelerator for Fast, Efficient LLM Decoding](https://arxiv.org/abs/2601.10953)
*Junming Zhang,Qinyan Zhang,Huajun Sun,Feiyang Gao,Sheng Hu,Rui Nie,Xiangshui Miao*

Main category: cs.AR

TL;DR: SwiftKV Attention：一种针对边缘加速器的单通道注意力推理算法，无需分数物化或二次处理，实现7.16倍加速。SwiftKV-MHA加速器支持多头并行解码，进一步降低延迟13.48倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备上的加速至关重要，但在资源受限的边缘加速器上实现快速注意力推理和高效解码仍然具有挑战性。现有方法存在计算冗余和硬件支持不足的问题。

Method: 提出SwiftKV Attention算法：基于每令牌流水线的单通道注意力推理，每个(k_t, v_t)在KV缓存中仅处理一次，无需分数物化、分块softmax或二次处理。设计SwiftKV-MHA加速器，在同一处理器阵列上实现高精度注意力和低精度GEMV，支持多头并行解码。

Result: 在边缘加速器上，SwiftKV Attention相比原生注意力实现7.16倍加速，显著优于其他注意力算法。SwiftKV-MHA进一步降低注意力延迟13.48倍，在相同设置下相比最先进工作提高生成速度17.4%，提升令牌效率1.98倍。

Conclusion: SwiftKV Attention算法和SwiftKV-MHA加速器有效解决了边缘设备上LLM推理的挑战，通过创新的单通道处理架构和硬件设计，实现了显著的性能提升和效率改进。

Abstract: Edge acceleration for large language models is crucial for their widespread application; however, achieving fast attention inference and efficient decoding on resource-constrained edge accelerators remains challenging. This paper presents SwiftKV Attention, a per-token pipelined, low-latency single-pass attention inference algorithm, where every (kt, vt) in the KV cache is processed exactly once in a uniform per-token pipeline without score materialization, blockwise softmax, or a second pass, thereby enabling fast execution on edge accelerators with a single hardware set and no resource-intensive parallelism. Furthermore, to address the limited support for multi-head LLM decoding in existing accelerators, we design the SwiftKV-MHA accelerator, which enables high precision attention and low precision GEMV on the same processor array, achieving fast and efficient multi-head parallel decoding. Experimental results show that, on the edge accelerator, the SwiftKV Attention algorithm achieves a 7.16* speedup over native attention and significantly outperforms other attention algorithms. SwiftKV-MHA further reduces attention latency by 13.48*; under the same settings, it improves generation speed by 17.4% and increases token efficiency by 1.98* compared with state-of-the-art works.

</details>


### [2] [RidgeWalker: Perfectly Pipelined Graph Random Walks on FPGAs](https://arxiv.org/abs/2601.11057)
*Hongshi Tan,Yao Chen,Xinyu Chen,Qizhen Zhang,Cheng Chen,Weng-Fai Wong,Bingsheng He*

Main category: cs.AR

TL;DR: RidgeWalker是一个基于FPGA的高性能图随机游走加速器，通过马尔可夫性质分解任务和异步流水线架构，实现了7.0倍于现有FPGA方案和8.1倍于GPU方案的加速效果。


<details>
  <summary>Details</summary>
Motivation: 图随机游走（GRW）在众多应用中用于近似图属性，但由于其强数据依赖性、不规则内存访问模式和负载不均衡等问题，传统加速方法难以充分发挥硬件潜力，现有FPGA方案因低效流水线和静态调度而表现不佳。

Method: 基于GRW的马尔可夫性质，将任务分解为无状态的细粒度任务，支持乱序执行而不影响正确性。设计了异步流水线架构，采用基于排队理论的反馈驱动调度器，实现完美流水化和自适应负载均衡。

Result: 在数据中心FPGA上原型实现，实验显示平均比最先进FPGA方案快7.0倍，比GPU方案快8.1倍，峰值加速分别达到71.0倍和22.9倍。

Conclusion: RidgeWalker通过创新的任务分解和自适应调度架构，显著提升了图随机游走的计算效率，为数据中心FPGA上的图计算加速提供了有效解决方案。

Abstract: Graph Random Walks (GRWs) offer efficient approximations of key graph properties and have been widely adopted in many applications. However, GRW workloads are notoriously difficult to accelerate due to their strong data dependencies, irregular memory access patterns, and imbalanced execution behavior. While recent work explores FPGA-based accelerators for GRWs, existing solutions fall far short of hardware potential due to inefficient pipelining and static scheduling. This paper presents RidgeWalker, a high-performance GRW accelerator designed for datacenter FPGAs. The key insight behind RidgeWalker is that the Markov property of GRWs allows decomposition into stateless, fine-grained tasks that can be executed out-of-order without compromising correctness. Building on this, RidgeWalker introduces an asynchronous pipeline architecture with a feedback-driven scheduler grounded in queuing theory, enabling perfect pipelining and adaptive load balancing. We prototype RidgeWalker on datacenter FPGAs and evaluated it across a range of GRW algorithms and real-world graph datasets. Experimental results demonstrate that RidgeWalker achieves an average speedup of 7.0x over state-of-the-art FPGA solutions and 8.1x over GPU solutions, with peak speedups of up to 71.0x and 22.9x, respectively. The source code is publicly available at https://github.com/Xtra-Computing/RidgeWalker.

</details>


### [3] [OpenACM: An Open-Source SRAM-Based Approximate CiM Compiler](https://arxiv.org/abs/2601.11292)
*Yiqi Zhou,JunHao Ma,Xingyang Li,Yule Sheng,Yue Yuan,Yikai Wang,Bochang Wang,Yiheng Wu,Shan Shen,Wei Xing,Daying Sun,Li Li,Zhiqiang Xiao*

Main category: cs.AR

TL;DR: OpenACM是首个开源、精度感知的SRAM近似计算内存编译器，通过集成可配置精度乘法器库，在保证AI应用精度的同时实现高达64%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 数据密集型AI工作负载加剧了"内存墙"瓶颈，数字计算内存(DCiM)虽提供可扩展解决方案，但其庞大设计空间使手动设计不切实际。现有DCiM编译器专注于精确计算，未能利用AI应用的误差容忍特性进行能耗优化。

Method: 开发OpenACM编译器，集成精度可配置乘法器库（精确、可调近似和对数），自动化生成DCiM架构，将晶体管级可定制SRAM宏与变异感知特性集成到基于OpenROAD和FreePDK45的开源物理设计流程中。

Result: 在代表性卷积神经网络上的实验表明，OpenACM在保持应用精度损失可忽略的情况下，实现了高达64%的能耗节省。该框架完全开源，确保可重现性和可访问性。

Conclusion: OpenACM填补了应用误差容忍与硬件自动化之间的空白，为SRAM近似DCiM架构提供了首个开源、精度感知的编译器解决方案，实现了细粒度精度-能耗权衡。

Abstract: The rise of data-intensive AI workloads has exacerbated the ``memory wall'' bottleneck. Digital Compute-in-Memory (DCiM) using SRAM offers a scalable solution, but its vast design space makes manual design impractical, creating a need for automated compilers. A key opportunity lies in approximate computing, which leverages the error tolerance of AI applications for significant energy savings. However, existing DCiM compilers focus on exact arithmetic, failing to exploit this optimization. This paper introduces OpenACM, the first open-source, accuracy-aware compiler for SRAM-based approximate DCiM architectures. OpenACM bridges the gap between application error tolerance and hardware automation. Its key contribution is an integrated library of accuracy-configurable multipliers (exact, tunable approximate, and logarithmic), enabling designers to make fine-grained accuracy-energy trade-offs. The compiler automates the generation of the DCiM architecture, integrating a transistor-level customizable SRAM macro with variation-aware characterization into a complete, open-source physical design flow based on OpenROAD and the FreePDK45 library. This ensures full reproducibility and accessibility, removing dependencies on proprietary tools. Experimental results on representative convolutional neural networks (CNNs) demonstrate that OpenACM achieves energy savings of up to 64\% with negligible loss in application accuracy. The framework is available on \href{https://github.com/ShenShan123/OpenACM}{OpenACM:URL}

</details>
