<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach](https://arxiv.org/abs/2510.06513)
*Debendra Das Sharma,Swadesh Choudhary,Peter Onufryk,Rob Pelt*

Main category: cs.AR

TL;DR: 提出通过增强UCIe接口支持内存语义，为AI等计算应用提供高带宽密度、低延迟、低功耗和低成本的封装内内存解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有封装内内存解决方案无法满足AI等新兴计算应用对高能效带宽的需求，面临内存墙问题。

Method: 通过逻辑芯片复用LPDDR6和HBM内存连接到SoC的UCIe接口，以及让DRAM芯片原生支持UCIe而非LPDDR6总线接口。

Result: 相比现有HBM4和LPDDR封装内内存解决方案，实现了显著更高的带宽密度（最高10倍）、更低延迟（最高3倍）、更低功耗（最高3倍）和更低成本。

Conclusion: 增强UCIe支持内存语义是解决计算连续体中封装内内存需求的有效方法。

Abstract: Emerging computing applications such as Artificial Intelligence (AI) are
facing a memory wall with existing on-package memory solutions that are unable
to meet the power-efficient bandwidth demands. We propose to enhance UCIe with
memory semantics to deliver power-efficient bandwidth and cost-effective
on-package memory solutions applicable across the entire computing continuum.
We propose approaches by reusing existing LPDDR6 and HBM memory through a logic
die that connects to the SoC using UCIe. We also propose an approach where the
DRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our
approaches result in significantly higher bandwidth density (up to 10x), lower
latency (up to 3x), lower power (up to 3x), and lower cost compared to existing
HBM4 and LPDDR on-package memory solutions.

</details>


### [2] [RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction](https://arxiv.org/abs/2510.06644)
*Leshu Li,Jiayin Qin,Jie Peng,Zishen Wan,Huaizhi Qu,Ye Han,Pingqing Zheng,Hongsen Zhang,Yu,Cao,Tianlong Chen,Yang,Zhao*

Main category: cs.AR

TL;DR: RTGS是一个算法-硬件协同设计框架，通过减少3D高斯泼溅SLAM管道中的冗余实现边缘设备上的实时性能，在保持质量的同时达到30 FPS并提升82.5倍能效。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅SLAM系统虽然具有优异的渲染效率和精度，但由于速度不足尚未在资源受限的边缘设备上应用，需要解决管道中的冗余问题以实现实时性能。

Method: 算法层面：自适应高斯剪枝和动态下采样技术；硬件层面：子瓦片级流式策略、像素级配对调度、渲染与反向传播缓冲区、梯度合并单元。

Result: 在四个数据集和三种算法上实现实时性能（≥30 FPS），能效比基线提升82.5倍，质量损失可忽略不计。

Conclusion: RTGS通过算法-硬件协同设计有效解决了3DGS-SLAM在边缘设备上的实时运行问题，为资源受限环境提供了可行的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping
(SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering
efficiency and accuracy, but have not yet been adopted in resource-constrained
edge devices due to insufficient speed. Addressing this, we identify notable
redundancies across the SLAM pipeline for acceleration. While conceptually
straightforward, practical approaches are required to minimize the overhead
associated with identifying and eliminating these redundancies. In response, we
propose RTGS, an algorithm-hardware co-design framework that comprehensively
reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the
overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline.
On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to
remove the redundant Gaussians by reusing gradients computed during
backpropagation; and (2) a dynamic downsampling technique that directly reuses
the keyframe identification and alpha computing steps to eliminate redundant
pixels. On the hardware side, we propose (1) a subtile-level streaming strategy
and a pixel-level pairwise scheduling strategy that mitigates workload
imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration
information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates
the rendering backpropagation by reusing intermediate data computed during
rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory
accesses caused by atomic operations while enabling pipelined aggregation.
Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on
four datasets and three algorithms, with up to 82.5x energy efficiency over the
baseline and negligible quality loss. Code is available at
https://github.com/UMN-ZhaoLab/RTGS.

</details>


### [3] [Hardware-Efficient CNNs: Interleaved Approximate FP32 Multipliers for Kernel Computation](https://arxiv.org/abs/2510.06767)
*Bindu G Gowda,Yogesh Goyal,Yash Gupta,Madhav Rao*

Main category: cs.AR

TL;DR: 该论文提出了一种用于CNN推理的近似FP32乘法器，通过使用误差可变的近似压缩器来近似尾数乘法，显著降低硬件成本，并采用NSGA-II算法优化近似乘法器在卷积层中的布局以平衡精度和硬件效率。


<details>
  <summary>Details</summary>
Motivation: FP32乘法在硬件实现上计算成本高且复杂，但在神经网络推理等应用中，完美精度并非必需，可以牺牲一定精度来换取面积、功耗和速度的提升。

Method: 使用误差可变的近似压缩器来近似FP32乘法器的尾数乘法部分，并采用NSGA-II算法优化不同近似程度的FP32乘法器在卷积层核内的交错布局和排序。

Result: 提出的近似FP32乘法器显著降低了硬件成本，同时通过优化布局策略在CNN推理中实现了精度和硬件效率的良好平衡。

Conclusion: 近似FP32乘法器结合智能布局优化策略，为CNN推理提供了一种有效的硬件优化方案，在可接受的精度损失下显著提升了硬件效率。

Abstract: Single-precision floating point (FP32) data format, defined by the IEEE 754
standard, is widely employed in scientific computing, signal processing, and
deep learning training, where precision is critical. However, FP32
multiplication is computationally expensive and requires complex hardware,
especially for precisely handling mantissa multiplication. In practical
applications like neural network inference, perfect accuracy is not always
necessary, minor multiplication errors often have little impact on final
accuracy. This enables trading precision for gains in area, power, and speed.
This work focuses on CNN inference using approximate FP32 multipliers, where
the mantissa multiplication is approximated by employing error-variant
approximate compressors, that significantly reduce hardware cost. Furthermore,
this work optimizes CNN performance by employing differently approximated FP32
multipliers and studying their impact when interleaved within the kernels
across the convolutional layers. The placement and ordering of these
approximate multipliers within each kernel are carefully optimized using the
Non-dominated Sorting Genetic Algorithm-II, balancing the trade-off between
accuracy and hardware efficiency.

</details>


### [4] [Cocoon: A System Architecture for Differentially Private Training with Correlated Noises](https://arxiv.org/abs/2510.07304)
*Donghwan Kim,Xin Gu,Jinho Baek,Timothy Lo,Younghoon Min,Kwangsik Shin,Jongryool Kim,Jongse Park,Kiwan Maeng*

Main category: cs.AR

TL;DR: 本文研究了使用相关噪声的差分隐私训练方法，分析了其在大模型和大嵌入表上的性能开销，并提出了Cocoon硬件-软件协同设计框架来加速相关噪声训练。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型会记忆和泄露训练数据，带来严重的隐私问题。虽然差分隐私训练(如DP-SGD)可以解决这个问题，但会降低模型准确性。新的相关噪声方法能提高准确性，但在大模型和大嵌入表上会产生显著开销。

Method: 提出Cocoon框架：1) Cocoon-Emb通过预计算和存储合并格式的相关噪声来加速带嵌入表的模型；2) Cocoon-NMP通过定制近内存处理设备支持大模型。

Result: 在基于FPGA的NMP设备原型上，Cocoon将性能提升了2.33-10.82倍(Cocoon-Emb)和1.55-3.06倍(Cocoon-NMP)。

Conclusion: Cocoon硬件-软件协同设计框架能有效解决相关噪声差分隐私训练在大模型和大嵌入表上的性能瓶颈，显著提升训练效率。

Abstract: Machine learning (ML) models memorize and leak training data, causing serious
privacy issues to data owners. Training algorithms with differential privacy
(DP), such as DP-SGD, have been gaining attention as a solution. However,
DP-SGD adds a noise at each training iteration, which degrades the accuracy of
the trained model. To improve accuracy, a new family of approaches adds
carefully designed correlated noises, so that noises cancel out each other
across iterations. We performed an extensive characterization study of these
new mechanisms, for the first time to the best of our knowledge, and show they
incur non-negligible overheads when the model is large or uses large embedding
tables. Motivated by the analysis, we propose Cocoon, a hardware-software
co-designed framework for efficient training with correlated noises. Cocoon
accelerates models with embedding tables through pre-computing and storing
correlated noises in a coalesced format (Cocoon-Emb), and supports large models
through a custom near-memory processing device (Cocoon-NMP). On a real system
with an FPGA-based NMP device prototype, Cocoon improves the performance by
2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).

</details>
