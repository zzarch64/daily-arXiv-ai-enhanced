<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 10]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Accelerating Physics-Based Electromigration Analysis via Rational Krylov Subspaces](https://arxiv.org/abs/2602.00330)
*Sheldon X. -D. Tan,Haotian Lu*

Main category: cs.AR

TL;DR: 提出两种基于有理Krylov子空间降阶的快速电迁移应力分析方法，相比传统有限差分法实现20-500倍加速，仅需4-6阶即可达到亚0.1%误差。


<details>
  <summary>Details</summary>
Motivation: 电迁移引起的应力演化是纳米级VLSI互连的主要可靠性挑战。传统有限差分法求解大规模互连树上的应力控制偏微分方程计算成本高昂，需要高效的快速分析方法。

Method: 提出两种基于有理Krylov子空间降阶的框架：频域扩展有理Krylov方法(ExtRaKrylovEM)和时域有理Krylov指数积分方法(EiRaKrylovEM)。通过坐标下降优化策略自动确定最优降阶阶数和移位时间。

Result: 在合成结构和工业级电源网络上，仅需4-6阶Krylov降阶即可实现亚0.1%的成核时间和电阻变化预测误差，获得20-500倍的加速。而标准扩展Krylov方法需要50多阶且仍有10-20%误差。

Conclusion: 提出的有理Krylov方法显著提高了电迁移应力分析的效率和精度，为电迁移感知优化和随机电迁移分析提供了实用工具，相比传统方法具有明显优势。

Abstract: Electromigration (EM) induced stress evolution is a major reliability challenge in nanometer-scale VLSI interconnects. Accurate EM analysis requires solving stress-governing partial differential equations over large interconnect trees, which is computationally expensive using conventional finite-difference methods. This work proposes two fast EM stress analysis techniques based on rational Krylov subspace reduction. Unlike traditional Krylov methods that expand around zero frequency, rational Krylov methods enable expansion at selected time constants, aligning directly with metrics such as nucleation and steady-state times and producing compact reduced models with minimal accuracy loss. Two complementary frameworks are developed: a frequency-domain extended rational Krylov method, ExtRaKrylovEM, and a time-domain rational Krylov exponential integration method, EiRaKrylovEM. We show that the accuracy of both methods depends strongly on the choice of expansion point, or shift time, and demonstrate that effective shift times are typically close to times of interest such as nucleation or post-void steady state. Based on this observation, a coordinate descent optimization strategy is introduced to automatically determine optimal reduction orders and shift times for both nucleation and post-void phases. Experimental results on synthesized structures and industry-scale power grids show that the proposed methods achieve orders-of-magnitude improvements in efficiency and accuracy over finite-difference solutions. Using only 4 to 6 Krylov orders, the methods achieve sub-0.1 percent error in nucleation time and resistance change predictions while delivering 20 to 500 times speedup. In contrast, standard extended Krylov methods require more than 50 orders and still incur 10 to 20 percent nucleation time error, limiting their practicality for EM-aware optimization and stochastic EM analysis.

</details>


### [2] [Optimal Engagement of Residential Battery Storage to Alleviate Grid Upgrades Caused by EVs and Solar Systems](https://arxiv.org/abs/2602.00342)
*Rafi Zahedi,Amirhossein Ahmadian,Chen Zhang,Shashank Narayana Gowda,Kourosh SedghiSigarchi,Rajit Gadh*

Main category: cs.AR

TL;DR: 提出一种基于电池储能太阳能系统的允许百分比分配方法，用于缓解分布式能源接入带来的电压波动和功率损耗问题，特别针对电动汽车充电需求增长场景。


<details>
  <summary>Details</summary>
Motivation: 分布式能源资源（特别是太阳能系统和电动汽车充电桩）的接入给配电网带来了电压波动和功率损耗等电能质量问题，随着电动卡车等新型电动汽车的普及，这些问题将更加严峻。

Method: 提出允许百分比分配方法，通过控制住宅区电池储能太阳能系统的充放电来调节电网。使用IEEE 33节点配电网作为测试平台，采用粒子群优化算法确定最优充放电指令。

Result: 仿真结果表明，通过合理分配电池储能太阳能系统的允许百分比，能够有效缓解电压偏差和功率损耗，平衡无电池和有电池太阳能住宅的需求，提升电网韧性。

Conclusion: 随着电动汽车的快速增长，需要精心设计的控制策略来应对现代电网动态复杂性，电池储能太阳能系统的智能充放电管理是提升电网韧性的有效途径。

Abstract: The integration of distributed energy resources has ushered in a host of complex challenges, significantly impacting power quality in distribution networks. This work studies these challenges, exploring issues such as voltage fluctuations and escalating power losses caused by the integration of solar systems and electric vehicle (EV) chargers. We present a robust methodology focused on mitigating voltage deviations and power losses, emphasizing the allocation of a Permitted Percentage (PP) of battery-based solar systems within residential areas endowed with storage capabilities. A key facet of this research lies in its adaptability to the changing landscape of electric transportation. With the rapid increase of electric trucks on the horizon, our proposed model gains relevance. By tactically deploying PP to oversee the charging and discharging of batteries within residential solar systems, utilities are poised not only to assist with grid resilience but also to cater to the upcoming demands spurred by the advent of new EVs, notably trucks. To validate the efficacy of our proposed model, rigorous simulations were conducted using the IEEE 33-bus distribution network as a designed testbed. Leveraging advanced Particle Swarm Optimization techniques, we have deciphered the optimal charging and discharging commands issued by utilities to energy storage systems. The outcomes of these simulations help us understand the transformative potential of various PP allocations, shedding light on the balance between non-battery-based and battery-based solar residences. This research underscores the need for carefully crafted approaches in navigating the complexities of modern grid dynamics amid the anticipated increase in electric vehicles.

</details>


### [3] [AutoGNN: End-to-End Hardware-Driven Graph Preprocessing for Enhanced GNN Performance](https://arxiv.org/abs/2602.00803)
*Seungkwan Kang,Seungjun Lee,Donghyun Gouk,Miryeong Kwon,Hyunkyu Choi,Junhyeok Jang,Sangwon Lee,Huiwon Choi,Jie Zhang,Wonil Choi,Mahmut Taylan Kandemir,Myoungsoo Jung*

Main category: cs.AR

TL;DR: AutoGNN是一个基于FPGA的GNN预处理加速器，通过可重构架构和专用组件解决GNN推理中的预处理瓶颈，相比传统和GPU加速系统分别实现9.0倍和2.1倍的加速。


<details>
  <summary>Details</summary>
Motivation: GNN推理中的预处理阶段（如图转换、采样等）通常占据整体推理延迟的主要部分，成为性能瓶颈。现有GPU方案存在序列化和同步限制，需要更高效的专用硬件加速器。

Method: 采用FPGA可重构架构，设计统一处理单元（UPEs）用于并行处理边排序和顶点选择，单周期归约器（SCRs）处理指针数组构建和子图重索引。用户级软件框架动态分析图输入特征，确定最优配置并重编程FPGA。

Result: 在7nm企业级FPGA上实现，相比传统预处理系统实现9.0倍加速，相比GPU加速系统实现2.1倍加速，能够高效处理多样化的图数据集。

Conclusion: AutoGNN通过FPGA可重构性和专用硬件组件有效解决了GNN预处理瓶颈，为高性能GNN推理提供了高效的硬件加速解决方案。

Abstract: Graph neural network (GNN) inference faces significant bottlenecks in preprocessing, which often dominate overall inference latency. We introduce AutoGNN, an FPGA-based accelerator designed to address these challenges by leveraging FPGA's reconfigurability and specialized components. AutoGNN adapts to diverse graph inputs, efficiently performing computationally intensive tasks such as graph conversion and sampling. By utilizing components like adder trees, AutoGNN executes reduction operations in constant time, overcoming the limitations of serialization and synchronization on GPUs.
  AutoGNN integrates unified processing elements (UPEs) and single-cycle reducers (SCRs) to streamline GNN preprocessing. UPEs enable scalable parallel processing for edge sorting and unique vertex selection, while SCRs efficiently handle sequential tasks such as pointer array construction and subgraph reindexing. A user-level software framework dynamically profiles graph inputs, determines optimal configurations, and reprograms AutoGNN to handle varying workloads. Implemented on a 7$n$m enterprise FPGA, AutoGNN achieves up to 9.0$\times$ and 2.1$\times$ speedup compared to conventional and GPU-accelerated preprocessing systems, respectively, enabling high-performance GNN preprocessing across diverse datasets.

</details>


### [4] [Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators](https://arxiv.org/abs/2602.00838)
*Prabhu Vellaisamy,Harideep Nair,Di Wu,Shawn Blanton,John Paul Shen*

Main category: cs.AR

TL;DR: 本文对三种最新的单精度GEMM设计（uGEMM、tuGEMM、tubGEMM）与传统的二进制GEMM进行了详细评估，展示了单精度GEMM在未来边缘AI加速器中实现高效能计算的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习向低精度发展，需要评估新型单精度GEMM设计作为传统二进制GEMM硬件替代方案的潜力，为未来DL计算提供参考。

Method: 对三种最新单精度GEMM设计（uGEMM、tuGEMM、tubGEMM）与常规二进制GEMM进行详细评估，包括不同位宽和矩阵尺寸下的综合后评估，并对8个预训练CNN和LLaMA2大语言模型进行权重稀疏性分析。

Result: 通过严格的评估确定了各种设计的权衡点和最优工作点，展示了单精度GEMM在能量效率方面的优势。

Conclusion: 单精度GEMM可以有效地用于未来边缘AI加速器的高效能计算，为低精度深度学习硬件设计提供了重要参考。

Abstract: General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM designs is needed to assess the potential of unary hardware for future DL compute. This paper focuses on unary GEMM designs for integer-based DL inference and performs a detailed evaluation of three latest unary design proposals, namely, uGEMM, tuGEMM and tubGEMM, by comparing them to a conventional binary GEMM. Rigorous post-synthesis evaluations beyond prior works are performed across varying bit-widths and matrix sizes to assess the designs' tradeoffs and determine optimal sweetspots. Further, we perform weight sparsity analysis across eight pretrained convolutional neural networks (CNNs) and the LLaMA2 large language model (LLM). In this work, we demonstrate how unary GEMM can be effectively used for energy-efficient compute in future edge AI accelerators.

</details>


### [5] [ENFOR-SA: End-to-end Cross-layer Transient Fault Injector for Efficient and Accurate DNN Reliability Assessment on Systolic Arrays](https://arxiv.org/abs/2602.00909)
*Rafael Billig Tonetto,Marcello Traiola,Fernando Fernandes dos Santos,Angeliki Kritikakou*

Main category: cs.AR

TL;DR: ENFOR-SA：针对脉动阵列架构的端到端DNN瞬态故障分析框架，通过跨层仿真实现RTL级精度，相比软件注入仅6%性能损失，相比全SoC RTL仿真加速569倍。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型变得越来越大且复杂，传统故障注入技术不实用。精确故障分析需要RTL级硬件模型，但这会显著降低评估速度，特别是结合昂贵的HDL仪器时。对于脉动阵列架构，这种高开销方法是不必要的。

Method: 提出ENFOR-SA框架，采用两步法：跨层仿真，仅在故障注入时使用RTL脉动阵列组件，其余部分在软件层面执行。

Result: 在CNN和Vision Transformer上的实验表明，ENFOR-SA实现RTL级精度故障注入，相比软件注入仅6%平均性能损失，相比全SoC RTL仿真加速569倍，相比最先进的跨层RTL注入工具提升2.03倍。

Conclusion: ENFOR-SA为脉动阵列架构提供高效准确的DNN故障分析框架，证明高开销方法不必要，代码已开源。

Abstract: Recent advances in deep learning have produced highly accurate but increasingly large and complex DNNs, making traditional fault-injection techniques impractical. Accurate fault analysis requires RTL-accurate hardware models. However, this significantly slows evaluation compared with software-only approaches, particularly when combined with expensive HDL instrumentation. In this work, we show that such high-overhead methods are unnecessary for systolic array (SA) architectures and propose ENFOR-SA, an end-to-end framework for DNN transient fault analysis on SAs. Our two-step approach employs cross-layer simulation and uses RTL SA components only during fault injection, with the rest executed at the software level. Experiments on CNNs and Vision Transformers demonstrate that ENFOR-SA achieves RTL-accurate fault injection with only 6% average slowdown compared to software-based injection, while delivering at least two orders of magnitude speedup (average $569\times$) over full-SoC RTL simulation and a $2.03\times$ improvement over a state-of-the-art cross-layer RTL injection tool. ENFOR-SA code is publicly available at https://github.com/rafaabt/ENFOR-SA.

</details>


### [6] [NeuroAI Temporal Neural Networks (NeuTNNs): Microarchitecture and Design Framework for Specialized Neuromorphic Processing Units](https://arxiv.org/abs/2602.01546)
*Shanmuga Venkatachalam,Prabhu Vellaisamy,Harideep Nair,Wei-Che Huang,Youngseok Na,Yuyang Kang,Quinn Jacobson,John Paul Shen*

Main category: cs.AR

TL;DR: 提出NeuroAI TNNs (NeuTNNs) - 一种结合神经科学发现的新型时序神经网络，通过PyTorch-to-layout工具套件(NeuTNNGen)实现应用特定设计，在性能和硬件效率上优于传统TNNs。


<details>
  <summary>Details</summary>
Motivation: 神经科学与人工智能的融合(NeuroAI)被认为是推动下一代AI创新的关键。现有时序神经网络(TNNs)虽然是有前景的神经形态方法，但需要进一步结合神经科学发现来提升能力和硬件效率。

Method: 提出NeuroAI TNNs (NeuTNNs)，采用包含主动树突和远端/近端段层次结构的神经元模型。开发NeuTNNGen工具套件，将PyTorch模型转换为硬件布局，支持应用特定设计。通过突触剪枝技术进一步减少30-50%的突触数量。

Result: NeuTNNs在UCR时间序列基准、MNIST设计探索和位置细胞设计三个应用中表现出优越性能和效率。突触剪枝能在保持模型精度的同时显著降低硬件成本。

Conclusion: NeuTNNGen工具套件能够促进应用特定、高能效的NeuTNNs设计，为下一代NeuroAI计算系统的发展提供支持，展示了神经科学与AI融合的实际价值。

Abstract: Leading experts from both communities have suggested the need to (re)connect research in neuroscience and artificial intelligence (AI) to accelerate the development of next-generation AI innovations. They term this convergence as NeuroAI. Previous research has established temporal neural networks (TNNs) as a promising neuromorphic approach toward biological intelligence and efficiency. We fully embrace NeuroAI and propose a new category of TNNs we call NeuroAI TNNs (NeuTNNs) with greater capability and hardware efficiency by adopting neuroscience findings, including a neuron model with active dendrites and a hierarchy of distal and proximal segments. This work introduces a PyTorch-to-layout tool suite (NeuTNNGen) to design application-specific NeuTNNs. Compared to previous TNN designs, NeuTNNs achieve superior performance and efficiency. We demonstrate NeuTNNGen's capabilities using three example applications: 1) UCR time series benchmarks, 2) MNIST design exploration, and 3) Place Cells design for neocortical reference frames. We also explore using synaptic pruning to further reduce synapse counts and hardware costs by 30-50% while maintaining model precision across diverse sensory modalities. NeuTNNGen can facilitate the design of application-specific energy-efficient NeuTNNs for the next generation of NeuroAI computing systems.

</details>


### [7] [In-Pipeline Integration of Digital In-Memory-Computing into RISC-V Vector Architecture to Accelerate Deep Learning](https://arxiv.org/abs/2602.01827)
*Tommaso Spagnolo,Cristina Silvano,Riccardo Massa,Filippo Grillotti,Thomas Boesch,Giuseppe Desoli*

Main category: cs.AR

TL;DR: 提出了一种将数字内存计算单元紧密集成到向量RISC-V ISA中的新架构，通过自定义指令加速边缘深度学习推理，实现217倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 边缘计算需要高性能、高效率的深度学习架构，但面临严格的功耗和内存限制。数字内存计算通过将计算移至内存阵列来减少数据移动、提高能效。

Method: 扩展向量RISC-V ISA，将DIMC单元紧密集成到流水线执行阶段，添加四个自定义指令用于数据加载、计算和写回，实现对推理执行的灵活优化控制。

Result: DIMC tile利用率高，在ResNet-50模型上保持稳定吞吐量，峰值性能达137 GOP/s。相比基线核心实现217倍加速，在硬件资源限制下仍实现50倍面积归一化加速。

Conclusion: 该架构展示了作为可扩展、高效解决方案加速边缘深度学习推理的巨大潜力，验证了DIMC与向量RISC-V紧密集成的有效性。

Abstract: Expanding Deep Learning applications toward edge computing demands architectures capable of delivering high computational performance and efficiency while adhering to tight power and memory constraints. Digital In-Memory Computing (DIMC) addresses this need by moving part of the computation directly within memory arrays, significantly reducing data movement and improving energy efficiency. This paper introduces a novel architecture that extends the Vector RISC-V Instruction Set Architecture (ISA) to integrate a tightly coupled DIMC unit directly into the execution stage of the pipeline, to accelerate Deep Learning inference at the edge. Specifically, the proposed approach adds four custom instructions dedicated to data loading, computation, and write-back, enabling flexible and optimal control of the inference execution on the target architecture. Experimental results demonstrate high utilization of the DIMC tile in Vector RISC-V and sustained throughput across the ResNet-50 model, achieving a peak performance of 137 GOP/s. The proposed architecture achieves a speedup of 217x over the baseline core and 50x area-normalized speedup even when operating near the hardware resource limits. The experimental results confirm the high potential of the proposed architecture as a scalable and efficient solution to accelerate Deep Learning inference on the edge.

</details>


### [8] [Position: The Need for Ultrafast Training](https://arxiv.org/abs/2602.02005)
*Duc Hoang*

Main category: cs.AR

TL;DR: 论文主张从仅推理的FPGA加速器转向超快速片上学习，使推理和训练都能在FPGA架构中以亚微秒延迟执行，实现与物理过程同步的实时自适应系统。


<details>
  <summary>Details</summary>
Motivation: 当前领域专用FPGA主要专注于静态模型的低延迟推理，将学习和适应任务交给较慢的CPU/GPU，这限制了在非平稳、高频环境中需要模型更新与物理过程同步的系统性能。

Method: 提出重新思考算法、架构和工具流程的联合设计方法，将学习功能集成到与推理相同的实时数据路径中，实现FPGA上的确定性亚微秒延迟训练。

Result: 通过将FPGA从静态推理引擎转变为实时学习机器，能够实现与物理过程同步的闭环自适应系统，应用于量子纠错、低温量子比特校准、等离子体与聚变控制、加速器调谐和自主科学实验等领域。

Conclusion: 需要从仅推理加速器转向超快速片上学习，通过算法、架构和工具流程的协同重构，使FPGA成为能够实时学习和适应的计算平台，开启实时自适应系统的新范式。

Abstract: Domain-specialized FPGAs have delivered unprecedented performance for low-latency inference across scientific and industrial workloads, yet nearly all existing accelerators assume static models trained offline, relegating learning and adaptation to slower CPUs or GPUs. This separation fundamentally limits systems that must operate in non-stationary, high-frequency environments, where model updates must occur at the timescale of the underlying physics. In this paper, I argue for a shift from inference-only accelerators to ultrafast on-chip learning, in which both inference and training execute directly within the FPGA fabric under deterministic, sub-microsecond latency constraints. Bringing learning into the same real-time datapath as inference would enable closed-loop systems that adapt as fast as the physical processes they control, with applications spanning quantum error correction, cryogenic qubit calibration, plasma and fusion control, accelerator tuning, and autonomous scientific experiments. Enabling such regimes requires rethinking algorithms, architectures, and toolflows jointly, but promises to transform FPGAs from static inference engines into real-time learning machines.

</details>


### [9] [Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02056)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

TL;DR: KANs在FPGA上实现亚微秒级在线学习，比MLPs更高效、表达能力更强，适用于量子计算和核聚变等高频率系统控制。


<details>
  <summary>Details</summary>
Motivation: 高频率系统（如量子计算和核聚变控制）需要亚微秒级的超快速在线学习，传统MLPs在低延迟、固定精度计算和严格内存约束下效率低下且数值不稳定。

Method: 利用KANs的B样条局部性实现稀疏更新，展示其对定点量化的固有鲁棒性，在FPGA上实现定点在线训练。

Result: KAN-based在线学习器在低延迟和资源受限任务中比MLPs显著更高效且表达能力更强，首次实现亚微秒延迟的无模型在线学习。

Conclusion: KANs特别适合高频率系统的超快速在线学习，其稀疏更新特性和量化鲁棒性使其在FPGA等资源受限平台上具有显著优势。

Abstract: Ultrafast online learning is essential for high-frequency systems, such as controls for quantum computing and nuclear fusion, where adaptation must occur on sub-microsecond timescales. Meeting these requirements demands low-latency, fixed-precision computation under strict memory constraints, a regime in which conventional Multi-Layer Perceptrons (MLPs) are both inefficient and numerically unstable. We identify key properties of Kolmogorov-Arnold Networks (KANs) that align with these constraints. Specifically, we show that: (i) KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling, and (ii) KANs are inherently robust to fixed-point quantization. By implementing fixed-point online training on Field-Programmable Gate Arrays (FPGAs), a representative platform for on-chip computation, we demonstrate that KAN-based online learners are significantly more efficient and expressive than MLPs across a range of low-latency and resource-constrained tasks. To our knowledge, this work is the first to demonstrate model-free online learning at sub-microsecond latencies.

</details>


### [10] [CHAOS: Controlled Hardware fAult injectOr System for gem5](https://arxiv.org/abs/2602.02119)
*Elio Vinciguerra,Enrico Russo,Giuseppe Ascia,Maurizio Palesi*

Main category: cs.AR

TL;DR: CHAOS是一个为gem5模拟器设计的模块化、开源、可配置的故障注入框架，用于评估计算系统的可靠性和弹性。


<details>
  <summary>Details</summary>
Motivation: 故障注入器对于评估计算系统的可靠性和弹性至关重要，能够模拟硬件和软件故障来分析系统在错误条件下的行为，并评估其在干扰下的正确运行能力。这种分析对于识别漏洞和提高系统鲁棒性非常关键。

Method: CHAOS是一个模块化、开源、完全可配置的故障注入框架，专为gem5模拟器设计。它支持跨多个架构级别的精确和系统化故障注入，能够全面评估容错机制和弹性策略。其高可配置性和与gem5的无缝集成允许研究人员探索广泛的故障模型和复杂场景。

Result: CHAOS框架使得研究人员能够进行全面的故障容忍机制评估和弹性策略分析，为可靠和高性能计算系统的研究提供了有价值的工具。

Conclusion: CHAOS作为一个模块化、开源、可配置的故障注入框架，为gem5模拟器提供了强大的故障注入能力，是推进可靠和高性能计算系统研究的重要工具。

Abstract: Fault injectors are essential tools for evaluating the reliability and resilience of computing systems. They enable the simulation of hardware and software faults to analyze system behavior under error conditions and assess its ability to operate correctly despite disruptions. Such analysis is critical for identifying vulnerabilities and improving system robustness. CHAOS is a modular, open-source, and fully configurable fault injection framework designed for the gem5 simulator. It facilitates precise and systematic fault injection across multiple architectural levels, supporting comprehensive evaluations of fault tolerance mechanisms and resilience strategies. Its high configurability and seamless integration with gem5 allow researchers to explore a wide range of fault models and complex scenarios, making CHAOS a valuable tool for advancing research in dependable and high-performance computing systems.

</details>
