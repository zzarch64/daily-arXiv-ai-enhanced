{"id": "2509.25391", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25391", "abs": "https://arxiv.org/abs/2509.25391", "authors": ["Fernanda Zapata Bascu\u00f1\u00e1n", "Alan Ezequiel Fuster"], "title": "smallNet: Implementation of a convolutional layer in tiny FPGAs", "comment": null, "summary": "Since current neural network development systems in Xilinx and VLSI require\ncodevelopment with Python libraries, the first stage of a convolutional network\nhas been implemented by developing a convolutional layer entirely in Verilog.\nThis handcoded design, free of IP cores and based on a filter polynomial like\nstructure, enables straightforward deployment not only on low cost FPGAs but\nalso on SoMs, SoCs, and ASICs. We analyze the limitations of numerical\nrepresentations and compare our implemented architecture, smallNet, with its\ncomputer based counterpart, demonstrating a 5.1x speedup, over 81%\nclassification accuracy, and a total power consumption of just 1.5 W. The\nalgorithm is validated on a single-core Cora Z7, demonstrating its feasibility\nfor real time, resource-constrained embedded applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u5168\u7528Verilog\u5b9e\u73b0\u7684\u5377\u79ef\u5c42\uff0c\u65e0\u9700Python\u5e93\u548cIP\u6838\uff0c\u53ef\u5728\u4f4e\u6210\u672cFPGA\u3001SoM\u3001SoC\u548cASIC\u4e0a\u90e8\u7f72\uff0c\u76f8\u6bd4\u8ba1\u7b97\u673a\u7248\u672c\u5b9e\u73b05.1\u500d\u52a0\u901f\uff0c81%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u529f\u8017\u4ec51.5W\u3002", "motivation": "\u5f53\u524dXilinx\u548cVLSI\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u5f00\u53d1\u7cfb\u7edf\u9700\u8981\u4e0ePython\u5e93\u534f\u540c\u5f00\u53d1\uff0c\u9650\u5236\u4e86\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u57fa\u4e8e\u6ee4\u6ce2\u5668\u591a\u9879\u5f0f\u7ed3\u6784\uff0c\u5b8c\u5168\u7528Verilog\u624b\u5199\u8bbe\u8ba1\u5377\u79ef\u5c42\uff0c\u907f\u514d\u4f7f\u7528IP\u6838\u3002", "result": "\u5728\u5355\u6838Cora Z7\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b05.1\u500d\u52a0\u901f\uff0c\u8d85\u8fc781%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u603b\u529f\u80171.5W\u3002", "conclusion": "\u8be5\u67b6\u6784\u9002\u7528\u4e8e\u5b9e\u65f6\u3001\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5728FPGA\u4e0a\u9ad8\u6548\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.25626", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25626", "abs": "https://arxiv.org/abs/2509.25626", "authors": ["Yi Hu", "Huiyang Zhou"], "title": "LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels", "comment": null, "summary": "3D Gaussian splatting (3DGS) is a transformative technique with profound\nimplications on novel view synthesis and real-time rendering. Given its\nimportance, there have been many attempts to improve its performance. However,\nwith the increasing complexity of GPU architectures and the vast search space\nof performance-tuning parameters, it is a challenging task. Although manual\noptimizations have achieved remarkable speedups, they require domain expertise\nand the optimization process can be highly time consuming and error prone. In\nthis paper, we propose to exploit large language models (LLMs) to analyze and\noptimize Gaussian splatting kernels. To our knowledge, this is the first work\nto use LLMs to optimize highly specialized real-world GPU kernels. We reveal\nthe intricacies of using LLMs for code optimization and analyze the code\noptimization techniques from the LLMs. We also propose ways to collaborate with\nLLMs to further leverage their capabilities. For the original 3DGS code on the\nMipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and\n24% with GPT-5, demonstrating the different capabilities of different LLMs. By\nfeeding additional information from performance profilers, the performance\nimprovement from LLM-optimized code is enhanced to up to 42% and 38% on\naverage. In comparison, our best-effort manually optimized version can achieve\na performance improvement up to 48% and 39% on average, showing that there are\nstill optimizations beyond the capabilities of current LLMs. On the other hand,\neven upon a newly proposed 3DGS framework with algorithmic optimizations,\nSeele, LLMs can still further enhance its performance by 6%, showing that there\nare optimization opportunities missed by domain experts. This highlights the\npotential of collaboration between domain experts and LLMs.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09GPU\u5185\u6838\uff0c\u5728MipNeRF360\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08\u6700\u9ad842%\uff09\uff0c\u5c55\u793a\u4e86LLMs\u5728\u4e13\u4e1a\u4ee3\u7801\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "3DGS\u5728\u5b9e\u65f6\u6e32\u67d3\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46GPU\u67b6\u6784\u590d\u6742\u4e14\u8c03\u4f18\u53c2\u6570\u7a7a\u95f4\u5927\uff0c\u624b\u52a8\u4f18\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u8017\u65f6\u6613\u9519\u3002", "method": "\u5229\u7528LLMs\u5206\u6790\u548c\u4f18\u5316\u9ad8\u65af\u6cfc\u6e85\u5185\u6838\uff0c\u7ed3\u5408\u6027\u80fd\u5206\u6790\u5668\u63d0\u4f9b\u989d\u5916\u4fe1\u606f\uff0c\u5e76\u4e0eLLMs\u534f\u4f5c\u4f18\u5316\u3002", "result": "\u539f\u59cb3DGS\u4ee3\u7801\u4e0a\uff0cDeepseek\u548cGPT-5\u5206\u522b\u5b9e\u73b019%\u548c24%\u52a0\u901f\uff1b\u7ed3\u5408\u6027\u80fd\u5206\u6790\u5668\u540e\u63d0\u5347\u81f3\u6700\u9ad842%\uff1b\u5728Seele\u6846\u67b6\u4e0a\u4ecd\u80fd\u8fdb\u4e00\u6b65\u4f18\u53166%\u3002", "conclusion": "LLMs\u5728GPU\u5185\u6838\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u6709\u624b\u52a8\u4f18\u5316\u65e0\u6cd5\u66ff\u4ee3\u7684\u90e8\u5206\uff0c\u4e13\u5bb6\u4e0eLLMs\u534f\u4f5c\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.25853", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25853", "abs": "https://arxiv.org/abs/2509.25853", "authors": ["Jingyao Zhang", "Jaewoo Park", "Jongeun Lee", "Elaheh Sadredini"], "title": "SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV", "comment": null, "summary": "Large Language Model (LLM) inference requires substantial computational\nresources, yet CPU-based inference remains essential for democratizing AI due\nto the widespread availability of CPUs compared to specialized accelerators.\nHowever, efficient LLM inference on CPUs faces two fundamental challenges: (1)\nexisting CPU architectures struggle with low-precision arithmetic required by\nquantized models, where optimal bit precision varies across models and layers;\nand (2) the memory-bound nature of the token generation phase creates severe\nperformance bottlenecks. To address these challenges, we propose SAIL\n(SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that\nefficiently supports arbitrary bit precisions with minimal overhead. SAIL\nintegrates three key innovations: First, we introduce Batched LUT-based General\nMatrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory,\nenabling high data reuse through lookup tables and reducing memory movement.\nSecond, our Pattern-Aware LUT optimization identifies and exploits redundancy\nin input activation patterns, reducing computation cycles by 13.8\\%. Third, we\ndevelop an in-memory type conversion algorithm that leverages PIM's parallelism\nfor efficient de-/quantization operations, alleviating pressure on CPU's vector\nunits. Our architecture requires only 2\\% hardware overhead and a single new\ninstruction, while maintaining dual functionality as both compute and storage\nunits. Experimental evaluations using a modified gem5 simulator demonstrate\nthat SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar\ncompared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost\nefficiency than NVIDIA V100 GPUs, establishing a practical path for efficient\nCPU-based LLM inference.", "AI": {"tldr": "SAIL\u662f\u4e00\u79cd\u57fa\u4e8eCPU\u7684LLM\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7SRAM\u52a0\u901f\u548c\u67e5\u627e\u8868\u6280\u672f\uff0c\u652f\u6301\u4efb\u610f\u6bd4\u7279\u7cbe\u5ea6\uff0c\u5728CPU\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "motivation": "CPU\u63a8\u7406\u5bf9AI\u6c11\u4e3b\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1aCPU\u67b6\u6784\u96be\u4ee5\u5904\u7406\u91cf\u5316\u6a21\u578b\u7684\u4f4e\u7cbe\u5ea6\u7b97\u672f\uff0c\u4ee5\u53catoken\u751f\u6210\u9636\u6bb5\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51faSAIL\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u6279\u5904\u7406LUT-GEMV\u4e0eSRAM\u5185\u5b58\u8ba1\u7b97\u3001\u6a21\u5f0f\u611f\u77e5LUT\u4f18\u5316\u3001\u5185\u5b58\u5185\u7c7b\u578b\u8f6c\u6362\u7b97\u6cd5\uff0c\u4ec5\u97002%\u786c\u4ef6\u5f00\u9500\u548c\u4e00\u4e2a\u65b0\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSAIL\u76f8\u6bd4ARM Neoverse-N1 CPU\u57fa\u7ebf\u5b9e\u73b010.7\u500d\u52a0\u901f\u548c19.9\u500d\u6bcf\u7f8e\u5143token\u6570\u63d0\u5347\uff0c\u6bd4NVIDIA V100 GPU\u6210\u672c\u6548\u7387\u9ad87.04\u500d\u3002", "conclusion": "SAIL\u4e3a\u57fa\u4e8eCPU\u7684\u9ad8\u6548LLM\u63a8\u7406\u5efa\u7acb\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u89e3\u51b3\u4e86CPU\u63a8\u7406\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2509.26065", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.26065", "abs": "https://arxiv.org/abs/2509.26065", "authors": ["Alberto Scionti", "Paolo Savio", "Francesco Lubrano", "Olivier Terzo", "Marco Ferretti", "Florin Apopei", "Juri Bellucci", "Ennio Spano", "Luca Carriere"], "title": "Runtime Energy Monitoring for RISC-V Soft-Cores", "comment": null, "summary": "Energy efficiency is one of the major concern in designing advanced computing\ninfrastructures. From single nodes to large-scale systems (data centers),\nmonitoring the energy consumption of the computing system when applications run\nis a critical task. Designers and application developers often rely on software\ntools and detailed architectural models to extract meaningful information and\ndetermine the system energy consumption. However, when a design space\nexploration is required, designers may incur in continuous tuning of the models\nto match with the system under evaluation. To overcome such limitations, we\npropose a holistic approach to monitor energy consumption at runtime without\nthe need of running complex (micro-)architectural models. Our approach is based\non a measurement board coupled with a FPGA-based System-on-Module. The\nmeasuring board captures currents and voltages (up to tens measuring points)\ndriving the FPGA and exposes such values through a specific memory region. A\nrunning service reads and computes energy consumption statistics without\nconsuming extra resources on the FPGA device. Our approach is also scalable to\nmonitoring of multi-nodes infrastructures (clusters). We aim to leverage this\nframework to perform experiments in the context of an aeronautical design\napplication; specifically, we will look at optimizing performance and energy\nconsumption of a shallow artificial neural network on RISC-V based soft-cores.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u8fd0\u884c\u65f6\u80fd\u8017\u76d1\u63a7\u65b9\u6cd5\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u6a21\u578b\u5373\u53ef\u5b9e\u65f6\u6d4b\u91cf\u8ba1\u7b97\u7cfb\u7edf\u7684\u80fd\u8017", "motivation": "\u4f20\u7edf\u80fd\u8017\u76d1\u63a7\u4f9d\u8d56\u590d\u6742\u7684\u8f6f\u4ef6\u5de5\u5177\u548c\u67b6\u6784\u6a21\u578b\uff0c\u5728\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65f6\u9700\u8981\u4e0d\u65ad\u8c03\u6574\u6a21\u578b\u53c2\u6570\uff0c\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u91c7\u7528\u6d4b\u91cf\u677f\u4e0eFPGA\u7cfb\u7edf\u6a21\u5757\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u677f\u6355\u83b7\u7535\u6d41\u548c\u7535\u538b\uff08\u6700\u591a\u6570\u5341\u4e2a\u6d4b\u91cf\u70b9\uff09\uff0c\u5e76\u901a\u8fc7\u7279\u5b9a\u5185\u5b58\u533a\u57df\u66b4\u9732\u8fd9\u4e9b\u503c\uff0c\u8fd0\u884c\u670d\u52a1\u8bfb\u53d6\u5e76\u8ba1\u7b97\u80fd\u8017\u7edf\u8ba1\u6570\u636e", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u76d1\u63a7\u80fd\u8017\uff0c\u4e0d\u6d88\u8017FPGA\u8bbe\u5907\u7684\u989d\u5916\u8d44\u6e90\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u591a\u8282\u70b9\u57fa\u7840\u8bbe\u65bd\uff08\u96c6\u7fa4\uff09\u7684\u76d1\u63a7", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u822a\u7a7a\u8bbe\u8ba1\u5e94\u7528\u4e2d\u7684\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u5173\u6ce8\u5728RISC-V\u8f6f\u6838\u4e0a\u4f18\u5316\u6d45\u5c42\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u548c\u80fd\u8017"}}
