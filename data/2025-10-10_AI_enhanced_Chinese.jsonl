{"id": "2510.07449", "categories": ["cs.AR", "C.1.0"], "pdf": "https://arxiv.org/pdf/2510.07449", "abs": "https://arxiv.org/abs/2510.07449", "authors": ["Georgia Antoniou", "Haris Volos", "Jawad Haj Yahya", "Yiannakis Sazeides"], "title": "How long can you sleep? Idle Time System Inefficiencies and Opportunities", "comment": "3 pages, 3 figures, accepted at the 1st International Workshop on\n  Data Center Energy Efficiency (DCEE2025) 2025", "summary": "This work introduces a model-based framework that reveals the idle\nopportunity of modern servers running latency-critical applications.\nSpecifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to\nestimate the theoretical idle time distribution at the CPU core and system\n(package) level. A comparison of the actual idleness of a real server and that\nfrom the theoretical models reveals significant missed opportunities to enter\ndeep idle states. This inefficiency is attributed to the idle-governor\ninaccuracy and the high latency to transition to/from legacy deep-idle states.\nThe proposed methodology offers the means for an early-stage design exploration\nand insights into idle time behavior and opportunities for varying server\nsystem configurations and load.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6392\u961f\u8bba\u7684\u6a21\u578b\u6846\u67b6\uff0c\u63ed\u793a\u8fd0\u884c\u5ef6\u8fdf\u5173\u952e\u5e94\u7528\u7684\u73b0\u4ee3\u670d\u52a1\u5668\u5b58\u5728\u5927\u91cf\u7a7a\u95f2\u673a\u4f1a\u672a\u88ab\u5229\u7528\u3002", "motivation": "\u73b0\u4ee3\u670d\u52a1\u5668\u5728\u8fd0\u884c\u5ef6\u8fdf\u5173\u952e\u5e94\u7528\u65f6\u5b58\u5728\u663e\u8457\u7684\u7a7a\u95f2\u65f6\u95f4\u6d6a\u8d39\uff0c\u9700\u8981\u91cf\u5316\u5206\u6790\u8fd9\u79cd\u4f4e\u6548\u73b0\u8c61\u3002", "method": "\u4f7f\u7528M/M/1\u3001cxM/M/1\u548cM/M/c\u4e09\u79cd\u6392\u961f\u6a21\u578b\u6765\u4f30\u8ba1CPU\u6838\u5fc3\u548c\u7cfb\u7edf\u7ea7\u522b\u7684\u7406\u8bba\u7a7a\u95f2\u65f6\u95f4\u5206\u5e03\u3002", "result": "\u5b9e\u9645\u670d\u52a1\u5668\u7a7a\u95f2\u65f6\u95f4\u4e0e\u7406\u8bba\u6a21\u578b\u5bf9\u6bd4\u663e\u793a\uff0c\u5b58\u5728\u5927\u91cf\u8fdb\u5165\u6df1\u5ea6\u7a7a\u95f2\u72b6\u6001\u7684\u673a\u4f1a\u88ab\u9519\u8fc7\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7a7a\u95f2\u7ba1\u7406\u5668\u7684\u51c6\u786e\u6027\u548c\u4f20\u7edf\u6df1\u5ea6\u7a7a\u95f2\u72b6\u6001\u8f6c\u6362\u5ef6\u8fdf\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e9\u671f\u8bbe\u8ba1\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b9\u6cd5\uff0c\u80fd\u591f\u6d1e\u5bdf\u4e0d\u540c\u670d\u52a1\u5668\u914d\u7f6e\u548c\u8d1f\u8f7d\u4e0b\u7684\u7a7a\u95f2\u65f6\u95f4\u884c\u4e3a\u548c\u4f18\u5316\u673a\u4f1a\u3002"}}
{"id": "2510.07719", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.07719", "abs": "https://arxiv.org/abs/2510.07719", "authors": ["Parker Hao Tian", "Zahra Yousefijamarani", "Alaa Alameldeen"], "title": "DL-PIM: Improving Data Locality in Processing-in-Memory Systems", "comment": null, "summary": "PIM architectures aim to reduce data transfer costs between processors and\nmemory by integrating processing units within memory layers. Prior PIM\narchitectures have shown potential to improve energy efficiency and\nperformance. However, such advantages rely on data proximity to the processing\nunits performing computations. Data movement overheads can degrade PIM's\nperformance and energy efficiency due to the need to move data between a\nprocessing unit and a distant memory location. %they face challenges due to the\noverhead of transferring data from remote memory locations to processing units\ninside memory for computation. In this paper, we demonstrate that a large\nfraction of PIM's latency per memory request is attributed to data transfers\nand queuing delays from remote memory accesses. To improve PIM's data locality,\nwe propose DL-PIM, a novel architecture that dynamically detects the overhead\nof data movement, and proactively moves data to a reserved area in the local\nmemory of the requesting processing unit. DL-PIM uses a distributed\naddress-indirection hardware lookup table to redirect traffic to the current\ndata location. We propose DL-PIM implementations on two 3D stacked memories:\nHMC and HBM. While some workloads benefit from DL-PIM, others are negatively\nimpacted by the additional latency due to indirection accesses. Therefore, we\npropose an adaptive mechanism that assesses the cost and benefit of indirection\nand dynamically enables or disables it to prevent degrading workloads that\nsuffer from indirection. Overall, DL-PIM reduces the average memory latency per\nrequest by 54% in HMC and 50% in HBM which resulted in performance improvement\nof 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all\nrepresentative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup\nin HBM, showing that DL-PIM enhances data locality and overall system\nperformance.", "AI": {"tldr": "DL-PIM\u662f\u4e00\u79cd\u65b0\u578bPIM\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u6d4b\u6570\u636e\u79fb\u52a8\u5f00\u9500\u5e76\u4e3b\u52a8\u5c06\u6570\u636e\u79fb\u52a8\u5230\u672c\u5730\u5185\u5b58\u7684\u4fdd\u7559\u533a\u57df\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u5730\u5740\u95f4\u63a5\u786c\u4ef6\u67e5\u627e\u8868\u91cd\u5b9a\u5411\u6d41\u91cf\uff0c\u4ece\u800c\u51cf\u5c11\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u7684\u6570\u636e\u4f20\u8f93\u548c\u6392\u961f\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709PIM\u67b6\u6784\u867d\u7136\u80fd\u63d0\u9ad8\u80fd\u6548\u548c\u6027\u80fd\uff0c\u4f46\u5176\u4f18\u52bf\u4f9d\u8d56\u4e8e\u6570\u636e\u4e0e\u5904\u7406\u5355\u5143\u7684\u90bb\u8fd1\u6027\u3002\u6570\u636e\u79fb\u52a8\u5f00\u9500\u4f1a\u964d\u4f4ePIM\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u56e0\u4e3a\u9700\u8981\u5728\u5904\u7406\u5355\u5143\u548c\u8fdc\u7a0b\u5185\u5b58\u4f4d\u7f6e\u4e4b\u95f4\u79fb\u52a8\u6570\u636e\u3002", "method": "\u63d0\u51faDL-PIM\u67b6\u6784\uff0c\u52a8\u6001\u68c0\u6d4b\u6570\u636e\u79fb\u52a8\u5f00\u9500\uff0c\u4e3b\u52a8\u5c06\u6570\u636e\u79fb\u52a8\u5230\u8bf7\u6c42\u5904\u7406\u5355\u5143\u7684\u672c\u5730\u5185\u5b58\u4fdd\u7559\u533a\u57df\u3002\u4f7f\u7528\u5206\u5e03\u5f0f\u5730\u5740\u95f4\u63a5\u786c\u4ef6\u67e5\u627e\u8868\u91cd\u5b9a\u5411\u6d41\u91cf\u5230\u5f53\u524d\u6570\u636e\u4f4d\u7f6e\u3002\u5728HMC\u548cHBM\u4e24\u79cd3D\u5806\u53e0\u5185\u5b58\u4e0a\u5b9e\u73b0\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u673a\u5236\u8bc4\u4f30\u95f4\u63a5\u8bbf\u95ee\u7684\u6210\u672c\u548c\u6536\u76ca\uff0c\u52a8\u6001\u542f\u7528\u6216\u7981\u7528\u95f4\u63a5\u8bbf\u95ee\u3002", "result": "DL-PIM\u5c06HMC\u4e2d\u6bcf\u4e2a\u8bf7\u6c42\u7684\u5e73\u5747\u5185\u5b58\u5ef6\u8fdf\u964d\u4f4e54%\uff0cHBM\u4e2d\u964d\u4f4e50%\u3002\u5bf9\u4e8e\u5177\u6709\u5927\u91cf\u6570\u636e\u91cd\u7528\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0cHMC\u6027\u80fd\u63d0\u534715%\uff0cHBM\u63d0\u53475%\u3002\u6240\u6709\u4ee3\u8868\u6027\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0cHMC\u5b9e\u73b06%\u52a0\u901f\uff0cHBM\u5b9e\u73b03%\u52a0\u901f\u3002", "conclusion": "DL-PIM\u901a\u8fc7\u589e\u5f3a\u6570\u636e\u5c40\u90e8\u6027\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u6570\u636e\u79fb\u52a8\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86PIM\u67b6\u6784\u7684\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.08137", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.08137", "abs": "https://arxiv.org/abs/2510.08137", "authors": ["Anastasios Petropoulos", "Theodore Antonakopoulos"], "title": "A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations", "comment": null, "summary": "Deep neural network (DNN) inference relies increasingly on specialized\nhardware for high computational efficiency. This work introduces a\nfield-programmable gate array (FPGA)-based dynamically configurable accelerator\nfeaturing systolic arrays, high-bandwidth memory, and UltraRAMs. We present two\nprocessing unit (PU) configurations with different computing capabilities using\nthe same interfaces and peripheral blocks. By instantiating multiple PUs and\nemploying a heuristic weight transfer schedule, the architecture achieves\nnotable throughput efficiency over prior works. Moreover, we outline how the\narchitecture can be extended to emulate analog in-memory computing (AIMC)\ndevices to aid next-generation heterogeneous AIMC chip designs and investigate\ndevice-level noise behavior. Overall, this brief presents a versatile DNN\ninference acceleration architecture adaptable to various models and future FPGA\ndesigns.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u53ef\u52a8\u6001\u914d\u7f6eDNN\u63a8\u7406\u52a0\u901f\u5668\u67b6\u6784\uff0c\u91c7\u7528\u8109\u52a8\u9635\u5217\u3001\u9ad8\u5e26\u5bbd\u5185\u5b58\u548cUltraRAM\uff0c\u652f\u6301\u591a\u79cd\u5904\u7406\u5355\u5143\u914d\u7f6e\u548c\u6743\u91cd\u4f20\u8f93\u8c03\u5ea6\uff0c\u5e76\u80fd\u6a21\u62df\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\u8bbe\u5907\u3002", "motivation": "\u968f\u7740DNN\u63a8\u7406\u5bf9\u4e13\u7528\u786c\u4ef6\u8ba1\u7b97\u6548\u7387\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u5f00\u53d1\u7075\u6d3b\u53ef\u914d\u7f6e\u7684\u52a0\u901f\u5668\u67b6\u6784\u6765\u9002\u5e94\u4e0d\u540c\u6a21\u578b\u548c\u672a\u6765FPGA\u8bbe\u8ba1\u3002", "method": "\u4f7f\u7528FPGA\u6784\u5efa\u52a8\u6001\u53ef\u914d\u7f6e\u52a0\u901f\u5668\uff0c\u5305\u542b\u8109\u52a8\u9635\u5217\u3001\u9ad8\u5e26\u5bbd\u5185\u5b58\u548cUltraRAM\uff1b\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4e0d\u540c\u8ba1\u7b97\u80fd\u529b\u7684\u5904\u7406\u5355\u5143\u914d\u7f6e\uff1b\u91c7\u7528\u542f\u53d1\u5f0f\u6743\u91cd\u4f20\u8f93\u8c03\u5ea6\uff1b\u652f\u6301\u6a21\u62df\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\u8bbe\u5907\u3002", "result": "\u8be5\u67b6\u6784\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u541e\u5410\u6548\u7387\u63d0\u5347\uff0c\u5e76\u80fd\u6a21\u62dfAIMC\u8bbe\u5907\u4ee5\u8f85\u52a9\u4e0b\u4e00\u4ee3\u5f02\u6784AIMC\u82af\u7247\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5404\u79cd\u6a21\u578b\u548c\u672a\u6765FPGA\u8bbe\u8ba1\u7684\u901a\u7528DNN\u63a8\u7406\u52a0\u901f\u67b6\u6784\uff0c\u5177\u6709\u9ad8\u5ea6\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.08351", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.08351", "abs": "https://arxiv.org/abs/2510.08351", "authors": ["Qingxiu Liu", "Jiazhen Cai", "Siyuan Sheng", "Yuhui Chen", "Lu Tang", "Zhirong Shen", "Patrick P. C. Lee"], "title": "FMCache: File-System Metadata Caching in Programmable Switches", "comment": "14 pages", "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.", "AI": {"tldr": "FMCache\u662f\u4e00\u4e2a\u5229\u7528\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a\u5728\u6570\u636e\u5e73\u9762\u76f4\u63a5\u670d\u52a1\u6587\u4ef6\u7cfb\u7edf\u5143\u6570\u636e\u8bf7\u6c42\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u5ba2\u6237\u7aef\u573a\u666f\u4e0b\u5143\u6570\u636e\u7f13\u5b58\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edfHDFS\u5b9e\u73b0\u4e86\u9ad8\u8fbe181.6%\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u8de8\u591a\u4e2a\u5143\u6570\u636e\u670d\u52a1\u5668\u7684\u5feb\u901f\u53ef\u6269\u5c55\u5143\u6570\u636e\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u5ba2\u6237\u7aef\u7f13\u5b58\u867d\u7136\u80fd\u51cf\u8f7b\u670d\u52a1\u5668\u8d1f\u8f7d\uff0c\u4f46\u5728\u5ba2\u6237\u7aef\u6570\u91cf\u589e\u52a0\u65f6\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u7ef4\u62a4\u5f00\u9500\u548c\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faFMCache\u6846\u67b6\uff0c\u5229\u7528\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a\u5728\u4ea4\u6362\u673a\u6570\u636e\u5e73\u9762\u76f4\u63a5\u670d\u52a1\u6765\u81ea\u591a\u4e2a\u5ba2\u6237\u7aef\u7684\u6587\u4ef6\u7cfb\u7edf\u5143\u6570\u636e\u8bf7\u6c42\uff0c\u89e3\u51b3\u4e86\u6587\u4ef6\u7cfb\u7edf\u7279\u5b9a\u7684\u8def\u5f84\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5728\u4e25\u683c\u7684\u4ea4\u6362\u673a\u8d44\u6e90\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7f13\u5b58\u3002", "result": "\u5728Tofino\u4ea4\u6362\u673a\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u4f7f\u7528\u771f\u5b9e\u6587\u4ef6\u7cfb\u7edf\u5143\u6570\u636e\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u8bc4\u4f30\uff0cFMCache\u76f8\u6bd4\u539f\u751fHDFS\u5b9e\u73b0\u4e86\u9ad8\u8fbe181.6%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4e0e\u5ba2\u6237\u7aef\u7f13\u5b58\u7ed3\u5408\u65f6\u8fd8\u80fd\u5e26\u6765\u989d\u5916139.6%\u7684\u541e\u5410\u91cf\u589e\u76ca\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u548c\u6709\u9650\u7684\u4ea4\u6362\u673a\u8d44\u6e90\u4f7f\u7528\u3002", "conclusion": "FMCache\u901a\u8fc7\u5229\u7528\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6587\u4ef6\u7cfb\u7edf\u5143\u6570\u636e\u7f13\u5b58\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u591a\u5ba2\u6237\u7aef\u7f13\u5b58\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002"}}
{"id": "2510.08544", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08544", "abs": "https://arxiv.org/abs/2510.08544", "authors": ["Hengrui Zhang", "Pratyush Patel", "August Ning", "David Wentzlaff"], "title": "SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference", "comment": null, "summary": "Large Language Models (LLMs) have gained popularity in recent years, driving\nup the demand for inference. LLM inference is composed of two phases with\ndistinct characteristics: a compute-bound prefill phase followed by a\nmemory-bound decode phase. To efficiently serve LLMs, prior work proposes\nprefill-decode disaggregation to run each phase on separate hardware. However,\nexisting hardware poorly matches the different requirements of each phase.\nCurrent datacenter GPUs and TPUs follow a more-is-better design philosophy that\nmaximizes compute and memory resources, causing memory bandwidth\nunderutilization in the prefill phase and compute underutilization in the\ndecode phase. Such underutilization directly translates into increased serving\ncosts.\n  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting\na less-is-more methodology to design specialized chips tailored to the distinct\ncharacteristics of prefill and decode phases. The proposed Prefill Chips have\nlarger systolic arrays and use cost-effective GDDR memory, whereas the proposed\nDecode Chips retain high memory bandwidth but reduce compute capacity. Compared\nto modeled H100s, simulations show that the proposed Prefill Chips deliver 8%\nhigher prefill performance on average at 52% lower hardware cost, while the\nproposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.\n  End-to-end simulations on production traces show that SPAD reduces hardware\ncost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while\noffering the same performance. Even when models and workloads change, SPAD can\nreallocate either type of chip to run either phase and still achieve 11%-43%\nlower hardware costs, demonstrating the longevity of the SPAD design.", "AI": {"tldr": "SPAD\u63d0\u51fa\u4e13\u95e8\u9488\u5bf9LLM\u63a8\u7406\u7684\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u8bbe\u8ba1\u4e13\u7528\u82af\u7247\uff0c\u901a\u8fc7\u786c\u4ef6\u89e3\u8026\u548c\u4e13\u4e1a\u5316\u8bbe\u8ba1\u964d\u4f4e\u6210\u672c\u548c\u529f\u8017\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u4e2d\u5fc3GPU/TPU\u91c7\u7528\"\u8d8a\u591a\u8d8a\u597d\"\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u5bfc\u81f4\u9884\u586b\u5145\u9636\u6bb5\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u4e0d\u8db3\u548c\u89e3\u7801\u9636\u6bb5\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\uff0c\u589e\u52a0\u4e86\u670d\u52a1\u6210\u672c\u3002", "method": "\u8bbe\u8ba1\u4e13\u95e8\u7684\u9884\u586b\u5145\u82af\u7247\uff08\u66f4\u5927\u8109\u52a8\u9635\u5217+GDDR\u5185\u5b58\uff09\u548c\u89e3\u7801\u82af\u7247\uff08\u4fdd\u6301\u9ad8\u5185\u5b58\u5e26\u5bbd\u4f46\u51cf\u5c11\u8ba1\u7b97\u80fd\u529b\uff09\uff0c\u91c7\u7528\u786c\u4ef6\u89e3\u8026\u67b6\u6784\u3002", "result": "\u76f8\u6bd4H100\uff0c\u9884\u586b\u5145\u82af\u7247\u6027\u80fd\u63d0\u53478%\uff0c\u786c\u4ef6\u6210\u672c\u964d\u4f4e52%\uff1b\u89e3\u7801\u82af\u7247\u8fbe\u523097%\u6027\u80fd\uff0cTDP\u964d\u4f4e28%\u3002\u7aef\u5230\u7aef\u6a21\u62df\u663e\u793a\u786c\u4ef6\u6210\u672c\u964d\u4f4e19%-41%\uff0cTDP\u964d\u4f4e2%-17%\u3002", "conclusion": "SPAD\u901a\u8fc7\u4e13\u4e1a\u5316\u786c\u4ef6\u8bbe\u8ba1\u663e\u8457\u964d\u4f4eLLM\u63a8\u7406\u6210\u672c\uff0c\u4e14\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\uff0c\u5373\u4f7f\u6a21\u578b\u548c\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u4e5f\u80fd\u4fdd\u6301\u6210\u672c\u4f18\u52bf\u3002"}}
