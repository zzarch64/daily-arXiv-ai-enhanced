<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Open-source Stand-Alone Versatile Tensor Accelerator](https://arxiv.org/abs/2509.19790)
*Anthony Faure-Gignoux,Kevin Delmas,Adrien Gauffriau,Claire Pagetti*

Main category: cs.AR

TL;DR: 开发了一个用于VTA的开源独立Python编译器管道，解决了VTA依赖TVM编译器且不符合认证要求的问题，展示了在VTA模拟器上编译和执行LeNet-5 CNN的有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习应用需要大量计算资源，在航空等安全关键领域面临挑战。VTA是基于FPGA的有前景解决方案，但其依赖TVM编译器且代码不符合认证要求，阻碍了其应用。

Method: 从头开发了一个开源、独立的Python编译器管道，设计时考虑了认证要求、模块化和可扩展性。使用VTA模拟器编译和执行LeNet-5卷积神经网络来验证编译器效果。

Result: 初步结果表明，该编译器具有扩展到更大CNN架构的强大潜力。所有贡献都已公开可用。

Conclusion: 成功开发了一个符合认证要求的VTA编译器，为安全关键领域的ML应用提供了可行的FPGA加速解决方案。

Abstract: Machine Learning (ML) applications demand significant computational
resources, posing challenges for safety-critical domains like aeronautics. The
Versatile Tensor Accelerator (VTA) is a promising FPGA-based solution, but its
adoption was hindered by its dependency on the TVM compiler and by other code
non-compliant with certification requirements. This paper presents an
open-source, standalone Python compiler pipeline for the VTA, developed from
scratch and designed with certification requirements, modularity, and
extensibility in mind. The compiler's effectiveness is demonstrated by
compiling and executing LeNet-5 Convolutional Neural Network (CNN) using the
VTA simulators, and preliminary results indicate a strong potential for scaling
its capabilities to larger CNN architectures. All contributions are publicly
available.

</details>


### [2] [SpecMamba: Accelerating Mamba Inference on FPGA with Speculative Decoding](https://arxiv.org/abs/2509.19873)
*Linfeng Zhong,Songqiang Xu,Huifeng Wen,Tong Xie,Qingyu Guo,Yuan Wang,Meng Li*

Main category: cs.AR

TL;DR: SpecMamba是首个基于FPGA的Mamba状态空间模型加速器，采用推测解码技术，通过系统、算法和硬件协同设计解决SSM在推测解码中的三个关键挑战，相比GPU基线实现2.27倍加速和5.41倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 边缘设备对长序列建模的需求日益增长，Mamba等状态空间模型因其计算效率和可扩展性而广泛应用。但其自回归生成过程仍受内存限制，推测解码技术面临隐藏状态回溯困难、树形并行验证不兼容和硬件工作负载不匹配三大挑战。

Method: 提出SpecMamba三层次设计：系统层采用内存感知混合回溯策略协调双模型；算法层提出基于FIFO的树形验证与分块技术减少内存访问；硬件层定制数据流，并行计算线性层、串行计算SSM层以实现最大重叠。

Result: 在AMD FPGA平台（VHK158和VCK190）上实现，相比GPU基线获得2.27倍加速，相比现有FPGA方案提升2.85倍，能效分别提高5.41倍和1.26倍。

Conclusion: SpecMamba成功解决了SSM推测解码的关键技术挑战，通过协同设计实现了显著的性能提升和能效优化，为边缘设备的长序列建模提供了高效解决方案。

Abstract: The growing demand for efficient long-sequence modeling on edge devices has
propelled widespread adoption of State Space Models (SSMs) like Mamba, due to
their superior computational efficiency and scalability. As its autoregressive
generation process remains memory-bound, speculative decoding has been proposed
that incorporates draft model generation and target model verification.
However, directly applying speculative decoding to SSMs faces three key
challenges: (1) hidden state backtracking difficulties, (2) tree-based parallel
verification incompatibility, and (3) hardware workload mismatch. To address
these challenges, we propose SpecMamba, the first FPGA-based accelerator for
Mamba with speculative decoding, which features system, algorithm, and hardware
co-design. At the system level, we present a memory-aware hybrid backtracking
strategy to coordinate both models. At the algorithm level, we propose
first-in-first-out (FIFO)-based tree verification with tiling to minimize
memory access. At the hardware level, we customize a dataflow that computes
linear layers in parallel and SSM layers in series to enable maximal
overlapping. Implemented on AMD FPGA platforms (VHK158 and VCK190), SpecMamba
achieves a 2.27x speedup over GPU baselines and a 2.85x improvement compared to
prior FPGA solutions, while demonstrating 5.41x and 1.26x higher energy
efficiency, respectively.

</details>


### [3] [OpenGL GPU-Based Rowhammer Attack (Work in Progress)](https://arxiv.org/abs/2509.19959)
*Antoine Plin,Frédéric Fauberteau,Nga Nguyen*

Main category: cs.AR

TL;DR: 本文提出了一种基于GPU计算着色器的自适应多面Rowhammer攻击方法，通过统计分布优化行目标选择并规避现有缓解措施，在树莓派4上实现了比传统CPU攻击更高的比特翻转率。


<details>
  <summary>Details</summary>
Motivation: Rowhammer攻击已成为现代DRAM内存系统的重大威胁，利用频繁内存访问诱导相邻内存单元比特翻转。本研究旨在探索GPU并行处理能力在加速Rowhammer攻击方面的有效性。

Method: 采用OpenGL计算着色器实现高效行锤击，方法包括：初始化已知模式内存、迭代锤击受害行、监控诱导错误、动态调整参数以最大化成功率。利用GPU并行处理能力加速锤击操作。

Result: 在树莓派4上的实验结果表明，基于GPU的方法相比传统CPU锤击实现了更高的比特翻转率，证实了其在破坏DRAM完整性方面的有效性。

Conclusion: 本研究有助于理解GPU辅助的故障注入攻击，并强调了未来内存架构中需要改进缓解策略的必要性，与异构系统中微架构攻击的现有研究结果一致。

Abstract: Rowhammer attacks have emerged as a significant threat to modern DRAM-based
memory systems, leveraging frequent memory accesses to induce bit flips in
adjacent memory cells. This work-in-progress paper presents an adaptive,
many-sided Rowhammer attack utilizing GPU compute shaders to systematically
achieve high-frequency memory access patterns. Our approach employs statistical
distributions to optimize row targeting and avoid current mitigations. The
methodology involves initializing memory with known patterns, iteratively
hammering victim rows, monitoring for induced errors, and dynamically adjusting
parameters to maximize success rates. The proposed attack exploits the parallel
processing capabilities of GPUs to accelerate hammering operations, thereby
increasing the probability of successful bit flips within a constrained
timeframe. By leveraging OpenGL compute shaders, our implementation achieves
highly efficient row hammering with minimal software overhead. Experimental
results on a Raspberry Pi 4 demonstrate that the GPU-based approach attains a
high rate of bit flips compared to traditional CPU-based hammering, confirming
its effectiveness in compromising DRAM integrity. Our findings align with
existing research on microarchitectural attacks in heterogeneous systems that
highlight the susceptibility of GPUs to security vulnerabilities. This study
contributes to the understanding of GPU-assisted fault-injection attacks and
underscores the need for improved mitigation strategies in future memory
architectures.

</details>


### [4] [Automated Multi-Agent Workflows for RTL Design](https://arxiv.org/abs/2509.20182)
*Amulya Bhattaram,Janani Ramamoorthy,Ranit Gupta,Diana Marculescu,Dimitrios Stamoulis*

Main category: cs.AR

TL;DR: VeriMaAS是一个多智能体框架，通过集成形式验证反馈来自动生成RTL代码生成的工作流，在少量训练样本下实现5-7%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对程序合成等专业领域，HDL和EDA资源的稀缺性带来了挑战，需要任务特定的微调、高推理成本和手动编排智能体工作流。

Method: 提出VeriMaAS多智能体框架，将HDL工具的形式验证反馈直接集成到工作流生成中，减少基于梯度的更新或长时间推理轨迹的成本。

Result: 在pass@k指标上比微调基线提高了5-7%，仅需几百个训练样本，监督成本降低了一个数量级。

Conclusion: 该方法通过形式验证反馈的集成，有效解决了专业领域智能体工作流生成的挑战，显著降低了监督成本并提升了性能。

Abstract: The rise of agentic AI workflows unlocks novel opportunities for computer
systems design and optimization. However, for specialized domains such as
program synthesis, the relative scarcity of HDL and proprietary EDA resources
online compared to more common programming tasks introduces challenges, often
necessitating task-specific fine-tuning, high inference costs, and
manually-crafted agent orchestration. In this work, we present VeriMaAS, a
multi-agent framework designed to automatically compose agentic workflows for
RTL code generation. Our key insight is to integrate formal verification
feedback from HDL tools directly into workflow generation, reducing the cost of
gradient-based updates or prolonged reasoning traces. Our method improves
synthesis performance by 5-7% for pass@k over fine-tuned baselines, while
requiring only a few hundred training examples, representing an
order-of-magnitude reduction in supervision cost.

</details>
