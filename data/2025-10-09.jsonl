{"id": "2510.06513", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06513", "abs": "https://arxiv.org/abs/2510.06513", "authors": ["Debendra Das Sharma", "Swadesh Choudhary", "Peter Onufryk", "Rob Pelt"], "title": "On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach", "comment": "10 pages", "summary": "Emerging computing applications such as Artificial Intelligence (AI) are\nfacing a memory wall with existing on-package memory solutions that are unable\nto meet the power-efficient bandwidth demands. We propose to enhance UCIe with\nmemory semantics to deliver power-efficient bandwidth and cost-effective\non-package memory solutions applicable across the entire computing continuum.\nWe propose approaches by reusing existing LPDDR6 and HBM memory through a logic\ndie that connects to the SoC using UCIe. We also propose an approach where the\nDRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our\napproaches result in significantly higher bandwidth density (up to 10x), lower\nlatency (up to 3x), lower power (up to 3x), and lower cost compared to existing\nHBM4 and LPDDR on-package memory solutions."}
{"id": "2510.06644", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.06644", "abs": "https://arxiv.org/abs/2510.06644", "authors": ["Leshu Li", "Jiayin Qin", "Jie Peng", "Zishen Wan", "Huaizhi Qu", "Ye Han", "Pingqing Zheng", "Hongsen Zhang", "Yu", "Cao", "Tianlong Chen", "Yang", "Zhao"], "title": "RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction", "comment": "Accepted by MICRO2025", "summary": "3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping\n(SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering\nefficiency and accuracy, but have not yet been adopted in resource-constrained\nedge devices due to insufficient speed. Addressing this, we identify notable\nredundancies across the SLAM pipeline for acceleration. While conceptually\nstraightforward, practical approaches are required to minimize the overhead\nassociated with identifying and eliminating these redundancies. In response, we\npropose RTGS, an algorithm-hardware co-design framework that comprehensively\nreduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the\noverhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline.\nOn the algorithm side, we introduce (1) an adaptive Gaussian pruning step to\nremove the redundant Gaussians by reusing gradients computed during\nbackpropagation; and (2) a dynamic downsampling technique that directly reuses\nthe keyframe identification and alpha computing steps to eliminate redundant\npixels. On the hardware side, we propose (1) a subtile-level streaming strategy\nand a pixel-level pairwise scheduling strategy that mitigates workload\nimbalance via a Workload Scheduling Unit (WSU) guided by previous iteration\ninformation; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates\nthe rendering backpropagation by reusing intermediate data computed during\nrendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory\naccesses caused by atomic operations while enabling pipelined aggregation.\nIntegrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on\nfour datasets and three algorithms, with up to 82.5x energy efficiency over the\nbaseline and negligible quality loss. Code is available at\nhttps://github.com/UMN-ZhaoLab/RTGS."}
{"id": "2510.06767", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.06767", "abs": "https://arxiv.org/abs/2510.06767", "authors": ["Bindu G Gowda", "Yogesh Goyal", "Yash Gupta", "Madhav Rao"], "title": "Hardware-Efficient CNNs: Interleaved Approximate FP32 Multipliers for Kernel Computation", "comment": null, "summary": "Single-precision floating point (FP32) data format, defined by the IEEE 754\nstandard, is widely employed in scientific computing, signal processing, and\ndeep learning training, where precision is critical. However, FP32\nmultiplication is computationally expensive and requires complex hardware,\nespecially for precisely handling mantissa multiplication. In practical\napplications like neural network inference, perfect accuracy is not always\nnecessary, minor multiplication errors often have little impact on final\naccuracy. This enables trading precision for gains in area, power, and speed.\nThis work focuses on CNN inference using approximate FP32 multipliers, where\nthe mantissa multiplication is approximated by employing error-variant\napproximate compressors, that significantly reduce hardware cost. Furthermore,\nthis work optimizes CNN performance by employing differently approximated FP32\nmultipliers and studying their impact when interleaved within the kernels\nacross the convolutional layers. The placement and ordering of these\napproximate multipliers within each kernel are carefully optimized using the\nNon-dominated Sorting Genetic Algorithm-II, balancing the trade-off between\naccuracy and hardware efficiency."}
{"id": "2510.07304", "categories": ["cs.AR", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07304", "abs": "https://arxiv.org/abs/2510.07304", "authors": ["Donghwan Kim", "Xin Gu", "Jinho Baek", "Timothy Lo", "Younghoon Min", "Kwangsik Shin", "Jongryool Kim", "Jongse Park", "Kiwan Maeng"], "title": "Cocoon: A System Architecture for Differentially Private Training with Correlated Noises", "comment": null, "summary": "Machine learning (ML) models memorize and leak training data, causing serious\nprivacy issues to data owners. Training algorithms with differential privacy\n(DP), such as DP-SGD, have been gaining attention as a solution. However,\nDP-SGD adds a noise at each training iteration, which degrades the accuracy of\nthe trained model. To improve accuracy, a new family of approaches adds\ncarefully designed correlated noises, so that noises cancel out each other\nacross iterations. We performed an extensive characterization study of these\nnew mechanisms, for the first time to the best of our knowledge, and show they\nincur non-negligible overheads when the model is large or uses large embedding\ntables. Motivated by the analysis, we propose Cocoon, a hardware-software\nco-designed framework for efficient training with correlated noises. Cocoon\naccelerates models with embedding tables through pre-computing and storing\ncorrelated noises in a coalesced format (Cocoon-Emb), and supports large models\nthrough a custom near-memory processing device (Cocoon-NMP). On a real system\nwith an FPGA-based NMP device prototype, Cocoon improves the performance by\n2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP)."}
