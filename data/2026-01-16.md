<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization](https://arxiv.org/abs/2601.09773)
*Binglei Lou,Ruilin Wu,Philip Leong*

Main category: cs.AR

TL;DR: SparseLUT框架通过架构增强和训练算法优化，解决了LUT-based DNNs中LUT大小指数增长和稀疏连接效率低的问题，在保持精度的同时显著降低了硬件资源消耗和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 在FPGA等资源受限的边缘设备上部署DNN需要平衡延迟、功耗和硬件资源使用，同时保持高精度。现有基于查找表(LUT)的DNN（如LogicNets、PolyLUT、NeuraLUT）面临两个关键挑战：LUT大小的指数增长和低效的随机稀疏连接。

Method: 提出SparseLUT框架，包含两个正交优化：1) 架构增强：通过加法器聚合多个PolyLUT子神经元，显著减少LUT消耗和推理延迟；2) 非贪婪训练算法：通过选择性剪枝不重要的输入并策略性地重新生长更有效的连接来优化神经元连接。

Result: 架构优化将LUT消耗降低2.0x-13.9x，推理延迟降低1.2x-1.6x，同时保持可比的精度。训练优化在基准测试中带来一致的精度提升：在MNIST上达到2.13%增益，在Jet Substructure Classification上达到0.94%增益，且不增加额外面积和延迟开销。

Conclusion: SparseLUT框架有效解决了LUT-based DNNs的关键挑战，通过架构增强和训练优化，在资源受限的边缘设备上实现了更好的性能-精度权衡，为FPGA上的高效DNN部署提供了实用解决方案。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained edge devices such as FPGAs requires a careful balance among latency, power, and hardware resource usage, while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs -- such as LogicNets, PolyLUT, and NeuraLUT -- face two critical challenges: the exponential growth of LUT size and inefficient random sparse connectivity. This paper presents SparseLUT, a comprehensive framework that addresses these challenges through two orthogonal optimizations. First, we propose an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, significantly reducing LUT consumption by 2.0x-13.9x and lowering inference latency by 1.2x-1.6x, all while maintaining comparable accuracy. Building upon this foundation, we further introduce a non-greedy training algorithm that optimizes neuron connectivity by selectively pruning less significant inputs and strategically regrowing more effective ones. This training optimization, which incurs no additional area and latency overhead, delivers consistent accuracy improvements across benchmarks -- achieving up to a 2.13% gain on MNIST and 0.94% on Jet Substructure Classification compared to existing LUT-DNN approaches.

</details>


### [2] [Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications](https://arxiv.org/abs/2601.10463)
*Xinyu Shi,Simei Yang,Francky Catthoor*

Main category: cs.AR

TL;DR: 论文提出了一种跨层方法对XR工作负载进行架构分类，识别出容量受限和开销敏感等核心工作负载原型，为下一代XR SoC设计提供指导，强调需要从通用资源扩展转向阶段感知调度和弹性资源分配。


<details>
  <summary>Details</summary>
Motivation: XR平台需要在严格的功耗和面积约束下提供确定性超低延迟性能，但XR工作负载多样性快速增加，具有异构算子类型和复杂数据流结构，这对传统以CNN为中心的加速器架构构成挑战，且缺乏对完整XR管道的系统性架构理解。

Method: 采用跨层方法整合模型驱动的高层设计空间探索和商用GPU/CPU硬件上的经验性性能分析，对12个不同XR内核的代表性工作负载集进行分析，将复杂架构特征提炼为少量跨层工作负载原型。

Result: 识别出容量受限、开销敏感等关键工作负载原型，基于这些原型提取了重要的架构洞察，为下一代XR SoC提供了可操作的设计指南。

Conclusion: XR架构设计必须从通用资源扩展转向阶段感知调度和弹性资源分配，以实现未来XR系统更高的能效和性能。

Abstract: Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems.

</details>
