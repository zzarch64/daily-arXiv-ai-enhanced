<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement](https://arxiv.org/abs/2512.14151)
*Songze Liu,Hongkun Du,Shaowen Wang*

Main category: cs.AR

TL;DR: 本文提出了一种针对LLM推理工作负载的自适应缓存污染控制机制（ACPC），通过TCN预测访问模式和优先级感知替换策略，显著减少缓存污染并提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM推理过程中频繁的token序列查找和嵌入向量检索会产生高度不规则和突发的访问模式，导致传统预取和替换策略误判，引发严重的缓存污染，从而降低系统性能。

Method: 提出自适应缓存污染控制机制（ACPC），集成基于时间卷积网络（TCN）的访问预测和优先级感知替换策略。TCN模块学习token访问序列的时间依赖性以识别潜在高重用缓存行，替换策略根据预测的重用可能性和缓存占用情况动态调整驱逐优先级。

Result: 实验结果表明，与最先进的基于机器学习的替换基准相比，ACPC减少缓存污染41.7%，提高缓存命中率8.9%，降低L2缺失惩罚60.0%。此外，ACPC框架提高token生成吞吐量15.9%，达到最低最终损失0.21。

Conclusion: ACPC能有效识别有用缓存行并在动态LLM访问行为下减少冗余预取，为大规模LLM服务和推理系统提供可扩展的学习驱动解决方案，优化内存效率和延迟。

Abstract: Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.

</details>


### [2] [ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework](https://arxiv.org/abs/2512.14172)
*Qijun Zhang,Shang Liu,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: ReadyPower是一个新的分析型功耗建模框架，通过引入架构级、实现级和技术级参数来改进McPAT模型，解决了传统分析模型精度低和机器学习模型不可靠、难解释、难使用的问题。


<details>
  <summary>Details</summary>
Motivation: 现代处理器设计中功耗是主要目标，需要准确高效的功耗建模技术。传统分析模型（如McPAT）精度不足，而基于机器学习的方法虽然精度高但存在不可靠、可解释性差、使用困难等问题，限制了工业界应用。

Method: 提出ReadyPower框架，通过向McPAT分析模型中引入三个层次的参数来弥合实际处理器实现与分析模型之间的差异：架构级、实现级和技术级参数。这些参数通过不同方式确定，以提高模型准确性。

Result: 在不同训练场景下，ReadyPower在BOOM和XiangShan CPU架构上，相比基于机器学习的基线方法，平均绝对百分比误差降低超过20%，相关系数R提高超过0.2。

Conclusion: ReadyPower提供了一个可靠、可解释且易于使用的功耗建模框架，解决了传统分析模型精度不足和机器学习模型实际应用困难的问题，为早期功耗优化和设计空间探索提供了更好的工具。

Abstract: Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.

</details>


### [3] [TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips](https://arxiv.org/abs/2512.14256)
*Huizheng Wang,Taiquan Wei,Zichuan Wang,Dingcheng Jiang,Qize Yang,Jiaxin Liu,Jingxiang Hou,Chao Li,Jinyi Deng,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: TEMP框架通过拓扑感知的张量流分区、流量感知映射和双层晶圆求解，优化了在晶圆级芯片上训练大语言模型的性能和内存效率，实现了1.7倍的平均吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 晶圆级芯片虽然提供高计算能力和带宽，但面临片上内存与计算资源的权衡，且现有张量并行策略无法充分利用其通信优势同时保持内存效率。

Method: 提出张量流分区范式，并开发TEMP框架，包含：1) 拓扑感知的张量流分区，2) 流量感知映射，3) 双层晶圆求解，以克服硬件限制和并行挑战。

Result: TEMP在各种模型上相比最先进的大语言模型训练系统实现了1.7倍的平均吞吐量提升。

Conclusion: TEMP框架通过集成方法优化内存效率和吞吐量，充分释放了张量流分区范式在晶圆级芯片上的潜力，有效解决了现有方法的不足。

Abstract: Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.
  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.
  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.

</details>


### [4] [PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion](https://arxiv.org/abs/2512.14322)
*Huizheng Wang,Hongbin Wang,Zichuan Wang,Zhiheng Yue,Yang Wang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: PADE提出了一种无需预测器的算法-硬件协同设计，用于动态稀疏注意力加速，通过位级不确定性区间保护过滤、双向稀疏乱序执行和交错稀疏分块注意力等技术，在22个基准测试中实现了7.43倍加速和31.1倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的模型虽然革命性，但自注意力的二次计算成本带来严重的计算和内存开销。现有稀疏注意力方法因需要额外的稀疏性预测器而缺乏实用性，预测器会严重降低硬件效率。

Method: 提出PADE，一种无需预测器的算法-硬件协同设计，包含三个关键技术：1) 位级不确定性区间保护过滤策略，在每个位轮次准确识别无关紧要的token；2) 基于双向稀疏性的乱序执行，提高硬件利用率；3) 基于交错的稀疏分块注意力，降低I/O和计算复杂度。结合定制加速器设计，实现无需稀疏预测器的实用稀疏加速。

Result: 在22个基准测试中，PADE相比Nvidia H100 GPU实现了7.43倍加速和31.1倍能效提升。相比SOTA加速器，PADE分别比Sanger、DOTA和SOFA节省5.1倍、4.3倍和3.4倍能耗。

Conclusion: PADE通过创新的算法-硬件协同设计，成功解决了动态稀疏注意力加速中的关键挑战，实现了无需额外预测器的高效稀疏注意力加速，显著提升了计算效率和能效。

Abstract: Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.
  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.
  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.

</details>


### [5] [Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models](https://arxiv.org/abs/2512.14661)
*Chiyue Wei,Cong Guo,Junyao Zhang,Haoxuan Shan,Yifan Xu,Ziyue Zhang,Yudong Liu,Qinsi Wang,Changchun Zhou,Hai "Helen" Li,Yiran Chen*

Main category: cs.AR

TL;DR: Focus提出了一种流式压缩架构，通过多级冗余消除高效加速视觉语言模型推理，实现2.4倍加速和3.3倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视频理解任务中表现出色，但模型规模增大和视频级输入导致计算和内存开销显著，难以在硬件加速器上实时部署。现有方法通常采用粗粒度的token剪枝或合并，存在运行时开销高的问题。

Method: 提出Focus流式压缩架构，采用三级层次化压缩：1) 基于文本提示的语义引导token剪枝；2) 使用局部比较的空间-时间块级压缩；3) 通过运动感知匹配的向量级冗余消除。所有压缩步骤与架构协同设计，支持流式友好的片上执行，利用GEMM分块、卷积式布局和跨模态注意力最小化片外访问。

Result: 在脉动阵列加速器中作为模块单元实现，Focus实现了2.4倍加速和3.3倍能耗降低，在性能和能效方面显著优于最先进的加速器。

Conclusion: Focus通过渐进式细粒度冗余消除有效解决了VLM推理的计算和内存瓶颈，为实时部署提供了高效解决方案，已开源完整实现。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on tasks such as video captioning and visual question answering. However, their growing scale and video-level inputs lead to significant computational and memory overhead, posing challenges for real-time deployment on hardware accelerators. While prior work attempts to reduce redundancy via token pruning or merging, these methods typically operate at coarse granularity and incur high runtime overhead due to global token-level operations. In this study, we propose Focus, a Streaming Concentration Architecture that efficiently accelerates VLM inference through progressive, fine-grained redundancy elimination. Focus introduces a multilevel concentration paradigm that hierarchically compresses vision-language inputs at three levels: (1) semantic-guided token pruning based on textual prompts, (2) spatial-temporal block-level concentration using localized comparisons, and (3) vector-level redundancy removal via motion-aware matching. All concentration steps are tightly co-designed with the architecture to support streaming-friendly, on-chip execution. Focus leverages GEMM tiling, convolution-style layout, and cross-modal attention to minimize off-chip access while enabling high throughput. Implemented as a modular unit within a systolic-array accelerator, Focus achieves a 2.4x speedup and 3.3x reduction in energy, significantly outperforming state-of-the-art accelerators in both performance and energy efficiency. Full-stack implementation of Focus is open-sourced at https://github.com/dubcyfor3/Focus.

</details>
