<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity](https://arxiv.org/abs/2511.10760)
*Emad Haque,Pragnya Sudershan Nalla,Jeff Zhang,Sachin S. Sapatnekar,Chaitali Chakrabarti,Yu Cao*

Main category: cs.AR

TL;DR: 本文重新评估了2.5D/3D异构集成中芯片接口的可靠性要求，提出可以大幅简化ESD保护和芯片间信号传输，为更小尺寸的chiplet铺平道路。


<details>
  <summary>Details</summary>
Motivation: 传统I/O电路（包括ESD保护和信号传输）在先进封装技术中引入了显著的面积开销，这成为将chiplet尺寸减小到100mm2以下的主要制约因素。

Method: 通过寄生参数提取和SPICE仿真，从芯片接口设计的角度重新审视可靠性要求。

Result: 研究表明在未来的2.5D/3D封装技术中，ESD保护和芯片间信号传输可以大幅简化。

Conclusion: 这种简化为进一步chiplet小型化铺平了道路，并提高了微小chiplet的可组合性和可重用性。

Abstract: The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets.

</details>


### [2] [MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores](https://arxiv.org/abs/2511.10909)
*Peichen Xie,Yang Wang,Fan Yang,Mao Yang*

Main category: cs.AR

TL;DR: MMA-Sim是首个比特级精确的参考模型，揭示了10个GPU架构中矩阵乘法加速器的详细算术行为，通过目标测试和随机测试推导出9种算法来模拟浮点矩阵乘法。


<details>
  <summary>Details</summary>
Motivation: 由于不同GPU厂商的矩阵乘法加速器具有独特且未公开的算术规范，可能导致数值不精确和不一致，从而影响深度神经网络训练的稳定性和可重现性。

Method: 结合目标测试和随机测试，分析10个GPU架构的矩阵乘法加速器，推导出9种算术算法来模拟浮点矩阵乘法行为。

Result: 大规模验证确认MMA-Sim与真实硬件在比特级别上等价，并识别出可能引发显著错误的未记录行为。

Conclusion: MMA-Sim能够准确模拟矩阵乘法加速器的算术行为，有助于研究影响DNN训练稳定性的算术行为，并发现潜在的错误来源。

Abstract: The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.
  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors.

</details>


### [3] [T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup](https://arxiv.org/abs/2511.11248)
*Jianyu Wei,Qingtao Li,Shijie Cao,Lingxiao Ma,Zixu Hao,Yanyong Zhang,Xiaoyan Hu,Ting Cao*

Main category: cs.AR

TL;DR: 提出了一种基于表查找的方法来优化NPU上的LLM推理，通过融合两级表去量化和并发层次引导的平铺技术，实现了预填充和解码阶段的性能提升和能耗降低。


<details>
  <summary>Details</summary>
Motivation: 当前NPU在LLM推理中性能不如CPU，主要因为NPU在除GEMM外的计算（如去量化）上表现不佳。现有方法要么在NPU和CPU间拆分预填充和解码，要么在NPU上运行但损失精度。

Method: 使用表查找替代硬件不支持的操作，设计统一表布局和平铺：(1) 融合两级表去量化 (2) 并发层次引导平铺。预填充采用三阶段流水线，解码阶段将表查找映射到NPU向量单元。

Result: 相比基线NPU方法，预填充速度提升1.4倍，解码速度提升3.1倍，能耗节省84%。

Conclusion: 表查找方法能有效解决NPU在LLM推理中的性能瓶颈，实现显著的性能提升和能耗降低。

Abstract: Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man.

</details>
