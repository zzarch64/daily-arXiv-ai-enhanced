<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Accelerating Detailed Routing Convergence through Offline Reinforcement Learning](https://arxiv.org/abs/2512.03594)
*Afsara Khan,Austin Rovinski*

Main category: cs.AR

TL;DR: 使用强化学习动态调整布线成本权重，加速详细布线收敛，相比基线路由器平均提速1.56倍，最多提速3.01倍，同时保持或减少设计规则违规


<details>
  <summary>Details</summary>
Motivation: 传统详细布线器使用静态成本权重，无法根据设计或技术变化动态调整，导致收敛到零设计规则违规的运行时成本过高

Method: 采用保守Q学习（CQL）模型，学习动态选择布线成本权重以最小化算法迭代次数，从先前设计中学习

Result: 在ISPD19基准测试中，平均运行时比基线路由器快1.56倍，最多快3.01倍，所有情况下设计规则违规数量保持或减少，学习效果在不同技术间具有泛化能力

Conclusion: 强化学习能够有效加速详细布线收敛，通过动态调整成本权重显著减少运行时，同时保持布线质量，且学习成果可在不同技术间迁移

Abstract: Detailed routing remains one of the most complex and time-consuming steps in modern physical design due to the challenges posed by shrinking feature sizes and stricter design rules. Prior detailed routers achieve state-of-the-art results by leveraging iterative pathfinding algorithms to route each net. However, runtimes are a major issue in detailed routers, as converging to a solution with zero design rule violations (DRVs) can be prohibitively expensive.
  In this paper, we propose leveraging reinforcement learning (RL) to enable rapid convergence in detailed routing by learning from previous designs. We make the key observation that prior detailed routers statically schedule the cost weights used in their routing algorithms, meaning they do not change in response to the design or technology. By training a conservative Q-learning (CQL) model to dynamically select the routing cost weights which minimize the number of algorithm iterations, we find that our work completes the ISPD19 benchmarks with 1.56x average and up to 3.01x faster runtime than the baseline router while maintaining or improving the DRV count in all cases. We also find that this learning shows signs of generalization across technologies, meaning that learning designs in one technology can translate to improved outcomes in other technologies.

</details>


### [2] [KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing](https://arxiv.org/abs/2512.03608)
*Lishuo Deng,Shaojie Xu,Jinwu Chen,Changwei Yan,Jiajie Wang,Zhe Jiang,Weiwei Shan*

Main category: cs.AR

TL;DR: KVNAND：首个基于闪存内计算的DRAM-free架构，将模型权重和KV缓存完全存储在3D NAND闪存中，解决长上下文推理中的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大语言模型面临内存瓶颈：传统方法中KV缓存随上下文长度增长会超过模型权重大小，导致DRAM成本过高且容量不足，而将KV缓存卸载到闪存会带来严重的性能惩罚。

Method: 提出KVNAND架构：1）利用闪存内计算处理所有内存受限操作以减少数据传输开销；2）引入头组并行性提升吞吐量；3）采用页面级KV缓存映射使令牌访问模式与闪存组织对齐；4）设计空间探索框架评估不同变体以平衡权重和KV缓存放置。

Result: 在MHA 7B和GQA 70B LLM上的评估显示，KVNAND在128/1K/10K令牌上下文长度下相比DRAM-equipped IFC设计分别实现1.98×/1.94×/2.05×几何平均加速，并在100K上下文长度下解决内存不足问题。

Conclusion: KVNAND通过将闪存内计算扩展到KV缓存管理，解决了长上下文LLM推理中的内存瓶颈，使闪存成为KV缓存存储的实用介质，为边缘设备上的隐私保护、低成本个性化AI代理提供了可行方案。

Abstract: Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.
  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\(\times\)/1.94\(\times\)/2.05\(\times\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.

</details>


### [3] [Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State](https://arxiv.org/abs/2512.03616)
*Christian Ewert,Amrit Sharma Poudel,Mouadh Ayache,Andrija Neskovic,Rainer Buchty,Mladen Berekovic,Sebastian Berndt,Saleh Mulhem*

Main category: cs.AR

TL;DR: 提出统一哈希引擎支持Sha-3和Shake，采用字节级原位分区机制，并通过二维奇偶校验实现故障检测，在面积开销上优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 哈希函数已成为后量子密码标准方案的关键部分，需要轻量级实现。故障弹性设计对于确保整个PQC系统可靠性至关重要。

Method: 1) 提出统一哈希引擎，支持Sha-3和Shake，采用Keccak状态的字节级原位分区机制；2) 基于Keccak立方体结构部署二维奇偶校验进行故障检测。

Result: 在面积需求上优于现有方案，实现100%的三故障检测和接近100%的更高数量故障检测。多维交叉奇偶校验机制实现3.7倍面积开销改进，整体故障弹性引擎设计小4.5倍。集成到RISC-V环境中仅增加不到8%的面积开销。

Conclusion: 该方法为资源受限的PQC应用中部署的哈希函数提供了鲁棒且轻量级的故障检测解决方案。

Abstract: Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications.

</details>


### [4] [The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates](https://arxiv.org/abs/2512.03781)
*Joscha Ilmberger,Johannes Schemmel*

Main category: cs.AR

TL;DR: BrainScaleS-2系统通过FPGA互连扩展计算规模，实现芯片间低延迟通信


<details>
  <summary>Details</summary>
Motivation: 扩展BrainScaleS-2系统的计算规模，通过FPGA互连实现多芯片协同工作，构建更大规模的神经形态计算系统

Method: 使用基于FPGA的聚合器单元进行互连，每个聚合器提供12个收发器连接到节点FPGA背板，以及4个收发器用于进一步扩展。将两个互连背板集成到标准19英寸4U机架中

Result: 在所有脉冲速率下，每个背板内的芯片间延迟（经过三个FPGA的四跳）低于1.3微秒

Conclusion: FPGA互连方案成功扩展了BrainScaleS-2系统的计算规模，实现了低延迟的芯片间通信，为构建大规模神经形态计算系统提供了可行方案

Abstract: The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$μ$s are achieved within each backplane.

</details>
