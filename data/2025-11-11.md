<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS](https://arxiv.org/abs/2511.05502)
*Varun Rajesh,Om Jodhpurkar,Pooja Anbuselvan,Mantinder Singh,Ashok Jallepali,Shantanu Godbole,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AR

TL;DR: 对苹果芯片上5个本地LLM运行时的系统评估：MLX、MLC-LLM、llama.cpp、Ollama和PyTorch MPS。MLX获得最高持续生成吞吐量，MLC-LLM在中等提示大小下TTFT最低，llama.cpp轻量级单流效率高，Ollama开发者体验好但性能较差，PyTorch MPS受内存限制。


<details>
  <summary>Details</summary>
Motivation: 系统评估苹果芯片上本地LLM运行时的性能，为私有、设备端LLM推理提供基于证据的推荐，明确苹果中心化LLM部署的设计权衡。

Method: 在配备M2 Ultra处理器和192GB统一内存的Mac Studio上，使用Qwen-2.5模型家族，测试从几百到10万token的提示，测量TTFT、吞吐量、延迟百分位数、长上下文行为、量化支持、流性能、批处理和并发行为以及部署复杂性。

Result: MLX实现最高持续生成吞吐量；MLC-LLM在中等提示大小下提供一致更低的TTFT和更强的开箱即用推理功能；llama.cpp轻量级单流使用高效；Ollama开发者体验好但吞吐量和TTFT落后；PyTorch MPS在大模型和长上下文上受内存限制。

Conclusion: 苹果芯片推理框架虽然绝对性能仍落后于NVIDIA GPU系统，但正快速成熟为私有、设备端LLM推理的可行生产级解决方案。所有框架完全在设备上执行，无遥测，确保强隐私保证。

Abstract: We present a systematic, empirical evaluation of five local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp, Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with an M2 Ultra processor and 192 GB of unified memory. Using the Qwen-2.5 model family across prompts ranging from a few hundred to 100,000 tokens, we measure time-to-first-token (TTFT), steady-state throughput, latency percentiles, long-context behavior (key-value and prompt caching), quantization support, streaming performance, batching and concurrency behavior, and deployment complexity.
  Under our settings, MLX achieves the highest sustained generation throughput, while MLC-LLM delivers consistently lower TTFT for moderate prompt sizes and offers stronger out-of-the-box inference features. llama.cpp is highly efficient for lightweight single-stream use, Ollama emphasizes developer ergonomics but lags in throughput and TTFT, and PyTorch MPS remains limited by memory constraints on large models and long contexts.
  All frameworks execute fully on-device with no telemetry, ensuring strong privacy guarantees. We release scripts, logs, and plots to reproduce all results. Our analysis clarifies the design trade-offs in Apple-centric LLM deployments and provides evidence-based recommendations for interactive and long-context processing. Although Apple Silicon inference frameworks still trail NVIDIA GPU-based systems such as vLLM in absolute performance, they are rapidly maturing into viable, production-grade solutions for private, on-device LLM inference.

</details>


### [2] [Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration](https://arxiv.org/abs/2511.06313)
*Stef Cuyckens,Xiaoling Yi,Robin Geens,Joren Dumoulin,Martin Wiesner,Chao Fang,Marian Verhelst*

Main category: cs.AR

TL;DR: 提出了一种混合精度可扩展的归约树设计，用于MX MAC单元，解决了现有MX MAC设计中整数累加和浮点累加之间的权衡问题，并在SNAX NPU平台上实现了高能效的混合精度计算。


<details>
  <summary>Details</summary>
Motivation: 新兴的持续学习应用需要NPU平台同时支持训练和推理操作，而现有的MX MAC设计面临关键权衡：整数累加需要昂贵的窄浮点乘积转换，而FP32累加则存在量化损失和昂贵的归一化问题。

Method: 提出混合精度可扩展归约树用于MX MAC，结合了整数和浮点累加的优势；将8x8 MAC阵列集成到SNAX NPU平台，为优化的精度可扩展MX数据路径提供高效控制和数据传输。

Result: 在MAC和系统级别评估设计，与SotA相比，集成系统在MXINT8、MXFP8/6和MXFP4下分别实现了657、1438-1675和4065 GOPS/W的能效，吞吐量分别为64、256和512 GOPS。

Conclusion: 该混合精度可扩展归约树设计成功解决了MX MAC中的精度-效率权衡问题，在NPU平台上实现了高能效的混合精度计算，为持续学习应用提供了有效的硬件支持。

Abstract: Emerging continual learning applications necessitate next-generation neural processing unit (NPU) platforms to support both training and inference operations. The promising Microscaling (MX) standard enables narrow bit-widths for inference and large dynamic ranges for training. However, existing MX multiply-accumulate (MAC) designs face a critical trade-off: integer accumulation requires expensive conversions from narrow floating-point products, while FP32 accumulation suffers from quantization losses and costly normalization. To address these limitations, we propose a hybrid precision-scalable reduction tree for MX MACs that combines the benefits of both approaches, enabling efficient mixed-precision accumulation with controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to provide efficient control and data transfer to our optimized precision-scalable MX datapath. We evaluate our design both on MAC and system level and compare it to the SotA. Our integrated system achieves an energy efficiency of 657, 1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with a throughput of 64, 256, and 512 GOPS.

</details>


### [3] [EONSim: An NPU Simulator for On-Chip Memory and Embedding Vector Operations](https://arxiv.org/abs/2511.06679)
*Sangun Choi,Yunho Oh*

Main category: cs.AR

TL;DR: EONSim是一个NPU模拟器，能够整体建模矩阵和嵌入向量操作，支持灵活的片上内存架构探索，验证显示其推理时间误差为1.4%，内存访问计数误差为2.2%。


<details>
  <summary>Details</summary>
Motivation: 现有NPU模拟器主要关注具有确定性访问模式的矩阵计算，缺乏对嵌入向量操作中数据依赖性和非确定性内存访问的建模能力，而下一代NPU需要支持嵌入工作负载的灵活片上内存架构。

Method: 开发EONSim模拟器，集成经过验证的矩阵计算性能模型和详细的嵌入访问内存模拟，支持多种片上内存管理策略。

Result: 与TPUv6e验证对比，EONSim实现了平均推理时间误差1.4%和平均片上内存访问计数误差2.2%。

Conclusion: EONSim为新兴NPU架构的灵活探索和设计提供了有效的模拟平台，能够准确建模嵌入向量操作的内存访问行为。

Abstract: Embedding vector operations are a key component of modern deep neural network workloads. Unlike matrix operations with deterministic access patterns, embedding vector operations exhibit input data-dependent and non-deterministic memory accesses. Existing neural processing unit (NPU) simulators focus on matrix computations with simple double-buffered on-chip memory systems, lacking the modeling capability for realistic embedding behavior. Next-generation NPUs, however, call for more flexible on-chip memory architectures that can support diverse access and management schemes required by embedding workloads. To enable flexible exploration and design of emerging NPU architectures, we present EONSim, an NPU simulator that holistically models both matrix and embedding vector operations. EONSim integrates a validated performance model for matrix computations with detailed memory simulation for embedding accesses, supporting various on-chip memory management policies. Validated against TPUv6e, EONSim achieves an average inference time error of 1.4\% and an average on-chip memory access count error of 2.2\%.

</details>
