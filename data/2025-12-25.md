<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [NotSoTiny: A Large, Living Benchmark for RTL Code Generation](https://arxiv.org/abs/2512.20823)
*Razine Moundir Ghorab,Emanuele Parisi,Cristian Gutierrez,Miquel Alberti-Binimelis,Miquel Moreto,Dario Garcia-Gasulla,Gokcen Kestor*

Main category: cs.AR

TL;DR: NotSoTiny是一个评估LLM生成RTL代码能力的基准测试，基于Tiny Tapeout社区的真实硬件设计，解决了现有基准规模小、设计简单、验证不严谨和数据污染等问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在RTL代码生成方面的评估存在挑战：现有基准测试规模有限、偏向简单设计、验证不严谨且容易受到数据污染。需要更真实、更具挑战性的评估方法来推动该领域发展。

Method: 从Tiny Tapeout社区的数百个真实硬件设计中构建自动化流水线，包括去重、验证正确性，并定期纳入新设计以匹配Tiny Tapeout发布计划，从而缓解数据污染问题。

Result: 评估结果显示NotSoTiny任务比现有基准更具挑战性，证明了其在克服当前LLM应用于硬件设计局限性方面的有效性，并能指导该有前景技术的改进。

Conclusion: NotSoTiny基准测试通过基于真实硬件设计的结构化丰富、上下文感知的RTL生成评估，为LLM在硬件设计领域的应用提供了更严谨、更实用的评估框架。

Abstract: LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.

</details>


### [2] [ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update](https://arxiv.org/abs/2512.21153)
*Zhe Su,Giacomo Indiveri*

Main category: cs.AR

TL;DR: ElfCore是一款28nm数字脉冲神经网络处理器，专为事件驱动传感信号处理设计，集成了自监督学习、稀疏训练和动态权重更新机制，在功耗、内存和网络容量效率方面显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有脉冲神经网络处理器在处理事件驱动传感信号时面临功耗高、内存需求大、网络容量有限等挑战，需要更高效的硬件架构来支持复杂的在线学习和稀疏处理。

Method: ElfCore集成了三个关键技术：1) 本地在线自监督学习引擎，支持多层时序学习无需标注输入；2) 动态结构化稀疏训练引擎，支持高精度稀疏到稀疏学习；3) 基于活动的稀疏权重更新机制，仅根据输入活动和网络动态选择性更新权重。

Result: 在姿态识别、语音和生物医学信号处理等任务上，ElfCore相比现有最佳方案：功耗降低16倍，片上内存需求减少3.8倍，网络容量效率提升5.9倍。

Conclusion: ElfCore通过创新的硬件架构设计，实现了高效的事件驱动传感信号处理，在功耗、内存和网络容量方面取得了显著优势，为边缘智能设备提供了有前景的解决方案。

Abstract: In this paper, we present ElfCore, a 28nm digital spiking neural network processor tailored for event-driven sensory signal processing. ElfCore is the first to efficiently integrate: (1) a local online self-supervised learning engine that enables multi-layer temporal learning without labeled inputs; (2) a dynamic structured sparse training engine that supports high-accuracy sparse-to-sparse learning; and (3) an activity-dependent sparse weight update mechanism that selectively updates weights based solely on input activity and network dynamics. Demonstrated on tasks including gesture recognition, speech, and biomedical signal processing, ElfCore outperforms state-of-the-art solutions with up to 16X lower power consumption, 3.8X reduced on-chip memory requirements, and 5.9X greater network capacity efficiency.

</details>
