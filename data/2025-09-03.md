<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 11]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection](https://arxiv.org/abs/2509.00433)
*Houshu He,Naifeng Jing,Li Jiang,Xiaoyao Liang,Zhuoran Song*

Main category: cs.AR

TL;DR: AGS是一个算法-硬件协同设计框架，通过利用SLAM系统中相邻帧的高相似性来加速3D高斯溅射SLAM系统，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS-SLAM系统由于每帧需要多次训练迭代和海量高斯计算，导致吞吐量不足，无法满足实时应用需求。

Method: 软件层面：1）提出粗粒度到细粒度的位姿跟踪方法；2）通过跨帧共享高斯贡献信息避免冗余计算。硬件层面：设计帧共视性检测引擎、位姿跟踪引擎和建图引擎，并配备工作负载调度器。

Result: AGS相比移动和高性能GPU以及最先进的3DGS加速器GSCore，分别实现了17.12倍、6.71倍和5.41倍的加速比。

Conclusion: AGS框架通过算法-硬件协同设计，有效利用了SLAM系统中帧间相似性，显著提升了3D高斯溅射SLAM系统的效率，为实时应用提供了可行解决方案。

Abstract: Simultaneous Localization and Mapping (SLAM) is a critical task that enables
autonomous vehicles to construct maps and localize themselves in unknown
environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting
(3DGS) to achieve exceptional reconstruction fidelity. However, existing
3DGS-SLAM systems provide insufficient throughput due to the need for multiple
training iterations per frame and the vast number of Gaussians.
  In this paper, we propose AGS, an algorithm-hardware co-design framework to
boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems
process frames in a streaming manner, where adjacent frames exhibit high
similarity that can be utilized for acceleration. On the software level: 1) We
propose a coarse-then-fine-grained pose tracking method with respect to the
robot's movement. 2) We avoid redundant computations of Gaussians by sharing
their contribution information across frames. On the hardware level, we propose
a frame covisibility detection engine to extract intermediate data from the
video CODEC. We also implement a pose tracking engine and a mapping engine with
workload schedulers to efficiently deploy the AGS algorithm. Our evaluation
shows that AGS achieves up to $17.12\times$, $6.71\times$, and $5.41\times$
speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS
accelerator, GSCore.

</details>


### [2] [Bit Transition Reduction by Data Transmission Ordering in NoC-based DNN Accelerator](https://arxiv.org/abs/2509.00500)
*Yizhi Chen,Jingwei Li,Wenyao Zhu,Zhonghai Lu*

Main category: cs.AR

TL;DR: 提出基于'1'比特计数的排序方法来减少神经网络加速器中NoC链路的比特翻转，降低功耗


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络加速器采用NoC架构，减少链路比特翻转以节省功耗成为重要研究方向

Method: 提出两种数据排序方法（关联排序和分离排序）来处理权重和输入数据，通过数学证明排序方法的有效性

Result: 无NoC时浮点32数据减少20.38%比特翻转，定点8数据减少55.71%；在NoC中浮点32数据减少32.01%，定点8数据减少40.85%

Conclusion: 该方法在不同DNN模型、NoC配置和数据精度下都能有效降低链路功耗

Abstract: As Deep Neural Networks (DNN) are becoming essential, Network-on-Chip
(NoC)-based DNN accelerators gained increasing popularity. To save link power
in NoC, many researchers focus on reducing the Bit Transition (BT). We propose
'1'-bit count-based ordering method to reduce BT for DNN workloads. We provide
a mathematical proof of the efficacy of proposed ordering. We evaluate our
method through experiments without NoC and with NoC. Without NoC, our proposed
ordering method achieves up to 20.38% BT reduction for floating-point-32 data
and 55.71% for fixed-point-8 data, respectively. We propose two data ordering
methods, affiliated-ordering and separated-ordering to process weight and input
jointly or individually and apply them to run full DNNs in NoC-based DNN
accelerator. We evaluate our approaches under various configurations, including
different DNN models such as LeNet and DarkNet, various NoC sizes with
different numbers of memory controllers, random weights and trained weights,
and different data precision. Our approach efficiently reduces the link power
by achieving up to 32.01% BT reduction for floating-point-32 data and 40.85% BT
reduction for fixed-point-8 data.

</details>


### [3] [Real-Time Piano Note Frequency Detection Using FPGA and FFT Core](https://arxiv.org/abs/2509.00589)
*Shafayet M. Anik,D. G. Perera*

Main category: cs.AR

TL;DR: 使用FPGA实现钢琴音频信号的实时FFT频率分析，相比传统软件DSP方法具有更低延迟和更高计算效率


<details>
  <summary>Details</summary>
Motivation: 传统软件DSP方法在实时音乐频率分析中存在延迟和计算资源消耗大的问题，而FPGA的并行处理能力可以提供更快、更确定性的分析性能

Method: 采用基于FPGA的实时快速傅里叶变换(FFT)系统来分析数字钢琴的模拟音频信号

Result: FPGA平台能够实现高速、低延迟的音乐频率分析，适用于电子调音器、音乐可视化器和现场声音监控等应用

Conclusion: FPGA硬件平台在实时音频频率分析方面相比软件DSP方法具有显著优势，特别适合需要快速响应和确定性性能的音乐应用场景

Abstract: Real-time frequency analysis of musical instruments, such as the piano, is an
essential feature in areas like electronic tuners, music visualizers, and live
sound monitoring. Traditional methods often rely on software-based digital
signal processing (DSP), which may introduce latency and require significant
computational power. In contrast, hardware platforms such as FPGAs (Field
Programmable Gate Arrays) offer the ability to perform such analyses with
greater speed and determinism due to their parallel processing capabilities.
The primary objective of this project was to analyze analog audio signals from
a digital piano using an FPGA-based real-time Fast Fourier Transform (FFT)
system.

</details>


### [4] [COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives](https://arxiv.org/abs/2509.00599)
*Shubham Negi,Manik Singhal,Aayush Ankit,Sudeep Bhoja,Kaushik Roy*

Main category: cs.AR

TL;DR: COMET是一个针对机器学习加速器中复合操作的数据流建模和优化框架，通过显式建模跨空间集群的集体通信，实现了比未融合基线最高3.46倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习加速器主要优化单一操作，但新兴DNN模型（如大语言模型、状态空间模型）越来越多地依赖复合操作，现有框架缺乏对集体通信成本的显式建模，无法有效处理现代工作负载。

Method: 提出COMET框架，引入新颖的表示方法显式建模跨空间集群的集体通信，建立包含GEMM和非GEMM操作级依赖关系的延迟和能耗成本模型，支持复合操作的数据流分析和优化。

Result: 优化的数据流在GEMM-Softmax上实现1.42倍加速，GEMM-LayerNorm实现3.46倍加速，自注意力机制实现1.82倍加速，相比未融合基线显著提升性能和能效。

Conclusion: COMET框架通过集体通信感知建模扩展了映射空间探索能力，为现代复合操作提供了有效的性能优化方案，在边缘和云端加速器配置上都表现出色。

Abstract: Modern machine learning accelerators are designed to efficiently execute deep
neural networks (DNNs) by optimizing data movement, memory hierarchy, and
compute throughput. However, emerging DNN models such as large language models,
state space models increasingly rely on compound operations-structured
compositions of multiple basic operations-which introduce new challenges for
dataflow optimization and minimizing off-chip memory traffic. Moreover, as
model size continues to grow, deployment across spatially distributed compute
clusters becomes essential, requiring frequent and complex collective
communication. Existing dataflow optimization frameworks and performance models
either focus on single operations or lack explicit modeling of collective
communication cost, limiting their applicability to modern workloads.
  To address these limitations, we propose, a framework for modeling and
optimizing dataflow for compound operations on machine learning accelerators.
COMET introduces a novel representation that explicitly models collective
communication across spatial clusters, along with latency and energy cost
models that account for both GEMM and non-GEMM operation level dependencies
within compound operations. We demonstrate COMET's capabilities to analyze and
optimize dataflows for compound operations such as GEMM--Softmax,
GEMM--LayerNorm, and self-attention, across both edge and cloud accelerator
configurations. Our collective-aware modeling enables exploration of a broader
mapping space, leading to improved performance and energy efficiency.
Specifically, our optimized dataflows achieve up to 1.42$\times$ speedup for
GEMM-Softmax, 3.46$\times$ for GEMM-LayerNorm and 1.82$\times$ for
self-attention compared to unfused baselines.

</details>


### [5] [On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures](https://arxiv.org/abs/2509.00633)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed A. Badawy,Ahmad Patooghy*

Main category: cs.AR

TL;DR: 基于3D堆叠高带宽内存(HBM)架构的热攻击，利用垂直和水平邻近性向目标内存波注入短暴热波，引起性能劣化但能穿越安全检测


<details>
  <summary>Details</summary>
Motivation: HBM架构虽提供高性能内存访问，但垂直邻近性使其容易受到热攻击，攻击者可利用这一弱点进行隐藏性强的性能劣化攻击

Method: 通过垂直和/或水平邻近的内存款向目标内存波注入短暴热波，产生聚焦热波最大化影响效果，延迟目标应用访问数据/指令

Result: 攻击能够穿越设计时安全测试和操作系统内存管理策略，因为攻击模仿合法工作负荷而难以检测

Conclusion: HBM架构存在新型热安全漏洞，需要发展新的热安全防护机制来应对这种隐藏在合法工作负荷中的性能劣化攻击

Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance
memory interactions to address the well-known performance challenge, namely the
memory wall. However, these architectures are susceptible to thermal
vulnerabilities due to the inherent vertical adjacency that occurs during the
manufacturing process of HBM architectures. We anticipate that adversaries may
exploit the intense vertical and lateral adjacency to design and develop
thermal performance degradation attacks on the memory banks that host
data/instructions from victim applications. In such attacks, the adversary
manages to inject short and intense heat pulses from vertically and/or
laterally adjacent memory banks, creating a convergent thermal wave that
maximizes impact and delays the victim application from accessing its
data/instructions. As the attacking application does not access any
out-of-range memory locations, it can bypass both design-time security tests
and the operating system's memory management policies. In other words, since
the attack mimics legitimate workloads, it will be challenging to detect.

</details>


### [6] [Low Power Approximate Multiplier Architecture for Deep Neural Networks](https://arxiv.org/abs/2509.00764)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 提出一种用于深度神经网络应用的低功耗近似乘法器架构，通过设计仅产生单一组合误差的4:2压缩器，在保持低错误率的同时显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络应用对计算能效要求高，传统精确乘法器功耗大，需要开发在保持计算精度的同时显著降低能耗的近似乘法器架构。

Method: 设计仅产生单一组合误差的4:2压缩器，并将其集成到8x8无符号乘法器中，减少精确压缩器的使用，构建自定义卷积层并在神经网络任务中进行评估。

Result: 硬件评估显示能耗节省达30.24%，图像去噪任务中PSNR和SSIM指标优于其他近似设计，手写数字识别保持高分类准确率。

Conclusion: 该架构在能效和计算精度之间实现了良好平衡，适合低功耗AI硬件实现。

Abstract: This paper proposes an low power approximate multiplier architecture for deep
neural network (DNN) applications. A 4:2 compressor, introducing only a single
combination error, is designed and integrated into an 8x8 unsigned multiplier.
This integration significantly reduces the usage of exact compressors while
preserving low error rates. The proposed multiplier is employed within a custom
convolution layer and evaluated on neural network tasks, including image
recognition and denoising. Hardware evaluation demonstrates that the proposed
design achieves up to 30.24% energy savings compared to the best among existing
multipliers. In image denoising, the custom approximate convolution layer
achieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity
Index Measure (SSIM) compared to other approximate designs. Additionally, when
applied to handwritten digit recognition, the model maintains high
classification accuracy. These results demonstrate that the proposed
architecture offers a favorable balance between energy efficiency and
computational precision, making it suitable for low-power AI hardware
implementations.

</details>


### [7] [Energy Efficient Exact and Approximate Systolic Array Architecture for Matrix Multiplication](https://arxiv.org/abs/2509.00778)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 本文提出了一种包含新型精确和近似处理单元的脉动阵列架构，用于深度神经网络的高效能矩阵乘法计算，相比现有设计分别实现22%和32%的能耗节省，同时在图像处理应用中保持良好输出质量。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络需要高效的矩阵乘法引擎来处理复杂计算，传统设计在能耗方面存在优化空间，需要开发既能节省能量又能保持计算精度的新型处理架构。

Method: 采用能量高效的正部分积单元(PPC)和负部分积单元(NPPC)设计8位精确和近似处理单元(PE)，并将其集成到8x8脉动阵列中，应用于离散余弦变换(DCT)计算和边缘检测卷积应用。

Result: 精确PE实现22%能耗节省，近似PE实现32%能耗节省；DCT计算获得38.21dB PSNR高质量输出；边缘检测应用达到30.45dB PSNR。

Conclusion: 所提出的设计能够在保持竞争力的输出质量的同时提供显著的能效优势，特别适用于容错图像和视觉处理应用。

Abstract: Deep Neural Networks (DNNs) require highly efficient matrix multiplication
engines for complex computations. This paper presents a systolic array
architecture incorporating novel exact and approximate processing elements
(PEs), designed using energy-efficient positive partial product and negative
partial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit
exact and approximate PE designs are employed in a 8x8 systolic array, which
achieves a energy savings of 22% and 32%, respectively, compared to the
existing design. To demonstrate their effectiveness, the proposed PEs are
integrated into a systolic array (SA) for Discrete Cosine Transform (DCT)
computation, achieving high output quality with a PSNR of 38.21,dB.
Furthermore, in an edge detection application using convolution, the
approximate PE achieves a PSNR of 30.45,dB. These results highlight the
potential of the proposed design to deliver significant energy efficiency while
maintaining competitive output quality, making it well-suited for
error-resilient image and vision processing applications.

</details>


### [8] [GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency](https://arxiv.org/abs/2509.00911)
*Joongho Jo,Jongsun Park*

Main category: cs.AR

TL;DR: GS-TG是一种基于瓦片分组的3D高斯泼溅加速器，通过减少冗余排序操作并保持光栅化效率，实现了1.54倍的平均加速效果。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然比NeRF速度快，但仍无法满足实时应用的帧率需求，存在瓦片大小选择上的权衡问题：大瓦片减少排序冗余但增加光栅化计算，小瓦片则相反。

Method: 提出瓦片分组方法：在排序阶段将小瓦片分组形成大瓦片来共享排序操作；在光栅化阶段使用位掩码标识相关小瓦片，保持小瓦片的光栅化效率。这是一种无需重新训练的无损方法。

Result: 实验结果显示GS-TG相比最先进的3D-GS加速器实现了平均1.54倍的加速效果。

Conclusion: GS-TG成功解决了3D-GS渲染中的关键权衡问题，通过创新的瓦片分组和位掩码技术，在保持图像质量的同时显著提升了渲染速度，且能与现有优化技术无缝集成。

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to
neural radiance fields (NeRF) as it offers high speed as well as high image
quality in novel view synthesis. Despite these advancements, 3D-GS still
struggles to meet the frames per second (FPS) demands of real-time
applications. In this paper, we introduce GS-TG, a tile-grouping-based
accelerator that enhances 3D-GS rendering speed by reducing redundant sorting
operations and preserving rasterization efficiency. GS-TG addresses a critical
trade-off issue in 3D-GS rendering: increasing the tile size effectively
reduces redundant sorting operations, but it concurrently increases unnecessary
rasterization computations. So, during sorting of the proposed approach, GS-TG
groups small tiles (for making large tiles) to share sorting operations across
tiles within each group, significantly reducing redundant computations. During
rasterization, a bitmask assigned to each Gaussian identifies relevant small
tiles, to enable efficient sharing of sorting results. Consequently, GS-TG
enables sorting to be performed as if a large tile size is used by grouping
tiles during the sorting stage, while allowing rasterization to proceed with
the original small tiles by using bitmasks in the rasterization stage. GS-TG is
a lossless method requiring no retraining or fine-tuning and it can be
seamlessly integrated with previous 3D-GS optimization techniques. Experimental
results show that GS-TG achieves an average speed-up of 1.54 times over
state-of-the-art 3D-GS accelerators.

</details>


### [9] [GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs](https://arxiv.org/abs/2509.01020)
*Elena Espinosa,Rubén Rodríguez Álvarez,José Miranda,Rafael Larrosa,Miguel Peón-Quirós,Oscar Plata,David Atienza*

Main category: cs.AR

TL;DR: 本文提出了GeneTEK，一种基于FPGA的可扩展加速器模板，用于基因组序列比对，在速度和能耗效率方面均优于现有CPU和GPU解决方案。


<details>
  <summary>Details</summary>
Motivation: 下一代测序技术产生了海量基因组数据，序列比对作为生物信息学流程中的关键步骤，计算耗时且能耗高。现有FPGA解决方案在比对长序列时存在可扩展性限制。

Method: 采用高级综合技术和基于工作者的架构，实现了Myers算法的FPGA加速器模板GeneTEK，部署在Xilinx Zynq UltraScale+ FPGA上。

Result: GeneTEK相比领先的CPU和GPU解决方案，执行速度至少提升19.4%，能耗降低高达62倍，同时支持比先前FPGA解决方案大72%的比对矩阵。

Conclusion: FPGA作为可扩展基因组工作负载的能效平台具有巨大潜力，GeneTEK成功克服了当前FPGA方法的可扩展性限制。

Abstract: The advent of next-generation sequencing (NGS) has revolutionized genomic
research by enabling high-throughput data generation through parallel
sequencing of a diverse range of organisms at significantly reduced costs. This
breakthrough has unleashed a "Cambrian explosion" in genomic data volume and
diversity. This volume of workloads places genomics among the top four big data
challenges anticipated for this decade. In this context, pairwise sequence
alignment represents a very time- and energy-consuming step in common
bioinformatics pipelines. Speeding up this step requires the implementation of
heuristic approaches, optimized algorithms, and/or hardware acceleration.
  Whereas state-of-the-art CPU and GPU implementations have demonstrated
significant performance gains, recent field programmable gate array (FPGA)
implementations have shown improved energy efficiency. However, the latter
often suffer from limited scalability due to constraints on hardware resources
when aligning longer sequences. In this work, we present a scalable and
flexible FPGA-based accelerator template that implements Myers's algorithm
using high-level synthesis and a worker-based architecture. GeneTEK, an
instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA,
outperforms state-of-the-art CPU and GPU implementations in both speed and
energy efficiency, while overcoming scalability limitations of current FPGA
approaches. Specifically, GeneTEK achieves at least a 19.4% increase in
execution speed and up to 62x reduction in energy consumption compared to
leading CPU and GPU solutions, while fitting comparison matrices up to 72%
larger compared to previous FPGA solutions. These results reaffirm the
potential of FPGAs as an energy-efficient platform for scalable genomic
workloads.

</details>


### [10] [LinkBo: An Adaptive Single-Wire, Low-Latency, and Fault-Tolerant Communications Interface for Variable-Distance Chip-to-Chip Systems](https://arxiv.org/abs/2509.01339)
*Bochen Ye,Gustavo Naspolini,Kimmo Salo,Manil Dev Gomony*

Main category: cs.AR

TL;DR: LinkBo是一种创新的单线通信协议，相比现有的1-wire和UNI/O协议，延迟降低20倍以上，吞吐量提升显著，支持最长15米线缆距离和最高7.5 Mbps数据速率


<details>
  <summary>Details</summary>
Motivation: 现有单线通信协议存在延迟高、吞吐量受限和鲁棒性不足的问题，无法满足嵌入式系统对高效可靠通信的需求

Method: 提出LinkBo协议，采用硬件中断机制实现可变距离芯片间通信，提供错误检测功能和高优先级消息传输保证，并在FPGA硬件平台上实现和评估

Result: 高优先级消息传输延迟仅50.4μs，比1-wire和UNI/O分别快20倍和6.3倍；支持300 kbps数据速率下15米线缆长度，11厘米线缆下可达7.5 Mbps

Conclusion: LinkBo协议在延迟、吞吐量和鲁棒性方面显著优于现有商用单线协议，为嵌入式系统提供了高性能的芯片间通信解决方案

Abstract: Cost-effective embedded systems necessitate utilizing the single-wire
communication protocol for inter-chip communication, thanks to its reduced pin
count in comparison to the multi-wire I2C or SPI protocols. However, current
single-wire protocols suffer from increased latency, restricted throughput, and
lack of robustness. This paper presents LinkBo, an innovative single-wire
protocol that offers reduced latency, enhanced throughput, and greater
robustness with hardware-interrupt for variable-distance inter-chip
communication. The LinkBo protocol-level guarantees that high-priority messages
are delivered with an error detection feature in just 50.4 $\mu$s, surpassing
current commercial options, 1-wire and UNI/O by at least 20X and 6.3X,
respectively. In addition, we present the hardware architecture for this new
protocol and its performance evaluation on a hardware platform consisting of
two FPGAs. Our findings demonstrate that the protocol reliably supports wire
lengths up to 15 meters with a data rate of 300 kbps, while reaching a maximum
data rate of 7.5 Mbps over an 11 cm wire, providing reliable performance for
varying inter-chip communication distances.

</details>


### [11] [Guidance and Control Neural Network Acceleration using Memristors](https://arxiv.org/abs/2509.02369)
*Zacharia A. Rudge,Dario Izzo,Moritz Fieback,Anteneh Gebregiorgis,Said Hamdioui,Dominik Dold*

Main category: cs.AR

TL;DR: 这篇论文探索了使用相变内存(PCM)和阻变存储器(RRAM)在太空应用中实现内存计算AI加速的可行性，通过模拟指导和控制神经网络来评估性能和面对设备非理想性的强锐性。


<details>
  <summary>Details</summary>
Motivation: 太空应用中AI技术发展受限于小占占和立方卡占的有限能源预算以及现代芯片的放射问题，需要研究能满足这些要求的神经网络加速器。

Method: 通过模拟使用PCM和RRAM阻变存储器加速的指导控制神经网络(G&CNET)，考虑器件的非理想性如噪声和导电率飘移，在多种场景下进行性能评估。

Result: 阻变存储器加速器能够学习专家行为，但噪声对准确性的影响仍是挑战；在性能退化后重新训练能够恢复到标称水平。

Conclusion: 这项研究为未来太空应用中阻变存储器基AI加速器的研究奠定了基础，展示了其潜力并指出了需要进一步研究的领域。

Abstract: In recent years, the space community has been exploring the possibilities of
Artificial Intelligence (AI), specifically Artificial Neural Networks (ANNs),
for a variety of on board applications. However, this development is limited by
the restricted energy budget of smallsats and cubesats as well as radiation
concerns plaguing modern chips. This necessitates research into neural network
accelerators capable of meeting these requirements whilst satisfying the
compute and performance needs of the application. This paper explores the use
of Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM)
memristors for on-board in-memory computing AI acceleration in space
applications. A guidance and control neural network (G\&CNET) accelerated using
memristors is simulated in a variety of scenarios and with both device types to
evaluate the performance of memristor-based accelerators, considering device
non-idealities such as noise and conductance drift. We show that the memristive
accelerator is able to learn the expert actions, though challenges remain with
the impact of noise on accuracy. We also show that re-training after
degradation is able to restore performance to nominal levels. This study
provides a foundation for future research into memristor-based AI accelerators
for space, highlighting their potential and the need for further investigation.

</details>
