<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 11]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection](https://arxiv.org/abs/2509.00433)
*Houshu He,Naifeng Jing,Li Jiang,Xiaoyao Liang,Zhuoran Song*

Main category: cs.AR

TL;DR: AGS是一个算法-硬件协同设计框架，通过利用SLAM系统中相邻帧的高相似性来加速3D高斯溅射SLAM系统，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS-SLAM系统由于每帧需要多次训练迭代和大量高斯分布计算，导致吞吐量不足，无法满足实时应用需求。

Method: 软件层面：1）提出粗粒度到细粒度的位姿跟踪方法；2）通过跨帧共享高斯分布贡献信息避免冗余计算。硬件层面：设计帧共视性检测引擎、位姿跟踪引擎和建图引擎，并配备工作负载调度器。

Result: AGS相比移动和高性能GPU实现了17.12倍和6.71倍的加速，相比最先进的3DGS加速器GSCore实现了5.41倍的加速。

Conclusion: AGS框架通过算法-硬件协同设计有效解决了3DGS-SLAM系统的效率瓶颈，为实时SLAM应用提供了可行的解决方案。

Abstract: Simultaneous Localization and Mapping (SLAM) is a critical task that enables
autonomous vehicles to construct maps and localize themselves in unknown
environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting
(3DGS) to achieve exceptional reconstruction fidelity. However, existing
3DGS-SLAM systems provide insufficient throughput due to the need for multiple
training iterations per frame and the vast number of Gaussians.
  In this paper, we propose AGS, an algorithm-hardware co-design framework to
boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems
process frames in a streaming manner, where adjacent frames exhibit high
similarity that can be utilized for acceleration. On the software level: 1) We
propose a coarse-then-fine-grained pose tracking method with respect to the
robot's movement. 2) We avoid redundant computations of Gaussians by sharing
their contribution information across frames. On the hardware level, we propose
a frame covisibility detection engine to extract intermediate data from the
video CODEC. We also implement a pose tracking engine and a mapping engine with
workload schedulers to efficiently deploy the AGS algorithm. Our evaluation
shows that AGS achieves up to $17.12\times$, $6.71\times$, and $5.41\times$
speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS
accelerator, GSCore.

</details>


### [2] [Bit Transition Reduction by Data Transmission Ordering in NoC-based DNN Accelerator](https://arxiv.org/abs/2509.00500)
*Yizhi Chen,Jingwei Li,Wenyao Zhu,Zhonghai Lu*

Main category: cs.AR

TL;DR: 提出基于'1'比特计数的数据排序方法，用于减少NoC-based DNN加速器中的比特翻转(BT)，从而降低链路功耗


<details>
  <summary>Details</summary>
Motivation: 随着DNN变得日益重要，基于NoC的DNN加速器越来越受欢迎。为了节省NoC中的链路功耗，许多研究者专注于减少比特翻转(BT)

Method: 提出'1'比特计数排序方法，提供数学证明验证其有效性。提出两种数据排序方法：affiliated-ordering和separated-ordering，分别用于联合或单独处理权重和输入数据

Result: 无NoC情况下：浮点32数据BT减少20.38%，定点8数据BT减少55.71%。在NoC-based加速器中：浮点32数据BT减少32.01%，定点8数据BT减少40.85%

Conclusion: 该方法在不同DNN模型(LeNet、DarkNet)、不同NoC配置、随机权重和训练权重、不同数据精度下都能有效降低链路功耗

Abstract: As Deep Neural Networks (DNN) are becoming essential, Network-on-Chip
(NoC)-based DNN accelerators gained increasing popularity. To save link power
in NoC, many researchers focus on reducing the Bit Transition (BT). We propose
'1'-bit count-based ordering method to reduce BT for DNN workloads. We provide
a mathematical proof of the efficacy of proposed ordering. We evaluate our
method through experiments without NoC and with NoC. Without NoC, our proposed
ordering method achieves up to 20.38% BT reduction for floating-point-32 data
and 55.71% for fixed-point-8 data, respectively. We propose two data ordering
methods, affiliated-ordering and separated-ordering to process weight and input
jointly or individually and apply them to run full DNNs in NoC-based DNN
accelerator. We evaluate our approaches under various configurations, including
different DNN models such as LeNet and DarkNet, various NoC sizes with
different numbers of memory controllers, random weights and trained weights,
and different data precision. Our approach efficiently reduces the link power
by achieving up to 32.01% BT reduction for floating-point-32 data and 40.85% BT
reduction for fixed-point-8 data.

</details>


### [3] [Real-Time Piano Note Frequency Detection Using FPGA and FFT Core](https://arxiv.org/abs/2509.00589)
*Shafayet M. Anik,D. G. Perera*

Main category: cs.AR

TL;DR: 使用FPGA实现钢琴音频信号的实时FFT频率分析，相比传统软件DSP方法具有更低延迟和更高计算效率


<details>
  <summary>Details</summary>
Motivation: 传统软件DSP方法在实时音乐频率分析中存在延迟高、计算资源需求大的问题，而FPGA的并行处理能力可以提供更快速和确定性的分析

Method: 采用基于FPGA的实时快速傅里叶变换(FFT)系统来分析数字钢琴的模拟音频信号

Result: FPGA平台能够实现实时频率分析，具有比软件DSP更低的延迟和更高的处理速度

Conclusion: FPGA是实现音乐乐器实时频率分析的理想硬件平台，特别适用于电子调音器、音乐可视化器和现场声音监控等应用场景

Abstract: Real-time frequency analysis of musical instruments, such as the piano, is an
essential feature in areas like electronic tuners, music visualizers, and live
sound monitoring. Traditional methods often rely on software-based digital
signal processing (DSP), which may introduce latency and require significant
computational power. In contrast, hardware platforms such as FPGAs (Field
Programmable Gate Arrays) offer the ability to perform such analyses with
greater speed and determinism due to their parallel processing capabilities.
The primary objective of this project was to analyze analog audio signals from
a digital piano using an FPGA-based real-time Fast Fourier Transform (FFT)
system.

</details>


### [4] [COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives](https://arxiv.org/abs/2509.00599)
*Shubham Negi,Manik Singhal,Aayush Ankit,Sudeep Bhoja,Kaushik Roy*

Main category: cs.AR

TL;DR: COMET是一个针对机器学习加速器的数据流建模和优化框架，专门处理复合操作和集体通信，相比未融合基准实现了1.42-3.46倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代DNN模型（如大语言模型、状态空间模型）越来越多地依赖复合操作，但现有数据流优化框架要么专注于单一操作，要么缺乏对集体通信成本的显式建模，无法满足现代工作负载需求。

Method: 提出COMET框架，引入新颖的表示方法显式建模跨空间集群的集体通信，包含考虑GEMM和非GEMM操作级依赖关系的延迟和能耗成本模型。

Result: 优化的数据流在GEMM-Softmax上实现1.42倍加速，GEMM-LayerNorm上3.46倍加速，自注意力机制上1.82倍加速。

Conclusion: COMET通过集体感知建模能够探索更广泛的映射空间，显著提升复合操作的性能和能效，适用于边缘和云端加速器配置。

Abstract: Modern machine learning accelerators are designed to efficiently execute deep
neural networks (DNNs) by optimizing data movement, memory hierarchy, and
compute throughput. However, emerging DNN models such as large language models,
state space models increasingly rely on compound operations-structured
compositions of multiple basic operations-which introduce new challenges for
dataflow optimization and minimizing off-chip memory traffic. Moreover, as
model size continues to grow, deployment across spatially distributed compute
clusters becomes essential, requiring frequent and complex collective
communication. Existing dataflow optimization frameworks and performance models
either focus on single operations or lack explicit modeling of collective
communication cost, limiting their applicability to modern workloads.
  To address these limitations, we propose, a framework for modeling and
optimizing dataflow for compound operations on machine learning accelerators.
COMET introduces a novel representation that explicitly models collective
communication across spatial clusters, along with latency and energy cost
models that account for both GEMM and non-GEMM operation level dependencies
within compound operations. We demonstrate COMET's capabilities to analyze and
optimize dataflows for compound operations such as GEMM--Softmax,
GEMM--LayerNorm, and self-attention, across both edge and cloud accelerator
configurations. Our collective-aware modeling enables exploration of a broader
mapping space, leading to improved performance and energy efficiency.
Specifically, our optimized dataflows achieve up to 1.42$\times$ speedup for
GEMM-Softmax, 3.46$\times$ for GEMM-LayerNorm and 1.82$\times$ for
self-attention compared to unfused baselines.

</details>


### [5] [On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures](https://arxiv.org/abs/2509.00633)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed A. Badawy,Ahmad Patooghy*

Main category: cs.AR

TL;DR: 高带宽内存(HBM)架构存在热攻击漏洞，攻击者可利用垂直/水平邻近性注入短暴热波延迟受害应用访存，而不违反内存管理策略


<details>
  <summary>Details</summary>
Motivation: HBM架构虽提供高性能内存访问，但垂直集成特性使其容易受到热攻击，需要揭露这种新型安全风险

Method: 攻击者通过水平或垂直邻近内存链注入短暴热波，产生聚焦热波影响受害应用的内存访问

Result: 这种攻击能够窃取主动权延迟受害应用，且因为模仿合法工作负荷而难以检测

Conclusion: HBM架构存在重大热安全漏洞，需要新的防御机制来应对这种隐藏在合法工作负荷中的热攻击

Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance
memory interactions to address the well-known performance challenge, namely the
memory wall. However, these architectures are susceptible to thermal
vulnerabilities due to the inherent vertical adjacency that occurs during the
manufacturing process of HBM architectures. We anticipate that adversaries may
exploit the intense vertical and lateral adjacency to design and develop
thermal performance degradation attacks on the memory banks that host
data/instructions from victim applications. In such attacks, the adversary
manages to inject short and intense heat pulses from vertically and/or
laterally adjacent memory banks, creating a convergent thermal wave that
maximizes impact and delays the victim application from accessing its
data/instructions. As the attacking application does not access any
out-of-range memory locations, it can bypass both design-time security tests
and the operating system's memory management policies. In other words, since
the attack mimics legitimate workloads, it will be challenging to detect.

</details>


### [6] [Low Power Approximate Multiplier Architecture for Deep Neural Networks](https://arxiv.org/abs/2509.00764)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 提出一种用于深度神经网络应用的低功耗近似乘法器架构，通过设计仅产生单一组合错误的4:2压缩器，在保持低错误率的同时显著降低精确压缩器的使用。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络应用对计算能效有较高要求，传统精确乘法器功耗较大，需要开发在保持计算精度的同时显著降低能耗的近似乘法器架构。

Method: 设计仅引入单一组合错误的4:2压缩器，并将其集成到8x8无符号乘法器中，减少精确压缩器的使用，构建自定义卷积层并在神经网络任务中进行评估。

Result: 硬件评估显示相比现有最佳乘法器节能30.24%，图像去噪任务中PSNR和SSIM指标优于其他近似设计，手写数字识别保持高分类准确率。

Conclusion: 所提出的架构在能效和计算精度之间实现了良好平衡，适用于低功耗AI硬件实现。

Abstract: This paper proposes an low power approximate multiplier architecture for deep
neural network (DNN) applications. A 4:2 compressor, introducing only a single
combination error, is designed and integrated into an 8x8 unsigned multiplier.
This integration significantly reduces the usage of exact compressors while
preserving low error rates. The proposed multiplier is employed within a custom
convolution layer and evaluated on neural network tasks, including image
recognition and denoising. Hardware evaluation demonstrates that the proposed
design achieves up to 30.24% energy savings compared to the best among existing
multipliers. In image denoising, the custom approximate convolution layer
achieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity
Index Measure (SSIM) compared to other approximate designs. Additionally, when
applied to handwritten digit recognition, the model maintains high
classification accuracy. These results demonstrate that the proposed
architecture offers a favorable balance between energy efficiency and
computational precision, making it suitable for low-power AI hardware
implementations.

</details>


### [7] [Energy Efficient Exact and Approximate Systolic Array Architecture for Matrix Multiplication](https://arxiv.org/abs/2509.00778)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 本文提出了一种包含新型精确和近似处理元件的脉动阵列架构，用于深度神经网络的矩阵乘法计算，实现了显著的能耗节省和良好的输出质量。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络需要高效的矩阵乘法引擎来处理复杂计算，现有设计在能耗方面仍有优化空间，需要开发更节能的架构来满足实际应用需求。

Method: 采用能量高效的正部分积和负部分积单元（PPC和NPPC）设计8位精确和近似处理元件，并将其集成到8x8脉动阵列中，应用于离散余弦变换和卷积计算。

Result: 相比现有设计，精确和近似PE分别实现了22%和32%的能耗节省。在DCT计算中达到38.21dB PSNR，在边缘检测应用中达到30.45dB PSNR。

Conclusion: 提出的设计能够在保持竞争力的输出质量的同时提供显著的能效优势，特别适用于容错图像和视觉处理应用。

Abstract: Deep Neural Networks (DNNs) require highly efficient matrix multiplication
engines for complex computations. This paper presents a systolic array
architecture incorporating novel exact and approximate processing elements
(PEs), designed using energy-efficient positive partial product and negative
partial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit
exact and approximate PE designs are employed in a 8x8 systolic array, which
achieves a energy savings of 22% and 32%, respectively, compared to the
existing design. To demonstrate their effectiveness, the proposed PEs are
integrated into a systolic array (SA) for Discrete Cosine Transform (DCT)
computation, achieving high output quality with a PSNR of 38.21,dB.
Furthermore, in an edge detection application using convolution, the
approximate PE achieves a PSNR of 30.45,dB. These results highlight the
potential of the proposed design to deliver significant energy efficiency while
maintaining competitive output quality, making it well-suited for
error-resilient image and vision processing applications.

</details>


### [8] [GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency](https://arxiv.org/abs/2509.00911)
*Joongho Jo,Jongsun Park*

Main category: cs.AR

TL;DR: GS-TG是一种基于瓦片分组的3D高斯泼溅加速器，通过减少冗余排序操作和保持光栅化效率，实现了1.54倍的平均加速效果


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然比NeRF速度快，但仍无法满足实时应用的高帧率需求，存在瓦片大小与计算效率之间的权衡问题

Method: 提出瓦片分组策略：在排序阶段将小瓦片分组形成大瓦片来共享排序操作，在光栅化阶段使用位掩码标识相关小瓦片来保持效率

Result: 实验结果显示GS-TG相比最先进的3D-GS加速器平均加速1.54倍

Conclusion: GS-TG是一种无损方法，无需重新训练或微调，可与现有优化技术无缝集成，有效解决了3D-GS渲染中的效率瓶颈

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to
neural radiance fields (NeRF) as it offers high speed as well as high image
quality in novel view synthesis. Despite these advancements, 3D-GS still
struggles to meet the frames per second (FPS) demands of real-time
applications. In this paper, we introduce GS-TG, a tile-grouping-based
accelerator that enhances 3D-GS rendering speed by reducing redundant sorting
operations and preserving rasterization efficiency. GS-TG addresses a critical
trade-off issue in 3D-GS rendering: increasing the tile size effectively
reduces redundant sorting operations, but it concurrently increases unnecessary
rasterization computations. So, during sorting of the proposed approach, GS-TG
groups small tiles (for making large tiles) to share sorting operations across
tiles within each group, significantly reducing redundant computations. During
rasterization, a bitmask assigned to each Gaussian identifies relevant small
tiles, to enable efficient sharing of sorting results. Consequently, GS-TG
enables sorting to be performed as if a large tile size is used by grouping
tiles during the sorting stage, while allowing rasterization to proceed with
the original small tiles by using bitmasks in the rasterization stage. GS-TG is
a lossless method requiring no retraining or fine-tuning and it can be
seamlessly integrated with previous 3D-GS optimization techniques. Experimental
results show that GS-TG achieves an average speed-up of 1.54 times over
state-of-the-art 3D-GS accelerators.

</details>


### [9] [GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs](https://arxiv.org/abs/2509.01020)
*Elena Espinosa,Rubén Rodríguez Álvarez,José Miranda,Rafael Larrosa,Miguel Peón-Quirós,Oscar Plata,David Atienza*

Main category: cs.AR

TL;DR: 这篇论文提出了一种可扩展的FPGA加速器GeneTEK，通过高级综合和工作者架构实现Myers算法，在基因组序列比对对准任务中较CPU和GPU方案更快更节能，同时充分利用FPGA资源解决了可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 下一代序列技术产生的巨量基因组数据使得序列比对对准成为计算密集型步骤。虽然CPU和GPU实现已有显著性能提升，FPGA方案在能源效率方面更优，但面临长序列对准时硬件资源限制导致的可扩展性问题。

Method: 设计了一种可扩展和灵活的FPGA加速器模板，使用高级综合技术实现Myers算法，采用工作者基础架构。GeneTEK是该模板在Xilinx Zynq UltraScale+ FPGA上的实例化。

Result: GeneTEK在执行速度上比领先的CPU和GPU实现提高了19.4%，能源消耗减少62倍，同时能够处理比之前FPGA方案大72%的比对矩阵。

Conclusion: 这些结果确认了FPGA作为能源效率高的平台，在可扩展基因组工作负载方面具有巨大潜力。

Abstract: The advent of next-generation sequencing (NGS) has revolutionized genomic
research by enabling high-throughput data generation through parallel
sequencing of a diverse range of organisms at significantly reduced costs. This
breakthrough has unleashed a "Cambrian explosion" in genomic data volume and
diversity. This volume of workloads places genomics among the top four big data
challenges anticipated for this decade. In this context, pairwise sequence
alignment represents a very time- and energy-consuming step in common
bioinformatics pipelines. Speeding up this step requires the implementation of
heuristic approaches, optimized algorithms, and/or hardware acceleration.
  Whereas state-of-the-art CPU and GPU implementations have demonstrated
significant performance gains, recent field programmable gate array (FPGA)
implementations have shown improved energy efficiency. However, the latter
often suffer from limited scalability due to constraints on hardware resources
when aligning longer sequences. In this work, we present a scalable and
flexible FPGA-based accelerator template that implements Myers's algorithm
using high-level synthesis and a worker-based architecture. GeneTEK, an
instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA,
outperforms state-of-the-art CPU and GPU implementations in both speed and
energy efficiency, while overcoming scalability limitations of current FPGA
approaches. Specifically, GeneTEK achieves at least a 19.4% increase in
execution speed and up to 62x reduction in energy consumption compared to
leading CPU and GPU solutions, while fitting comparison matrices up to 72%
larger compared to previous FPGA solutions. These results reaffirm the
potential of FPGAs as an energy-efficient platform for scalable genomic
workloads.

</details>


### [10] [LinkBo: An Adaptive Single-Wire, Low-Latency, and Fault-Tolerant Communications Interface for Variable-Distance Chip-to-Chip Systems](https://arxiv.org/abs/2509.01339)
*Bochen Ye,Gustavo Naspolini,Kimmo Salo,Manil Dev Gomony*

Main category: cs.AR

TL;DR: LinkBo是一种创新的单线通信协议，相比现有的1-wire和UNI/O协议，延迟降低20倍以上，吞吐量提升，支持最长15米距离的可靠通信


<details>
  <summary>Details</summary>
Motivation: 现有单线通信协议存在延迟高、吞吐量受限和鲁棒性不足的问题，需要开发更高效的单线协议来满足嵌入式系统对低成本、高性能通信的需求

Method: 提出了LinkBo协议，采用硬件中断机制实现可变距离的芯片间通信，包含错误检测功能和高优先级消息传输机制，并在FPGA硬件平台上实现和评估

Result: 高优先级消息可在50.4μs内传输，比1-wire快20倍，比UNI/O快6.3倍；支持最长15米距离的300kbps通信，在11厘米距离可达7.5Mbps

Conclusion: LinkBo协议在延迟、吞吐量和鲁棒性方面显著优于现有商业单线协议，为嵌入式系统提供了高性能的单线通信解决方案

Abstract: Cost-effective embedded systems necessitate utilizing the single-wire
communication protocol for inter-chip communication, thanks to its reduced pin
count in comparison to the multi-wire I2C or SPI protocols. However, current
single-wire protocols suffer from increased latency, restricted throughput, and
lack of robustness. This paper presents LinkBo, an innovative single-wire
protocol that offers reduced latency, enhanced throughput, and greater
robustness with hardware-interrupt for variable-distance inter-chip
communication. The LinkBo protocol-level guarantees that high-priority messages
are delivered with an error detection feature in just 50.4 $\mu$s, surpassing
current commercial options, 1-wire and UNI/O by at least 20X and 6.3X,
respectively. In addition, we present the hardware architecture for this new
protocol and its performance evaluation on a hardware platform consisting of
two FPGAs. Our findings demonstrate that the protocol reliably supports wire
lengths up to 15 meters with a data rate of 300 kbps, while reaching a maximum
data rate of 7.5 Mbps over an 11 cm wire, providing reliable performance for
varying inter-chip communication distances.

</details>


### [11] [Guidance and Control Neural Network Acceleration using Memristors](https://arxiv.org/abs/2509.02369)
*Zacharia A. Rudge,Dario Izzo,Moritz Fieback,Anteneh Gebregiorgis,Said Hamdioui,Dominik Dold*

Main category: cs.AR

TL;DR: 研究使用相变存储器和阻变存储器等忆阻器技术，为空间应用中的星载AI加速提供内存计算解决方案，通过模拟验证了在考虑器件非理想特性情况下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 小型卫星和立方星的能量预算有限且存在辐射问题，限制了传统AI技术在星载应用中的发展，需要研究能够满足这些特殊要求的神经网络加速器。

Method: 使用相变存储器(PCM)和阻变存储器(RRAM)忆阻器进行星载内存计算AI加速，通过模拟指导控制神经网络(G&CNET)在各种场景下的性能，考虑噪声和电导漂移等器件非理想特性。

Result: 忆阻器加速器能够学习专家动作，但噪声对准确性仍有影响；在性能退化后重新训练可以恢复到标称性能水平。

Conclusion: 该研究为未来基于忆阻器的空间AI加速器研究奠定了基础，展示了其潜力并指出需要进一步研究的需求。

Abstract: In recent years, the space community has been exploring the possibilities of
Artificial Intelligence (AI), specifically Artificial Neural Networks (ANNs),
for a variety of on board applications. However, this development is limited by
the restricted energy budget of smallsats and cubesats as well as radiation
concerns plaguing modern chips. This necessitates research into neural network
accelerators capable of meeting these requirements whilst satisfying the
compute and performance needs of the application. This paper explores the use
of Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM)
memristors for on-board in-memory computing AI acceleration in space
applications. A guidance and control neural network (G\&CNET) accelerated using
memristors is simulated in a variety of scenarios and with both device types to
evaluate the performance of memristor-based accelerators, considering device
non-idealities such as noise and conductance drift. We show that the memristive
accelerator is able to learn the expert actions, though challenges remain with
the impact of noise on accuracy. We also show that re-training after
degradation is able to restore performance to nominal levels. This study
provides a foundation for future research into memristor-based AI accelerators
for space, highlighting their potential and the need for further investigation.

</details>
