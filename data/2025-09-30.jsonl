{"id": "2509.22980", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22980", "abs": "https://arxiv.org/abs/2509.22980", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "\\textit{No One-Size-Fits-All}: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory", "comment": null, "summary": "Processing-in-Memory (PIM) is a promising approach to overcoming the\nmemory-wall bottleneck. However, the PIM community has largely treated its two\nfundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they\nwere interchangeable. This implicit \"one-layout-fits-all\" assumption, often\nhard-coded into existing evaluation frameworks, creates a critical gap:\narchitects lack systematic, workload-driven guidelines for choosing the optimal\ndata layout for their target applications.\n  To address this gap, this paper presents the first systematic,\nworkload-driven characterization of BP and BS PIM architectures. We develop\niso-area, cycle-accurate BP and BS PIM architectural models and conduct a\ncomprehensive evaluation using a diverse set of benchmarks. Our suite includes\nboth fine-grained microworkloads from MIMDRAM to isolate specific operational\ncharacteristics, and large-scale applications from the PIMBench suite, such as\nthe VGG network, to represent realistic end-to-end workloads.\n  Our results quantitatively demonstrate that no single layout is universally\nsuperior; the optimal choice is strongly dependent on workload characteristics.\nBP excels on control-flow-intensive tasks with irregular memory access\npatterns, whereas BS shows substantial advantages in massively parallel,\nlow-precision (e.g., INT4/INT8) computations common in AI. Based on this\ncharacterization, we distill a set of actionable design guidelines for\narchitects. This work challenges the prevailing one-size-fits-all view on PIM\ndata layouts and provides a principled foundation for designing\nnext-generation, workload-aware, and potentially hybrid PIM systems."}
{"id": "2509.22999", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22999", "abs": "https://arxiv.org/abs/2509.22999", "authors": ["Sachin Sachdeva", "Jincong Lu", "Wantong Li", "Sheldon X. -D. Tan"], "title": "Enhanced Hybrid Temporal Computing Using Deterministic Summations for Ultra-Low-Power Accelerators", "comment": "8 pages", "summary": "This paper presents an accuracy-enhanced Hybrid Temporal Computing (E-HTC)\nframework for ultra-low-power hardware accelerators with deterministic\nadditions. Inspired by the recently proposed HTC architecture, which leverages\npulse-rate and temporal data encoding to reduce switching activity and energy\nconsumption but loses accuracy due to its multiplexer (MUX)-based scaled\naddition, we propose two bitstream addition schemes: (1) an Exact\nMultiple-input Binary Accumulator (EMBA), which performs precise binary\naccumulation, and (2) a Deterministic Threshold-based Scaled Adder (DTSA),\nwhich employs threshold logic for scaled addition. These adders are integrated\ninto a multiplier accumulator (MAC) unit supporting both unipolar and bipolar\nencodings. To validate the framework, we implement two accelerators: a Finite\nImpulse Response (FIR) filter and an 8-point Discrete Cosine Transform\n(DCT)/iDCT engine. Results on a 4x4 MAC show that, in unipolar mode, E-HTC\nmatches the RMSE of state-of-the-art Counter-Based Stochastic Computing (CBSC)\nMAC, improves accuracy by 94% over MUX-based HTC, and reduces power and area by\n23% and 7% compared to MUX-based HTC and 64% and 74% compared to CBSC. In\nbipolar mode, E-HTC MAC achieves 2.09% RMSE -- an 83% improvement over\nMUX-based HTC -- and approaches CBSC's 1.40% RMSE with area and power savings\nof 28% and 43% vs. MUX-based HTC and about 76% vs. CBSC. In FIR experiments,\nboth E-HTC variants yield PSNR gains of 3--5 dB (30--45% RMSE reduction) while\nsaving 13% power and 3% area. For DCT/iDCT, E-HTC boosts PSNR by 10--13 dB\n(70--75% RMSE reduction) while saving area and power over both MUX- and\nCBSC-based designs."}
{"id": "2509.23179", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23179", "abs": "https://arxiv.org/abs/2509.23179", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "A Near-Cache Architectural Framework for Cryptographic Computing", "comment": null, "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."}
{"id": "2509.23674", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23674", "abs": "https://arxiv.org/abs/2509.23674", "authors": ["Hongqin Lyu", "Yonghao Wang", "Yunlin Du", "Mingyu Shi", "Zhiteng Chao", "Wenxing Li", "Tiancheng Wang", "Huawei Li"], "title": "AssertGen: Enhancement of LLM-aided Assertion Generation through Cross-Layer Signal Bridging", "comment": "6 pages, 7 figures", "summary": "Assertion-based verification (ABV) serves as a crucial technique for ensuring\nthat register-transfer level (RTL) designs adhere to their specifications.\nWhile Large Language Model (LLM) aided assertion generation approaches have\nrecently achieved remarkable progress, existing methods are still unable to\neffectively identify the relationship between design specifications and RTL\ndesigns, which leads to the insufficiency of the generated assertions. To\naddress this issue, we propose AssertGen, an assertion generation framework\nthat automatically generates SystemVerilog assertions (SVA). AssertGen first\nextracts verification objectives from specifications using a chain-of-thought\n(CoT) reasoning strategy, then bridges corresponding signals between these\nobjectives and the RTL code to construct a cross-layer signal chain, and\nfinally generates SVAs based on the LLM. Experimental results demonstrate that\nAssertGen outperforms the existing state-of-the-art methods across several key\nmetrics, such as pass rate of formal property verification (FPV), cone of\ninfluence (COI), proof core and mutation testing coverage."}
{"id": "2509.23693", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.23693", "abs": "https://arxiv.org/abs/2509.23693", "authors": ["Tao Lu", "Jiapin Wang", "Yelin Shan", "Xiangping Zhang", "Xiang Chen"], "title": "ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights", "comment": "16 pages", "summary": "Lossless compression imposes significant computational over head on\ndatacenters when performed on CPUs. Hardware compression and decompression\nprocessing units (CDPUs) can alleviate this overhead, but optimal algorithm\nselection, microarchitectural design, and system-level placement of CDPUs are\nstill not well understood. We present the design of an ASIC-based in-storage\nCDPU and provide a comprehensive end-to-end evaluation against two leading ASIC\naccelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant\nCDPU placement regimes: peripheral, on-chip, and in-storage. Our results\nreveal: (i) acute sensitivity of throughput and latency to CDPU placement and\ninterconnection, (ii) strong correlation between compression efficiency and\ndata patterns/layouts, (iii) placement-driven divergences between\nmicrobenchmark gains and real-application speedups, (iv) discrepancies between\nmodule and system-level power efficiency, and (v) scalability and multi-tenant\ninterference is sues of various CDPUs. These findings motivate a\nplacement-aware, cross-layer rethinking of hardware (de)compression for\nhyperscale storage infrastructures."}
{"id": "2509.23972", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23972", "abs": "https://arxiv.org/abs/2509.23972", "authors": ["Hongqin Lyu", "Yunlin Du", "Yonghao Wang", "Zhiteng Chao", "Tiancheng Wang", "Huawei Li"], "title": "AssertFix: Empowering Automated Assertion Fix via Large Language Models", "comment": "6 pages, 6 figures", "summary": "Assertion-based verification (ABV) is critical in ensuring that\nregister-transfer level (RTL) designs conform to their functional\nspecifications. SystemVerilog Assertions (SVA) effectively specify design\nproperties, but writing and maintaining them manually is challenging and\nerror-prone. Although recent progress of assertion generation methods\nleveraging large language models (LLMs) have shown great potential in improving\nassertion quality, they typically treat assertion generation as a final step,\nleaving the burden of fixing of the incorrect assertions to human effects,\nwhich may significantly limits the application of these methods. To address the\nabove limitation, we propose an automatic assertion fix framework based on\nLLMs, named AssertFix. AsserFix accurately locates the RTL code related to the\nincorrect assertion, systematically identifies the root causes of the assertion\nerrors, classifies the error type and finally applies dedicated fix strategies\nto automatically correct these errors, improving the overall quality of the\ngenerated assertions. Experimental results show that AssertFix achieves\nnoticeable improvements in both fix rate and verification coverage across the\nOpencore benchmarks."}
{"id": "2509.24929", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.24929", "abs": "https://arxiv.org/abs/2509.24929", "authors": ["Hongwei Zhao", "Vianney Lapotre", "Guy Gogniat"], "title": "Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI", "comment": "12 pages, 7 tables", "summary": "Fault injection attacks exploit physical disturbances to compromise the\nfunctionality and security of integrated circuits. As System on Chip (SoC)\narchitectures grow in complexity, the vulnerability of on chip communication\nfabrics has become increasingly prominent. Buses, serving as interconnects\namong various IP cores, represent potential vectors for fault-based\nexploitation. In this study, we perform simulation-driven fault injection\nacross three mainstream bus protocols Wishbone, AXI Lite, and AXI. We\nsystematically examine fault success rates, spatial vulnerability\ndistributions, and timing dependencies to characterize how faults interact with\nbus-level transactions. The results uncover consistent behavioral patterns\nacross protocols, offering practical insights for both attack modeling and the\ndevelopment of resilient SoC designs."}
