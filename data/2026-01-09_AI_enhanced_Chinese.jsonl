{"id": "2601.04476", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.04476", "abs": "https://arxiv.org/abs/2601.04476", "authors": ["Chuanzhen Wang", "Leo Zhang", "Eric Liu"], "title": "Memory-Guided Unified Hardware Accelerator for Mixed-Precision Scientific Computing", "comment": "22 pages", "summary": "Recent hardware acceleration advances have enabled powerful specialized accelerators for finite element computations, spiking neural network inference, and sparse tensor operations. However, existing approaches face fundamental limitations: (1) finite element methods lack comprehensive rounding error analysis for reduced-precision implementations and use fixed precision assignment strategies that cannot adapt to varying numerical conditioning; (2) spiking neural network accelerators cannot handle non-spike operations and suffer from bit-width escalation as network depth increases; and (3) FPGA tensor accelerators optimize only for dense computations while requiring manual configuration for each sparsity pattern. To address these challenges, we introduce \\textbf{Memory-Guided Unified Hardware Accelerator for Mixed-Precision Scientific Computing}, a novel framework that integrates three enhanced modules with memory-guided adaptation for efficient mixed-workload processing on unified platforms. Our approach employs memory-guided precision selection to overcome fixed precision limitations, integrates experience-driven bit-width management and dynamic parallelism adaptation for enhanced spiking neural network acceleration, and introduces curriculum learning for automatic sparsity pattern discovery. Extensive experiments on FEniCS, COMSOL, ANSYS benchmarks, MNIST, CIFAR-10, CIFAR-100, DVS-Gesture datasets, and COCO 2017 demonstrate 2.8\\% improvement in numerical accuracy, 47\\% throughput increase, 34\\% energy reduction, and 45-65\\% throughput improvement compared to specialized accelerators. Our work enables unified processing of finite element methods, spiking neural networks, and sparse computations on a single platform while eliminating data transfer overhead between separate units.", "AI": {"tldr": "\u63d0\u51faMemory-Guided Unified Hardware Accelerator\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5b58\u5f15\u5bfc\u7684\u7cbe\u5ea6\u9009\u62e9\u3001\u7ecf\u9a8c\u9a71\u52a8\u7684\u4f4d\u5bbd\u7ba1\u7406\u548c\u8bfe\u7a0b\u5b66\u4e60\u81ea\u52a8\u53d1\u73b0\u7a00\u758f\u6a21\u5f0f\uff0c\u5728\u7edf\u4e00\u5e73\u53f0\u4e0a\u9ad8\u6548\u5904\u7406\u6709\u9650\u5143\u65b9\u6cd5\u3001\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u7a00\u758f\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u4e13\u7528\u52a0\u901f\u5668\u5b58\u5728\u4e09\u4e2a\u6839\u672c\u9650\u5236\uff1a\u6709\u9650\u5143\u65b9\u6cd5\u7f3a\u4e4f\u5168\u9762\u7684\u820d\u5165\u8bef\u5dee\u5206\u6790\u4e14\u4f7f\u7528\u56fa\u5b9a\u7cbe\u5ea6\u7b56\u7565\uff1b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u65e0\u6cd5\u5904\u7406\u975e\u8109\u51b2\u64cd\u4f5c\u4e14\u4f4d\u5bbd\u968f\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\u800c\u6269\u5927\uff1bFPGA\u5f20\u91cf\u52a0\u901f\u5668\u4ec5\u4f18\u5316\u5bc6\u96c6\u8ba1\u7b97\u4e14\u9700\u8981\u624b\u52a8\u914d\u7f6e\u7a00\u758f\u6a21\u5f0f\u3002", "method": "\u63d0\u51faMemory-Guided Unified Hardware Accelerator\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u589e\u5f3a\u6a21\u5757\uff1a1) \u5185\u5b58\u5f15\u5bfc\u7684\u7cbe\u5ea6\u9009\u62e9\u514b\u670d\u56fa\u5b9a\u7cbe\u5ea6\u9650\u5236\uff1b2) \u7ecf\u9a8c\u9a71\u52a8\u7684\u4f4d\u5bbd\u7ba1\u7406\u548c\u52a8\u6001\u5e76\u884c\u9002\u914d\u589e\u5f3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\uff1b3) \u8bfe\u7a0b\u5b66\u4e60\u81ea\u52a8\u53d1\u73b0\u7a00\u758f\u6a21\u5f0f\u3002", "result": "\u5728FEniCS\u3001COMSOL\u3001ANSYS\u57fa\u51c6\u6d4b\u8bd5\uff0cMNIST\u3001CIFAR-10\u3001CIFAR-100\u3001DVS-Gesture\u6570\u636e\u96c6\u548cCOCO 2017\u4e0a\u5b9e\u9a8c\u663e\u793a\uff1a\u6570\u503c\u7cbe\u5ea6\u63d0\u9ad82.8%\uff0c\u541e\u5410\u91cf\u589e\u52a047%\uff0c\u80fd\u8017\u964d\u4f4e34%\uff0c\u76f8\u6bd4\u4e13\u7528\u52a0\u901f\u5668\u541e\u5410\u91cf\u63d0\u534745-65%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b9e\u73b0\u4e86\u5728\u5355\u4e00\u5e73\u53f0\u4e0a\u7edf\u4e00\u5904\u7406\u6709\u9650\u5143\u65b9\u6cd5\u3001\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u7a00\u758f\u8ba1\u7b97\uff0c\u6d88\u9664\u4e86\u5355\u72ec\u5355\u5143\u95f4\u7684\u6570\u636e\u4f20\u8f93\u5f00\u9500\uff0c\u4e3a\u6df7\u5408\u7cbe\u5ea6\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04801", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04801", "abs": "https://arxiv.org/abs/2601.04801", "authors": ["Lei Xu", "Shanshan Wang", "Chenglong Xiao"], "title": "MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration", "comment": null, "summary": "High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE.", "AI": {"tldr": "\u63d0\u51faMPM-LLM4DSE\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\u548cLLM\u4f18\u5316\u5668\uff0c\u663e\u8457\u63d0\u5347HLS\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709GNN\u9884\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u6355\u6349\u884c\u4e3a\u63cf\u8ff0\u7684\u4e30\u5bcc\u8bed\u4e49\u7279\u5f81\uff0c\u4f20\u7edf\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u7f3a\u4e4f\u5bf9pragma\u6307\u4ee4\u5982\u4f55\u5f71\u54cdQoR\u7684\u9886\u57df\u77e5\u8bc6\u8003\u8651", "method": "\u63d0\u51faMPM-LLM4DSE\u6846\u67b6\uff1a1) \u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\u878d\u5408\u884c\u4e3a\u63cf\u8ff0\u548c\u63a7\u5236\u6570\u636e\u6d41\u56fe\u7279\u5f81\uff1b2) \u4f7f\u7528LLM\u4f5c\u4e3a\u4f18\u5316\u5668\uff0c\u914d\u5408\u4e13\u95e8\u8bbe\u8ba1\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5305\u542bpragma\u5bf9QoR\u5f71\u54cd\u5206\u6790", "result": "\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5ProgSG\u63d0\u5347\u9ad8\u8fbe10.25\u500d\uff1b\u5728DSE\u4efb\u52a1\u4e2d\uff0cLLM4DSE\u6bd4\u5148\u524d\u65b9\u6cd5\u5e73\u5747\u6027\u80fd\u63d0\u534739.90%", "conclusion": "\u63d0\u51fa\u7684MPM-LLM4DSE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86HLS DSE\u4e2d\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u4f18\u5316\u6548\u7387\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548cLLM\u5f15\u5bfc\u4f18\u5316\u7684\u6709\u6548\u6027"}}
{"id": "2601.05047", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05047", "abs": "https://arxiv.org/abs/2601.05047", "authors": ["Xiaoyu Ma", "David Patterson"], "title": "Challenges and Research Directions for Large Language Model Inference Hardware", "comment": "Accepted for publication by IEEE Computer, 2026", "summary": "Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86LLM\u63a8\u7406\u7684\u6311\u6218\uff0c\u6307\u51fa\u5185\u5b58\u548c\u4e92\u8fde\u662f\u4e3b\u8981\u74f6\u9888\u800c\u975e\u8ba1\u7b97\uff0c\u63d0\u51fa\u4e86\u56db\u79cd\u67b6\u6784\u7814\u7a76\u673a\u4f1a\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "motivation": "LLM\u63a8\u7406\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u81ea\u56de\u5f52\u89e3\u7801\u9636\u6bb5\u4f7f\u63a8\u7406\u4e0e\u8bad\u7ec3\u6839\u672c\u4e0d\u540c\uff0c\u4e14\u53d7AI\u8d8b\u52bf\u5f71\u54cd\uff0c\u5185\u5b58\u548c\u4e92\u8fde\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u800c\u975e\u8ba1\u7b97\u80fd\u529b", "method": "\u63d0\u51fa\u56db\u79cd\u67b6\u6784\u7814\u7a76\u673a\u4f1a\uff1a1) \u9ad8\u5e26\u5bbd\u95ea\u5b58\u5b9e\u73b010\u500d\u5185\u5b58\u5bb9\u91cf\u548cHBM\u7ea7\u5e26\u5bbd\uff1b2) \u8fd1\u5185\u5b58\u5904\u7406\uff1b3) 3D\u5185\u5b58-\u903b\u8f91\u5806\u53e0\u5b9e\u73b0\u9ad8\u5185\u5b58\u5e26\u5bbd\uff1b4) \u4f4e\u5ef6\u8fdf\u4e92\u8fde\u52a0\u901f\u901a\u4fe1", "result": "\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u4e2d\u5fc3AI\uff0c\u4f46\u8bba\u6587\u4e5f\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u67b6\u6784\u65b9\u6848\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u9002\u7528\u6027", "conclusion": "\u9700\u8981\u65b0\u7684\u67b6\u6784\u521b\u65b0\u6765\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u548c\u4e92\u8fde\u74f6\u9888\uff0c\u63d0\u51fa\u7684\u56db\u79cd\u7814\u7a76\u65b9\u5411\u6709\u671b\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387"}}
