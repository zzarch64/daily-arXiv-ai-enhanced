{"id": "2511.03079", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03079", "abs": "https://arxiv.org/abs/2511.03079", "authors": ["Changhong Li", "Biswajit Basu", "Shreejith Shanker"], "title": "LogicSparse: Enabling Engine-Free Unstructured Sparsity for Quantised Deep-learning Accelerators", "comment": "Accepted by ICFPT 2025", "summary": "FPGAs have been shown to be a promising platform for deploying Quantised\nNeural Networks (QNNs) with high-speed, low-latency, and energy-efficient\ninference. However, the complexity of modern deep-learning models limits the\nperformance on resource-constrained edge devices. While quantisation and\npruning alleviate these challenges, unstructured sparsity remains\nunderexploited due to irregular memory access. This work introduces a framework\nthat embeds unstructured sparsity into dataflow accelerators, eliminating the\nneed for dedicated sparse engines and preserving parallelism. A hardware-aware\npruning strategy is introduced to improve efficiency and design flow further.\nOn LeNet-5, the framework attains 51.6 x compression and 1.23 x throughput\nimprovement using only 5.12% of LUTs, effectively exploiting unstructured\nsparsity for QNN acceleration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u5d4c\u5165\u6570\u636e\u6d41\u52a0\u901f\u5668\u7684\u6846\u67b6\uff0c\u65e0\u9700\u4e13\u7528\u7a00\u758f\u5f15\u64ce\u5373\u53ef\u4fdd\u6301\u5e76\u884c\u6027\uff0c\u5728LeNet-5\u4e0a\u5b9e\u73b051.6\u500d\u538b\u7f29\u548c1.23\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6027\u80fd\u53d7\u9650\uff0c\u91cf\u5316\u4e0e\u526a\u679d\u867d\u80fd\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u7531\u4e8e\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u5f15\u5165\u5c06\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u5d4c\u5165\u6570\u636e\u6d41\u52a0\u901f\u5668\u7684\u6846\u67b6\uff0c\u91c7\u7528\u786c\u4ef6\u611f\u77e5\u526a\u679d\u7b56\u7565\u63d0\u9ad8\u6548\u7387\uff0c\u65e0\u9700\u4e13\u7528\u7a00\u758f\u5f15\u64ce\u5373\u53ef\u4fdd\u6301\u5e76\u884c\u6027\u3002", "result": "\u5728LeNet-5\u4e0a\u5b9e\u73b051.6\u500d\u538b\u7f29\u548c1.23\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4ec5\u4f7f\u75285.12%\u7684LUT\u8d44\u6e90\uff0c\u6709\u6548\u5229\u7528\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u52a0\u901fQNN\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u96c6\u6210\u5230FPGA\u6570\u636e\u6d41\u52a0\u901f\u5668\u4e2d\uff0c\u663e\u8457\u63d0\u5347QNN\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03203", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03203", "abs": "https://arxiv.org/abs/2511.03203", "authors": ["Deyang Yu", "Chenchen Liu", "Chuanjie Zhang", "Xiao Fang", "Weisheng Zhao"], "title": "An Event-Driven Spiking Compute-In-Memory Macro based on SOT-MRAM", "comment": "5 pages, 7 figures. Under review for ISCAS", "summary": "The application of Magnetic Random-Access Memory (MRAM) in\ncomputing-in-memory (CIM) has gained significant attention. However, existing\ndesigns often suffer from high energy consumption due to their reliance on\ncomplex analog circuits for computation. In this work, we present a Spin-Orbit-\nTorque MRAM(SOT-MRAM)-based CIM macro that employs an event-driven spiking\nprocessing for high energy efficiency. The SOT-MRAM crossbar adopts a hybrid\nseries-parallel cell structure to efficiently support matrix-vector\nmultiplication (MVM). Signal information is (en) decoded as spikes using\nlightweight circuits, eliminating the need for conventional area- and\npowerintensive analog circuits. The SOT-MRAM macro is designed and evaluated in\n28nm technology, and experimental results show that it achieves a peak energy\nefficiency of 243.6 TOPS/W, significantly outperforming existing designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u65cb\u8f68\u9053\u8f6c\u77e9MRAM\u7684\u4e8b\u4ef6\u9a71\u52a8\u8109\u51b2\u5904\u7406\u8ba1\u7b97\u5185\u5b58\u5b8f\uff0c\u91c7\u7528\u6df7\u5408\u4e32\u5e76\u8054\u5355\u5143\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6548\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\uff0c\u572828nm\u5de5\u827a\u4e0b\u8fbe\u5230243.6 TOPS/W\u7684\u5cf0\u503c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709MRAM\u8ba1\u7b97\u5185\u5b58\u8bbe\u8ba1\u4f9d\u8d56\u590d\u6742\u7684\u6a21\u62df\u7535\u8def\uff0c\u5bfc\u81f4\u80fd\u8017\u8fc7\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528SOT-MRAM\u4ea4\u53c9\u9635\u5217\uff0c\u91c7\u7528\u6df7\u5408\u4e32\u5e76\u8054\u5355\u5143\u7ed3\u6784\u652f\u6301\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\uff1b\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7535\u8def\u5c06\u4fe1\u53f7\u7f16\u7801\u4e3a\u8109\u51b2\uff0c\u907f\u514d\u4f20\u7edf\u6a21\u62df\u7535\u8def\u3002", "result": "\u572828nm\u5de5\u827a\u4e0b\u5b9e\u73b0\uff0c\u5cf0\u503c\u80fd\u6548\u8fbe\u5230243.6 TOPS/W\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8bbe\u8ba1\u3002", "conclusion": "\u8be5SOT-MRAM\u8ba1\u7b97\u5185\u5b58\u5b8f\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u8109\u51b2\u5904\u7406\u548c\u6df7\u5408\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u7684\u8ba1\u7b97\u5185\u5b58\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03427", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03427", "abs": "https://arxiv.org/abs/2511.03427", "authors": ["Florentia Afentaki", "Maha Shatta", "Konstantinos Balaskas", "Georgios Panagopoulos", "Georgios Zervakis", "Mehdi B. Tahoori"], "title": "Design and Optimization of Mixed-Kernel Mixed-Signal SVMs for Flexible Electronics", "comment": "Accepted for publication at IEEE Design Automation & Testing in\n  Europe (DATE 2026)", "summary": "Flexible Electronics (FE) have emerged as a promising alternative to\nsilicon-based technologies, offering on-demand low-cost fabrication,\nconformality, and sustainability. However, their large feature sizes severely\nlimit integration density, imposing strict area and power constraints, thus\nprohibiting the realization of Machine Learning (ML) circuits, which can\nsignificantly enhance the capabilities of relevant near-sensor applications.\nSupport Vector Machines (SVMs) offer high accuracy in such applications at\nrelatively low computational complexity, satisfying FE technologies'\nconstraints. Existing SVM designs rely solely on linear or Radial Basis\nFunction (RBF) kernels, forcing a trade-off between hardware costs and\naccuracy. Linear kernels, implemented digitally, minimize overhead but\nsacrifice performance, while the more accurate RBF kernels are prohibitively\nlarge in digital, and their analog realization contains inherent functional\napproximation. In this work, we propose the first mixed-kernel and mixed-signal\nSVM design in FE, which unifies the advantages of both implementations and\nbalances the cost/accuracy trade-off. To that end, we introduce a\nco-optimization approach that trains our mixed-kernel SVMs and maps binary SVM\nclassifiers to the appropriate kernel (linear/RBF) and domain (digital/analog),\naiming to maximize accuracy whilst reducing the number of costly RBF\nclassifiers. Our designs deliver 7.7% higher accuracy than state-of-the-art\nsingle-kernel linear SVMs, and reduce area and power by 108x and 17x on average\ncompared to digital RBF implementations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u67d4\u6027\u7535\u5b50\u4e2d\u7684\u6df7\u5408\u6838\u51fd\u6570\u548c\u6df7\u5408\u4fe1\u53f7SVM\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5728\u6570\u5b57/\u6a21\u62df\u57df\u548c\u7ebf\u6027/RBF\u6838\u51fd\u6570\u95f4\u5206\u914d\u5206\u7c7b\u5668\uff0c\u5728\u63d0\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u3002", "motivation": "\u67d4\u6027\u7535\u5b50\u6280\u672f\u56e0\u7279\u5f81\u5c3a\u5bf8\u5927\u800c\u9650\u5236\u4e86\u96c6\u6210\u5bc6\u5ea6\uff0c\u65e0\u6cd5\u5b9e\u73b0\u673a\u5668\u5b66\u4e60\u7535\u8def\u3002\u73b0\u6709SVM\u8bbe\u8ba1\u5728\u786c\u4ef6\u6210\u672c\u548c\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u7ebf\u6027\u6838\u7cbe\u5ea6\u4f4e\u4f46\u786c\u4ef6\u5f00\u9500\u5c0f\uff0cRBF\u6838\u7cbe\u5ea6\u9ad8\u4f46\u786c\u4ef6\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6838\u51fd\u6570\u548c\u6df7\u5408\u4fe1\u53f7SVM\u8bbe\u8ba1\uff0c\u91c7\u7528\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u8bad\u7ec3\u6df7\u5408\u6838SVM\uff0c\u5e76\u5c06\u4e8c\u5206\u7c7bSVM\u6620\u5c04\u5230\u9002\u5f53\u7684\u6838\u51fd\u6570\uff08\u7ebf\u6027/RBF\uff09\u548c\u57df\uff08\u6570\u5b57/\u6a21\u62df\uff09\uff0c\u65e8\u5728\u6700\u5927\u5316\u7cbe\u5ea6\u540c\u65f6\u51cf\u5c11\u6602\u8d35\u7684RBF\u5206\u7c7b\u5668\u6570\u91cf\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5355\u6838\u7ebf\u6027SVM\uff0c\u7cbe\u5ea6\u63d0\u9ad87.7%\uff1b\u76f8\u6bd4\u6570\u5b57RBF\u5b9e\u73b0\uff0c\u9762\u79ef\u548c\u529f\u8017\u5206\u522b\u5e73\u5747\u964d\u4f4e108\u500d\u548c17\u500d\u3002", "conclusion": "\u6df7\u5408\u6838\u51fd\u6570\u548c\u6df7\u5408\u4fe1\u53f7SVM\u8bbe\u8ba1\u6210\u529f\u5e73\u8861\u4e86\u67d4\u6027\u7535\u5b50\u4e2d\u786c\u4ef6\u6210\u672c\u4e0e\u7cbe\u5ea6\u7684\u6743\u8861\uff0c\u4e3a\u8fd1\u4f20\u611f\u5668\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
