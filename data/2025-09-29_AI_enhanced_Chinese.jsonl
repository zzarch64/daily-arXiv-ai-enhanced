{"id": "2509.21762", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.21762", "abs": "https://arxiv.org/abs/2509.21762", "authors": ["Ian McDougall", "Michael Davies", "Rahul Chatterjee", "Somesh Jha", "Karthikeyan Sankaralingam"], "title": "Privacy-Preserving Performance Profiling of In-The-Wild GPUs", "comment": "26 pages, 10 figures", "summary": "GPUs are the dominant platform for many important applications today\nincluding deep learning, accelerated computing, and scientific simulation.\nHowever, as the complexity of both applications and hardware increases, GPU\nchip manufacturers face a significant challenge: how to gather comprehensive\nperformance characteristics and value profiles from GPUs deployed in real-world\nscenarios. Such data, encompassing the types of kernels executed and the time\nspent in each, is crucial for optimizing chip design and enhancing application\nperformance. Unfortunately, despite the availability of low-level tools like\nNSYS and NCU, current methodologies fall short, offering data collection\ncapabilities only on an individual user basis rather than a broader, more\ninformative fleet-wide scale. This paper takes on the problem of realizing a\nsystem that allows planet-scale real-time GPU performance profiling of\nlow-level hardware characteristics. The three fundamental problems we solve\nare: i) user experience of achieving this with no slowdown; ii) preserving user\nprivacy, so that no 3rd party is aware of what applications any user runs; iii)\nefficacy in showing we are able to collect data and assign it applications even\nwhen run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,\nrunning applications from the Torchbench suite, showing our system addresses\nall 3 problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u884c\u661f\u7ea7\u5b9e\u65f6GPU\u6027\u80fd\u5206\u6790\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21GPU\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u6570\u636e\u6536\u96c6\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u96f6\u6027\u80fd\u5f71\u54cd\u548c\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u968f\u7740GPU\u5e94\u7528\u548c\u786c\u4ef6\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u5236\u9020\u5546\u9700\u8981\u4ece\u771f\u5b9e\u90e8\u7f72\u73af\u5883\u4e2d\u6536\u96c6\u5168\u9762\u7684\u6027\u80fd\u7279\u5f81\u6570\u636e\u6765\u4f18\u5316\u82af\u7247\u8bbe\u8ba1\u548c\u5e94\u7528\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u53ea\u80fd\u63d0\u4f9b\u5355\u4e2a\u7528\u6237\u7684\u6570\u636e\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u5168\u5c40\u89c6\u89d2\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u884c\u661f\u7ea7\u5b9e\u65f6GPU\u6027\u80fd\u5206\u6790\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u96f6\u6027\u80fd\u5f71\u54cd\u7684\u7528\u6237\u4f53\u9a8c\u3001\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff08\u7b2c\u4e09\u65b9\u65e0\u6cd5\u83b7\u77e5\u7528\u6237\u8fd0\u884c\u7684\u5e94\u7528\uff09\u3001\u5728\u6570\u5343GPU\u4e0a\u6709\u6548\u6536\u96c6\u6570\u636e\u5e76\u5173\u8054\u5230\u5177\u4f53\u5e94\u7528\u3002", "result": "\u5728\u6a21\u62df10\u4e07\u89c4\u6a21GPU\u90e8\u7f72\u73af\u5883\u4e2d\u8fd0\u884cTorchbench\u5957\u4ef6\u5e94\u7528\uff0c\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u6240\u6709\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21GPU\u6027\u80fd\u6570\u636e\u7684\u6709\u6548\u6536\u96c6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u5927\u89c4\u6a21GPU\u90e8\u7f72\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u5206\u6790\uff0c\u540c\u65f6\u4fdd\u8bc1\u96f6\u6027\u80fd\u5f71\u54cd\u548c\u7528\u6237\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3aGPU\u82af\u7247\u8bbe\u8ba1\u548c\u5e94\u7528\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2509.22410", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22410", "abs": "https://arxiv.org/abs/2509.22410", "authors": ["Shayne Wadle", "Yanxin Zhang", "Vikas Singh", "Karthikeyan Sankaralingam"], "title": "NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction", "comment": null, "summary": "The evaluation of new microprocessor designs is constrained by slow,\ncycle-accurate simulators that rely on unrepresentative benchmark traces. This\npaper introduces a novel deep learning framework for high-fidelity,\n``in-the-wild'' simulation on production hardware. Our core contribution is a\nDL model trained on microarchitecture-independent features to predict\ncycle-level performance for hypothetical processor designs. This unique\napproach allows the model to be deployed on existing silicon to evaluate future\nhardware. We propose a complete system featuring a lightweight hardware trace\ncollector and a principled sampling strategy to minimize user impact. This\nsystem achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a\nmere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip\naccelerator improves performance by 85x over the GPU. We demonstrate that this\nframework enables accurate performance analysis and large-scale hardware A/B\ntesting on a massive scale using real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9ad8\u4fdd\u771f\u5fae\u5904\u7406\u5668\u4eff\u771f\u6846\u67b6\uff0c\u53ef\u5728\u73b0\u6709\u786c\u4ef6\u4e0a\u8bc4\u4f30\u672a\u6765\u5904\u7406\u5668\u8bbe\u8ba1\uff0c\u5b9e\u73b05 MIPS\u4eff\u771f\u901f\u5ea6\uff0c\u4ec5\u4ea7\u751f0.1%\u6027\u80fd\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u5fae\u5904\u7406\u5668\u8bbe\u8ba1\u8bc4\u4f30\u53d7\u9650\u4e8e\u7f13\u6162\u7684\u5468\u671f\u7cbe\u786e\u4eff\u771f\u5668\u548c\u7f3a\u4e4f\u4ee3\u8868\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4eff\u771f\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e\u5fae\u67b6\u6784\u65e0\u5173\u7279\u5f81\u9884\u6d4b\u5468\u671f\u7ea7\u6027\u80fd\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u786c\u4ef6\u8ffd\u8e2a\u6536\u96c6\u5668\u548c\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u5546\u7528GPU\u4e0a\u5b9e\u73b05 MIPS\u4eff\u771f\u901f\u5ea6\uff0c\u4ec50.1%\u6027\u80fd\u5f00\u9500\uff1b\u4e13\u7528Neutrino\u52a0\u901f\u5668\u6027\u80fd\u63d0\u534785\u500d\uff1b\u652f\u6301\u5927\u89c4\u6a21\u786c\u4ef6A/B\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u771f\u5b9e\u5e94\u7528\u73af\u5883\u4e2d\u8fdb\u884c\u51c6\u786e\u6027\u80fd\u5206\u6790\u548c\u5927\u89c4\u6a21\u786c\u4ef6\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u5fae\u5904\u7406\u5668\u8bbe\u8ba1\u6548\u7387\u3002"}}
{"id": "2509.22512", "categories": ["cs.AR", "n/a"], "pdf": "https://arxiv.org/pdf/2509.22512", "abs": "https://arxiv.org/abs/2509.22512", "authors": ["Soroush Ahadi", "Mehdi Modarressi", "Masoud Daneshtalab"], "title": "AxLLM: accelerator architecture for large language models with computation reuse capability", "comment": "7 pages, 9 figures", "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.", "AI": {"tldr": "AxLLM\u662f\u4e00\u79cd\u9488\u5bf9\u91cf\u5316LLM\u7684\u786c\u4ef6\u52a0\u901f\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\uff0c\u53ef\u51cf\u5c1190%\u8ba1\u7b97\u91cf\uff0c\u80fd\u8017\u964d\u4f4e28%\uff0c\u901f\u5ea6\u63d0\u53471.7\u500d", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u90e8\u7f72\u6548\u7387\u9762\u4e34\u6311\u6218\u3002\u91cf\u5316\u4e0d\u4ec5\u80fd\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u8fd8\u80fd\u589e\u52a0\u53c2\u6570\u5c40\u90e8\u6027\uff0c\u4e3a\u8ba1\u7b97\u91cd\u7528\u521b\u9020\u673a\u4f1a", "method": "\u63d0\u51faAxLLM\u786c\u4ef6\u52a0\u901f\u5668\u67b6\u6784\uff0c\u91c7\u7528\u65b0\u9896\u7684\u5197\u4f59\u6d88\u9664\u6280\u672f\uff0c\u7f13\u5b58\u5e76\u91cd\u7528\u91cd\u590d\u6743\u91cd\u503c\u7684\u4e58\u6cd5\u7ed3\u679c\u3002\u67b6\u6784\u5177\u6709\u53cc\u4e58\u6cd5\u548c\u91cd\u7528\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u57fa\u7840\u6a21\u578b\u548cLoRA\u5fae\u8c03\u6a21\u578b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aAxLLM\u53ef\u51cf\u5c11\u9ad8\u8fbe90%\u7684\u8ba1\u7b97\u91cf\uff0c\u80fd\u8017\u964d\u4f4e28%\uff0c\u901f\u5ea6\u6bd4\u57fa\u7ebf\u6267\u884c\u5feb1.7\u500d", "conclusion": "AxLLM\u662f\u4e13\u95e8\u786c\u4ef6\u4e0a\u52a0\u901fLLM\u7684\u53ef\u6269\u5c55\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u6539\u53d8\u53c2\u6570\u3001\u91cd\u65b0\u8bad\u7ec3\u6216\u79bb\u7ebf\u9884\u5904\u7406"}}
