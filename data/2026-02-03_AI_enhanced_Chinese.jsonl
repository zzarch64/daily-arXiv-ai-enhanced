{"id": "2602.00330", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00330", "abs": "https://arxiv.org/abs/2602.00330", "authors": ["Sheldon X. -D. Tan", "Haotian Lu"], "title": "Accelerating Physics-Based Electromigration Analysis via Rational Krylov Subspaces", "comment": null, "summary": "Electromigration (EM) induced stress evolution is a major reliability challenge in nanometer-scale VLSI interconnects. Accurate EM analysis requires solving stress-governing partial differential equations over large interconnect trees, which is computationally expensive using conventional finite-difference methods. This work proposes two fast EM stress analysis techniques based on rational Krylov subspace reduction. Unlike traditional Krylov methods that expand around zero frequency, rational Krylov methods enable expansion at selected time constants, aligning directly with metrics such as nucleation and steady-state times and producing compact reduced models with minimal accuracy loss. Two complementary frameworks are developed: a frequency-domain extended rational Krylov method, ExtRaKrylovEM, and a time-domain rational Krylov exponential integration method, EiRaKrylovEM. We show that the accuracy of both methods depends strongly on the choice of expansion point, or shift time, and demonstrate that effective shift times are typically close to times of interest such as nucleation or post-void steady state. Based on this observation, a coordinate descent optimization strategy is introduced to automatically determine optimal reduction orders and shift times for both nucleation and post-void phases. Experimental results on synthesized structures and industry-scale power grids show that the proposed methods achieve orders-of-magnitude improvements in efficiency and accuracy over finite-difference solutions. Using only 4 to 6 Krylov orders, the methods achieve sub-0.1 percent error in nucleation time and resistance change predictions while delivering 20 to 500 times speedup. In contrast, standard extended Krylov methods require more than 50 orders and still incur 10 to 20 percent nucleation time error, limiting their practicality for EM-aware optimization and stochastic EM analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6709\u7406Krylov\u5b50\u7a7a\u95f4\u964d\u9636\u7684\u5feb\u901f\u7535\u8fc1\u79fb\u5e94\u529b\u5206\u6790\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6cd5\u5b9e\u73b020-500\u500d\u52a0\u901f\uff0c\u4ec5\u97004-6\u9636\u5373\u53ef\u8fbe\u5230\u4e9a0.1%\u8bef\u5dee\u3002", "motivation": "\u7535\u8fc1\u79fb\u5f15\u8d77\u7684\u5e94\u529b\u6f14\u5316\u662f\u7eb3\u7c73\u7ea7VLSI\u4e92\u8fde\u7684\u4e3b\u8981\u53ef\u9760\u6027\u6311\u6218\u3002\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6cd5\u6c42\u89e3\u5927\u89c4\u6a21\u4e92\u8fde\u6811\u4e0a\u7684\u5e94\u529b\u63a7\u5236\u504f\u5fae\u5206\u65b9\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5feb\u901f\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6709\u7406Krylov\u5b50\u7a7a\u95f4\u964d\u9636\u7684\u6846\u67b6\uff1a\u9891\u57df\u6269\u5c55\u6709\u7406Krylov\u65b9\u6cd5(ExtRaKrylovEM)\u548c\u65f6\u57df\u6709\u7406Krylov\u6307\u6570\u79ef\u5206\u65b9\u6cd5(EiRaKrylovEM)\u3002\u901a\u8fc7\u5750\u6807\u4e0b\u964d\u4f18\u5316\u7b56\u7565\u81ea\u52a8\u786e\u5b9a\u6700\u4f18\u964d\u9636\u9636\u6570\u548c\u79fb\u4f4d\u65f6\u95f4\u3002", "result": "\u5728\u5408\u6210\u7ed3\u6784\u548c\u5de5\u4e1a\u7ea7\u7535\u6e90\u7f51\u7edc\u4e0a\uff0c\u4ec5\u97004-6\u9636Krylov\u964d\u9636\u5373\u53ef\u5b9e\u73b0\u4e9a0.1%\u7684\u6210\u6838\u65f6\u95f4\u548c\u7535\u963b\u53d8\u5316\u9884\u6d4b\u8bef\u5dee\uff0c\u83b7\u5f9720-500\u500d\u7684\u52a0\u901f\u3002\u800c\u6807\u51c6\u6269\u5c55Krylov\u65b9\u6cd5\u9700\u898150\u591a\u9636\u4e14\u4ecd\u670910-20%\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u6709\u7406Krylov\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7535\u8fc1\u79fb\u5e94\u529b\u5206\u6790\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u4e3a\u7535\u8fc1\u79fb\u611f\u77e5\u4f18\u5316\u548c\u968f\u673a\u7535\u8fc1\u79fb\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2602.00342", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00342", "abs": "https://arxiv.org/abs/2602.00342", "authors": ["Rafi Zahedi", "Amirhossein Ahmadian", "Chen Zhang", "Shashank Narayana Gowda", "Kourosh SedghiSigarchi", "Rajit Gadh"], "title": "Optimal Engagement of Residential Battery Storage to Alleviate Grid Upgrades Caused by EVs and Solar Systems", "comment": "Published at Advances in Science, Technology and Engineering Systems Journal (ASTESJ)- https://www.astesj.com/v09/i02/p01/", "summary": "The integration of distributed energy resources has ushered in a host of complex challenges, significantly impacting power quality in distribution networks. This work studies these challenges, exploring issues such as voltage fluctuations and escalating power losses caused by the integration of solar systems and electric vehicle (EV) chargers. We present a robust methodology focused on mitigating voltage deviations and power losses, emphasizing the allocation of a Permitted Percentage (PP) of battery-based solar systems within residential areas endowed with storage capabilities. A key facet of this research lies in its adaptability to the changing landscape of electric transportation. With the rapid increase of electric trucks on the horizon, our proposed model gains relevance. By tactically deploying PP to oversee the charging and discharging of batteries within residential solar systems, utilities are poised not only to assist with grid resilience but also to cater to the upcoming demands spurred by the advent of new EVs, notably trucks. To validate the efficacy of our proposed model, rigorous simulations were conducted using the IEEE 33-bus distribution network as a designed testbed. Leveraging advanced Particle Swarm Optimization techniques, we have deciphered the optimal charging and discharging commands issued by utilities to energy storage systems. The outcomes of these simulations help us understand the transformative potential of various PP allocations, shedding light on the balance between non-battery-based and battery-based solar residences. This research underscores the need for carefully crafted approaches in navigating the complexities of modern grid dynamics amid the anticipated increase in electric vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7535\u6c60\u50a8\u80fd\u592a\u9633\u80fd\u7cfb\u7edf\u7684\u5141\u8bb8\u767e\u5206\u6bd4\u5206\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f13\u89e3\u5206\u5e03\u5f0f\u80fd\u6e90\u63a5\u5165\u5e26\u6765\u7684\u7535\u538b\u6ce2\u52a8\u548c\u529f\u7387\u635f\u8017\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u9700\u6c42\u589e\u957f\u573a\u666f\u3002", "motivation": "\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\uff08\u7279\u522b\u662f\u592a\u9633\u80fd\u7cfb\u7edf\u548c\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u6869\uff09\u7684\u63a5\u5165\u7ed9\u914d\u7535\u7f51\u5e26\u6765\u4e86\u7535\u538b\u6ce2\u52a8\u548c\u529f\u7387\u635f\u8017\u7b49\u7535\u80fd\u8d28\u91cf\u95ee\u9898\uff0c\u968f\u7740\u7535\u52a8\u5361\u8f66\u7b49\u65b0\u578b\u7535\u52a8\u6c7d\u8f66\u7684\u666e\u53ca\uff0c\u8fd9\u4e9b\u95ee\u9898\u5c06\u66f4\u52a0\u4e25\u5cfb\u3002", "method": "\u63d0\u51fa\u5141\u8bb8\u767e\u5206\u6bd4\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u4f4f\u5b85\u533a\u7535\u6c60\u50a8\u80fd\u592a\u9633\u80fd\u7cfb\u7edf\u7684\u5145\u653e\u7535\u6765\u8c03\u8282\u7535\u7f51\u3002\u4f7f\u7528IEEE 33\u8282\u70b9\u914d\u7535\u7f51\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u91c7\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u786e\u5b9a\u6700\u4f18\u5145\u653e\u7535\u6307\u4ee4\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5408\u7406\u5206\u914d\u7535\u6c60\u50a8\u80fd\u592a\u9633\u80fd\u7cfb\u7edf\u7684\u5141\u8bb8\u767e\u5206\u6bd4\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u7535\u538b\u504f\u5dee\u548c\u529f\u7387\u635f\u8017\uff0c\u5e73\u8861\u65e0\u7535\u6c60\u548c\u6709\u7535\u6c60\u592a\u9633\u80fd\u4f4f\u5b85\u7684\u9700\u6c42\uff0c\u63d0\u5347\u7535\u7f51\u97e7\u6027\u3002", "conclusion": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u7684\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63a7\u5236\u7b56\u7565\u6765\u5e94\u5bf9\u73b0\u4ee3\u7535\u7f51\u52a8\u6001\u590d\u6742\u6027\uff0c\u7535\u6c60\u50a8\u80fd\u592a\u9633\u80fd\u7cfb\u7edf\u7684\u667a\u80fd\u5145\u653e\u7535\u7ba1\u7406\u662f\u63d0\u5347\u7535\u7f51\u97e7\u6027\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.00803", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00803", "abs": "https://arxiv.org/abs/2602.00803", "authors": ["Seungkwan Kang", "Seungjun Lee", "Donghyun Gouk", "Miryeong Kwon", "Hyunkyu Choi", "Junhyeok Jang", "Sangwon Lee", "Huiwon Choi", "Jie Zhang", "Wonil Choi", "Mahmut Taylan Kandemir", "Myoungsoo Jung"], "title": "AutoGNN: End-to-End Hardware-Driven Graph Preprocessing for Enhanced GNN Performance", "comment": null, "summary": "Graph neural network (GNN) inference faces significant bottlenecks in preprocessing, which often dominate overall inference latency. We introduce AutoGNN, an FPGA-based accelerator designed to address these challenges by leveraging FPGA's reconfigurability and specialized components. AutoGNN adapts to diverse graph inputs, efficiently performing computationally intensive tasks such as graph conversion and sampling. By utilizing components like adder trees, AutoGNN executes reduction operations in constant time, overcoming the limitations of serialization and synchronization on GPUs.\n  AutoGNN integrates unified processing elements (UPEs) and single-cycle reducers (SCRs) to streamline GNN preprocessing. UPEs enable scalable parallel processing for edge sorting and unique vertex selection, while SCRs efficiently handle sequential tasks such as pointer array construction and subgraph reindexing. A user-level software framework dynamically profiles graph inputs, determines optimal configurations, and reprograms AutoGNN to handle varying workloads. Implemented on a 7$n$m enterprise FPGA, AutoGNN achieves up to 9.0$\\times$ and 2.1$\\times$ speedup compared to conventional and GPU-accelerated preprocessing systems, respectively, enabling high-performance GNN preprocessing across diverse datasets.", "AI": {"tldr": "AutoGNN\u662f\u4e00\u4e2a\u57fa\u4e8eFPGA\u7684GNN\u9884\u5904\u7406\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u67b6\u6784\u548c\u4e13\u7528\u7ec4\u4ef6\u89e3\u51b3GNN\u63a8\u7406\u4e2d\u7684\u9884\u5904\u7406\u74f6\u9888\uff0c\u76f8\u6bd4\u4f20\u7edf\u548cGPU\u52a0\u901f\u7cfb\u7edf\u5206\u522b\u5b9e\u73b09.0\u500d\u548c2.1\u500d\u7684\u52a0\u901f\u3002", "motivation": "GNN\u63a8\u7406\u4e2d\u7684\u9884\u5904\u7406\u9636\u6bb5\uff08\u5982\u56fe\u8f6c\u6362\u3001\u91c7\u6837\u7b49\uff09\u901a\u5e38\u5360\u636e\u6574\u4f53\u63a8\u7406\u5ef6\u8fdf\u7684\u4e3b\u8981\u90e8\u5206\uff0c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002\u73b0\u6709GPU\u65b9\u6848\u5b58\u5728\u5e8f\u5217\u5316\u548c\u540c\u6b65\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u5668\u3002", "method": "\u91c7\u7528FPGA\u53ef\u91cd\u6784\u67b6\u6784\uff0c\u8bbe\u8ba1\u7edf\u4e00\u5904\u7406\u5355\u5143\uff08UPEs\uff09\u7528\u4e8e\u5e76\u884c\u5904\u7406\u8fb9\u6392\u5e8f\u548c\u9876\u70b9\u9009\u62e9\uff0c\u5355\u5468\u671f\u5f52\u7ea6\u5668\uff08SCRs\uff09\u5904\u7406\u6307\u9488\u6570\u7ec4\u6784\u5efa\u548c\u5b50\u56fe\u91cd\u7d22\u5f15\u3002\u7528\u6237\u7ea7\u8f6f\u4ef6\u6846\u67b6\u52a8\u6001\u5206\u6790\u56fe\u8f93\u5165\u7279\u5f81\uff0c\u786e\u5b9a\u6700\u4f18\u914d\u7f6e\u5e76\u91cd\u7f16\u7a0bFPGA\u3002", "result": "\u57287nm\u4f01\u4e1a\u7ea7FPGA\u4e0a\u5b9e\u73b0\uff0c\u76f8\u6bd4\u4f20\u7edf\u9884\u5904\u7406\u7cfb\u7edf\u5b9e\u73b09.0\u500d\u52a0\u901f\uff0c\u76f8\u6bd4GPU\u52a0\u901f\u7cfb\u7edf\u5b9e\u73b02.1\u500d\u52a0\u901f\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u591a\u6837\u5316\u7684\u56fe\u6570\u636e\u96c6\u3002", "conclusion": "AutoGNN\u901a\u8fc7FPGA\u53ef\u91cd\u6784\u6027\u548c\u4e13\u7528\u786c\u4ef6\u7ec4\u4ef6\u6709\u6548\u89e3\u51b3\u4e86GNN\u9884\u5904\u7406\u74f6\u9888\uff0c\u4e3a\u9ad8\u6027\u80fdGNN\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00838", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00838", "abs": "https://arxiv.org/abs/2602.00838", "authors": ["Prabhu Vellaisamy", "Harideep Nair", "Di Wu", "Shawn Blanton", "John Paul Shen"], "title": "Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators", "comment": null, "summary": "General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM designs is needed to assess the potential of unary hardware for future DL compute. This paper focuses on unary GEMM designs for integer-based DL inference and performs a detailed evaluation of three latest unary design proposals, namely, uGEMM, tuGEMM and tubGEMM, by comparing them to a conventional binary GEMM. Rigorous post-synthesis evaluations beyond prior works are performed across varying bit-widths and matrix sizes to assess the designs' tradeoffs and determine optimal sweetspots. Further, we perform weight sparsity analysis across eight pretrained convolutional neural networks (CNNs) and the LLaMA2 large language model (LLM). In this work, we demonstrate how unary GEMM can be effectively used for energy-efficient compute in future edge AI accelerators.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e09\u79cd\u6700\u65b0\u7684\u5355\u7cbe\u5ea6GEMM\u8bbe\u8ba1\uff08uGEMM\u3001tuGEMM\u3001tubGEMM\uff09\u4e0e\u4f20\u7edf\u7684\u4e8c\u8fdb\u5236GEMM\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5355\u7cbe\u5ea6GEMM\u5728\u672a\u6765\u8fb9\u7f18AI\u52a0\u901f\u5668\u4e2d\u5b9e\u73b0\u9ad8\u6548\u80fd\u8ba1\u7b97\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5411\u4f4e\u7cbe\u5ea6\u53d1\u5c55\uff0c\u9700\u8981\u8bc4\u4f30\u65b0\u578b\u5355\u7cbe\u5ea6GEMM\u8bbe\u8ba1\u4f5c\u4e3a\u4f20\u7edf\u4e8c\u8fdb\u5236GEMM\u786c\u4ef6\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765DL\u8ba1\u7b97\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u5bf9\u4e09\u79cd\u6700\u65b0\u5355\u7cbe\u5ea6GEMM\u8bbe\u8ba1\uff08uGEMM\u3001tuGEMM\u3001tubGEMM\uff09\u4e0e\u5e38\u89c4\u4e8c\u8fdb\u5236GEMM\u8fdb\u884c\u8be6\u7ec6\u8bc4\u4f30\uff0c\u5305\u62ec\u4e0d\u540c\u4f4d\u5bbd\u548c\u77e9\u9635\u5c3a\u5bf8\u4e0b\u7684\u7efc\u5408\u540e\u8bc4\u4f30\uff0c\u5e76\u5bf98\u4e2a\u9884\u8bad\u7ec3CNN\u548cLLaMA2\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6743\u91cd\u7a00\u758f\u6027\u5206\u6790\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u8bc4\u4f30\u786e\u5b9a\u4e86\u5404\u79cd\u8bbe\u8ba1\u7684\u6743\u8861\u70b9\u548c\u6700\u4f18\u5de5\u4f5c\u70b9\uff0c\u5c55\u793a\u4e86\u5355\u7cbe\u5ea6GEMM\u5728\u80fd\u91cf\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u5355\u7cbe\u5ea6GEMM\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u672a\u6765\u8fb9\u7f18AI\u52a0\u901f\u5668\u7684\u9ad8\u6548\u80fd\u8ba1\u7b97\uff0c\u4e3a\u4f4e\u7cbe\u5ea6\u6df1\u5ea6\u5b66\u4e60\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2602.00909", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00909", "abs": "https://arxiv.org/abs/2602.00909", "authors": ["Rafael Billig Tonetto", "Marcello Traiola", "Fernando Fernandes dos Santos", "Angeliki Kritikakou"], "title": "ENFOR-SA: End-to-end Cross-layer Transient Fault Injector for Efficient and Accurate DNN Reliability Assessment on Systolic Arrays", "comment": null, "summary": "Recent advances in deep learning have produced highly accurate but increasingly large and complex DNNs, making traditional fault-injection techniques impractical. Accurate fault analysis requires RTL-accurate hardware models. However, this significantly slows evaluation compared with software-only approaches, particularly when combined with expensive HDL instrumentation. In this work, we show that such high-overhead methods are unnecessary for systolic array (SA) architectures and propose ENFOR-SA, an end-to-end framework for DNN transient fault analysis on SAs. Our two-step approach employs cross-layer simulation and uses RTL SA components only during fault injection, with the rest executed at the software level. Experiments on CNNs and Vision Transformers demonstrate that ENFOR-SA achieves RTL-accurate fault injection with only 6% average slowdown compared to software-based injection, while delivering at least two orders of magnitude speedup (average $569\\times$) over full-SoC RTL simulation and a $2.03\\times$ improvement over a state-of-the-art cross-layer RTL injection tool. ENFOR-SA code is publicly available at https://github.com/rafaabt/ENFOR-SA.", "AI": {"tldr": "ENFOR-SA\uff1a\u9488\u5bf9\u8109\u52a8\u9635\u5217\u67b6\u6784\u7684\u7aef\u5230\u7aefDNN\u77ac\u6001\u6545\u969c\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5c42\u4eff\u771f\u5b9e\u73b0RTL\u7ea7\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u8f6f\u4ef6\u6ce8\u5165\u4ec56%\u6027\u80fd\u635f\u5931\uff0c\u76f8\u6bd4\u5168SoC RTL\u4eff\u771f\u52a0\u901f569\u500d\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u5927\u4e14\u590d\u6742\uff0c\u4f20\u7edf\u6545\u969c\u6ce8\u5165\u6280\u672f\u4e0d\u5b9e\u7528\u3002\u7cbe\u786e\u6545\u969c\u5206\u6790\u9700\u8981RTL\u7ea7\u786c\u4ef6\u6a21\u578b\uff0c\u4f46\u8fd9\u4f1a\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u901f\u5ea6\uff0c\u7279\u522b\u662f\u7ed3\u5408\u6602\u8d35\u7684HDL\u4eea\u5668\u65f6\u3002\u5bf9\u4e8e\u8109\u52a8\u9635\u5217\u67b6\u6784\uff0c\u8fd9\u79cd\u9ad8\u5f00\u9500\u65b9\u6cd5\u662f\u4e0d\u5fc5\u8981\u7684\u3002", "method": "\u63d0\u51faENFOR-SA\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u8de8\u5c42\u4eff\u771f\uff0c\u4ec5\u5728\u6545\u969c\u6ce8\u5165\u65f6\u4f7f\u7528RTL\u8109\u52a8\u9635\u5217\u7ec4\u4ef6\uff0c\u5176\u4f59\u90e8\u5206\u5728\u8f6f\u4ef6\u5c42\u9762\u6267\u884c\u3002", "result": "\u5728CNN\u548cVision Transformer\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cENFOR-SA\u5b9e\u73b0RTL\u7ea7\u7cbe\u5ea6\u6545\u969c\u6ce8\u5165\uff0c\u76f8\u6bd4\u8f6f\u4ef6\u6ce8\u5165\u4ec56%\u5e73\u5747\u6027\u80fd\u635f\u5931\uff0c\u76f8\u6bd4\u5168SoC RTL\u4eff\u771f\u52a0\u901f569\u500d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8de8\u5c42RTL\u6ce8\u5165\u5de5\u5177\u63d0\u53472.03\u500d\u3002", "conclusion": "ENFOR-SA\u4e3a\u8109\u52a8\u9635\u5217\u67b6\u6784\u63d0\u4f9b\u9ad8\u6548\u51c6\u786e\u7684DNN\u6545\u969c\u5206\u6790\u6846\u67b6\uff0c\u8bc1\u660e\u9ad8\u5f00\u9500\u65b9\u6cd5\u4e0d\u5fc5\u8981\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.01546", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.01546", "abs": "https://arxiv.org/abs/2602.01546", "authors": ["Shanmuga Venkatachalam", "Prabhu Vellaisamy", "Harideep Nair", "Wei-Che Huang", "Youngseok Na", "Yuyang Kang", "Quinn Jacobson", "John Paul Shen"], "title": "NeuroAI Temporal Neural Networks (NeuTNNs): Microarchitecture and Design Framework for Specialized Neuromorphic Processing Units", "comment": null, "summary": "Leading experts from both communities have suggested the need to (re)connect research in neuroscience and artificial intelligence (AI) to accelerate the development of next-generation AI innovations. They term this convergence as NeuroAI. Previous research has established temporal neural networks (TNNs) as a promising neuromorphic approach toward biological intelligence and efficiency. We fully embrace NeuroAI and propose a new category of TNNs we call NeuroAI TNNs (NeuTNNs) with greater capability and hardware efficiency by adopting neuroscience findings, including a neuron model with active dendrites and a hierarchy of distal and proximal segments. This work introduces a PyTorch-to-layout tool suite (NeuTNNGen) to design application-specific NeuTNNs. Compared to previous TNN designs, NeuTNNs achieve superior performance and efficiency. We demonstrate NeuTNNGen's capabilities using three example applications: 1) UCR time series benchmarks, 2) MNIST design exploration, and 3) Place Cells design for neocortical reference frames. We also explore using synaptic pruning to further reduce synapse counts and hardware costs by 30-50% while maintaining model precision across diverse sensory modalities. NeuTNNGen can facilitate the design of application-specific energy-efficient NeuTNNs for the next generation of NeuroAI computing systems.", "AI": {"tldr": "\u63d0\u51faNeuroAI TNNs (NeuTNNs) - \u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\u7684\u65b0\u578b\u65f6\u5e8f\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7PyTorch-to-layout\u5de5\u5177\u5957\u4ef6(NeuTNNGen)\u5b9e\u73b0\u5e94\u7528\u7279\u5b9a\u8bbe\u8ba1\uff0c\u5728\u6027\u80fd\u548c\u786c\u4ef6\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edfTNNs\u3002", "motivation": "\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u878d\u5408(NeuroAI)\u88ab\u8ba4\u4e3a\u662f\u63a8\u52a8\u4e0b\u4e00\u4ee3AI\u521b\u65b0\u7684\u5173\u952e\u3002\u73b0\u6709\u65f6\u5e8f\u795e\u7ecf\u7f51\u7edc(TNNs)\u867d\u7136\u662f\u6709\u524d\u666f\u7684\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\u6765\u63d0\u5347\u80fd\u529b\u548c\u786c\u4ef6\u6548\u7387\u3002", "method": "\u63d0\u51faNeuroAI TNNs (NeuTNNs)\uff0c\u91c7\u7528\u5305\u542b\u4e3b\u52a8\u6811\u7a81\u548c\u8fdc\u7aef/\u8fd1\u7aef\u6bb5\u5c42\u6b21\u7ed3\u6784\u7684\u795e\u7ecf\u5143\u6a21\u578b\u3002\u5f00\u53d1NeuTNNGen\u5de5\u5177\u5957\u4ef6\uff0c\u5c06PyTorch\u6a21\u578b\u8f6c\u6362\u4e3a\u786c\u4ef6\u5e03\u5c40\uff0c\u652f\u6301\u5e94\u7528\u7279\u5b9a\u8bbe\u8ba1\u3002\u901a\u8fc7\u7a81\u89e6\u526a\u679d\u6280\u672f\u8fdb\u4e00\u6b65\u51cf\u5c1130-50%\u7684\u7a81\u89e6\u6570\u91cf\u3002", "result": "NeuTNNs\u5728UCR\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u3001MNIST\u8bbe\u8ba1\u63a2\u7d22\u548c\u4f4d\u7f6e\u7ec6\u80de\u8bbe\u8ba1\u4e09\u4e2a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u548c\u6548\u7387\u3002\u7a81\u89e6\u526a\u679d\u80fd\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u3002", "conclusion": "NeuTNNGen\u5de5\u5177\u5957\u4ef6\u80fd\u591f\u4fc3\u8fdb\u5e94\u7528\u7279\u5b9a\u3001\u9ad8\u80fd\u6548\u7684NeuTNNs\u8bbe\u8ba1\uff0c\u4e3a\u4e0b\u4e00\u4ee3NeuroAI\u8ba1\u7b97\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u652f\u6301\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u878d\u5408\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2602.01827", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.01827", "abs": "https://arxiv.org/abs/2602.01827", "authors": ["Tommaso Spagnolo", "Cristina Silvano", "Riccardo Massa", "Filippo Grillotti", "Thomas Boesch", "Giuseppe Desoli"], "title": "In-Pipeline Integration of Digital In-Memory-Computing into RISC-V Vector Architecture to Accelerate Deep Learning", "comment": null, "summary": "Expanding Deep Learning applications toward edge computing demands architectures capable of delivering high computational performance and efficiency while adhering to tight power and memory constraints. Digital In-Memory Computing (DIMC) addresses this need by moving part of the computation directly within memory arrays, significantly reducing data movement and improving energy efficiency. This paper introduces a novel architecture that extends the Vector RISC-V Instruction Set Architecture (ISA) to integrate a tightly coupled DIMC unit directly into the execution stage of the pipeline, to accelerate Deep Learning inference at the edge. Specifically, the proposed approach adds four custom instructions dedicated to data loading, computation, and write-back, enabling flexible and optimal control of the inference execution on the target architecture. Experimental results demonstrate high utilization of the DIMC tile in Vector RISC-V and sustained throughput across the ResNet-50 model, achieving a peak performance of 137 GOP/s. The proposed architecture achieves a speedup of 217x over the baseline core and 50x area-normalized speedup even when operating near the hardware resource limits. The experimental results confirm the high potential of the proposed architecture as a scalable and efficient solution to accelerate Deep Learning inference on the edge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u5355\u5143\u7d27\u5bc6\u96c6\u6210\u5230\u5411\u91cfRISC-V ISA\u4e2d\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u6307\u4ee4\u52a0\u901f\u8fb9\u7f18\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\uff0c\u5b9e\u73b0217\u500d\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u9700\u8981\u9ad8\u6027\u80fd\u3001\u9ad8\u6548\u7387\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4f46\u9762\u4e34\u4e25\u683c\u7684\u529f\u8017\u548c\u5185\u5b58\u9650\u5236\u3002\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u901a\u8fc7\u5c06\u8ba1\u7b97\u79fb\u81f3\u5185\u5b58\u9635\u5217\u6765\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u3001\u63d0\u9ad8\u80fd\u6548\u3002", "method": "\u6269\u5c55\u5411\u91cfRISC-V ISA\uff0c\u5c06DIMC\u5355\u5143\u7d27\u5bc6\u96c6\u6210\u5230\u6d41\u6c34\u7ebf\u6267\u884c\u9636\u6bb5\uff0c\u6dfb\u52a0\u56db\u4e2a\u81ea\u5b9a\u4e49\u6307\u4ee4\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u3001\u8ba1\u7b97\u548c\u5199\u56de\uff0c\u5b9e\u73b0\u5bf9\u63a8\u7406\u6267\u884c\u7684\u7075\u6d3b\u4f18\u5316\u63a7\u5236\u3002", "result": "DIMC tile\u5229\u7528\u7387\u9ad8\uff0c\u5728ResNet-50\u6a21\u578b\u4e0a\u4fdd\u6301\u7a33\u5b9a\u541e\u5410\u91cf\uff0c\u5cf0\u503c\u6027\u80fd\u8fbe137 GOP/s\u3002\u76f8\u6bd4\u57fa\u7ebf\u6838\u5fc3\u5b9e\u73b0217\u500d\u52a0\u901f\uff0c\u5728\u786c\u4ef6\u8d44\u6e90\u9650\u5236\u4e0b\u4ecd\u5b9e\u73b050\u500d\u9762\u79ef\u5f52\u4e00\u5316\u52a0\u901f\u3002", "conclusion": "\u8be5\u67b6\u6784\u5c55\u793a\u4e86\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u52a0\u901f\u8fb9\u7f18\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u9a8c\u8bc1\u4e86DIMC\u4e0e\u5411\u91cfRISC-V\u7d27\u5bc6\u96c6\u6210\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.02005", "categories": ["cs.AR", "cs.LG", "eess.SY", "hep-ex", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02005", "abs": "https://arxiv.org/abs/2602.02005", "authors": ["Duc Hoang"], "title": "Position: The Need for Ultrafast Training", "comment": "Position paper at the 2nd Workshop on Domain-Specialized FPGAs (WDSFPGA 2026)", "summary": "Domain-specialized FPGAs have delivered unprecedented performance for low-latency inference across scientific and industrial workloads, yet nearly all existing accelerators assume static models trained offline, relegating learning and adaptation to slower CPUs or GPUs. This separation fundamentally limits systems that must operate in non-stationary, high-frequency environments, where model updates must occur at the timescale of the underlying physics. In this paper, I argue for a shift from inference-only accelerators to ultrafast on-chip learning, in which both inference and training execute directly within the FPGA fabric under deterministic, sub-microsecond latency constraints. Bringing learning into the same real-time datapath as inference would enable closed-loop systems that adapt as fast as the physical processes they control, with applications spanning quantum error correction, cryogenic qubit calibration, plasma and fusion control, accelerator tuning, and autonomous scientific experiments. Enabling such regimes requires rethinking algorithms, architectures, and toolflows jointly, but promises to transform FPGAs from static inference engines into real-time learning machines.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u4ece\u4ec5\u63a8\u7406\u7684FPGA\u52a0\u901f\u5668\u8f6c\u5411\u8d85\u5feb\u901f\u7247\u4e0a\u5b66\u4e60\uff0c\u4f7f\u63a8\u7406\u548c\u8bad\u7ec3\u90fd\u80fd\u5728FPGA\u67b6\u6784\u4e2d\u4ee5\u4e9a\u5fae\u79d2\u5ef6\u8fdf\u6267\u884c\uff0c\u5b9e\u73b0\u4e0e\u7269\u7406\u8fc7\u7a0b\u540c\u6b65\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u9886\u57df\u4e13\u7528FPGA\u4e3b\u8981\u4e13\u6ce8\u4e8e\u9759\u6001\u6a21\u578b\u7684\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u5c06\u5b66\u4e60\u548c\u9002\u5e94\u4efb\u52a1\u4ea4\u7ed9\u8f83\u6162\u7684CPU/GPU\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u975e\u5e73\u7a33\u3001\u9ad8\u9891\u73af\u5883\u4e2d\u9700\u8981\u6a21\u578b\u66f4\u65b0\u4e0e\u7269\u7406\u8fc7\u7a0b\u540c\u6b65\u7684\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u91cd\u65b0\u601d\u8003\u7b97\u6cd5\u3001\u67b6\u6784\u548c\u5de5\u5177\u6d41\u7a0b\u7684\u8054\u5408\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06\u5b66\u4e60\u529f\u80fd\u96c6\u6210\u5230\u4e0e\u63a8\u7406\u76f8\u540c\u7684\u5b9e\u65f6\u6570\u636e\u8def\u5f84\u4e2d\uff0c\u5b9e\u73b0FPGA\u4e0a\u7684\u786e\u5b9a\u6027\u4e9a\u5fae\u79d2\u5ef6\u8fdf\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5c06FPGA\u4ece\u9759\u6001\u63a8\u7406\u5f15\u64ce\u8f6c\u53d8\u4e3a\u5b9e\u65f6\u5b66\u4e60\u673a\u5668\uff0c\u80fd\u591f\u5b9e\u73b0\u4e0e\u7269\u7406\u8fc7\u7a0b\u540c\u6b65\u7684\u95ed\u73af\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u91cf\u5b50\u7ea0\u9519\u3001\u4f4e\u6e29\u91cf\u5b50\u6bd4\u7279\u6821\u51c6\u3001\u7b49\u79bb\u5b50\u4f53\u4e0e\u805a\u53d8\u63a7\u5236\u3001\u52a0\u901f\u5668\u8c03\u8c10\u548c\u81ea\u4e3b\u79d1\u5b66\u5b9e\u9a8c\u7b49\u9886\u57df\u3002", "conclusion": "\u9700\u8981\u4ece\u4ec5\u63a8\u7406\u52a0\u901f\u5668\u8f6c\u5411\u8d85\u5feb\u901f\u7247\u4e0a\u5b66\u4e60\uff0c\u901a\u8fc7\u7b97\u6cd5\u3001\u67b6\u6784\u548c\u5de5\u5177\u6d41\u7a0b\u7684\u534f\u540c\u91cd\u6784\uff0c\u4f7fFPGA\u6210\u4e3a\u80fd\u591f\u5b9e\u65f6\u5b66\u4e60\u548c\u9002\u5e94\u7684\u8ba1\u7b97\u5e73\u53f0\uff0c\u5f00\u542f\u5b9e\u65f6\u81ea\u9002\u5e94\u7cfb\u7edf\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.02056", "categories": ["cs.AR", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02056", "abs": "https://arxiv.org/abs/2602.02056", "authors": ["Duc Hoang", "Aarush Gupta", "Philip Harris"], "title": "Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks", "comment": null, "summary": "Ultrafast online learning is essential for high-frequency systems, such as controls for quantum computing and nuclear fusion, where adaptation must occur on sub-microsecond timescales. Meeting these requirements demands low-latency, fixed-precision computation under strict memory constraints, a regime in which conventional Multi-Layer Perceptrons (MLPs) are both inefficient and numerically unstable. We identify key properties of Kolmogorov-Arnold Networks (KANs) that align with these constraints. Specifically, we show that: (i) KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling, and (ii) KANs are inherently robust to fixed-point quantization. By implementing fixed-point online training on Field-Programmable Gate Arrays (FPGAs), a representative platform for on-chip computation, we demonstrate that KAN-based online learners are significantly more efficient and expressive than MLPs across a range of low-latency and resource-constrained tasks. To our knowledge, this work is the first to demonstrate model-free online learning at sub-microsecond latencies.", "AI": {"tldr": "KANs\u5728FPGA\u4e0a\u5b9e\u73b0\u4e9a\u5fae\u79d2\u7ea7\u5728\u7ebf\u5b66\u4e60\uff0c\u6bd4MLPs\u66f4\u9ad8\u6548\u3001\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\uff0c\u9002\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u548c\u6838\u805a\u53d8\u7b49\u9ad8\u9891\u7387\u7cfb\u7edf\u63a7\u5236\u3002", "motivation": "\u9ad8\u9891\u7387\u7cfb\u7edf\uff08\u5982\u91cf\u5b50\u8ba1\u7b97\u548c\u6838\u805a\u53d8\u63a7\u5236\uff09\u9700\u8981\u4e9a\u5fae\u79d2\u7ea7\u7684\u8d85\u5feb\u901f\u5728\u7ebf\u5b66\u4e60\uff0c\u4f20\u7edfMLPs\u5728\u4f4e\u5ef6\u8fdf\u3001\u56fa\u5b9a\u7cbe\u5ea6\u8ba1\u7b97\u548c\u4e25\u683c\u5185\u5b58\u7ea6\u675f\u4e0b\u6548\u7387\u4f4e\u4e0b\u4e14\u6570\u503c\u4e0d\u7a33\u5b9a\u3002", "method": "\u5229\u7528KANs\u7684B\u6837\u6761\u5c40\u90e8\u6027\u5b9e\u73b0\u7a00\u758f\u66f4\u65b0\uff0c\u5c55\u793a\u5176\u5bf9\u5b9a\u70b9\u91cf\u5316\u7684\u56fa\u6709\u9c81\u68d2\u6027\uff0c\u5728FPGA\u4e0a\u5b9e\u73b0\u5b9a\u70b9\u5728\u7ebf\u8bad\u7ec3\u3002", "result": "KAN-based\u5728\u7ebf\u5b66\u4e60\u5668\u5728\u4f4e\u5ef6\u8fdf\u548c\u8d44\u6e90\u53d7\u9650\u4efb\u52a1\u4e2d\u6bd4MLPs\u663e\u8457\u66f4\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\uff0c\u9996\u6b21\u5b9e\u73b0\u4e9a\u5fae\u79d2\u5ef6\u8fdf\u7684\u65e0\u6a21\u578b\u5728\u7ebf\u5b66\u4e60\u3002", "conclusion": "KANs\u7279\u522b\u9002\u5408\u9ad8\u9891\u7387\u7cfb\u7edf\u7684\u8d85\u5feb\u901f\u5728\u7ebf\u5b66\u4e60\uff0c\u5176\u7a00\u758f\u66f4\u65b0\u7279\u6027\u548c\u91cf\u5316\u9c81\u68d2\u6027\u4f7f\u5176\u5728FPGA\u7b49\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.02119", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.02119", "abs": "https://arxiv.org/abs/2602.02119", "authors": ["Elio Vinciguerra", "Enrico Russo", "Giuseppe Ascia", "Maurizio Palesi"], "title": "CHAOS: Controlled Hardware fAult injectOr System for gem5", "comment": null, "summary": "Fault injectors are essential tools for evaluating the reliability and resilience of computing systems. They enable the simulation of hardware and software faults to analyze system behavior under error conditions and assess its ability to operate correctly despite disruptions. Such analysis is critical for identifying vulnerabilities and improving system robustness. CHAOS is a modular, open-source, and fully configurable fault injection framework designed for the gem5 simulator. It facilitates precise and systematic fault injection across multiple architectural levels, supporting comprehensive evaluations of fault tolerance mechanisms and resilience strategies. Its high configurability and seamless integration with gem5 allow researchers to explore a wide range of fault models and complex scenarios, making CHAOS a valuable tool for advancing research in dependable and high-performance computing systems.", "AI": {"tldr": "CHAOS\u662f\u4e00\u4e2a\u4e3agem5\u6a21\u62df\u5668\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u3001\u5f00\u6e90\u3001\u53ef\u914d\u7f6e\u7684\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5f39\u6027\u3002", "motivation": "\u6545\u969c\u6ce8\u5165\u5668\u5bf9\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5f39\u6027\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u6a21\u62df\u786c\u4ef6\u548c\u8f6f\u4ef6\u6545\u969c\u6765\u5206\u6790\u7cfb\u7edf\u5728\u9519\u8bef\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u5e72\u6270\u4e0b\u7684\u6b63\u786e\u8fd0\u884c\u80fd\u529b\u3002\u8fd9\u79cd\u5206\u6790\u5bf9\u4e8e\u8bc6\u522b\u6f0f\u6d1e\u548c\u63d0\u9ad8\u7cfb\u7edf\u9c81\u68d2\u6027\u975e\u5e38\u5173\u952e\u3002", "method": "CHAOS\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u5f00\u6e90\u3001\u5b8c\u5168\u53ef\u914d\u7f6e\u7684\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u4e13\u4e3agem5\u6a21\u62df\u5668\u8bbe\u8ba1\u3002\u5b83\u652f\u6301\u8de8\u591a\u4e2a\u67b6\u6784\u7ea7\u522b\u7684\u7cbe\u786e\u548c\u7cfb\u7edf\u5316\u6545\u969c\u6ce8\u5165\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u5bb9\u9519\u673a\u5236\u548c\u5f39\u6027\u7b56\u7565\u3002\u5176\u9ad8\u53ef\u914d\u7f6e\u6027\u548c\u4e0egem5\u7684\u65e0\u7f1d\u96c6\u6210\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u5e7f\u6cdb\u7684\u6545\u969c\u6a21\u578b\u548c\u590d\u6742\u573a\u666f\u3002", "result": "CHAOS\u6846\u67b6\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u80fd\u591f\u8fdb\u884c\u5168\u9762\u7684\u6545\u969c\u5bb9\u5fcd\u673a\u5236\u8bc4\u4f30\u548c\u5f39\u6027\u7b56\u7565\u5206\u6790\uff0c\u4e3a\u53ef\u9760\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002", "conclusion": "CHAOS\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u5f00\u6e90\u3001\u53ef\u914d\u7f6e\u7684\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u4e3agem5\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6545\u969c\u6ce8\u5165\u80fd\u529b\uff0c\u662f\u63a8\u8fdb\u53ef\u9760\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7814\u7a76\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
