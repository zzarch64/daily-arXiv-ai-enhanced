<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Sparsity-Aware Streaming SNN Accelerator with Output-Channel Dataflow for Automatic Modulation Classification](https://arxiv.org/abs/2601.02613)
*Kuilian Yang,Li Zhang,Ahmed M. Eltawil,Khaled Nabil Salama*

Main category: cs.AR

TL;DR: 提出一种面向自动调制分类任务的稀疏感知SNN流式加速器，通过GOAP算法和预计算优化，在FPGA上实现高吞吐量和低功耗


<details>
  <summary>Details</summary>
Motivation: 随着5G/6G和物联网的发展，频谱利用效率需求增加。自动调制分类在认知无线电系统中至关重要，但传统DNN计算和能耗高，SNN虽然能效高但难以同时实现高吞吐和低功耗

Method: 提出稀疏感知输出通道数据流加速器，利用推理时核固定的特性，采用GOAP算法仅计算非零输入-权重交集，将额外或空迭代预计算并嵌入推理数据流，实现完全流水线化、无控制层间执行

Result: 在FPGA上实现，在RadioML 2016数据集上达到23.5 MS/s吞吐量（约是基线的两倍），同时降低动态功耗并保持可比的分类准确率

Conclusion: 该设计展示了在边缘认知无线电系统中实现实时、低功耗部署的强大潜力，解决了传统架构在利用稀疏性和实现高吞吐量之间的权衡问题

Abstract: The rapid advancement of wireless communication technologies, including 5G, emerging 6G networks, and the large-scale deployment of the Internet of Things (IoT), has intensified the need for efficient spectrum utilization. Automatic modulation classification (AMC) plays a vital role in cognitive radio systems by enabling real-time identification of modulation schemes for dynamic spectrum access and interference mitigation. While deep neural networks (DNNs) offer high classification accuracy, their computational and energy demands pose challenges for real-time edge deployment. Spiking neural networks (SNNs), with their event-driven nature, offer inherent energy efficiency, but achieving both high throughput and low power under constrained hardware resources remains challenging. This work proposes a sparsity-aware SNN streaming accelerator optimized for AMC tasks. Unlike traditional systolic arrays that exploit sparsity but suffer from low throughput, or streaming architectures that achieve high throughput but cannot fully utilize input and weight sparsity, our design integrates both advantages. By leveraging the fixed nature of kernels during inference, we apply the gated one-to-all product (GOAP) algorithm to compute only on non-zero input-weight intersections. Extra or empty iterations are precomputed and embedded into the inference dataflow, eliminating dynamic data fetches and enabling fully pipelined, control-free inter-layer execution. Implemented on an FPGA, our sparsity-aware output-channel dataflow streaming (SAOCDS) accelerator achieves 23.5 MS/s (approximately double the baseline throughput) on the RadioML 2016 dataset, while reducing dynamic power and maintaining comparable classification accuracy. These results demonstrate strong potential for real-time, low-power deployment in edge cognitive radio systems.

</details>
