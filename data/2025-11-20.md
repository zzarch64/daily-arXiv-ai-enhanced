<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations](https://arxiv.org/abs/2511.14990)
*Zhuolun Jiang,Songyue Wang,Xiaokun Pei,Tianyue Lu,Mingyu Chen*

Main category: cs.AR

TL;DR: CoroAMU是一个软硬件协同设计的系统，通过编译器优化和硬件支持来提升协程在内存密集型应用中的性能，特别是在分解式内存系统中显著降低内存延迟影响。


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用面临内存延迟挑战，特别是在分解式内存系统中。现有协程方法在平衡延迟隐藏效率和运行时开销方面存在困难。

Method: 提出CoroAMU系统，包括编译器过程优化协程代码生成、最小化上下文和合并请求，以及硬件支持的解耦内存操作，增强异步内存单元，引入协程特定内存操作和内存引导分支预测机制。

Result: 在Intel服务器处理器上，CoroAMU编译器相比最先进的协程方法实现了1.51倍加速。结合优化的解耦内存访问硬件，在FPGA模拟的分解式系统上，200ns和800ns延迟下分别实现了3.39倍和4.87倍的平均性能提升。

Conclusion: CoroAMU通过软硬件协同设计有效解决了协程在内存密集型应用中的性能瓶颈，显著提升了分解式内存系统中的性能表现。

Abstract: Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.

</details>


### [2] [DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution](https://arxiv.org/abs/2511.15367)
*Xin Yang,Xin Fan,Zengshi Wang,Jun Han*

Main category: cs.AR

TL;DR: DARE是一种针对稀疏DNN的矩阵处理单元，通过密度化ISA和过滤式预执行技术解决内存访问不规则和计算利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 当前MPU在处理稀疏DNN时存在硬件-算法协同优化不足的问题：内存侧不规则访问导致高缓存缺失率，计算侧现有Matrix ISA的步长约束阻碍了多个逻辑相关稀疏操作的密度化，导致MPU处理单元利用率低下。

Method: 提出DARE MPU架构，包含两个关键技术：1) 扩展ISA支持稀疏操作的密度化；2) 配备具有过滤能力的轻量级预执行机制。

Result: 实验结果显示，DARE相比基线性能提升1.04-4.44倍，能效提升1.00-22.8倍，硬件开销比NVR低3.91倍。

Conclusion: DARE通过硬件-算法协同设计有效解决了稀疏DNN中的不规则性问题，显著提升了性能和能效，同时保持了较低的硬件开销。

Abstract: Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.
  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.
  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\times$ to 4.44$\times$ and increases energy efficiency by 1.00$\times$ to 22.8$\times$ over the baseline, with 3.91$\times$ lower hardware overhead than NVR.

</details>


### [3] [Hemlet: A Heterogeneous Compute-in-Memory Chiplet Architecture for Vision Transformers with Group-Level Parallelism](https://arxiv.org/abs/2511.15397)
*Cong Wang,Zexin Fu,Jiayi Huang,Shanshi Huang*

Main category: cs.AR

TL;DR: Hemlet是一个异构CIM小芯片系统，旨在加速Vision Transformers，通过集成模拟CIM、数字CIM和中间数据处理小芯片来解决单片CIM设计的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在视觉任务中表现出色，但对内存和计算资源需求大，硬件部署面临挑战。单片CIM设计存在可扩展性问题，而小芯片设计虽然可扩展但通信成本高。

Method: 提出Hemlet异构CIM小芯片系统，集成模拟CIM、数字CIM和中间数据处理小芯片，实现灵活的资源扩展，同时优化通信以减少开销。

Result: 未在提供的摘要中明确说明具体结果。

Conclusion: Hemlet通过异构CIM小芯片系统设计，为Vision Transformers的高效硬件部署提供了可扩展且通信优化的解决方案。

Abstract: Vision Transformers (ViTs) have established new performance benchmarks in vision tasks such as image recognition and object detection. However, these advancements come with significant demands for memory and computational resources, presenting challenges for hardware deployment. Heterogeneous compute-in-memory (CIM) accelerators have emerged as a promising solution for enabling energy-efficient deployment of ViTs. Despite this potential, monolithic CIM-based designs face scalability issues due to the size limitations of a single chip. To address this challenge, emerging chiplet-based techniques offer a more scalable alternative. However, chiplet designs come with their own costs, as they introduce more expensive communication through the network-on-package (NoP) compared to the network-on-chip (NoC), which can hinder improvements in throughput.
  This work introduces Hemlet, a heterogeneous CIM chiplet system designed to accelerate ViT. Hemlet facilitates flexible resource scaling through the integration of heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets. To improve throughput while reducing communication ove

</details>


### [4] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: DCC是一个面向PIM系统的数据中心化ML编译器，通过联合优化数据重排和计算代码，在统一调优过程中解决PIM设备与主机处理器数据布局不匹配的问题，显著提升ML内核和LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: PIM设备与主机处理器需要不同的数据布局：主机需要跨DRAM存储体的连续元素分布，而PIM核心需要本地存储体内的元素。这导致ML内核执行中的数据重排带来显著的性能和可编程性挑战，且现有编译方法缺乏对多样化PIM后端的系统性优化。

Method: 设计DCC编译器，集成多层PIM抽象，支持不同PIM后端上的各种数据分布和处理策略。通过将数据分区策略映射到计算循环分区，应用PIM特定代码优化，并利用快速准确的性能预测模型来选择最优配置。

Result: 在各种ML内核中，DCC在HBM-PIM上实现最高7.68倍（平均2.7倍）加速，在AttAcc PIM后端上实现最高13.17倍（平均5.75倍）加速。在端到端LLM推理中，DCC在AttAcc上对GPT-3和LLaMA-2实现最高7.71倍（平均4.88倍）加速。

Conclusion: 数据重排和计算代码优化是相互依赖的，需要在调优过程中联合优化。DCC通过统一的数据中心化方法有效解决了PIM系统中的数据布局挑战，显著提升了ML工作负载的性能。

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


### [5] [Instruction-Based Coordination of Heterogeneous Processing Units for Acceleration of DNN Inference](https://arxiv.org/abs/2511.15505)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出了一种基于指令的FPGA多处理器协调架构，用于加速DNN推理，支持可编程的多处理器同步和灵活的模型分区策略。


<details>
  <summary>Details</summary>
Motivation: 解决FPGA系统中多个高性能处理单元在加速DNN推理时的协调和同步问题，提高计算效率和吞吐量。

Method: 采用指令控制器单元和点对点指令同步单元，将指令分为加载、计算和存储功能组，通过编译框架将DNN模型转换为可执行指令程序。

Result: 在ResNet-50上的实验结果显示计算效率高达98%，吞吐量效率比先前工作提升2.7倍。

Conclusion: 该架构支持设计空间探索，在单批次和多批次性能之间实现动态权衡，无需FPGA重新配置即可在不同部署策略间切换。

Abstract: This paper presents an instruction-based coordination architecture for Field-Programmable Gate Array (FPGA)-based systems with multiple high-performance Processing Units (PUs) for accelerating Deep Neural Network (DNN) inference. This architecture enables programmable multi-PU synchronization through instruction controller units coupled with peer-to-peer instruction synchronization units, utilizing instruction types organized into load, compute, and store functional groups. A compilation framework is presented that transforms DNN models into executable instruction programs, enabling flexible partitioning of DNN models into topologically contiguous subgraphs mapped to available PUs. Multiple deployment strategies are supported, enabling pipeline parallelism among PUs and batch-level parallelism across different PU subsets, with runtime switching among them without FPGA reconfiguration. The proposed approach enables design space exploration, supporting dynamic trade-offs between single-batch and multi-batch performance. Experimental results on ResNet-50 demonstrate notable compute efficiency, up to $98\%$, and throughput efficiency gains, up to $2.7\times$, over prior works across different configurations.

</details>


### [6] [Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond](https://arxiv.org/abs/2511.15564)
*Paul Scheffler,Thomas Benz,Tim Fischer,Lorenzo Leone,Sina Arjmandpour,Luca Benini*

Main category: cs.AR

TL;DR: 提出了一个基于开源chiplet的RISC-V系统路线图，从Occamy双chiplet系统扩展到Ramora和Ogopogo多chiplet架构，旨在缩小与专有设计在高性能计算和人工智能领域的性能差距。


<details>
  <summary>Details</summary>
Motivation: 缩小开源RISC-V系统与专有设计在高性能计算和人工智能领域的性能差距，推动开放硬件生态系统的发展。

Method: 采用chiplet方法，从Occamy（12nm FinFET双chiplet RISC-V多核）开始，扩展到Ramora（基于mesh-NoC的双chiplet系统），再到Ogopogo（7nm四chiplet概念架构），并探索将开放性扩展到仿真、EDA、PDK和片外PHY等领域。

Result: 开发了硅验证的Occamy双chiplet系统，设计了Ramora和Ogopogo架构，其中Ogopogo在7nm工艺下实现了最先进的计算密度。

Conclusion: 通过chiplet方法和扩展开放性到更广泛的硬件生态系统，开源RISC-V系统有望在性能和功能上达到与专有设计相当的水平。

Abstract: We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.

</details>
