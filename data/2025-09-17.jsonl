{"id": "2509.12676", "categories": ["cs.AR", "cs.CR", "C.3; E.3; C.1"], "pdf": "https://arxiv.org/pdf/2509.12676", "abs": "https://arxiv.org/abs/2509.12676", "authors": ["Jiaao Ma", "Ceyu Xu", "Lisa Wu Wills"], "title": "A Scalable Architecture for Efficient Multi-bit Fully Homomorphic Encryption", "comment": "13 pages, 16 figures", "summary": "In the era of cloud computing, privacy-preserving computation offloading is\ncrucial for safeguarding sensitive data. Fully Homomorphic Encryption (FHE)\nenables secure processing of encrypted data, but the inherent computational\ncomplexity of FHE operations introduces significant computational overhead on\nthe server side. FHE schemes often face a tradeoff between efficiency and\nversatility. While the CKKS scheme is highly efficient for polynomial\noperations, it lacks the flexibility of the binary TFHE (Torus-FHE) scheme,\nwhich offers greater versatility but at the cost of efficiency. The recent\nmulti-bit TFHE extension offers greater flexibility and performance by\nsupporting native non-polynomial operations and efficient integer processing.\nHowever, current implementations of multi-bit TFHE are constrained by its\nnarrower numeric representation, which prevents its adoption in applications\nrequiring wider numeric representations.\n  To address this challenge, we introduce Taurus, a hardware accelerator\ndesigned to enhance the efficiency of multi-bit TFHE computations. Taurus\nsupports ciphertexts up to 10 bits by leveraging novel FFT units and optimizing\nmemory bandwidth through key reuse strategies. We also propose a compiler with\noperation deduplication to improve memory utilization. Our experiment results\ndemonstrate that Taurus achieves up to 2600x speedup over a CPU, 1200x speedup\nover a GPU, and up to 7x faster compared to the previous state-of-the-art TFHE\naccelerator. Moreover, Taurus is the first accelerator to demonstrate\nprivacy-preserving inference with large language models such as GPT-2. These\nadvancements enable more practical and scalable applications of\nprivacy-preserving computation in cloud environments."}
{"id": "2509.12993", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.12993", "abs": "https://arxiv.org/abs/2509.12993", "authors": ["Cenlin Duan", "Jianlei Yang", "Rubing Yang", "Yikun Wang", "Yiou Wang", "Lingkun Long", "Yingjie Qi", "Xiaolin He", "Ao Zhou", "Xueyan Wang", "Weisheng Zhao"], "title": "HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large Language Models Inference", "comment": null, "summary": "The deployment of large language models (LLMs) presents significant\nchallenges due to their enormous memory footprints, low arithmetic intensity,\nand stringent latency requirements, particularly during the autoregressive\ndecoding stage. Traditional compute-centric accelerators, such as GPUs, suffer\nfrom severe resource underutilization and memory bandwidth bottlenecks in these\nmemory-bound workloads. To overcome these fundamental limitations, we propose\nHPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)\naccelerator that integrates SRAM-PIM and HBM-PIM subsystems designed\nspecifically for LLM inference. HPIM employs a software-hardware co-design\napproach that combines a specialized compiler framework with a heterogeneous\nhardware architecture. It intelligently partitions workloads based on their\ncharacteristics: latency-critical attention operations are mapped to the\nSRAM-PIM subsystem to exploit its ultra-low latency and high computational\nflexibility, while weight-intensive GEMV computations are assigned to the\nHBM-PIM subsystem to leverage its high internal bandwidth and large storage\ncapacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy\nacross SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,\nthereby significantly mitigating serial dependency of the autoregressive\ndecoding stage. Comprehensive evaluations using a cycle-accurate simulator\ndemonstrate that HPIM significantly outperforms state-of-the-art accelerators,\nachieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.\nMoreover, HPIM exhibits superior performance over contemporary PIM-based\naccelerators, highlighting its potential as a highly practical and scalable\nsolution for accelerating large-scale LLM inference."}
{"id": "2509.13029", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13029", "abs": "https://arxiv.org/abs/2509.13029", "authors": ["Yi Ren", "Baokang Peng", "Chenhao Xue", "Kairong Guo", "Yukun Wang", "Guoyao Cheng", "Yibo Lin", "Lining Zhang", "Guangyu Sun"], "title": "Orthrus: Dual-Loop Automated Framework for System-Technology Co-Optimization", "comment": "Accepted by ICCAD 2025", "summary": "With the diminishing return from Moore's Law, system-technology\nco-optimization (STCO) has emerged as a promising approach to sustain the\nscaling trends in the VLSI industry. By bridging the gap between system\nrequirements and technology innovations, STCO enables customized optimizations\nfor application-driven system architectures. However, existing research lacks\nsufficient discussion on efficient STCO methodologies, particularly in\naddressing the information gap across design hierarchies and navigating the\nexpansive cross-layer design space. To address these challenges, this paper\npresents Orthrus, a dual-loop automated framework that synergizes system-level\nand technology-level optimizations. At the system level, Orthrus employs a\nnovel mechanism to prioritize the optimization of critical standard cells using\nsystem-level statistics. It also guides technology-level optimization via the\nnormal directions of the Pareto frontier efficiently explored by Bayesian\noptimization. At the technology level, Orthrus leverages system-aware insights\nto optimize standard cell libraries. It employs a neural network-assisted\nenhanced differential evolution algorithm to efficiently optimize technology\nparameters. Experimental results on 7nm technology demonstrate that Orthrus\nachieves 12.5% delay reduction at iso-power and 61.4% power savings at\niso-delay over the baseline approaches, establishing new Pareto frontiers in\nSTCO."}
