{"id": "2510.05245", "categories": ["cs.AR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05245", "abs": "https://arxiv.org/abs/2510.05245", "authors": ["Yue Pan", "Zihan Xia", "Po-Kai Hsu", "Lanxiang Hu", "Hyungyo Kim", "Janak Sharda", "Minxuan Zhou", "Nam Sung Kim", "Shimeng Yu", "Tajana Rosing", "Mingu Kang"], "title": "Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving", "comment": null, "summary": "As Large Language Models (LLMs) continue to evolve, Mixture of Experts (MoE)\narchitecture has emerged as a prevailing design for achieving state-of-the-art\nperformance across a wide range of tasks. MoE models use sparse gating to\nactivate only a handful of expert sub-networks per input, achieving\nbillion-parameter capacity with inference costs akin to much smaller models.\nHowever, such models often pose challenges for hardware deployment due to the\nmassive data volume introduced by the MoE layers. To address the challenges of\nserving MoE models, we propose Stratum, a system-hardware co-design approach\nthat combines the novel memory technology Monolithic 3D-Stackable DRAM (Mono3D\nDRAM), near-memory processing (NMP), and GPU acceleration. The logic and Mono3D\nDRAM dies are connected through hybrid bonding, whereas the Mono3D DRAM stack\nand GPU are interconnected via silicon interposer. Mono3D DRAM offers higher\ninternal bandwidth than HBM thanks to the dense vertical interconnect pitch\nenabled by its monolithic structure, which supports implementations of\nhigher-performance near-memory processing. Furthermore, we tackle the latency\ndifferences introduced by aggressive vertical scaling of Mono3D DRAM along the\nz-dimension by constructing internal memory tiers and assigning data across\nlayers based on access likelihood, guided by topic-based expert usage\nprediction to boost NMP throughput. The Stratum system achieves up to 8.29x\nimprovement in decoding throughput and 7.66x better energy efficiency across\nvarious benchmarks compared to GPU baselines."}
{"id": "2510.05327", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05327", "abs": "https://arxiv.org/abs/2510.05327", "authors": ["Zahin Ibnat", "Paul E. Calzada", "Rasin Mohammed Ihtemam", "Sujan Kumar Saha", "Jingbo Zhou", "Farimah Farahmandi", "Mark Tehranipoor"], "title": "DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base", "comment": "22 pages, 6 figures", "summary": "As large language models (LLMs) continue to be integrated into modern\ntechnology, there has been an increased push towards code generation\napplications, which also naturally extends to hardware design automation.\nLLM-based solutions for register transfer level (RTL) code generation for\nintellectual property (IP) designs have grown, especially with fine-tuned LLMs,\nprompt engineering, and agentic approaches becoming popular in literature.\nHowever, a gap has been exposed in these techniques, as they fail to integrate\nnovel IPs into the model's knowledge base, subsequently resulting in poorly\ngenerated code. Additionally, as general-purpose LLMs continue to improve,\nfine-tuned methods on older models will not be able to compete to produce more\naccurate and efficient designs. Although some retrieval augmented generation\n(RAG) techniques exist to mitigate challenges presented in fine-tuning\napproaches, works tend to leverage low-quality codebases, incorporate\ncomputationally expensive fine-tuning in the frameworks, or do not use RAG\ndirectly in the RTL generation step. In this work, we introduce DeepV: a\nmodel-agnostic RAG framework to generate RTL designs by enhancing context\nthrough a large, high-quality dataset without any RTL-specific training. Our\nframework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17%\nincrease in performance on the VerilogEval benchmark. We host DeepV for use by\nthe community in a Hugging Face (HF) Space:\nhttps://huggingface.co/spaces/FICS-LLM/DeepV."}
{"id": "2510.05632", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05632", "abs": "https://arxiv.org/abs/2510.05632", "authors": ["Tianhao Zhu", "Dahu Feng", "Erhu Feng", "Yubin Xia"], "title": "From Principles to Practice: A Systematic Study of LLM Serving on Multi-core NPUs", "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), the demand for\nhigh-performance LLM inference services continues to grow. To meet this demand,\na growing number of AI accelerators have been proposed, such as Google TPU,\nHuawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators\nadopt multi-core architectures to achieve enhanced scalability, but lack the\nflexibility of SIMT architectures. Therefore, without careful configuration of\nthe hardware architecture, as well as deliberate design of tensor parallelism\nand core placement strategies, computational resources may be underutilized,\nresulting in suboptimal inference performance.\n  To address these challenges, we first present a multi-level simulation\nframework with both transaction-level and performance-model-based simulation\nfor multi-core NPUs. Using this simulator, we conduct a systematic analysis and\nfurther propose the optimal solutions for tensor parallelism strategies, core\nplacement policies, memory management methods, as well as the selection between\nPD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive\nexperiments on representative LLMs and various NPU configurations. The\nevaluation results demonstrate that, our solution can achieve 1.32x-6.03x\nspeedup compared to SOTA designs for multi-core NPUs across different hardware\nconfigurations. As for LLM serving, our work offers guidance on designing\noptimal hardware architectures and serving strategies for multi-core NPUs\nacross various LLM workloads."}
{"id": "2510.05787", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.05787", "abs": "https://arxiv.org/abs/2510.05787", "authors": ["Panagiota Nikolaou", "Freddy Gabbay", "Jawad Haj-Yahya", "Yiannakis Sazeides"], "title": "An opportunity to improve Data Center Efficiency: Optimizing the Server's Upgrade Cycle", "comment": "This work was has been submitted and presented at the 1st\n  International Workshop on Data Center Energy Efficiency (DCEE-2025) at\n  ISCA-2025, June 21, 2025, Tokyo, Japan", "summary": "This work aims to improve a data center's efficiency by optimizing the server\nupgrade plan: determine the optimal timing for replacing old servers with new\nones. The opportunity presented by this approach is demonstrated through a\nstudy based on historical server data. The study establishes a significant\nopportunity to increase the QPS/(TCOxCO2) metric by formulating a global\nupgrade plan at the data center's design time covering its entire life cycle.\nThis plan leverages information, such as server entry year, performance, and\nactive power consumption for both existing and future servers. Our findings\nreveal that an optimal global upgrade plan, may involve upgrades at non fixed\ntime periods and outperforms local upgrade plans. Local upgrade plans follow a\nfixed, equal-length cycle and make decisions based only on currently available\nserver models. These local plans select the best available server at each\nupgrade cycle without accounting for future server releases."}
