<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems](https://arxiv.org/abs/2510.24112)
*Junchi Wu,Xinfei Wan,Zhuoran Li,Yuyang Jin,Guangyu Sun,Yun Liang,Diyu Zhou,Youwei Zhuo*

Main category: cs.AR

TL;DR: SlowPoke是一个轻量级硬件感知框架，用于在多核架构上进行片上故障慢速检测，通过编译器插桩、实时跟踪压缩和拓扑感知排名算法，显著降低存储开销并提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 多核架构对高性能计算至关重要，但其性能受到广泛存在的故障慢速问题的严重影响。现有的分布式系统方法由于严格的内存限制和无法跟踪硬件拓扑中的故障而不适用。

Method: 结合编译器插桩进行低开销监控、实时跟踪压缩以在千字节内存内运行，以及新颖的拓扑感知排名算法来定位故障的根本原因。

Result: 在代表性多核工作负载上评估显示，SlowPoke将检测跟踪的存储开销平均降低115.9倍，实现平均86.77%的故障慢速检测准确率和12.11%的误报率。

Conclusion: SlowPoke在不同多核架构上都能有效扩展，使其适用于大规模部署，为片上故障慢速检测提供了实用的解决方案。

Abstract: Many-core architectures are essential for high-performance computing, but
their performance is undermined by widespread fail-slow failures. Detecting
such failures on-chip is challenging, as prior methods from distributed systems
are unsuitable due to strict memory limits and their inability to track
failures across the hardware topology. This paper introduces SlowPoke, a
lightweight, hardware-aware framework for practical on-chip fail-slow
detection. SlowPoke combines compiler-based instrumentation for low-overhead
monitoring, on-the-fly trace compression to operate within kilobytes of memory,
and a novel topology-aware ranking algorithm to pinpoint a failure's root
cause. We evaluate SlowPoke on a wide range of representative many-core
workloads, and the results demonstrate that SlowPoke reduces the storage
overhead of detection traces by an average of 115.9$\times$, while achieving an
average fail-slow detection accuracy of 86.77% and a false positive rate (FPR)
of 12.11%. More importantly, SlowPoke scales effectively across different
many-core architectures, making it practical for large-scale deployments.

</details>


### [2] [Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on Chiplet-Based Accelerators](https://arxiv.org/abs/2510.24113)
*Arnav Shukla,Harsh Sharma,Srikant Bharadwaj,Vinayak Abrol,Sujay Deb*

Main category: cs.AR

TL;DR: 本文提出PARL方法，通过多目标优化生成异构chiplet系统中网络互连拓扑，解决内存驱动传输导致的尾部延迟问题。


<details>
  <summary>Details</summary>
Motivation: 异构chiplet系统通过解耦CPU/GPU和新兴技术(HBM/DRAM)来提升扩展性，但封装内解耦引入了网络互连延迟。在大模型推理中，参数和激活值在HBM/DRAM间频繁移动，产生突发流量，导致尾部延迟增加并违反SLA。

Method: 引入干扰评分量化最坏情况下的性能下降，将网络互连合成建模为多目标优化问题，开发PARL（分区感知强化学习）拓扑生成器，平衡吞吐量、延迟和功耗。

Result: PARL生成的拓扑减少内存切分处的争用，满足SLA要求，将最坏情况性能下降降至1.2倍，同时保持与链路丰富网格相当的均值吞吐量。

Conclusion: 该方法重新定义了异构chiplet加速器的网络互连设计，实现工作负载感知的目标优化。

Abstract: Heterogeneous chiplet-based systems improve scaling by disag-gregating
CPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package
disaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe
that in modern large-modelinference, parameters and activations routinely move
backand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.
These memory-driven transfers inflate tail latency andviolate Service Level
Agreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this
gap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown
under contention.We then formulate NoI synthesis as a multi-objective
optimization(MOO) problem. We develop PARL (Partition-Aware
ReinforcementLearner), a topology generator that balances throughput,
latency,and power. PARL-generated topologies reduce contention at the memory
cut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining
competitive mean throughput relative to link-rich meshes. Overall, this
reframes NoI design for heterogeneouschiplet accelerators with workload-aware
objectives.

</details>
