<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132)
*Mansi Choudhary,Karthik Sangaiah,Sonali Singh,Muhammad Osama,Lisa Wu Wills,Ganesh Dasika*

Main category: cs.AR

TL;DR: 提出Swizzled Head-first Mapping方法，通过空间感知调度策略将注意力头与GPU NUMA域对齐，在AMD MI300X架构上实现比传统注意力算法高50%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 分解式AI GPU的兴起暴露了大规模注意力工作负载中的关键瓶颈：非统一内存访问(NUMA)。多芯片设计成为扩展计算能力的常态，但内存延迟和带宽在不同计算区域间差异显著，破坏了传统GPU内核调度策略的性能。

Method: 提出Swizzled Head-first Mapping空间感知调度策略，将注意力头与GPU NUMA域对齐，利用片内缓存重用。

Result: 在AMD MI300X架构上，该方法比使用传统调度技术的最先进注意力算法性能提升高达50%，并保持80-97%的高L2缓存命中率。

Conclusion: NUMA感知调度对于在下一代分解式GPU上实现完全效率至关重要，为可扩展的AI训练和推理提供了前进路径。

Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in
large-scale attention workloads: non-uniform memory access (NUMA). As
multi-chiplet designs become the norm for scaling compute capabilities, memory
latency and bandwidth vary sharply across compute regions, undermining the
performance of traditional GPU kernel scheduling strategies that assume uniform
memory access. We identify how these NUMA effects distort locality in
multi-head attention (MHA) and present Swizzled Head-first Mapping, a
spatially-aware scheduling strategy that aligns attention heads with GPU NUMA
domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our
method achieves up to 50% higher performance over state-of-the-art attention
algorithms using conventional scheduling techniques and sustains consistently
high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware
scheduling is now fundamental to achieving full efficiency on next-generation
disaggregated GPUs, offering a path forward for scalable AI training and
inference.

</details>


### [2] [BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction](https://arxiv.org/abs/2511.02196)
*Liwei Ni,Jiaxi Zhang,Shenggen Zheng,Junfeng Liu,Xingyu Meng,Biwei Xie,Xingquan Li,Huawei Li*

Main category: cs.AR

TL;DR: BoolSkeleton是一种布尔网络骨架化方法，通过预处理和简化两个步骤，将布尔网络转换为布尔依赖图，定义同质和异质模式进行节点级模式简化，提高设计一致性评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 布尔等价性使得具有相同功能的布尔网络可以展现不同的图结构，这为逻辑优化提供了更多探索空间，但也给布尔网络间的一致性任务带来了挑战。

Method: 包含预处理和简化两个关键步骤：预处理将布尔网络转换为定义的布尔依赖图，为节点分配功能相关状态；然后定义同质和异质模式进行节点级模式简化，异质模式保留关键功能依赖，同质模式可简化，参数K控制模式扇入大小以精细控制图简化粒度。

Result: 在四个布尔网络分析/下游任务中验证有效性：压缩分析、分类、关键路径分析和时序预测，展示了在不同场景下的鲁棒性。时序预测任务中平均准确率比原始布尔网络提高了55%以上。

Conclusion: 实验证明了BoolSkeleton在增强逻辑综合中设计一致性方面的潜力。

Abstract: Boolean equivalence allows Boolean networks with identical functionality to
exhibit diverse graph structures. This gives more room for exploration in logic
optimization, while also posing a challenge for tasks involving consistency
between Boolean networks. To tackle this challenge, we introduce BoolSkeleton,
a novel Boolean network skeletonization method that improves the consistency
and reliability of design-specific evaluations. BoolSkeleton comprises two key
steps: preprocessing and reduction. In preprocessing, the Boolean network is
transformed into a defined Boolean dependency graph, where nodes are assigned
the functionality-related status. Next, the homogeneous and heterogeneous
patterns are defined for the node-level pattern reduction step. Heterogeneous
patterns are preserved to maintain critical functionality-related dependencies,
while homogeneous patterns can be reduced. Parameter K of the pattern further
constrains the fanin size of these patterns, enabling fine-tuned control over
the granularity of graph reduction. To validate BoolSkeleton's effectiveness,
we conducted four analysis/downstream tasks around the Boolean network:
compression analysis, classification, critical path analysis, and timing
prediction, demonstrating its robustness across diverse scenarios. Furthermore,
it improves above 55% in the average accuracy compared to the original Boolean
network for the timing prediction task. These experiments underscore the
potential of BoolSkeleton to enhance design consistency in logic synthesis.

</details>


### [3] [Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA](https://arxiv.org/abs/2511.02269)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次在粗粒度线性阵列（CGLA）加速器上实现了Whisper语音识别核心计算内核，通过软硬件协同设计，在FPGA原型上评估并预测28nm ASIC性能，展示了比NVIDIA Jetson AGX Orin和RTX 4090更优的能效表现。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在自动语音识别（ASR）任务中的兴起带来了严重的能耗挑战。ASIC虽然效率高但缺乏可编程性，无法适应不断演进的算法。

Method: 在IMAX通用CGLA加速器上实现Whisper核心计算内核，采用软硬件协同设计方法，通过FPGA原型进行评估，并预测28nm ASIC的性能。

Result: 投影的ASIC在能效方面表现优异：比NVIDIA Jetson AGX Orin能效高1.90倍，比NVIDIA RTX 4090高9.83倍（针对Q8_0模型）。

Conclusion: 这项工作将CGLA定位为在功耗受限的边缘设备上实现可持续ASR的有前景的平台。

Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR)
has created a critical energy consumption challenge. While ASICs offer high
efficiency, they lack the programmability to adapt to evolving algorithms. To
address this trade-off, we implement and evaluate Whisper's core computational
kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)
accelerator. To our knowledge, this is the first work to execute a Whisper
kernel on a CGRA and compare its performance against CPUs and GPUs. Using
hardware/software co-design, we evaluate our system via an FPGA prototype and
project performance for a 28 nm ASIC. Our results demonstrate superior energy
efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA
Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This
work positions CGLA as a promising platform for sustainable ASR on
power-constrained edge devices.

</details>


### [4] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus是一个三阶段框架，通过聚焦LLM推理到代码生成的关键决策点来增强Verilog生成，显著提高了功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖自一致性或模拟反馈来选择最佳候选，但未能将LLM推理聚焦到设计中最具信息量的部分，导致功能正确性保障不足。

Method: 三阶段框架：预排名阶段生成多个代码候选并应用密度引导过滤；排名阶段通过自动生成测试平台进行模拟和自一致性聚类；后排名精炼阶段进行不一致性挖掘和推理增强的LLM提示精炼。

Result: 在VerilogEval-Human基准测试中，VFocus显著提高了多个推理LLM的pass@1正确性。

Conclusion: VFocus通过聚焦LLM推理到关键决策点，有效增强了复杂硬件设计任务的Verilog生成能力。

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


### [5] [Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA](https://arxiv.org/abs/2511.02408)
*Takuto Ando,Yusuke Inoue*

Main category: cs.AR

TL;DR: 在SoC FPGA上使用DPU实现多线程的独立面部表情识别系统，通过DenseBox进行人脸检测和CNN进行表情识别，在相同DPU上运行两个推理，实现了25 FPS的系统吞吐量和2.4倍的能效比。


<details>
  <summary>Details</summary>
Motivation: 解决之前工作中Haar Cascade检测器在侧脸和变光照条件下精度不足的问题，以及避免为第二个DNN推理添加新加速器导致的FPGA资源限制问题。

Method: 使用DPU（脉动阵列型通用CNN加速器）同时运行DenseBox人脸检测和CNN表情识别两个推理，并开发多线程技术提高吞吐量和DPU利用效率。

Result: 实现了25 FPS的整体系统吞吐量，吞吐量功耗比为之前的2.4倍，同时保持了较小的电路尺寸。

Conclusion: 在相同DPU上运行两个DNN推理的方法能够高效利用FPGA资源，多线程技术显著提升了系统性能和能效。

Abstract: In this paper, we implement a stand-alone facial expression recognition
system on an SoC FPGA with multi-threading using a Deep learning Processor Unit
(DPU). The system consists of two steps: one for face detection step and one
for facial expression recognition. In the previous work, the Haar Cascade
detector was run on a CPU in the face detection step due to FPGA resource
limitations, but this detector is less accurate for profile and variable
illumination condition images. Moreover, the previous work used a dedicated
circuit accelerator, so running a second DNN inference for face detection on
the FPGA would require the addition of a new accelerator. As an alternative to
this approach, we run the two inferences by DNN on a DPU, which is a
general-purpose CNN accelerator of the systolic array type. Our method for face
detection using DenseBox and facial expression recognition using CNN on the
same DPU enables the efficient use of FPGA resources while maintaining a small
circuit size. We also developed a multi-threading technique that improves the
overall throughput while increasing the DPU utilization efficiency. With this
approach, we achieved an overall system throughput of 25 FPS and a throughput
per power consumption of 2.4 times.

</details>


### [6] [Digit-Recurrence Posit Division](https://arxiv.org/abs/2511.02494)
*Raul Murillo,Julio Villalba-Moreno,Alberto A. Del Barrio,Guillermo Botella*

Main category: cs.AR

TL;DR: 提出了基于数字递归算法的posit除法单元，首次在posit系统中实现radix-4数字递归技术，显著降低了能耗和迭代次数。


<details>
  <summary>Details</summary>
Motivation: Posit算术作为IEEE 754浮点表示的有前景替代方案，具有更高的精度和动态范围，但其除法操作由于硬件复杂性而具有挑战性。

Method: 采用基于数字递归算法的posit除法单元，结合冗余算术、在线商转换和操作数缩放等硬件优化技术，以简化除法过程并减少延迟、面积和功耗开销。

Result: 在多种posit配置下的综合评估显示，与现有方法相比实现了超过80%的能耗降低（仅需小面积开销），并显著减少了迭代次数。

Conclusion: 该适配算法有潜力显著提高基于posit的算术单元的效率。

Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754
floating-point representation, offering enhanced accuracy and dynamic range.
However, division operations in posit systems remain challenging due to their
inherent hardware complexity. In this work, we present posit division units
based on the digit-recurrence algorithm, marking the first implementation of
radix-4 digit-recurrence techniques within this context. Our approach
incorporates hardware-centric optimizations including redundant arithmetic,
on-the-fly quotient conversion, and operand scaling to streamline the division
process while mitigating latency, area, and power overheads. Comprehensive
synthesis evaluations across multiple posit configurations demonstrate
significant performance improvements, including more than 80% energy reduction
with small area overhead compared to existing methods, and a substantial
decrease in the number of iterations. These results underscore the potential of
our adapted algorithm to enhance the efficiency of posit-based arithmetic
units.

</details>


### [7] [Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator](https://arxiv.org/abs/2511.02530)
*Takuto Ando,Yu Eto,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 在IMAX3 CGRA加速器上首次实现并评估stable-diffusion.cpp图像生成核心计算，展示其作为通用平台在图像生成工作负载中的性能潜力。


<details>
  <summary>Details</summary>
Motivation: 评估通用CGRA加速器IMAX3在处理要求苛刻的图像生成工作负载时的能力，为下一代AI专用加速器设计提供指导。

Method: 在FPGA原型上实现stable-diffusion.cpp核心计算内核，评估性能并预测未来ASIC实现的潜力。

Result: IMAX3在通用架构下展现出有前景的性能和能效，特别是在预测的ASIC形式中表现更佳。

Conclusion: 这项工作为未来IMAX架构设计提供了具体指导，并为开发下一代AI专用CGLA加速器奠定了基础，有助于实现能效优化的设备端多模态AI平台。

Abstract: This paper presents the first implementation and in-depth evaluation of the
primary computational kernels from the stable-diffusion.cpp image generation
framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array
(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,
and this work assesses its capabilities by executing a demanding image
generation workload. We evaluate its performance on a current
Field-Programmable Gate Array (FPGA) prototype to establish a baseline and
project its potential for a future Application-Specific Integrated Circuit
(ASIC) implementation. Our results demonstrate that, despite its
general-purpose architecture, IMAX3 achieves promising performance and power
efficiency, particularly in its projected ASIC form. This work provides
concrete guidelines for future IMAX architectural designs and establishes a
foundation for developing next-generation, AI-specialized Coarse-Grained Linear
Array (CGLA) accelerators by refining this versatile platform. Ultimately, this
achievement contributes to the realization of energy-efficient, on-device,
multi-modal AI platforms.

</details>
