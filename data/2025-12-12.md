<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Algorithm-Driven On-Chip Integration for High Density and Low Cost](https://arxiv.org/abs/2512.10089)
*Jeongeun Kim,Sabrina Yarzada,Paul Chen,Christopher Torng*

Main category: cs.AR

TL;DR: 提出一种新的多项目芯片集成方法，通过自动化布局、窄区域互连架构和片上电源域技术，实现13倍面积缩减，为大规模流片提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着半导体人才培养需求增长，需要支持大量独立硬件设计的平台。传统多项目晶圆(MPW)服务在项目数量增加时扩展性受限，现有集成方法缺乏系统性的布局、互连和验证原则。

Method: 提出三种关键技术：1) 建立结构化设计空间公式化，实现自动化算法驱动的项目打包；2) 利用设计站点间的窄区域实现片外通信和其他共享需求；3) 提供片上电源域实用方法，支持每个项目在标准实验台上进行功耗表征。

Result: 实验结果显示，相比最先进的纯物理聚合方法，该方法实现了高达13倍的面积缩减，为大规模流片环境提供了可扩展且经济高效的解决方案。

Conclusion: 该方法通过系统性的设计空间公式化、窄区域互连架构和片上电源域技术，解决了多项目芯片集成的可扩展性问题，为半导体研究和培训提供了更高效的平台。

Abstract: Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.

</details>


### [2] [A Vertically Integrated Framework for Templatized Chip Design](https://arxiv.org/abs/2512.10155)
*Jeongeun Kim,Christopher Torng*

Main category: cs.AR

TL;DR: 提出一种从高级面向对象软件规范生成芯片的方法，通过软件对象与芯片区域的直接映射，降低软件开发者参与芯片设计的门槛。


<details>
  <summary>Details</summary>
Motivation: 软件开发者难以将定制硬件集成到应用中，尽管专用芯片能为机器学习和AI等领域带来显著优势。需要降低芯片设计的学习门槛，让软件开发者也能参与芯片创建。

Method: 采用模块化构建策略：1) 软件对象直接映射到芯片区域，保持结构对应关系；2) 垂直组合IP块实现软件中表达的行为协议；3) 使用基于序列的形式类型系统检查硬件模块交互是否符合软件模型通信模式；4) 开发适合对象对齐设计风格的硬件互连策略和布局技术。

Result: 实现了从软件到芯片设计的思维连续性保持，支持实用的布局生成，降低了软件开发者参与芯片创建所需的专业知识。

Conclusion: 该方法为芯片设计初学者提供了从软件规范生成芯片的途径，通过保持软件抽象在硬件设计流程中的连续性，显著降低了软件开发者参与芯片设计的门槛。

Abstract: Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.

</details>


### [3] [Neuromorphic Processor Employing FPGA Technology with Universal Interconnections](https://arxiv.org/abs/2512.10180)
*Pracheta Harlikar,Abdel-Hameed A. Badawy,Prasanna Date*

Main category: cs.AR

TL;DR: 在Xilinx Zynq-7000 FPGA上实现低成本神经形态处理器，支持全连接可配置架构，采用LIF神经元模型，通过UART接口实现运行时重配置，验证了Iris分类和MNIST识别任务，具有高能效和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算具有超低功耗和实时推理的潜力，但缺乏灵活、开源平台阻碍了广泛采用和实验。需要开发低成本、可访问的研究级平台。

Method: 在Xilinx Zynq-7000 FPGA平台上实现神经形态处理器，支持全连接可配置连接，采用泄漏积分发放(LIF)神经元模型，参数可定制（阈值、突触权重、不应期），通过UART接口与主机通信实现运行时重配置。

Result: 使用Iris分类和MNIST数字识别基准数据集验证了架构有效性。综合后结果显示设计具有高能效和可扩展性，证明其作为研究级神经形态平台的可行性。

Conclusion: 该实现提供了一个低成本、可访问且适应性强的神经形态平台，适用于真实世界脉冲神经网络应用，将在项目完成后作为开源发布。

Abstract: Neuromorphic computing, inspired by biological neural systems, holds immense promise for ultra-low-power and real-time inference applications. However, limited access to flexible, open-source platforms continues to hinder widespread adoption and experimentation. In this paper, we present a low-cost neuromorphic processor implemented on a Xilinx Zynq-7000 FPGA platform. The processor supports all-to-all configurable connectivity and employs the leaky integrate-and-fire (LIF) neuron model with customizable parameters such as threshold, synaptic weights, and refractory period. Communication with the host system is handled via a UART interface, enabling runtime reconfiguration without hardware resynthesis. The architecture was validated using benchmark datasets including the Iris classification and MNIST digit recognition tasks. Post-synthesis results highlight the design's energy efficiency and scalability, establishing its viability as a research-grade neuromorphic platform that is both accessible and adaptable for real-world spiking neural network applications. This implementation will be released as open source following project completion.

</details>


### [4] [SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation](https://arxiv.org/abs/2512.10231)
*Zhenguo Liu,Chengao Shi,Chen Ding,Jiang Xu*

Main category: cs.AR

TL;DR: SemanticBBV：一种新颖的两阶段框架，通过语义编码和集合变换器生成性能感知的程序签名，实现跨程序模拟重用，相比传统基本块向量（BBV）有显著优势。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的微架构模拟使用基本块向量（BBV）作为程序表示，但BBV存在两个根本限制：1）顺序依赖的ID阻碍跨程序知识重用；2）缺乏预测硬件性能的语义内容，导致大量优化潜力未被开发。

Method: 提出SemanticBBV两阶段框架：1）轻量级RWKV语义编码器将汇编基本块转换为丰富的BBE嵌入，捕获深度功能语义；2）顺序不变的集合变换器聚合BBE（按执行频率加权）生成最终签名，通过三元组损失（签名区分性）和CPI回归任务（性能敏感性）进行联合训练。

Result: 仅模拟14个通用程序点即可估计10个SPEC CPU基准测试的性能，平均准确率达86.3%，实现7143倍模拟加速。签名对新微架构表现出强适应性，仅需少量微调。

Conclusion: SemanticBBV不仅匹配传统BBV的单程序准确性，还实现了前所未有的跨程序分析能力，为微架构模拟开辟了新的可能性。

Abstract: For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.
  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.

</details>
