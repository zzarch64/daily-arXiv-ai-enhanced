<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration](https://arxiv.org/abs/2512.11550)
*Yifan Zhang,Zhiheng Chen,Ye Qiao,Sitao Huang*

Main category: cs.AR

TL;DR: PD-Swap：基于动态部分重配置的预填充-解码分离LLM加速器，通过时间复用注意力模块，在边缘FPGA上实现高效的长上下文推理


<details>
  <summary>Details</summary>
Motivation: 边缘FPGA部署的量化LLM在处理长上下文时性能急剧下降，主要原因是预填充和解码阶段的计算特性不同：预填充是计算密集型，解码是内存带宽密集型。静态加速器需要为两种模式提供资源，导致资源浪费和性能瓶颈。

Method: 提出PD-Swap加速器，使用动态部分重配置技术时间复用注意力模块。核心的三元矩阵乘法和权重缓冲引擎保持静态，注意力子系统作为可重配置分区，包含两个专门化架构：计算密集的token并行预填充引擎和带宽优化的KV缓存中心解码引擎。

Result: PD-Swap在边缘FPGA上实现高达27 tokens/s的解码吞吐量，比现有最优工作提升1.3-2.1倍（上下文越长提升越大），且不增加额外面积成本。

Conclusion: 通过预填充-解码分离架构和动态部分重配置，PD-Swap有效解决了边缘FPGA上LLM长上下文推理的性能瓶颈，为低功耗边缘设备部署生成式AI提供了高效解决方案。

Abstract: Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.

</details>
