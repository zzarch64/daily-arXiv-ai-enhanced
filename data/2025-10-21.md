<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 26]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Multimodal Chip Physical Design Engineer Assistant](https://arxiv.org/abs/2510.15872)
*Yun-Da Tsai,Chang-Yu Chao,Liang-Yeh Shen,Tsung-Han Lin,Haoyu Yang,Mark Ho,Yi-Chen Lu,Wen-Hao Liu,Shou-De Lin,Haoxing Ren*

Main category: cs.AR

TL;DR: 提出了一种多模态大语言模型助手，通过结合视觉、表格和文本输入来预测布线拥塞并提供可解释的设计建议，优于现有模型在准确性和可解释性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现代芯片物理设计依赖EDA工具，但这些工具难以提供可解释的反馈或可操作的设计改进指导，需要一种能够预测拥塞并提供人类可理解建议的方法。

Method: 结合MLLM引导的遗传提示进行自动特征生成，以及可解释的偏好学习框架，建模跨视觉、表格和文本输入的拥塞相关权衡，生成包含最有影响力布局特征和针对性优化的设计建议卡。

Result: 在CircuitNet基准测试中，该方法在准确性和可解释性方面均优于现有模型，设计建议指导案例研究和定性分析证实学习到的偏好与现实设计原则一致且对工程师具有可操作性。

Conclusion: 这项工作突显了MLLM作为交互式助手在可解释和上下文感知的物理设计优化方面的潜力。

Abstract: Modern chip physical design relies heavily on Electronic Design Automation
(EDA) tools, which often struggle to provide interpretable feedback or
actionable guidance for improving routing congestion. In this work, we
introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this
gap by not only predicting congestion but also delivering human-interpretable
design suggestions. Our method combines automated feature generation through
MLLM-guided genetic prompting with an interpretable preference learning
framework that models congestion-relevant tradeoffs across visual, tabular, and
textual inputs. We compile these insights into a "Design Suggestion Deck" that
surfaces the most influential layout features and proposes targeted
optimizations. Experiments on the CircuitNet benchmark demonstrate that our
approach outperforms existing models on both accuracy and explainability.
Additionally, our design suggestion guidance case study and qualitative
analyses confirm that the learned preferences align with real-world design
principles and are actionable for engineers. This work highlights the potential
of MLLMs as interactive assistants for interpretable and context-aware physical
design optimization.

</details>


### [2] [Putting the Context back into Memory](https://arxiv.org/abs/2510.15878)
*David A. Roberts*

Main category: cs.AR

TL;DR: 提出了一种在内存地址流中编码用户可见状态的方法，使内存设备能够获取程序上下文信息，从而改善内存优化。


<details>
  <summary>Details</summary>
Motivation: 解决硬件缓存预取、内存请求调度和交错导致的可观测性丧失问题，使内存设备能够获取有价值的程序上下文信息。

Method: 通过在内存读取地址流中以非破坏性方式编码用户可见状态作为可检测的数据包，构建了包含元数据注入的端到端系统原型。

Result: 成功实现了从内存地址跟踪中可靠检测和解码元数据，展示了精确代码执行标记和对象地址范围跟踪的用例。

Conclusion: 该方法为未来实时元数据解码和近内存计算提供了基础，可实现定制化遥测、统计和基于应用提示的内存优化功能。

Abstract: Requests arriving at main memory are often different from what programmers
can observe or estimate by using CPU-based monitoring. Hardware cache
prefetching, memory request scheduling and interleaving cause a loss of
observability that limits potential data movement and tiering optimizations. In
response, memory-side telemetry hardware like page access heat map units (HMU)
and page prefetchers were proposed to inform Operating Systems with accurate
usage data. However, it is still hard to map memory activity to software
program functions and objects because of the decoupled nature of host
processors and memory devices. Valuable program context is stripped out from
the memory bus, leaving only commands, addresses and data. Programmers have
expert knowledge of future data accesses, priorities, and access to processor
state, which could be useful hints for runtime memory device optimization. This
paper makes context visible at memory devices by encoding any user-visible
state as detectable packets in the memory read address stream, in a
nondestructive manner without significant capacity overhead, drivers or special
access privileges. We prototyped an end-to-end system with metadata injection
that can be reliably detected and decoded from a memory address trace, either
by a host processor, or a memory module. We illustrate a use case with precise
code execution markers and object address range tracking. In the future, real
time metadata decoding with near-memory computing (NMC) could provide
customized telemetry and statistics to users, or act on application hints to
perform functions like prioritizing requests, remapping data and reconfiguring
devices.

</details>


### [3] [Opportunities and Challenges for 3D Systems and Their Design](https://arxiv.org/abs/2510.15880)
*Philip Emma,Eren Kurshan*

Main category: cs.AR

TL;DR: 3D集成技术虽然不是一个新概念，但随着光刻缩放变得更具挑战性以及微型通孔制造能力的提升，正受到广泛关注。它像摩尔定律一样提高密度，但带来了更高的功率密度挑战，并对设计和制造技术提出了新要求。


<details>
  <summary>Details</summary>
Motivation: 随着光刻缩放面临挑战，3D集成技术因其能够提高密度而受到重视。微型通孔制造能力的改进使得3D集成更具可行性，但需要解决由此带来的设计和制造新问题。

Method: 论文讨论了3D集成系统的设计方法，包括跨层协同设计电路、通孔和宏单元的布局，确保组装后的空间对应关系正确。每个功能层需要独立可测试，并在组装后支持系统级测试。

Result: 分析表明3D集成确实能够提高系统密度，但同时也带来了功率密度增加、设计和制造复杂度提升等挑战。需要建立新的设计标准和测试方法来应对这些挑战。

Conclusion: 3D集成技术具有显著优势，但要充分发挥其潜力，必须系统性地解决设计、组装和测试过程中出现的新挑战，包括跨层协同设计、独立测试能力和成品率管理等方面。

Abstract: Although it is not a new concept, 3D integration increasingly receives
widespread interest and focus as lithographic scaling becomes more challenging,
and as the ability to make miniature vias greatly improves. Like Moores law, 3D
integration improves density. With improvements in packaging density, however,
come the challenges associated with its inherently higher power density. And
though it acts somewhat as a scaling accelerator, the vertical integration also
poses new challenges to design and manufacturing technologies. The placement of
circuits, vias, and macros in the planes of a 3D stack must be co-designed
across layers (or must conform to new standards) so that, when assembled, they
have correct spatial correspondence. Each layer, although perhaps being a mere
functional slice through a system (and we can slice the system in many
different ways), must be independently testable so that we can systematically
test and diagnose subsystems before and after final assembly. When those layers
are assembled, they must come together in a way that enables a sensible yield
and facilitates testing the finished product. To make the most of 3D
integration, we should articulate the leverages of 3D systems (other
researchers offer a more complete treatment elsewhere). Then we can enumerate
and elucidate many of the new challenges posed by the design, assembly, and
test of 3D systems.

</details>


### [4] [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern](https://arxiv.org/abs/2510.15882)
*Ao Shen,Rui Zhang,Junping Zhao*

Main category: cs.AR

TL;DR: FlexLink是一个集体通信框架，通过聚合NVLink、PCIe和RDMA NIC等异构链路来解决大语言模型多节点部署中的通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，多节点部署成为必需，通信成为关键性能瓶颈。现有通信库如NCCL仅使用单一互连（如NVLink），导致性能上限，而其他硬件资源如PCIe和RDMA NIC在密集工作负载期间基本闲置。

Method: FlexLink采用两阶段自适应负载均衡策略，动态地将通信流量分区到所有可用链路上，确保快速互连不会被较慢的互连所限制。

Result: 在8-GPU H800服务器上，与NCCL基线相比，AllReduce和AllGather等集体操作符的带宽分别提高了26%和27%，通过将2-22%的总通信流量卸载到之前未充分利用的PCIe和RDMA NIC上实现。

Conclusion: FlexLink作为无损的即插即用替代方案，与NCCL API兼容，确保易于采用，有效解决了异构链路利用率不足的问题。

Abstract: As large language models (LLMs) continue to scale, multi-node deployment has
become a necessity. Consequently, communication has become a critical
performance bottleneck. Current intra-node communication libraries, like NCCL,
typically make use of a single interconnect such as NVLink. This approach
creates performance ceilings, especially on hardware like the H800 GPU where
the primary interconnect's bandwidth can become a bottleneck, and leaves other
hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable
Network Interface Cards (NICs) largely idle during intensive workloads. We
propose FlexLink, the first collective communication framework to the best of
our knowledge designed to systematically address this by aggregating these
heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance
communication fabric. FlexLink employs an effective two-stage adaptive load
balancing strategy that dynamically partitions communication traffic across all
available links, ensuring that faster interconnects are not throttled by slower
ones. On an 8-GPU H800 server, our design improves the bandwidth of collective
operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL
baseline, respectively. This gain is achieved by offloading 2-22% of the total
communication traffic to the previously underutilized PCIe and RDMA NICs.
FlexLink provides these improvements as a lossless, drop-in replacement
compatible with the NCCL API, ensuring easy adoption.

</details>


### [5] [Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I](https://arxiv.org/abs/2510.15884)
*Faizan A Khattak,Mantas Mikaitis*

Main category: cs.AR

TL;DR: 提出了一种架构无关的测试方案，用于分析消费级NVIDIA GPU中矩阵乘法器的数值特性，包括舍入、归一化和累加器内部精度等特征。


<details>
  <summary>Details</summary>
Motivation: 扩展数据中心GPU数值特性分析方法到消费级GPU，开发不依赖设备特定常数的通用测试向量生成方法。

Method: 实现架构无关的测试方案，使用非穷举搜索且不依赖硬编码常数的测试向量生成方法，支持多种混合精度格式。

Result: 在RTX-3060和Ada RTX-1000上成功分析了binary16、TensorFloat32、bfloat16输入格式和binary16、binary32输出格式的数值特性，发现RTX-3060与数据中心GPU A100具有相同的数值特性。

Conclusion: 该方法具有很好的通用性，预计无需修改即可用于分析新一代NVIDIA GPU（如Hopper、Blackwell）及其未来继任者，以及包括最新8位浮点格式在内的各种输入输出格式组合。

Abstract: Numerical features of matrix multiplier hardware units in NVIDIA and AMD data
centre GPUs have recently been studied. Features such as rounding,
normalisation, and internal precision of the accumulators are of interest. In
this paper, we extend the methodology for analysing those features, to
consumer-grade NVIDIA GPUs by implementing an architecture-independent test
scheme for various input and output precision formats. Unlike current
approaches, the proposed test vector generation method neither performs an
exhaustive search nor relies on hard-coded {constants that are device-specific,
yet remains applicable to a wide range of mixed-precision formats. We have
applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada
Lovelace architecture) graphics cards and determined numerical features of
matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating
point formats and binary16 and binary32 IEEE 754 output formats. Our
methodology allowed us to determine that} the numerical features of RTX-3060, a
consumer-grade GPU, are identical to those of the A100, a data centre GPU. We
do not expect our code to require any changes for performing analysis of matrix
multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future
successors, and any input/output format combination, including the latest 8-bit
floating-point formats.

</details>


### [6] [ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices](https://arxiv.org/abs/2510.15885)
*Dingcui Yu,Zonghuan Yan,Jialin Liu,Yumiao Zhao,Yanyun Wang,Xinghui Duan,Yina Lv,Liang Shi*

Main category: cs.AR

TL;DR: ConZone+是一个模拟消费级分区闪存存储的仿真器，通过添加块接口支持解决了原版ConZone无法挂载文件系统的问题，提供了部署脚本和多项增强功能。


<details>
  <summary>Details</summary>
Motivation: 为了理解和高效改进消费级分区闪存存储的软硬件设计，需要能够模拟其资源约束和架构特征的仿真工具。

Method: ConZone+在ConZone基础上扩展了块接口支持，提供了部署脚本，并包含逻辑到物理映射缓存、受限写缓冲器和混合闪存媒体管理等消费级设备典型组件。

Result: 通过与代表性硬件架构和最先进技术比较验证了ConZone+的准确性，并通过案例研究探索了分区存储设计和当前文件系统的不足。

Conclusion: ConZone+使用户能够探索消费级分区闪存存储的内部架构，并将优化与系统软件集成，提高了仿真器的可用性。

Abstract: To facilitate the understanding and efficient enhancement of software and
hardware design for consumer-grade zoned flash storage, ConZone is proposed as
the first emulator designed to model the resource constraints and architectural
features typical of such systems. It incorporates essential components commonly
deployed in consumer-grade devices, including limited logical to physical
mapping caches, constrained write buffers, and hybrid flash media management.
However, ConZone cannot be mounted with the file system due to the lack of
in-place update capability, which is required by the metadata area of F2FS. To
improve the usability of the emulator, ConZone+ extends ConZone with support
for a block interface. We also provide a script to help the deployment and
introduces several enhancements over the original version. Users can explore
the internal architecture of consumer-grade zoned flash storage and integrate
their optimizations with system software using ConZone+. We validate the
accuracy of ConZone+ by comparing a hardware architecture representative of
consumer-grade zoned flash storage and comparing it with the state-of-the-art.
In addition, we conduct several case studies using ConZone+ to investigate the
design of zoned storage and explore the inadequacies of the current file
system.

</details>


### [7] [basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I](https://arxiv.org/abs/2510.15887)
*Hyun Woo Kang,Ji Woong Choi*

Main category: cs.AR

TL;DR: BASIC_RV32s是一个开源的RISC-V RV32I架构框架，从单周期核心演进到5级流水线设计，包含完整的前向转发、动态分支预测和异常处理，在FPGA上实现1.09 DMIPS/MHz性能，提供完整的RTL源码和开发文档。


<details>
  <summary>Details</summary>
Motivation: 解决RISC-V架构理论知识与硬件实现之间的差距，为开源硬件生态系统提供可复现的教学路径。

Method: 采用Patterson和Hennessy经典方法学，从单周期核心逐步演进到5级流水线设计，集成完整的前向转发、动态分支预测和异常处理机制，在FPGA上进行验证。

Result: 在Xilinx Artix-7 FPGA上实现SoC设计，集成UART通信，在50MHz频率下达到1.09 DMIPS/MHz的性能指标。

Conclusion: BASIC_RV32s框架成功填补了RISC-V理论教学与实际硬件实现之间的空白，通过开源RTL源码、逻辑框图和发展日志，为开源硬件社区提供了有价值的教学资源。

Abstract: This paper introduces BASIC_RV32s, an open-source framework providing a
practical microarchitectural roadmap for the RISC-V RV32I architecture,
addressing the gap between theoretical knowledge and hardware implementation.
Following the classic Patterson and Hennessy methodology, the design evolves
from a basic single-cycle core to a 5-stage pipelined core design with full
hazard forwarding, dynamic branch prediction, and exception handling. For
verification, the final core design is integrated into a System-on-Chip (SoC)
with Universal Asynchronous Receiver-Transmitter (UART) communication
implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving
1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50
MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level
logic block diagrams, and development logs under MIT license on GitHub,
BASIC_RV32s offers a reproducible instructional pathway for the open-source
hardware ecosystem.

</details>


### [8] [Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol](https://arxiv.org/abs/2510.15888)
*Konstantinos Kafousis*

Main category: cs.AR

TL;DR: 提出了一种基于现有Load-Linked和Store-Conditional指令的硬件事务内存实现，无需修改缓存一致性协议，仅需在L1数据缓存中进行修改，适用于读写集不超过8个缓存行的小型事务。


<details>
  <summary>Details</summary>
Motivation: 传统HTM实现硬件复杂度高，需要修改ISA和缓存一致性协议，导致采用率低。本文旨在简化HTM实现，降低硬件复杂性。

Method: 通过扩展Load-Linked和Store-Conditional指令语义实现HTM，限制事务读写集不超过8个缓存行，仅需修改L1数据缓存，提供两种基于重试检测的前进保证机制。

Result: 在Gem5模拟器中验证，成功实现多个流行并发数据结构，在低竞争情况下性能优于TTS锁，跨节点竞争时中止率很低。

Conclusion: 提出的简化HTM设计在保持良好性能的同时大幅降低了实现复杂度，特别适合小型事务的并发应用场景。

Abstract: Hardware Transactional Memory (HTM) allows lock-free programming as easy as
with traditional coarse-grain locks or similar, while benefiting from the
performance advantages of fine-grained locking. Many HTM implementations have
been proposed, but they have not received widespread adoption because of their
high hardware complexity, their need for additions to the Instruction Set
Architecture (ISA), and often for modifications to the cache coherence
protocol.
  We show that HTM can be implemented without adding new instructions -- merely
by extending the semantics of two existing, Load-Linked and Store-Conditional.
Also, our proposed design does not modify or extend standard coherence
protocols. We further propose to drastically simplify the implementation of HTM
-- confined to modifications in the L1 Data Cache only -- by restricting it to
applications where the write set plus the read set of each transaction do not
exceed a small number of cache lines. We also propose two alternative
mechanisms to guarantee forward progress, both based on detecting retrial
attempts.
  We simulated our proposed design in Gem5, and we used it to implement several
popular concurrent data structures, showing that a maximum of eight (8) words
(cache lines) suffice for the write plus read sets. We provide a detailed
explanation of selected implementations, clarifying the intended usage of our
HTM from a programmer's perspective. We evaluated our HTM under varying
contention levels to explore its scalability limits. The results indicate that
our HTM provides good performance in concurrent data structures when contention
is spread across multiple nodes: in such cases, the percentage of aborts
relative to successful commits is very low. In the atomic fetch-and-increment
benchmark for multiple shared counters, the results show that, under
low-congestion, our HTM improves performance relative to the TTS lock.

</details>


### [9] [Accelerating Frontier MoE Training with 3D Integrated Optics](https://arxiv.org/abs/2510.15893)
*Mikhail Bernadskiy,Peter Carson,Thomas Graham,Taylor Groves,Ho John Lee,Eric Yeh*

Main category: cs.AR

TL;DR: 本文探讨了3D集成光互连技术如何解决AI工作负载增长中的互连瓶颈，通过光互连实现跨机架的大规模GPU集群，提升训练万亿参数MoE模型的效率。


<details>
  <summary>Details</summary>
Motivation: 传统电互连受限于1米传输距离，只能在同一机架内扩展，无法满足大规模AI模型训练需求。3D堆叠光互连技术为解决跨机架大规模GPU集群互连提供了新的解决方案。

Method: 采用3D共封装光学(CPO)技术，将光互连与GPU芯片堆叠集成，实现高带宽、低延迟的跨机架互连。通过建模分析3D CPO在训练万亿参数MoE模型时的性能表现。

Result: 3D CPO技术使扩展能力提升8倍，支持新的多维并行策略，训练时间减少2.7倍，实现了前所未有的模型扩展能力。

Conclusion: 3D集成光互连技术是解决AI工作负载扩展瓶颈的关键，为训练超大规模模型提供了可行的技术路径。

Abstract: The unabated growth in AI workload demands is driving the need for concerted
advances in compute, memory, and interconnect performance. As traditional
semiconductor scaling slows, high-speed interconnects have emerged as the new
scaling engine, enabling the creation of larger logical GPUs by linking many
GPUs into a single, low-latency, high-bandwidth compute domain. While initial
scale-up fabrics leveraged copper interconnects for their power and cost
advantages, the maximum reach of passive electrical interconnects
(approximately 1 meter) effectively limits the scale-up domain to within a
single rack. The advent of 3D-stacked optics and logic offers a transformative,
power-efficient scale-up solution for connecting hundreds of GPU packages
(thousands of GPUs) across multiple data center racks. This work explores the
design tradeoffs of scale-up technologies and demonstrates how frontier LLMs
necessitate novel photonic solutions to achieve aggressive power and
performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and
switches within the scale-up domain when training Frontier Mixture of Experts
(MoE) models exceeding one trillion parameters. Our results show that the
substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X
increase in scale-up capability. This affords new opportunities for
multi-dimensional parallelism within the scale-up domain and results in a 2.7X
reduction in time-to-train, unlocking unprecedented model scaling.

</details>


### [10] [DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms](https://arxiv.org/abs/2510.15897)
*Kien Le Trung,Truong-Son Hy*

Main category: cs.AR

TL;DR: DiffPlace是一个基于条件去噪扩散过程的芯片布局框架，能够在不重新训练的情况下泛化到未见过的电路网表，实现可转移的布局策略。


<details>
  <summary>Details</summary>
Motivation: 传统芯片布局方法依赖分析优化或强化学习，难以处理硬性布局约束或需要为每个新电路设计进行昂贵的在线训练。DiffPlace旨在解决这些限制。

Method: 将芯片布局制定为条件去噪扩散过程，利用扩散模型的生成能力探索布局空间，同时基于电路连接性和相对质量指标进行条件约束，结合能量引导采样和约束流形扩散确保布局合法性。

Result: 在所有实验场景中实现了极低的重叠率，弥合了基于优化和基于学习方法之间的差距。

Conclusion: DiffPlace为现代VLSI设计提供了一条实现自动化、高质量芯片布局的实用路径。

Abstract: Chip placement, the task of determining optimal positions of circuit modules
on a chip canvas, is a critical step in the VLSI design flow that directly
impacts performance, power consumption, and routability. Traditional methods
rely on analytical optimization or reinforcement learning, which struggle with
hard placement constraints or require expensive online training for each new
circuit design. To address these limitations, we introduce DiffPlace, a
framework that formulates chip placement as a conditional denoising diffusion
process, enabling transferable placement policies that generalize to unseen
circuit netlists without retraining. DiffPlace leverages the generative
capabilities of diffusion models to efficiently explore the vast space of
placement while conditioning on circuit connectivity and relative quality
metrics to identify optimal solutions globally. Our approach combines
energy-guided sampling with constrained manifold diffusion to ensure placement
legality, achieving extremely low overlap across all experimental scenarios.
Our method bridges the gap between optimization-based and learning-based
approaches, offering a practical path toward automated, high-quality chip
placement for modern VLSI design. Our source code is publicly available at:
https://github.com/HySonLab/DiffPlace/

</details>


### [11] [LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models](https://arxiv.org/abs/2510.15899)
*Kiran Thorat,Jiahui Zhao,Yaotian Liu,Amit Hasan,Hongwu Peng,Xi Xie,Bin Lei,Caiwen Ding*

Main category: cs.AR

TL;DR: VeriPPA框架使用大语言模型优化芯片设计的PPA（功耗-性能-面积）并生成准确的Verilog代码，在语法和功能正确性方面均超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在芯片设计领域的应用潜力，特别是PPA优化和Verilog代码生成这两个关键环节，以推动芯片设计自动化进程。

Method: 采用两阶段方法：第一阶段改进生成Verilog代码的功能和语法正确性；第二阶段优化Verilog代码以满足电路设计的PPA约束。

Result: 在RTLLM数据集上达到81.37%语法正确率和62.06%功能正确率；在VerilogEval数据集上达到99.56%语法正确率和43.79%功能正确率，均超越现有最优方法。

Conclusion: 大语言模型在复杂技术领域具有巨大潜力，为芯片设计自动化带来了令人鼓舞的发展前景。

Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks
to their ability to generate high- quality content from human instructions.
This paper delves into the field of chip design using LLMs, specifically in
Power- Performance-Area (PPA) optimization and the generation of accurate
Verilog codes for circuit designs. We introduce a novel framework VeriPPA
designed to optimize PPA and generate Verilog code using LLMs. Our method
includes a two-stage process where the first stage focuses on improving the
functional and syntactic correctness of the generated Verilog codes, while the
second stage focuses on optimizing the Verilog codes to meet PPA constraints of
circuit designs, a crucial element of chip design. Our framework achieves an
81.37% success rate in syntactic correctness and 62.06% in functional
correctness for code genera- tion, outperforming current state-of-the-art
(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework
achieves 99.56% syntactic correctness and 43.79% functional correctness, also
surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%
for functional correctness. Furthermore, Our framework able to optimize the PPA
of the designs. These results highlight the potential of LLMs in handling
complex technical areas and indicate an encouraging development in the
automation of chip design processes.

</details>


### [12] [Fully Automated Verification Framework for Configurable IPs: From Requirements to Results](https://arxiv.org/abs/2510.15902)
*Shuhang Zhang,Jelena Radulovic,Thorsten Dworzak*

Main category: cs.AR

TL;DR: 提出全自动框架用于需求驱动的功能验证，自动化vPlan生成、测试平台创建、回归执行和报告等关键流程，大幅降低验证工作量


<details>
  <summary>Details</summary>
Motivation: 半导体行业竞争加剧导致芯片价格压力增大，而功能验证特别是可配置IP的验证是开发成本的主要贡献者，因其复杂性和资源密集型特性

Method: 开发全自动需求驱动功能验证框架，自动化vPlan生成、测试平台创建、回归执行和需求管理工具中的报告等关键流程

Result: 该框架大幅减少验证工作量，加速开发周期，最小化人为错误，提高覆盖率

Conclusion: 该框架为验证可配置IP提供了可扩展且高效的解决方案

Abstract: The increasing competition in the semiconductor industry has created
significant pressure to reduce chip prices while maintaining quality and
reliability. Functional verification, particularly for configurable IPs, is a
major contributor to development costs due to its complexity and
resource-intensive nature. To address this, we propose a fully automated
framework for requirements driven functional verification. The framework
automates key processes, including vPlan generation, testbench creation,
regression execution, and reporting in a requirements management tool,
drastically reducing verification effort. This approach accelerates development
cycles, minimizes human error, and enhances coverage, offering a scalable and
efficient solution to the challenges of verifying configurable IPs.

</details>


### [13] [NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme](https://arxiv.org/abs/2510.15904)
*Subhradip Chakraborty,Ankur Singh,Xuming Chen,Gourav Datta,Akhilesh R. Jaiswal*

Main category: cs.AR

TL;DR: 提出了一种将RRAM集成到传统6T-SRAM中的NVM-in-Cache架构，形成6T-2R混合单元，支持存内计算，在保持缓存数据的同时实现高能效的并行MAC操作。


<details>
  <summary>Details</summary>
Motivation: DNN工作负载快速增长导致对片上SRAM容量需求激增，SRAM阵列占用大量芯片面积，需要解决存储密度和计算效率的双重挑战。

Method: 在传统6T-SRAM单元中集成RRAM器件形成6T-2R混合单元，利用其固有特性实现存内计算模式，通过缓存电源线直接执行并行乘累加操作。

Result: 在22nm FDSOI技术中实现0.4 TOPS吞吐量和491.78 TOPS/W能效，使用Resnet-18网络在CIFAR-10分类任务上达到91.27%准确率。

Conclusion: NVM-in-Cache方法通过重新利用现有6T SRAM缓存架构，为下一代AI加速器和通用处理器提供可扩展、高能效的计算方案。

Abstract: The rapid growth of deep neural network (DNN) workloads has significantly
increased the demand for large-capacity on-chip SRAM in machine learning (ML)
applications, with SRAM arrays now occupying a substantial fraction of the
total die area. To address the dual challenges of storage density and
computation efficiency, this paper proposes an NVM-in-Cache architecture that
integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell,
forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory
(PIM) mode, which performs massively parallel multiply-and-accumulate (MAC)
operations directly on cache power lines while preserving stored cache data. By
exploiting the intrinsic properties of the 6T-2R structure, the architecture
achieves additional storage capability, high computational throughput without
any bit-cell area overhead. Circuit- and array-level simulations in
GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design
achieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel
operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18
neural network, achieving an accuracy of 91.27%. These results highlight the
potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient
computing method by re-purposing existing 6T SRAM cache architecture for
next-generation AI accelerators and general purpose processors.

</details>


### [14] [FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures](https://arxiv.org/abs/2510.15906)
*Yunsheng Bai,Ghaith Bany Hamad,Chia-Tung Ho,Syed Suhaib,Haoxing Ren*

Main category: cs.AR

TL;DR: FVDebug是一个自动化硬件设计形式验证失败调试的智能系统，通过结合波形、RTL代码和设计规范，将失败跟踪转化为可操作的见解，并提供具体的RTL修复方案。


<details>
  <summary>Details</summary>
Motivation: 解决硬件设计形式验证失败调试的瓶颈问题，该过程通常需要工程师手动分析复杂反例、波形和设计规范，耗时数小时甚至数天。现有工具无法处理设计意图与实现逻辑之间的复杂交互。

Method: 采用新颖的三阶段流水线：1) 因果图合成，将失败跟踪结构化为有向无环图；2) 图扫描器，使用批量大语言模型分析和正反提示来识别可疑节点；3) 洞察漫游器，利用代理叙事探索生成高级因果解释。系统还包含修复生成器提供具体RTL修复。

Result: 在开放基准测试中，FVDebug实现了高假设质量和强Pass@k修复率。在两个专有的生产级形式验证反例上的结果进一步证明了系统从学术基准到工业设计的适用性。

Conclusion: FVDebug成功展示了将形式验证失败跟踪转化为可操作见解的能力，通过智能分析多数据源实现了从学术基准到工业设计的有效调试解决方案。

Abstract: Debugging formal verification (FV) failures represents one of the most
time-consuming bottlenecks in modern hardware design workflows. When properties
fail, engineers must manually trace through complex counter-examples spanning
multiple cycles, analyze waveforms, and cross-reference design specifications
to identify root causes - a process that can consume hours or days per bug.
Existing solutions are largely limited to manual waveform viewers or simple
automated tools that cannot reason about the complex interplay between design
intent and implementation logic. We present FVDebug, an intelligent system that
automates root-cause analysis by combining multiple data sources - waveforms,
RTL code, design specifications - to transform failure traces into actionable
insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis
that structures failure traces into directed acyclic graphs, (2) Graph Scanner
using batched Large Language Model (LLM) analysis with for-and-against
prompting to identify suspicious nodes, and (3) Insight Rover leveraging
agentic narrative exploration to generate high-level causal explanations.
FVDebug further provides concrete RTL fixes through its Fix Generator.
Evaluated on open benchmarks, FVDebug attains high hypothesis quality and
strong Pass@k fix rates. We further report results on two proprietary,
production-scale FV counterexamples. These results demonstrate FVDebug's
applicability from academic benchmarks to industrial designs.

</details>


### [15] [Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions](https://arxiv.org/abs/2510.15907)
*Era Thaqi,Dennis Eigner,Arman Ferdowsi,Ulrich Schmid*

Main category: cs.AR

TL;DR: 提出了一种基于解析延迟公式的符号时序分析方法，通过计算内部信号转换时间的闭式解析表达式，实现无需仿真的时序分析和灵敏度分析。


<details>
  <summary>Details</summary>
Motivation: 传统时序分析通常依赖仿真，无法提供对时序特性与输入信号及门参数依赖关系的解析理解。需要一种能够进行符号化分析的方法来深入理解电路时序行为。

Method: 基于Ferdowsi等人开发的2输入NOR、NAND和Muller-C门的解析延迟公式，在固定信号转换顺序下，计算内部信号转换时间的闭式解析表达式，这些表达式依赖于输入信号的符号转换时间和相关门的模型参数。

Result: 实现了基于SageMath计算机代数系统的框架，并在c17 slack基准电路的NOR门版本上进行了验证，能够进行无仿真的时序分析和灵敏度分析。

Conclusion: 该方法不仅能够进行传统的时序分析，更重要的是能够解析地研究时序特性对输入信号和门参数的依赖关系，为电路设计和优化提供了新的分析工具。

Abstract: We propose a novel approach to symbolic timing analysis for digital
integrated circuits based on recently developed analytic delay formulas for
2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a
fixed order of the transitions of all input and internal signals of a circuit,
our framework computes closed-form analytic delay expressions for all the
internal signal transition times that depend on (i) the symbolic transition
times of the relevant input signals and (ii) the model parameters of the
relevant gates. The resulting formulas facilitate per-transition timing
analysis without any simulation, by instantiating the symbolic input transition
times and the gate parameters. More importantly, however, they also enable an
\emph{analytic} study of the dependencies of certain timing properties on input
signals and gate parameters. For instance, differentiating a symbolic delay
expression with respect to a gate parameter or input transition time enables
sensitivity analysis. As a proof of concept, we implement our approach using
the computer algebra system SageMath and apply it to the NOR-gate version of
the c17 slack benchmark circuit.

</details>


### [16] [Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations](https://arxiv.org/abs/2510.15908)
*Hana Chitsaz,Johnson Umeike,Amirmahdi Namjoo,Babak N. Safa,Bahar Asgari*

Main category: cs.AR

TL;DR: Belenos对生物力学有限元模拟进行了全面的工作负载分析，揭示了小工作负载存在前端瓶颈（约13.1%），大工作负载存在显著后端瓶颈（59.9%-82.2%），并通过gem5敏感性研究确定了DSAs的最佳硬件配置。


<details>
  <summary>Details</summary>
Motivation: 当前生物力学有限元模拟在硬件和软件架构上存在效率限制，特别是在材料参数识别等迭代任务中，往往需要在精度和可处理性之间权衡。可重构硬件如FPGA提供了领域特定加速的潜力，但其在生物力学中的应用尚未充分探索。

Method: 使用FEBio模拟器进行生物力学有限元工作负载表征，结合gem5敏感性研究和VTune性能分析，识别硬件瓶颈和优化配置。

Result: VTune分析显示小工作负载前端停滞约13.1%，大工作负载后端瓶颈达59.9%-82.2%。gem5研究表明次优的流水线、内存或分支预测器设置可使性能下降达37.1%。

Conclusion: 需要架构感知的协同设计来有效支持生物力学模拟工作负载，领域特定加速器需要针对特定工作负载特征进行优化配置。

Abstract: Finite element simulations are essential in biomechanics, enabling detailed
modeling of tissues and organs. However, architectural inefficiencies in
current hardware and software stacks limit performance and scalability,
especially for iterative tasks like material parameter identification. As a
result, workflows often sacrifice fidelity for tractability. Reconfigurable
hardware, such as FPGAs, offers a promising path to domain-specific
acceleration without the cost of ASICs, but its potential in biomechanics
remains underexplored. This paper presents Belenos, a comprehensive workload
characterization of finite element biomechanics using FEBio, a widely adopted
simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal
that smaller workloads experience moderate front-end stalls, typically around
13.1%, whereas larger workloads are dominated by significant back-end
bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.
Complementary gem5 sensitivity studies identify optimal hardware configurations
for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,
memory, or branch predictor settings can degrade performance by up to 37.1%.
These findings underscore the need for architecture-aware co-design to
efficiently support biomechanical simulation workloads.

</details>


### [17] [SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs](https://arxiv.org/abs/2510.15910)
*Marvin Fuchs,Lukas Scheller,Timo Muscheid,Oliver Sander,Luis E. Ardila-Perez*

Main category: cs.AR

TL;DR: SoCks是一个灵活可扩展的构建框架，通过将SoC镜像分区为高级单元（块）来降低复杂性，实现独立构建和最小化依赖，使构建速度比现有工具快3倍。


<details>
  <summary>Details</summary>
Motivation: 现代异构SoC设备集成复杂组件，开发工具日益复杂但缺乏足够支持，导致学习曲线陡峭和故障排除困难。

Method: 将SoC镜像分区为高层次的块单元，每个固件和软件块以封装方式独立构建，通过标准化接口实现块间信息交换，最小化依赖关系。

Result: SoCks能够比现有工具快3倍构建完整的SoC镜像，简化现有块实现的复用，促进版本间无缝替换。

Conclusion: SoCks框架有效降低了SoC开发的复杂性，通过模块化设计和标准化接口支持去中心化和部分自动化的开发流程。

Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced
components into a single package, offering powerful capabilities while also
introducing significant complexity. To manage these sophisticated devices,
firmware and software developers need powerful development tools. However, as
these tools become increasingly complex, they often lack adequate support,
resulting in a steep learning curve and challenging troubleshooting. To address
this, this work introduces System-on-Chip blocks (SoCks), a flexible and
expandable build framework that reduces complexity by partitioning the SoC
image into high-level units called blocks. SoCks builds each firmware and
software block in an encapsulated way, independently from other components of
the image, thereby reducing dependencies to a minimum. While some information
exchange between the blocks is unavoidable to ensure seamless runtime
integration, this interaction is standardized via interfaces. A small number of
dependencies and well-defined interfaces simplify the reuse of existing block
implementations and facilitate seamless substitution between versions-for
instance, when choosing root file systems for the embedded Linux operating
system. Additionally, this approach facilitates the establishment of a
decentralized and partially automated development flow through Continuous
Integration and Continuous Delivery (CI/CD). Measurement results demonstrate
that SoCks can build a complete SoC image up to three times faster than
established tools.

</details>


### [18] [VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts](https://arxiv.org/abs/2510.15914)
*Jiayu Zhao,Song Chen*

Main category: cs.AR

TL;DR: VeriGRAG框架通过图神经网络提取Verilog代码的结构图嵌入，结合多模态检索器选择相关嵌入，生成结构感知的软提示，显著提升LLM生成Verilog代码的正确性。


<details>
  <summary>Details</summary>
Motivation: Verilog代码编码了硬件电路的结构信息，但现有方法未能有效利用这些结构信息来提升LLM生成代码的功能和语法正确性。

Method: 使用GNN提取Verilog代码的结构图嵌入，多模态检索器选择相关嵌入，通过VeriFormer模块对齐代码模态生成结构感知软提示。

Result: 在VerilogEval和RTLLM基准测试中达到最先进或更优性能，显著提高了Verilog代码生成的正确性。

Conclusion: VeriGRAG框架通过有效利用Verilog代码的结构信息，成功提升了LLM生成Verilog代码的质量和正确性。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
generating Verilog code from natural language descriptions. However, Verilog
code inherently encodes structural information of hardware circuits.
Effectively leveraging this structural information to enhance the functional
and syntactic correctness of LLM-generated Verilog code remains a significant
challenge. To address this challenge, we propose VeriGRAG , a novel framework
that extracts structural graph embeddings from Verilog code using graph neural
networks (GNNs). A multimodal retriever then selects the graph embeddings most
relevant to the given generation task, which are aligned with the code modality
through the VeriFormer module to generate structure-aware soft prompts. Our
experiments demonstrate that VeriGRAG substantially improves the correctness of
Verilog code generation, achieving state-of-the-art or superior performance
across both VerilogEval and RTLLM benchmarks.

</details>


### [19] [Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding](https://arxiv.org/abs/2510.15917)
*Shai Bergman,Won Wook Song,Lukas Cavigelli,Konstantin Berestizshevsky,Ke Zhou,Ji Zhang*

Main category: cs.AR

TL;DR: 提出意图驱动存储系统(IDSS)，利用大语言模型从非结构化信号中推断工作负载意图，指导存储系统自适应和跨层参数重配置，在FileBench测试中可将IOPS提升至多2.45倍。


<details>
  <summary>Details</summary>
Motivation: 现有存储系统缺乏对工作负载意图的可见性，导致启发式方法脆弱且优化零散，无法适应现代大规模数据密集型应用的语义需求。

Method: 提出IDSS架构，将LLM集成到存储控制回路中，通过四个设计原则：意图推断、跨层重配置、策略护栏和结构化工作流，实现语义驱动的存储优化。

Result: 在FileBench工作负载测试中，IDSS通过解释意图并为缓存和预取等存储组件生成可操作的配置，将IOPS提升至多2.45倍。

Conclusion: 在策略护栏约束下，LLM可作为高层次语义优化器，弥合应用目标与底层系统控制之间的差距，推动存储系统向更自适应、自主和动态对齐的方向发展。

Abstract: Existing storage systems lack visibility into workload intent, limiting their
ability to adapt to the semantics of modern, large-scale data-intensive
applications. This disconnect leads to brittle heuristics and fragmented,
siloed optimizations. To address these limitations, we propose Intent-Driven
Storage Systems (IDSS), a vision for a new paradigm where large language models
(LLMs) infer workload and system intent from unstructured signals to guide
adaptive and cross-layer parameter reconfiguration. IDSS provides holistic
reasoning for competing demands, synthesizing safe and efficient decisions
within policy guardrails. We present four design principles for integrating
LLMs into storage control loops and propose a corresponding system
architecture. Initial results on FileBench workloads show that IDSS can improve
IOPS by up to 2.45X by interpreting intent and generating actionable
configurations for storage components such as caching and prefetching. These
findings suggest that, when constrained by guardrails and embedded within
structured workflows, LLMs can function as high-level semantic optimizers,
bridging the gap between application goals and low-level system control. IDSS
points toward a future in which storage systems are increasingly adaptive,
autonomous, and aligned with dynamic workload demands.

</details>


### [20] [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)
*Ye Qiao,Zhiheng Chen,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe是一个基于查表的1.58位三元LLM加速器，专为低功耗边缘FPGA设计，支持预填充和自回归解码，在5W功耗下实现高达25 tokens/s的解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备和嵌入式系统的普及，在边缘平台上部署大型语言模型成为迫切需求，但受限于计算资源、内存需求和功耗预算，特别是预填充阶段的长延迟问题。

Method: 采用表查找三元矩阵乘法引擎、细粒度URAM权重缓冲管理、流式数据流架构、反向重排序预填充注意力以及资源高效解码注意力等创新技术。

Result: 在5W功耗预算下，实现25 tokens/s的解码吞吐量，对于64-128个token的提示，首token延迟为0.45-0.96秒。

Conclusion: TeLLMe在边缘FPGA上的LLM推理能效方面取得了显著进步，为资源受限的边缘设备提供了高效的LLM部署解决方案。

Abstract: With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.

</details>


### [21] [UPMEM Unleashed: Software Secrets for Speed](https://arxiv.org/abs/2510.15927)
*Krystian Chmielewski,Jarosław Ławnicki,Uladzislau Lukyanau,Tadeusz Kobus,Maciej Maciejewski*

Main category: cs.AR

TL;DR: 本文揭示了UPMEM PIM平台软件栈中的低效问题，通过汇编优化、位串行处理等技术，在整数运算和矩阵向量乘法上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: PIM平台在数据管理和并行编程方面面临独特挑战，现有SDK仍有较大性能优化空间，需要探索非标准编程技术来提升效率。

Method: 修改UPMEM编译器生成的汇编代码，采用位串行处理低精度数据，扩展API以考虑NUMA架构，优化主机-PIM数据传输。

Result: 整数加法速度提升1.6-2倍，乘法提升1.4-5.9倍；INT4位串行点积计算速度提升2.7倍；主机-PIM数据传输吞吐量提升2.9倍；INT8 GEMV比CPU快3倍，INT4 GEMV快10倍。

Conclusion: 通过简单的软件栈优化和编程技术改进，可以显著提升PIM平台的性能，特别是在低精度矩阵运算方面具有明显优势。

Abstract: Developing kernels for Processing-In-Memory (PIM) platforms poses unique
challenges in data management and parallel programming on limited processing
units. Although software development kits (SDKs) for PIM, such as the UPMEM
SDK, provide essential tools, these emerging platforms still leave significant
room for performance optimization. In this paper, we reveal surprising
inefficiencies in UPMEM software stack and play with non-standard programming
techniques. By making simple modifications to the assembly generated by the
UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x
in integer multiplication, depending on the data type. We also demonstrate that
bit-serial processing of low precision data is a viable option for UPMEM: in
INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup
over the baseline. Minor API extensions for PIM allocation that account for the
non-uniform memory access (NUMA) architecture of the server further improve the
consistency and throughput of host-PIM data transfers by up to 2.9x. Finally,
we show that, when the matrix is preloaded into PIM, our optimized kernels
outperform a dual-socket CPU server by over 3x for INT8 generalized
matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized
INT8 GEMV kernel outperforms the baseline 3.5x.

</details>


### [22] [Implémentation Efficiente de Fonctions de Convolution sur FPGA à l'Aide de Blocs Paramétrables et d'Approximations Polynomiales](https://arxiv.org/abs/2510.15930)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: 提出可配置卷积块库和FPGA资源预测模型，优化CNN在FPGA上的部署，解决硬件知识需求和长设计周期问题。


<details>
  <summary>Details</summary>
Motivation: FPGA实现CNN具有低延迟、高能效和灵活性优势，但开发复杂，需要硬件知识且设计周期长，难以快速探索网络配置和资源优化。

Method: 设计可配置卷积块库以适应硬件资源约束，并开发数学模型预测FPGA资源利用率，通过参数相关性分析和误差指标验证。

Result: 设计的卷积块能够根据硬件约束调整卷积层，模型能准确预测资源消耗，为FPGA选择和CNN优化部署提供有用工具。

Conclusion: 该方法有效解决了FPGA上CNN实现的复杂性问题，实现了资源约束下的优化部署和快速设计迭代。

Abstract: Implementing convolutional neural networks (CNNs) on field-programmable gate
arrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower
latency, greater power efficiency and greater flexibility. However, this
development remains complex due to the hardware knowledge required and the long
synthesis, placement and routing stages, which slow down design cycles and
prevent rapid exploration of network configurations, making resource
optimisation under severe constraints particularly challenging. This paper
proposes a library of configurable convolution Blocks designed to optimize FPGA
implementation and adapt to available resources. It also presents a
methodological framework for developing mathematical models that predict FPGA
resources utilization. The approach is validated by analyzing the correlation
between the parameters, followed by error metrics. The results show that the
designed blocks enable adaptation of convolution layers to hardware
constraints, and that the models accurately predict resource consumption,
providing a useful tool for FPGA selection and optimized CNN deployment.

</details>


### [23] [Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing](https://arxiv.org/abs/2510.16040)
*Tianhua Xia,Sai Qian Zhang*

Main category: cs.AR

TL;DR: 提出Kelle软硬件协同设计方案，通过在边缘设备上使用eDRAM存储LLM的KV缓存，结合细粒度内存管理算法，实现3.9倍加速和4.5倍能耗节省


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上运行LLM可降低延迟、提升实时处理能力和隐私保护，但KV缓存随序列长度线性增长导致内存占用大，而边缘设备资源有限难以高效存储和访问大缓存

Method: 使用eDRAM作为边缘设备LLM服务的主要存储，提出Kelle软硬件协同设计，结合细粒度内存驱逐、重计算和刷新控制算法

Result: Kelle加速器相比现有基线方案实现3.9倍加速和4.5倍能耗节省

Conclusion: Kelle方案有效解决了边缘设备部署LLM时的KV缓存内存瓶颈问题，通过eDRAM和优化算法显著提升了性能和能效

Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing
latency, improving real-time processing, and enhancing privacy. By performing
inference directly on the device, data does not need to be sent to the cloud,
ensuring faster responses and reducing reliance on network connectivity.
However, implementing LLMs on edge devices presents challenges, particularly
with managing key-value (KV) caches, which plays a pivotal role in LLM serving.
As the input text lengthens, the size of the KV cache increases linearly with
the sequence length, leading to a significant memory footprint and data access
costs. On the other hand, edge devices have limited memory and computational
power, making it hard to store and efficiently access the large caches needed
for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using
embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,
which offers higher storage density compared to SRAM. However, to ensure data
integrity, eDRAM needs periodic refresh operations, which are power-intensive.
To reduce eDRAM costs and improve overall system performance, we
propose~\textit{Kelle}, a software-hardware co-design solution optimized for
deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained
memory eviction, recomputation, and refresh control algorithms, the
\textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$
energy savings compared to existing baseline solutions.

</details>


### [24] [Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project](https://arxiv.org/abs/2510.16487)
*Giovanni Agosta,Stefano Cherubin,Derek Christ,Francesco Conti,Asbjørn Djupdal,Matthias Jung,Georgios Keramidas,Roberto Passerone,Paolo Rech,Elisa Ricci,Philippe Velha,Flavio Vella,Kasim Sinan Yildirim,Nils Wilbert*

Main category: cs.AR

TL;DR: ARCHYTAS项目旨在设计和评估非传统硬件加速器，包括光电、易失性和非易失性内存计算以及神经形态计算，以解决AI的功耗、效率和可扩展性瓶颈，重点关注国防应用。


<details>
  <summary>Details</summary>
Motivation: 解决AI在国防应用（如自动驾驶车辆、监控无人机、海上和太空平台）中的功耗、效率和可扩展性瓶颈问题。

Method: 开发系统架构和软件栈来集成和支持这些加速器，并开发用于全系统及其组件早期原型设计的仿真软件。

Result: 提出了ARCHYTAS项目的整体架构和软件方案，包括系统集成框架和仿真工具链。

Conclusion: ARCHYTAS项目通过开发非传统硬件加速器和相应的软件基础设施，为国防AI应用提供高效、可扩展的解决方案。

Abstract: ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,
in particular, optoelectronic, volatile and non-volatile processing-in-memory,
and neuromorphic, to tackle the power, efficiency, and scalability bottlenecks
of AI with an emphasis on defense use cases (e.g., autonomous vehicles,
surveillance drones, maritime and space platforms). In this paper, we present
the system architecture and software stack that ARCHYTAS will develop to
integrate and support those accelerators, as well as the simulation software
needed for early prototyping of the full system and its components.

</details>


### [25] [Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization](https://arxiv.org/abs/2510.16622)
*Kazi Ababil Azam,Hasan Masum,Masfiqur Rahaman,A. B. M. Alim Al Islam*

Main category: cs.AR

TL;DR: 开发适用于孟加拉国等发展中国家非车道化、异质交通的智能交通信号系统，使用RTSP视频流、树莓派4B处理和YOLO目标检测，结合NSGA-II多目标优化算法优化信号配时。


<details>
  <summary>Details</summary>
Motivation: 解决达卡等发展中国家城市因车辆密度高导致的严重交通拥堵问题，现有智能交通信号技术主要针对发达国家的结构化交通，不适用于非车道化、异质交通环境。

Method: 利用RTSP视频流采集交通数据，在树莓派4B上运行基于YOLO的目标检测模型（使用NHT-1071数据集训练），结合NSGA-II多目标优化算法生成优化的信号配时方案。

Result: 在达卡Palashi的五路交叉口进行测试，证明该系统能显著改善交通管理，减少等待时间并提高车辆通行能力。

Conclusion: 该测试平台为具有复杂交通动态的发展中国家地区开发更符合情境的智能交通信号解决方案铺平了道路。

Abstract: The vehicular density in urbanizing cities of developing countries such as
Dhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road
experiences. Traffic signaling is a key component in effective traffic
management for such situations, but the advancements in intelligent traffic
signaling have been exclusive to developed countries with structured traffic.
The non-lane-based, heterogeneous traffic of Dhaka City requires a contextual
approach. This study focuses on the development of an intelligent traffic
signaling system feasible in the context of developing countries such as
Bangladesh. We propose a pipeline leveraging Real Time Streaming Protocol
(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of
the art YOLO-based object detection model trained on the Non-lane-based and
Heterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous
traffic. A multi-objective optimization algorithm, NSGA-II, then generates
optimized signal timings, minimizing waiting time while maximizing vehicle
throughput. We test our implementation in a five-road intersection at Palashi,
Dhaka, demonstrating the potential to significantly improve traffic management
in similar situations. The developed testbed paves the way for more contextual
and effective Intelligent Traffic Signaling (ITS) solutions for developing
areas with complicated traffic dynamics such as Dhaka City.

</details>


### [26] [SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding](https://arxiv.org/abs/2510.17251)
*Chengxi Li,Yang Sun,Lei Chen,Yiwen Wang,Mingxuan Yuan,Evangeline F. Y. Young*

Main category: cs.AR

TL;DR: smaRTLy是一种针对RTL逻辑综合中多路复用器优化的新技术，通过移除冗余多路复用器树并重构剩余结构，显著减少门电路数量。


<details>
  <summary>Details</summary>
Motivation: 传统工具如Yosys通过遍历多路复用器树和监控控制端口值来优化，但未能充分利用信号间的内在逻辑关系或结构优化潜力。

Method: 开发创新策略来移除冗余多路复用器树并重构剩余结构，采用逻辑推理和结构重建技术。

Result: 在IWLS-2005和RISC-V基准测试中，相比Yosys额外减少8.95%的AIG面积；在百万门级工业基准测试中，比Yosys多移除47.2%的AIG面积。

Conclusion: smaRTLy的逻辑推理和结构重建技术能有效增强RTL优化过程，实现更高效的硬件设计。

Abstract: This paper proposes smaRTLy: a new optimization technique for multiplexers in
Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very
common in RTL designs, and traditional tools like Yosys optimize them by
traversing the tree and monitoring control port values. However, this method
does not fully exploit the intrinsic logical relationships among signals or the
potential for structural optimization. To address these limitations, we develop
innovative strategies to remove redundant multiplexer trees and restructure the
remaining ones, significantly reducing the overall gate count. We evaluate
smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%
reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an
industrial benchmark in the scale of millions of gates, results show that
smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate
the effectiveness of our logic inferencing and structural rebuilding techniques
in enhancing the RTL optimization process, leading to more efficient hardware
designs.

</details>
