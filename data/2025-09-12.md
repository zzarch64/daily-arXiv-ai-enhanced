<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Implementation of a 8-bit Wallace Tree Multiplier](https://arxiv.org/abs/2509.09178)
*Ayan Biswas,Jimmy Jin*

Main category: cs.AR

TL;DR: 这篇论文介绍了在Cadence Virtuoso中设计和布局Wallace树8位乘法器的过程，包括乘积累加单元的实现


<details>
  <summary>Details</summary>
Motivation: 设计并优化Wallace树乘法器的底层电路，以减小最坏情况下的时间复杂度，达到O(log(n))的性能

Method: 使用Cadence Virtuoso工具，在gpdk45技术上设计进行8位Wallace树乘法器的原理图和布局，并实现16位组合逻辑乘加单元

Result: 完成了Wallace树8位乘法器的设计和布局，以及16位乘积累加单元的实现

Conclusion: 项目成功实现了并行乘法器的优化设计，为高速数字电路提供了有效的解决方案

Abstract: Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.

</details>


### [2] [Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference](https://arxiv.org/abs/2509.09505)
*Haoran Wu,Can Xiao,Jiayi Nie,Xuan Guo,Binglei Lou,Jeffrey T. H. Wong,Zhiwen Mo,Cheng Zhang,Przemyslaw Forys,Wayne Luk,Hongxiang Fan,Jianyi Cheng,Timothy M. Jones,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: PLENA是一个硬件-软件协同设计的系统，专门针对长上下文LLM推理中的内存墙问题，通过非对称量化方案、扁平化脉动阵列架构和完整软件栈优化，实现了比现有加速器更高的利用率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: LLM代理推理任务（如工具使用、网页操作等）需要处理超长上下文，导致严重的离片内存流量问题，面临带宽和容量两个内存墙限制，使得计算单元利用率低下。

Method: 采用硬件-软件协同设计，包括：1）支持非对称量化方案的高效硬件实现；2）原生支持FlashAttention的扁平化脉动阵列架构；3）完整的软件栈（自定义ISA、编译器、周期仿真器和自动化设计空间探索流程）。

Result: 仿真结果显示：PLENA比现有加速器利用率提高8.5倍，在相同乘法器数量和内存配置下，吞吐量比A100 GPU高2.24倍，比TPU v6e高3.85倍。

Conclusion: PLENA系统有效解决了长上下文LLM推理中的内存墙问题，显著提升了硬件利用率和推理性能，该系统将开源发布。

Abstract: LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.

</details>
