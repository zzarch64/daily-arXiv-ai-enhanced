<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 11]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [NuRedact: Non-Uniform eFPGA Architecture for Low-Overhead and Secure IP Redaction](https://arxiv.org/abs/2601.11770)
*Voktho Das,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: NuRedact是一个非均匀eFPGA重删框架，通过定制化架构平衡安全性和效率，相比传统均匀结构实现高达9倍的面积缩减，同时保持强大的安全弹性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LUT和eFPGA的可重构重删技术虽然提高了安全性，但带来了巨大的开销，这些开销主要来自为阻碍逆向工程而引入的人工复杂性，而非真正的功能可重构需求，导致硬件利用率低且安全成本过高。

Method: 作为OpenFPGA基础设施的扩展，NuRedact采用三阶段方法：1) 生成具有引脚映射不规则性的定制化硬件结构；2) 在VPR级别进行修改，支持基于Python自动化优化器的非均匀布局；3) 对目标IP模块进行重删感知的重新配置和映射。

Result: 实验结果显示，相比传统均匀结构，NuRedact实现了高达9倍的面积缩减，在保持与LUT基甚至晶体管级重删技术竞争效率的同时，维持了强大的安全弹性。该框架能有效抵抗SAT基、循环和时序变体等先进攻击模型。

Conclusion: NuRedact通过引入架构非均匀性，在安全性和效率之间实现了更好的平衡，为集成电路供应链安全提供了一种更实用的重删解决方案，显著降低了安全开销同时保持了强大的攻击抵抗力。

Abstract: While logic locking has been extensively studied as a countermeasure against integrated circuit (IC) supply chain threats, recent research has shifted toward reconfigurable-based redaction techniques, e.g., LUT- and eFPGA-based schemes. While these approaches raise the bar against attacks, they incur substantial overhead, much of which arises not from genuine functional reconfigurability need, but from artificial complexity intended solely to frustrate reverse engineering (RE). As a result, fabrics are often underutilized, and security is achieved at disproportionate cost. This paper introduces NuRedact, the first full-custom eFPGA redaction framework that embraces architectural non-uniformity to balance security and efficiency. Built as an extension of the widely adopted OpenFPGA infrastructure, NuRedact introduces a three-stage methodology: (i) custom fabric generation with pin-mapping irregularity, (ii) VPR-level modifications to enable non-uniform placement guided by an automated Python-based optimizer, and (iii) redaction-aware reconfiguration and mapping of target IP modules. Experimental results show up to 9x area reduction compared to conventional uniform fabrics, achieving competitive efficiency with LUT-based and even transistor-level redaction techniques while retaining strong resilience. From a security perspective, NuRedact fabrics are evaluated against state-of-the-art attack models, including SAT-based, cyclic, and sequential variants, and show enhanced resilience while maintaining practical design overheads.

</details>


### [2] [Domain-specific Hardware Acceleration for Model Predictive Path Integral Control](https://arxiv.org/abs/2601.12089)
*Erwan Tanguy-Legac,Tommaso Belvedere,Gianluca Corsini,Marco Tognon,Marcello Traiola*

Main category: cs.AR

TL;DR: 提出基于硬件加速器的MPPI控制器，相比GPU实现更精确且能耗更低


<details>
  <summary>Details</summary>
Motivation: 机器人实时控制中，MPC难以应用于非线性系统，MPPI计算负载大，GPU加速能耗过高，不适合电池供电的自主系统

Method: 设计并模拟执行MPPI硬件加速器，采用定制化设计（可能基于FPGA）来加速MPPI控制算法

Result: MPPI定制加速器比基于GPU的MPPI实现能够生成更精确的轨迹

Conclusion: 硬件加速器为MPPI控制提供了高效低能耗的解决方案，特别适合电池供电的自主机器人系统

Abstract: Accurately controlling a robotic system in real time is a challenging problem. To address this, the robotics community has adopted various algorithms, such as Model Predictive Control (MPC) and Model Predictive Path Integral (MPPI) control. The first is difficult to implement on non-linear systems such as unmanned aerial vehicles, whilst the second requires a heavy computational load. GPUs have been successfully used to accelerate MPPI implementations; however, their power consumption is often excessive for autonomous or unmanned targets, especially when battery-powered. On the other hand, custom designs, often implemented on FPGAs, have been proposed to accelerate robotic algorithms while consuming considerably less energy than their GPU (or CPU) implementation. However, no MPPI custom accelerator has been proposed so far. In this work, we present a hardware accelerator for MPPI control and simulate its execution. Results show that the MPPI custom accelerator allows more accurate trajectories than GPU-based MPPI implementations.

</details>


### [3] [Biological Intuition on Digital Hardware: An RTL Implementation of Poisson-Encoded SNNs for Static Image Classification](https://arxiv.org/abs/2601.12156)
*Debabrata Das,Yogeeth G. K.,Arnav Gupta*

Main category: cs.AR

TL;DR: 该论文设计了一个基于SystemVerilog的精确周期硬件SNN核心，采用LIF神经元模型和定点运算，通过片上泊松编码和主动剪枝机制，在FPGA/ASIC平台上实现高效节能的神经形态计算。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的人工智能部署（TinyML）常受传统人工神经网络高功耗和延迟的限制，特别是其依赖密集的矩阵乘法运算。神经形态计算通过事件驱动处理模拟生物效率，提供了有吸引力的替代方案。

Method: 设计并实现了一个基于SystemVerilog的精确周期硬件SNN核心，采用LIF神经元模型，使用定点算术和位级原语（移位和加法）替代复杂浮点硬件。架构包含片上泊松编码器用于随机脉冲生成，以及新颖的主动剪枝机制，在分类后动态禁用神经元以减少动态功耗。

Result: 通过全连接层实现数字分类，仿真结果表明设计在有限时间步内快速收敛（89%准确率），同时相比传统密集架构显著降低了计算开销。

Conclusion: 这项工作为FPGA和ASIC平台上可扩展、高能效的神经形态硬件提供了基础构建模块，展示了硬件SNN核心在边缘计算中的潜力。

Abstract: The deployment of Artificial Intelligence on edge devices (TinyML) is often constrained by the high power consumption and latency associated with traditional Artificial Neural Networks (ANNs) and their reliance on intensive Matrix-Multiply (MAC) operations. Neuromorphic computing offers a compelling alternative by mimicking biological efficiency through event-driven processing. This paper presents the design and implementation of a cycle-accurate, hardware-oriented Spiking Neural Network (SNN) core implemented in SystemVerilog. Unlike conventional accelerators, this design utilizes a Leaky Integrate-and-Fire (LIF) neuron model powered by fixed-point arithmetic and bit-wise primitives (shifts and additions) to eliminate the need for complex floating-point hardware. The architecture features an on-chip Poisson encoder for stochastic spike generation and a novel active pruning mechanism that dynamically disables neurons post-classification to minimize dynamic power consumption. We demonstrate the hardware's efficacy through a fully connected layer implementation targeting digit classification. Simulation results indicate that the design achieves rapid convergence (89% accuracy) within limited timesteps while maintaining a significantly reduced computational footprint compared to traditional dense architectures. This work serves as a foundational building block for scalable, energy-efficient neuromorphic hardware on FPGA and ASIC platforms.

</details>


### [4] [CD-PIM: A High-Bandwidth and Compute-Efficient LPDDR5-Based PIM for Low-Batch LLM Acceleration on Edge-Device](https://arxiv.org/abs/2601.12298)
*Ye Lin,Chao Fang,Xiaoyong Song,Qi Wu,Anying Jiang,Yichuan Bai,Li Du*

Main category: cs.AR

TL;DR: CD-PIM提出了一种用于边缘部署低批次大语言模型的新型数字处理内存架构，通过高带宽计算高效模式、低批次交错模式和计算高效单元设计，解决了现有PIM架构的带宽限制、组件利用率低和计算能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 边缘部署低批次大语言模型面临内存带宽瓶颈，现有数字处理内存架构存在三个关键限制：带宽改进有限、混合工作负载中组件利用率低、计算单元计算能力不足。

Method: 1. 高带宽计算高效模式：通过分段全局位线将每个存储体分为四个伪存储体；2. 低批次交错模式：重叠GEMV和GEMM操作提高组件利用率；3. 计算高效单元：以流水线方式串行输入权重数据执行增强GEMV操作；4. 列向映射键缓存矩阵和行向映射值缓存矩阵。

Result: 在单批次高带宽计算高效模式下，相比GPU基准和先进PIM设计，CD-PIM分别实现平均11.42倍和4.25倍加速。在低批次下，低批次交错模式相比高带宽计算高效模式实现平均1.12倍加速。

Conclusion: CD-PIM通过创新的架构设计有效解决了边缘低批次大语言模型部署中的内存带宽瓶颈问题，显著提升了计算性能和组件利用率。

Abstract: Edge deployment of low-batch large language models (LLMs) faces critical memory bandwidth bottlenecks when executing memory-intensive general matrix-vector multiplications (GEMV) operations. While digital processing-in-memory (PIM) architectures promise to accelerate GEMV operations, existing PIM-equipped edge devices still suffer from three key limitations: limited bandwidth improvement, component under-utilization in mixed workloads, and low compute capacity of computing units (CUs). In this paper, we propose CD-PIM to address these challenges through three key innovations. First, we introduce a high-bandwidth compute-efficient mode (HBCEM) that enhances bandwidth by dividing each bank into four pseudo-banks through segmented global bitlines. Second, we propose a low-batch interleaving mode (LBIM) to improve component utilization by overlapping GEMV operations with GEMM operations. Third, we design a compute-efficient CU that performs enhanced GEMV operations in a pipelined manner by serially feeding weight data into the computing core. Forth, we adopt a column-wise mapping for the key-cache matrix and row-wise mapping for the value-cache matrix, which fully utilizes CU resources. Our evaluation shows that compared to a GPU-only baseline and state-of-the-art PIM designs, our CD-PIM achieves 11.42x and 4.25x speedup on average within a single batch in HBCEM mode, respectively. Moreover, for low-batch sizes, the CD-PIM achieves an average speedup of 1.12x in LBIM compared to HBCEM.

</details>


### [5] [Best Practices for Large Load Interconnections: A North American Perspective on Data Centers](https://arxiv.org/abs/2601.12686)
*Rafi Zahedi,Amin Zamani,Rahul Anilkumar*

Main category: cs.AR

TL;DR: 论文综述北美大型负荷（数据中心、加密货币挖矿等）并网的最佳实践，分析技术挑战并提出实用指导


<details>
  <summary>Details</summary>
Motivation: 北美大型负荷（特别是数据中心）快速增长，其规模、工作周期和变流器主导的接口给输电并网带来新挑战，包括扰动行为、稳态性能和运行可见性等问题

Method: 结合手册和手册分析、跨电力公司比较以及欧洲方向展望，综合北美公用事业和系统运营商指南形成一套连贯的技术要求

Result: 确定了电能质量、遥测、调试测试和保护协调等方面的要求，同时指出了穿越规范、负荷变化管理和扰动后恢复目标方面的差距

Conclusion: 基于研究发现，为开发商和公用事业公司提出了实用的指导建议，以应对大型负荷并网的技术挑战

Abstract: Large loads are expanding rapidly across North America, led by data centers, cryptocurrency mining, hydrogen production facilities, and heavy-duty charging stations. Each class presents distinct electrical characteristics, but data centers are drawing particular attention as AI deployment drives unprecedented capacity growth. Their scale, duty cycles, and converter-dominated interfaces introduce new challenges for transmission interconnections, especially regarding disturbance behavior, steady-state performance, and operational visibility. This paper reviews best practices for large-load interconnections across North America, synthesizing utility and system operator guidelines into a coherent set of technical requirements. The approach combines handbook and manual analysis with cross-utility comparisons and an outlook on European directions. The review highlights requirements on power quality, telemetry, commissioning tests, and protection coordination, while noting gaps in ride-through specifications, load-variation management, and post-disturbance recovery targets. Building on these findings, the paper proposes practical guidance for developers and utilities.

</details>


### [6] [PRIMAL: Processing-In-Memory Based Low-Rank Adaptation for LLM Inference Accelerator](https://arxiv.org/abs/2601.13628)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: PRIMAL是一个基于存内计算(PIM)的LLM推理加速器，集成了异构PIM处理单元和2D网格互连网络，通过SRAM重编程和电源门控方案实现流水线LoRA更新和次线性功耗扩展，相比NVIDIA H100在LoRA rank 8上实现了1.5倍吞吐量和25倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理面临计算密集和内存访问瓶颈问题，特别是结合LoRA微调时，传统架构难以高效处理动态参数更新。需要设计专门的硬件加速器来优化LLM推理与LoRA适配的结合。

Method: 1. 集成异构PIM处理单元和2D网格互连网络；2. 采用SRAM重编程和电源门控方案实现流水线LoRA更新；3. 优化空间映射和数据流编排以最小化通信开销；4. 支持重叠重配置与计算，门控空闲资源实现次线性功耗扩展。

Result: 在Llama-13B模型上，使用LoRA rank 8（Q,V）配置时，相比NVIDIA H100实现了1.5倍的吞吐量提升和25倍的能效提升。

Conclusion: PRIMAL展示了PIM架构在LLM推理加速中的潜力，特别是结合LoRA微调时，通过创新的硬件设计和优化策略能够显著提升性能和能效，为高效LLM部署提供了有前景的解决方案。

Abstract: This paper presents PRIMAL, a processing-in-memory (PIM) based large language model (LLM) inference accelerator with low-rank adaptation (LoRA). PRIMAL integrates heterogeneous PIM processing elements (PEs), interconnected by 2D-mesh inter-PE computational network (IPCN). A novel SRAM reprogramming and power gating (SRPG) scheme enables pipelined LoRA updates and sub-linear power scaling by overlapping reconfiguration with computation and gating idle resources. PRIMAL employs optimized spatial mapping and dataflow orchestration to minimize communication overhead, and achieves $1.5\times$ throughput and $25\times$ energy efficiency over NVIDIA H100 with LoRA rank 8 (Q,V) on Llama-13B.

</details>


### [7] [The Non-Predictability of Mispredicted Branches using Timing Information](https://arxiv.org/abs/2601.13804)
*Ioannis Constantinou,Arthur Perais,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 研究探索使用微架构时序信息（分支解析周期）来改进分支预测准确性，提出推测性分支解析（SBR）方法，但实验表明传统TAGE-SC预测器在整体性能上仍优于该方法。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中分支预测错误是性能下降和能耗浪费的主要原因。虽然现有预测器表现良好，但对于难以预测的分支仍存在高错误率。本研究探索是否可以利用微架构信息（除传统分支历史外）来提高预测准确性。

Method: 提出推测性分支解析（SBR）方法：在分支分配至重排序缓冲（ROB）N个周期后，收集时序信息（包括ROB中较老分支、已提交分支以及较新分支的解析周期）进行重新预测。使用gem5模拟器实现基于TAGE-Like预测器的SBR进行极限研究。

Result: 实验显示，所使用的分配后时序信息未能超越无限制TAGE-SC预测器的性能。但在两个难以预测的分支上，时序信息确实提供了优势。研究人员深入分析了其中一个案例以理解原因。

Conclusion: 特定微架构信息可能有助于提高某些难以预测分支的准确性，后端覆盖预测可能带来性能收益，但需要进一步研究确定有效的信息向量。整体而言，传统TAGE-SC仍优于基于时序信息的SBR方法。

Abstract: Branch misprediction latency is one of the most important contributors to performance degradation and wasted energy consumption in a modern core. State-of-the-art predictors generally perform very well but occasionally suffer from high Misprediction Per Kilo Instruction due to hard-to-predict branches. In this work, we investigate if predicting branches using microarchitectural information, in addition to traditional branch history, can improve prediction accuracy. Our approach considers branch timing information (resolution cycle) both for older branches in the Reorder Buffer (ROB) and recently committed, and for younger branches relative to the branch we re-predict. We propose Speculative Branch Resolution (SBR) in which, N cycles after a branch allocates in the ROB, various timing information is collected and used to re-predict. Using the gem5 simulator we implement and perform a limit-study of SBR using a TAGE-Like predictor. Our experiments show that the post-alloc timing information we used was not able to yield performance gains over an unbounded TAGE-SC. However, we find two hard to predict branches where timing information did provide an advantage and thoroughly analysed one of them to understand why. This finding suggests that predictors may benefit from specific microarchitectural information to increase accuracy on specific hard to predict branches and that overriding predictions in the backend may yet yield performance benefits, but that further research is needed to determine such information vectors.

</details>


### [8] [From RTL to Prompt Coding: Empowering the Next Generation of Chip Designers through LLMs](https://arxiv.org/abs/2601.13815)
*Lukas Krupp,Matthew Venn,Norbert Wehn*

Main category: cs.AR

TL;DR: 首个基于LLM的芯片设计教育平台，通过浏览器工作流和聊天助手帮助初学者完成从设计构思到流片的全过程，成功让高中生在没有经验的情况下完成可流片芯片设计。


<details>
  <summary>Details</summary>
Motivation: 芯片设计领域技术复杂，对初学者门槛高。需要一种能让没有经验的学习者也能接触和掌握芯片设计的教育平台，扩大该领域的受众群体。

Method: 将LLM聊天代理集成到基于Tiny Tapeout生态系统的浏览器工作流中，引导用户从设计构思、RTL代码生成到可流片芯片的完整流程。

Result: 18名高中生在90分钟课程中开发了8个功能性的VGA芯片设计（130nm工艺）。所有小组都成功实现了可流片项目，尽管他们之前没有任何芯片设计经验。

Conclusion: LLM辅助的芯片设计具有可行性和教育影响力，能够吸引和激励早期学习者，显著扩大芯片设计领域的受众范围。

Abstract: This paper presents an LLM-based learning platform for chip design education, aiming to make chip design accessible to beginners without overwhelming them with technical complexity. It represents the first educational platform that assists learners holistically across both frontend and backend design. The proposed approach integrates an LLM-based chat agent into a browser-based workflow built upon the Tiny Tapeout ecosystem. The workflow guides users from an initial design idea through RTL code generation to a tapeout-ready chip. To evaluate the concept, a case study was conducted with 18 high-school students. Within a 90-minute session they developed eight functional VGA chip designs in a 130 nm technology. Despite having no prior experience in chip design, all groups successfully implemented tapeout-ready projects. The results demonstrate the feasibility and educational impact of LLM-assisted chip design, highlighting its potential to attract and inspire early learners and significantly broaden the target audience for the field.

</details>


### [9] ['1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators](https://arxiv.org/abs/2601.14087)
*Ruichi Han,Yizhi Chen,Tong Lei,Jordi Altayo Gonzalez,Ahmed Hemani*

Main category: cs.AR

TL;DR: 提出一种用于CNN的免比较近似排序单元，通过粗粒度桶分组降低硬件面积，同时保持数据重排序的链路功耗优势


<details>
  <summary>Details</summary>
Motivation: 互连功耗是DNN加速器的瓶颈，基于'1'比特计数的数据排序可以减少开关活动，但实际硬件排序实现尚未充分探索

Method: 利用近似计算将人口计数分组到粗粒度桶中，设计优化的免比较排序单元，专为卷积神经网络定制

Result: 近似排序单元实现高达35.4%的面积减少，同时保持19.50%的BT减少，接近精确实现的20.42%

Conclusion: 该硬件排序设计在保持链路功耗优势的同时显著降低硬件面积，为DNN加速器的互连功耗优化提供了实用解决方案

Abstract: Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\% BT reduction compared to 20.42% of precise implementation.

</details>


### [10] [CREATE: Cross-Layer Resilience Characterization and Optimization for Efficient yet Reliable Embodied AI Systems](https://arxiv.org/abs/2601.14140)
*Tong Xie,Yijiahao Qi,Jinqi Wen,Zishen Wan,Yanchi Dong,Zihao Wang,Shaofei Cai,Yitao Liang,Tianyu Jia,Yuan Wang,Runsheng Wang,Meng Li*

Main category: cs.AR

TL;DR: CREATE提出了一种异构弹性设计原则，通过电路层、模型层和应用层的协同优化，在保证任务质量的同时显著降低具身AI系统的能耗


<details>
  <summary>Details</summary>
Motivation: 具身AI系统通常结合LLM规划器和RL控制器，但部署时面临高计算能耗挑战。降低工作电压虽能提高能效，但会引入比特错误导致任务失败，需要解决能效与可靠性的权衡问题

Method: 1) 电路层：异常检测与清除机制消除异常错误；2) 模型层：权重旋转增强规划算法提高LLM规划器的容错性；3) 应用层：自主性自适应电压调节动态调整控制器工作电压；4) 协同设计电压调节电路支持在线电压调整

Result: CREATE平均实现40.6%的计算能耗节省（相比标称电压基准），比现有技术节省35.0%。芯片级能耗节省29.5%-37.3%，电池寿命提升约15%-30%，且不损害任务质量

Conclusion: CREATE通过异构弹性协同优化方法，有效解决了具身AI系统的能效与可靠性权衡问题，为电池供电本地设备的具身AI部署提供了实用解决方案

Abstract: Embodied Artificial Intelligence (AI) has recently attracted significant attention as it bridges AI with the physical world. Modern embodied AI systems often combine a Large Language Model (LLM)-based planner for high-level task planning and a reinforcement learning (RL)-based controller for low-level action generation, enabling embodied agents to tackle complex tasks in real-world environments. However, deploying embodied agents remains challenging due to their high computation requirements, especially for battery-powered local devices. Although techniques like lowering operating voltage can improve energy efficiency, they can introduce bit errors and result in task failures. In this work, we propose CREATE, a general design principle that leverages heterogeneous resilience at different layers for synergistic energy-reliability co-optimization. For the first time, we conduct a comprehensive error injection study on modern embodied AI systems and observe an inherent but heterogeneous fault tolerance. Building upon these insights, we develop an anomaly detection and clearance mechanism at the circuit level to eliminate outlier errors. At the model level, we propose a weight-rotation-enhanced planning algorithm to improve the fault tolerance of the LLM-based planner. Furthermore, we introduce an application-level technique, autonomy-adaptive voltage scaling, to dynamically adjust the operating voltage of the controllers. The voltage scaling circuit is co-designed to enable online voltage adjustment. Extensive experiments demonstrate that without compromising task quality, CREATE achieves 40.6% computational energy savings on average over nominal-voltage baselines and 35.0% over prior-art techniques. This further leads to 29.5% to 37.3% chip-level energy savings and approximately a 15% to 30% improvement in battery life.

</details>


### [11] [The Quest for Reliable AI Accelerators: Cross-Layer Evaluation and Design Optimization](https://arxiv.org/abs/2601.14148)
*Meng Li,Tong Xie,Zuodong Zhang,Runsheng Wang*

Main category: cs.AR

TL;DR: 本文针对纳米级CMOS技术下AI加速器的可靠性挑战，提出了一系列跨层可靠性感知加速器设计方法，包括动态时序分析、数据流优化和LLM弹性架构设计，实现了可靠性与计算效率的协同优化。


<details>
  <summary>Details</summary>
Motivation: 随着CMOS技术进入纳米尺度，老化效应和工艺变异日益显著，传统基于保护带的设计方法牺牲了过多性能和计算效率，无法满足高性能AI计算需求。当前AI加速器设计面临两大挑战：缺乏跨层可靠性分析工具，以及可靠性优化与计算效率之间的根本性权衡。

Method: 提出三方面系统化设计方法：(1) 老化与变异感知的动态时序分析器；(2) 基于关键输入模式减少的加速器数据流优化；(3) 大语言模型的弹性特性表征与新颖架构设计。通过紧密集成跨层可靠性建模与AI工作负载特性，实现协同优化。

Result: 这些协同优化方法有效解决了传统设计中的可靠性-效率权衡问题，实现了可靠且高效的AI加速。通过跨层分析和针对性优化，在保证可靠性的同时提升了计算效率。

Conclusion: 本文提出的跨层可靠性感知加速器设计框架成功解决了纳米级CMOS技术下的AI加速器可靠性挑战，通过系统化的协同优化方法，在设备、电路、架构和应用多个层面实现了可靠性与计算效率的平衡，为高性能AI计算提供了有效解决方案。

Abstract: As the CMOS technology pushes to the nanoscale, aging effects and process variations have become increasingly pronounced, posing significant reliability challenges for AI accelerators. Traditional guardband-based design approaches, which rely on pessimistic timing margin, sacrifice significant performance and computational efficiency, rendering them inadequate for high-performance AI computing demands. Current reliability-aware AI accelerator design faces two core challenges: (1) the lack of systematic cross-layer analysis tools to capture coupling reliability effects across device, circuit, architecture, and application layers; and (2) the fundamental trade-off between conventional reliability optimization and computational efficiency. To address these challenges, this paper systematically presents a series of reliability-aware accelerator designs, encompassing (1) aging and variation-aware dynamic timing analyzer, (2) accelerator dataflow optimization using critical input pattern reduction, and (3) resilience characterization and novel architecture design for large language models (LLMs). By tightly integrating cross-layer reliability modeling and AI workload characteristics, these co-optimization approaches effectively achieve reliable and efficient AI acceleration.

</details>
