{"id": "2511.04682", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04682", "abs": "https://arxiv.org/abs/2511.04682", "authors": ["Eleni Bougioukou", "Theodore Antonakopoulos"], "title": "Efficient Deployment of CNN Models on Multiple In-Memory Computing Units", "comment": "5 pages, 4 figures, 2025 14th International Conference on Modern\n  Circuits and Systems Technologies (MOCAST)", "summary": "In-Memory Computing (IMC) represents a paradigm shift in deep learning\nacceleration by mitigating data movement bottlenecks and leveraging the\ninherent parallelism of memory-based computations. The efficient deployment of\nConvolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use\nof advanced task allocation strategies for achieving maximum computational\nefficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple\nProcessing Units (PUs) for investigating how the deployment of a CNN model in a\nmulti-processing system affects its performance, in terms of processing rate\nand latency. For that purpose, we introduce the Load-Balance-Longest-Path\n(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE\nPUs, for maximizing the processing rate and minimizing latency due to efficient\nresources utilization. We are benchmarking LBLP against other alternative\nscheduling strategies for a number of CNN models and experimental results\ndemonstrate the effectiveness of the proposed algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86Load-Balance-Longest-Path (LBLP)\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u5177\u6709\u591a\u4e2a\u5904\u7406\u5355\u5143\u7684\u5b58\u5185\u8ba1\u7b97\u6a21\u62df\u5668\u4e0a\u4f18\u5316CNN\u6a21\u578b\u7684\u90e8\u7f72\uff0c\u4ee5\u63d0\u9ad8\u5904\u7406\u901f\u7387\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5b58\u5185\u8ba1\u7b97(IMC)\u901a\u8fc7\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u74f6\u9888\u548c\u5229\u7528\u5185\u5b58\u8ba1\u7b97\u7684\u5e76\u884c\u6027\u6765\u52a0\u901f\u6df1\u5ea6\u5b66\u4e60\uff0c\u4f46\u5728\u591a\u5904\u7406\u5355\u5143IMC\u786c\u4ef6\u4e0a\u9ad8\u6548\u90e8\u7f72CNN\u9700\u8981\u5148\u8fdb\u7684\u4efb\u52a1\u5206\u914d\u7b56\u7565\u3002", "method": "\u4f7f\u7528IMC\u6a21\u62df\u5668(IMCE)\u548c\u591a\u5904\u7406\u5355\u5143\uff0c\u5f00\u53d1\u4e86LBLP\u7b97\u6cd5\uff0c\u52a8\u6001\u5730\u5c06CNN\u8282\u70b9\u5206\u914d\u7ed9\u53ef\u7528\u5904\u7406\u5355\u5143\uff0c\u4ee5\u6700\u5927\u5316\u5904\u7406\u901f\u7387\u5e76\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLBLP\u7b97\u6cd5\u5728\u591a\u4e2aCNN\u6a21\u578b\u4e0a\u76f8\u6bd4\u5176\u4ed6\u8c03\u5ea6\u7b56\u7565\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "LBLP\u7b97\u6cd5\u4e3aIMC\u786c\u4ef6\u4e0a\u7684CNN\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5904\u7406\u6027\u80fd\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002"}}
{"id": "2511.04684", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04684", "abs": "https://arxiv.org/abs/2511.04684", "authors": ["Yuchao Qin", "Anjunyi Fan", "Bonan Yan"], "title": "RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression", "comment": "5 pages, 4 figures", "summary": "Data centers handle vast volumes of data that require efficient lossless\ncompression, yet emerging probabilistic models based methods are often\ncomputationally slow. To address this, we introduce RAS, the Range Asymmetric\nNumeral System Acceleration System, a hardware architecture that integrates the\nrANS algorithm into a lossless compression pipeline and eliminates key\nbottlenecks. RAS couples an rANS core with a probabilistic generator, storing\ndistributions in BF16 format and converting them once into a fixed-point domain\nshared by a unified division/modulo datapath. A two-stage rANS update with\nbyte-level re-normalization reduces logic cost and memory traffic, while a\nprediction-guided decoding path speculatively narrows the cumulative\ndistribution function (CDF) search window and safely falls back to maintain\nbit-exactness. A multi-lane organization scales throughput and enables\nfine-grained clock gating for efficient scheduling. On image workloads, our\nRTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a\nPython rANS baseline, reducing average decoder binary-search steps from 7.00 to\n3.15 (approximately 55% fewer). When paired with neural probability models, RAS\nsustains higher compression ratios than classical codecs and outperforms\nCPU/GPU rANS implementations, offering a practical approach to fast neural\nlossless compression.", "AI": {"tldr": "RAS\u662f\u4e00\u4e2a\u786c\u4ef6\u52a0\u901f\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210rANS\u7b97\u6cd5\u5230\u65e0\u635f\u538b\u7f29\u6d41\u6c34\u7ebf\u4e2d\uff0c\u89e3\u51b3\u4e86\u6982\u7387\u6a21\u578b\u65b9\u6cd5\u8ba1\u7b97\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u7f16\u7801\u548c\u89e3\u7801\u52a0\u901f\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u5904\u7406\u5927\u91cf\u6570\u636e\u9700\u8981\u9ad8\u6548\u7684\u65e0\u635f\u538b\u7f29\uff0c\u4f46\u57fa\u4e8e\u6982\u7387\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u901f\u5ea6\u8f83\u6162\uff0c\u9700\u8981\u786c\u4ef6\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002", "method": "RAS\u5c06rANS\u6838\u5fc3\u4e0e\u6982\u7387\u751f\u6210\u5668\u8026\u5408\uff0c\u4f7f\u7528BF16\u683c\u5f0f\u5b58\u50a8\u5206\u5e03\u5e76\u8f6c\u6362\u4e3a\u56fa\u5b9a\u70b9\u57df\uff0c\u91c7\u7528\u4e24\u9636\u6bb5rANS\u66f4\u65b0\u548c\u5b57\u8282\u7ea7\u91cd\u5f52\u4e00\u5316\uff0c\u9884\u6d4b\u5f15\u5bfc\u7684\u89e3\u7801\u8def\u5f84\u7f29\u5c0fCDF\u641c\u7d22\u7a97\u53e3\uff0c\u591a\u901a\u9053\u7ec4\u7ec7\u6269\u5c55\u541e\u5410\u91cf\u3002", "result": "\u5728\u56fe\u50cf\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0cRTL\u6a21\u62df\u539f\u578b\u76f8\u6bd4Python rANS\u57fa\u7ebf\u5b9e\u73b0\u4e86121.2\u500d\u7f16\u7801\u548c70.9\u500d\u89e3\u7801\u52a0\u901f\uff0c\u5e73\u5747\u89e3\u7801\u4e8c\u5206\u641c\u7d22\u6b65\u9aa4\u4ece7.00\u51cf\u5c11\u52303.15\uff08\u7ea6\u51cf\u5c1155%\uff09\u3002", "conclusion": "RAS\u4e0e\u795e\u7ecf\u6982\u7387\u6a21\u578b\u914d\u5bf9\u65f6\uff0c\u6bd4\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u4fdd\u6301\u66f4\u9ad8\u7684\u538b\u7f29\u6bd4\uff0c\u5e76\u4f18\u4e8eCPU/GPU rANS\u5b9e\u73b0\uff0c\u4e3a\u5feb\u901f\u795e\u7ecf\u65e0\u635f\u538b\u7f29\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2511.04687", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04687", "abs": "https://arxiv.org/abs/2511.04687", "authors": ["Teona Bagashvili", "Tarikul Islam Papon", "Subhadeep Sarkar", "Manos Athanassoulis"], "title": "Eliminating the Hidden Cost of Zone Management in ZNS SSDs", "comment": null, "summary": "Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput\nand low-latency storage by eliminating device-side garbage collection. They\nexpose storage as append-only zones that give the host applications direct\ncontrol over data placement. However, current ZNS implementations suffer from\n(a) device-level write amplification (DLWA), (b) increased wear, and (c)\ninterference with host I/O due to zone mapping and management. We identify two\nprimary design decisions as the main cause: (i) fixed physical zones and (ii)\nfull-zone operations that lead to excessive physical writes. We propose\nSilentZNS, a new zone mapping and management approach that addresses the\naforementioned limitations by on-the-fly allocating available resources to\nzones, while minimizing wear, maintaining parallelism, and avoiding unnecessary\nwrites at the device-level. SilentZNS is a flexible zone allocation scheme that\ndeparts from the traditional logical-to-physical zone mapping and allows for\narbitrary collections of blocks to be assigned to a zone. We add the necessary\nconstraints to ensure wear-leveling and state-of-the-art read performance, and\nuse only the required blocks to avoid dummy writes during zone reset. We\nimplement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that\nit eliminates the undue burden of dummy writes by up to 20x, leading to lower\nDLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up\nto 3.7x faster workload execution.", "AI": {"tldr": "SilentZNS\u662f\u4e00\u79cd\u65b0\u7684ZNS SSD\u533a\u57df\u6620\u5c04\u548c\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u8d44\u6e90\u89e3\u51b3\u4f20\u7edfZNS\u5b9e\u73b0\u4e2d\u7684\u8bbe\u5907\u7ea7\u5199\u5165\u653e\u5927\u3001\u78e8\u635f\u589e\u52a0\u548c\u4e3b\u673aI/O\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfZNS SSD\u5b9e\u73b0\u5b58\u5728\u8bbe\u5907\u7ea7\u5199\u5165\u653e\u5927(DLWA)\u3001\u589e\u52a0\u78e8\u635f\u4ee5\u53ca\u4e0e\u4e3b\u673aI/O\u5e72\u6270\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u56fa\u5b9a\u7269\u7406\u533a\u57df\u548c\u5168\u533a\u57df\u64cd\u4f5c\u5bfc\u81f4\u8fc7\u591a\u7269\u7406\u5199\u5165\u3002", "method": "\u63d0\u51faSilentZNS\u65b9\u6cd5\uff0c\u91c7\u7528\u7075\u6d3b\u7684\u533a\u57df\u5206\u914d\u65b9\u6848\uff0c\u52a8\u6001\u5206\u914d\u53ef\u7528\u8d44\u6e90\u5230\u533a\u57df\uff0c\u907f\u514d\u4f20\u7edf\u903b\u8f91\u5230\u7269\u7406\u533a\u57df\u6620\u5c04\u7684\u9650\u5236\uff0c\u5141\u8bb8\u4efb\u610f\u5757\u96c6\u5408\u5206\u914d\u7ed9\u533a\u57df\uff0c\u540c\u65f6\u786e\u4fdd\u78e8\u635f\u5747\u8861\u548c\u8bfb\u53d6\u6027\u80fd\u3002", "result": "SilentZNS\u53ef\u6d88\u9664\u9ad8\u8fbe20\u500d\u7684\u865a\u62df\u5199\u5165\u8d1f\u62c5\uff0c\u51cf\u5c1186%\u7684\u8bbe\u5907\u7ea7\u5199\u5165\u653e\u5927(\u572810%\u533a\u57df\u5360\u7528\u65f6)\uff0c\u964d\u4f4e76.9%\u7684\u6574\u4f53\u78e8\u635f\uff0c\u5de5\u4f5c\u8d1f\u8f7d\u6267\u884c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe3.7\u500d\u3002", "conclusion": "SilentZNS\u901a\u8fc7\u521b\u65b0\u7684\u533a\u57df\u6620\u5c04\u548c\u7ba1\u7406\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86ZNS SSD\u7684\u5173\u952e\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8010\u7528\u6027\u3002"}}
{"id": "2511.04713", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.04713", "abs": "https://arxiv.org/abs/2511.04713", "authors": ["Mahek Desai", "Rowena Quinn", "Marjan Asadinia"], "title": "SMART-WRITE: Adaptive Learning-based Write Energy Optimization for Phase Change Memory", "comment": null, "summary": "As dynamic random access memory (DRAM) and other current transistor-based\nmemories approach their scalability limits, the search for alternative storage\nmethods becomes increasingly urgent. Phase-change memory (PCM) emerges as a\npromising candidate due to its scalability, fast access time, and zero leakage\npower compared to many existing memory technologies. However, PCM has\nsignificant drawbacks that currently hinder its viability as a replacement. PCM\ncells suffer from a limited lifespan because write operations degrade the\nphysical material, and these operations consume a considerable amount of\nenergy. For PCM to be a practical option for data storage-which involves\nfrequent write operations-its cell endurance must be enhanced, and write energy\nmust be reduced. In this paper, we propose SMART-WRITE, a method that\nintegrates neural networks (NN) and reinforcement learning (RL) to dynamically\noptimize write energy and improve performance. The NN model monitors real-time\noperating conditions and device characteristics to determine optimal write\nparameters, while the RL model dynamically adjusts these parameters to further\noptimize PCM's energy consumption. By continuously adjusting PCM write\nparameters based on real-time system conditions, SMART-WRITE reduces write\nenergy consumption by up to 63% and improves performance by up to 51% compared\nto the baseline and previous models.", "AI": {"tldr": "\u63d0\u51faSMART-WRITE\u65b9\u6cd5\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u76f8\u53d8\u5b58\u50a8\u5668\u7684\u5199\u5165\u80fd\u8017\u548c\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u51c6\u6a21\u578b\u53ef\u51cf\u5c1163%\u5199\u5165\u80fd\u8017\u5e76\u63d0\u534751%\u6027\u80fd\u3002", "motivation": "DRAM\u7b49\u4f20\u7edf\u5b58\u50a8\u5668\u9762\u4e34\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u76f8\u53d8\u5b58\u50a8\u5668(PCM)\u867d\u5177\u6f5c\u529b\u4f46\u5b58\u5728\u5199\u5165\u5bff\u547d\u6709\u9650\u548c\u80fd\u8017\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u63d0\u5347\u5176\u8010\u7528\u6027\u548c\u964d\u4f4e\u5199\u5165\u80fd\u8017\u3002", "method": "\u96c6\u6210\u795e\u7ecf\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\uff0cNN\u6a21\u578b\u76d1\u63a7\u5b9e\u65f6\u8fd0\u884c\u6761\u4ef6\u548c\u8bbe\u5907\u7279\u6027\u786e\u5b9a\u6700\u4f18\u5199\u5165\u53c2\u6570\uff0cRL\u6a21\u578b\u52a8\u6001\u8c03\u6574\u53c2\u6570\u4ee5\u4f18\u5316PCM\u80fd\u8017\u3002", "result": "\u901a\u8fc7\u5b9e\u65f6\u8c03\u6574PCM\u5199\u5165\u53c2\u6570\uff0c\u76f8\u6bd4\u57fa\u51c6\u548c\u5148\u524d\u6a21\u578b\uff0c\u5199\u5165\u80fd\u8017\u964d\u4f4e63%\uff0c\u6027\u80fd\u63d0\u534751%\u3002", "conclusion": "SMART-WRITE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86PCM\u7684\u5199\u5165\u80fd\u8017\u548c\u6027\u80fd\u95ee\u9898\uff0c\u4e3aPCM\u4f5c\u4e3a\u6570\u636e\u5b58\u50a8\u66ff\u4ee3\u65b9\u6848\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.04798", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04798", "abs": "https://arxiv.org/abs/2511.04798", "authors": ["Matheus Farias", "Wanghley Martins", "H. T. Kung"], "title": "MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars", "comment": "5 pages, 6 figures", "summary": "Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)\nweight mapping technique for memristive bit-sliced compute-in-memory (CIM)\ncrossbars that reduces parasitic resistance (PR) nonidealities.\n  PR limits crossbar efficiency by mapping DNN matrices into small crossbar\ntiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring\ndigital synchronization before the next layer. At this granularity, designers\neither deploy many small crossbars in parallel or reuse a few sequentially-both\nincreasing analog-to-digital conversions, latency, I/O pressure, and chip area.\n  MDM alleviates PR effects by optimizing active-memristor placement.\nExploiting bit-level structured sparsity, it feeds activations from the denser\nlow-order side and reorders rows according to the Manhattan distance,\nrelocating active cells toward regions less affected by PR and thus lowering\nthe nonideality factor (NF).\n  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and\nimproves accuracy under analog distortion by an average of 3.6% in ResNets.\nOverall, it provides a lightweight, spatially informed method for scaling CIM\nDNN accelerators.", "AI": {"tldr": "MDM\u662f\u4e00\u79cd\u540e\u8bad\u7ec3DNN\u6743\u91cd\u6620\u5c04\u6280\u672f\uff0c\u901a\u8fc7\u4f18\u5316\u6709\u6e90\u5fc6\u963b\u5668\u4f4d\u7f6e\u6765\u51cf\u5c11\u5fc6\u963b\u8ba1\u7b97\u5185\u5b58\u4ea4\u53c9\u9635\u5217\u4e2d\u7684\u5bc4\u751f\u7535\u963b\u975e\u7406\u60f3\u6027\uff0c\u63d0\u9ad8\u6a21\u62df\u7cbe\u5ea6\u3002", "motivation": "\u5bc4\u751f\u7535\u963b\u9650\u5236\u4e86\u4ea4\u53c9\u9635\u5217\u6548\u7387\uff0c\u9700\u8981\u5c06DNN\u77e9\u9635\u6620\u5c04\u5230\u5c0f\u5c3a\u5bf8\u4ea4\u53c9\u9635\u5217\u74e6\u7247\u4e2d\uff0c\u8fd9\u964d\u4f4e\u4e86CIM\u52a0\u901f\u6548\u679c\u5e76\u589e\u52a0\u4e86\u6570\u5b57\u540c\u6b65\u3001ADC\u8f6c\u6362\u3001\u5ef6\u8fdf\u548c\u82af\u7247\u9762\u79ef\u7684\u5f00\u9500\u3002", "method": "\u5229\u7528\u6bd4\u7279\u7ea7\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u4ece\u5bc6\u5ea6\u8f83\u9ad8\u7684\u4f4e\u9636\u4fa7\u8f93\u5165\u6fc0\u6d3b\uff0c\u5e76\u6839\u636e\u66fc\u54c8\u987f\u8ddd\u79bb\u91cd\u65b0\u6392\u5217\u884c\uff0c\u5c06\u6709\u6e90\u5355\u5143\u91cd\u65b0\u5b9a\u4f4d\u5230\u53d7\u5bc4\u751f\u7535\u963b\u5f71\u54cd\u8f83\u5c0f\u7684\u533a\u57df\u3002", "result": "\u5728ImageNet-1k\u4e0a\u7684DNN\u6a21\u578b\u4e2d\uff0cMDM\u5c06\u975e\u7406\u60f3\u6027\u56e0\u5b50\u964d\u4f4e\u9ad8\u8fbe46%\uff0c\u5728ResNets\u4e2d\u6a21\u62df\u5931\u771f\u4e0b\u7684\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad83.6%\u3002", "conclusion": "MDM\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u7a7a\u95f4\u611f\u77e5\u7684\u65b9\u6cd5\u6765\u6269\u5c55CIM DNN\u52a0\u901f\u5668\u3002"}}
{"id": "2511.05321", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.05321", "abs": "https://arxiv.org/abs/2511.05321", "authors": ["Maximilian Kirschner", "Konstantin Dudzik", "Ben Krusekamp", "J\u00fcrgen Becker"], "title": "MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for Neural Network Inference", "comment": null, "summary": "Real-time systems, particularly those used in domains like automated driving,\nare increasingly adopting neural networks. From this trend arises the need for\nhigh-performance hardware exhibiting predictable timing behavior. While\nstate-of-the-art real-time hardware often suffers from limited memory and\ncompute resources, modern AI accelerators typically lack the crucial\npredictability due to memory interference.\n  We present a new hardware architecture to bridge this gap between performance\nand predictability. The architecture features a multi-core vector processor\nwith predictable cores, each equipped with local scratchpad memories. A central\nmanagement core orchestrates access to shared external memory following a\nstatically determined schedule.\n  To evaluate the proposed hardware architecture, we analyze different variants\nof our parameterized design. We compare these variants to a baseline\narchitecture consisting of a single-core vector processor with large vector\nregisters. We find that configurations with a larger number of smaller cores\nachieve better performance due to increased effective memory bandwidth and\nhigher clock frequencies. Crucially for real-time systems, execution time\nfluctuation remains very low, demonstrating the platform's time predictability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u786c\u4ef6\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u6838\u5411\u91cf\u5904\u7406\u5668\u548c\u672c\u5730\u6682\u5b58\u5185\u5b58\u6765\u89e3\u51b3AI\u52a0\u901f\u5668\u5728\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u6027\u80fd\u548c\u53ef\u9884\u6d4b\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "motivation": "\u5b9e\u65f6\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\uff0c\u9700\u8981\u517c\u5177\u9ad8\u6027\u80fd\u548c\u53ef\u9884\u6d4b\u65f6\u5e8f\u884c\u4e3a\u7684\u786c\u4ef6\u3002\u73b0\u6709\u5b9e\u65f6\u786c\u4ef6\u8d44\u6e90\u6709\u9650\uff0c\u800c\u73b0\u4ee3AI\u52a0\u901f\u5668\u7531\u4e8e\u5185\u5b58\u5e72\u6270\u7f3a\u4e4f\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u8bbe\u8ba1\u591a\u6838\u5411\u91cf\u5904\u7406\u5668\u67b6\u6784\uff0c\u6bcf\u4e2a\u6838\u5fc3\u914d\u5907\u672c\u5730\u6682\u5b58\u5185\u5b58\uff0c\u7531\u4e2d\u592e\u7ba1\u7406\u6838\u5fc3\u6309\u7167\u9759\u6001\u8c03\u5ea6\u8868\u534f\u8c03\u5171\u4eab\u5916\u90e8\u5185\u5b58\u8bbf\u95ee\u3002", "result": "\u4e0e\u5355\u6838\u5927\u5411\u91cf\u5bc4\u5b58\u5668\u57fa\u7ebf\u67b6\u6784\u76f8\u6bd4\uff0c\u914d\u7f6e\u66f4\u591a\u5c0f\u6838\u5fc3\u7684\u53d8\u4f53\u7531\u4e8e\u6709\u6548\u5185\u5b58\u5e26\u5bbd\u589e\u52a0\u548c\u65f6\u949f\u9891\u7387\u66f4\u9ad8\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002\u6267\u884c\u65f6\u95f4\u6ce2\u52a8\u975e\u5e38\u4f4e\uff0c\u8bc1\u660e\u4e86\u5e73\u53f0\u7684\u65f6\u95f4\u53ef\u9884\u6d4b\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u6210\u529f\u5f25\u5408\u4e86\u6027\u80fd\u548c\u53ef\u9884\u6d4b\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u60f3\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002"}}
