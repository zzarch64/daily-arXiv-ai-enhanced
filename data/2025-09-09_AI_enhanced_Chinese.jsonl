{"id": "2509.05451", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.05451", "abs": "https://arxiv.org/abs/2509.05451", "authors": ["Niansong Zhang", "Wenbo Zhu", "Courtney Golden", "Dan Ilan", "Hongzheng Chen", "Christopher Batten", "Zhiru Zhang"], "title": "Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device", "comment": "MICRO 2025", "summary": "Compute-in-SRAM architectures offer a promising approach to achieving higher\nperformance and energy efficiency across a range of data-intensive\napplications. However, prior evaluations have largely relied on simulators or\nsmall prototypes, limiting the understanding of their real-world potential. In\nthis work, we present a comprehensive performance and energy characterization\nof a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.\nWe compare the GSI APU against established architectures, including CPUs and\nGPUs, to quantify its energy efficiency and performance potential. We introduce\nan analytical framework for general-purpose compute-in-SRAM devices that\nreveals fundamental optimization principles by modeling performance trade-offs,\nthereby guiding program optimizations.\n  Exploiting the fine-grained parallelism of tightly integrated memory-compute\narchitectures requires careful data management. We address this by proposing\nthree optimizations: communication-aware reduction mapping, coalesced DMA, and\nbroadcast-friendly data layouts. When applied to retrieval-augmented generation\n(RAG) over large corpora (10GB--200GB), these optimizations enable our\ncompute-in-SRAM system to accelerate retrieval by 4.8$\\times$--6.6$\\times$ over\nan optimized CPU baseline, improving end-to-end RAG latency by\n1.1$\\times$--1.8$\\times$. The shared off-chip memory bandwidth is modeled using\na simulated HBM, while all other components are measured on the real\ncompute-in-SRAM device. Critically, this system matches the performance of an\nNVIDIA A6000 GPU for RAG while being significantly more energy-efficient\n(54.4$\\times$-117.9$\\times$ reduction). These findings validate the viability\nof compute-in-SRAM for complex, real-world applications and provide guidance\nfor advancing the technology.", "AI": {"tldr": "\u8ba1\u7b97\u5728SRAM\u4e2d\u7684\u5546\u4e1a\u8bbe\u5907GSI APU\u5728\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8377\u4e0b\u7684\u6027\u80fd\u548c\u80fd\u6d88\u8868\u73b0\uff0c\u901a\u8fc7\u4f18\u5316\u5728RAG\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u6bd4CPU\u66f4\u5feb4.8-6.6\u500d\u7684\u68c0\u7d22\u901f\u5ea6\uff0c\u4e14\u6bd4GPU\u66f4\u8282\u80fd54.4-117.9\u500d", "motivation": "\u4e4b\u524d\u5bf9\u8ba1\u7b97\u5728SRAM\u67b6\u6784\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4eff\u771f\u5668\u6216\u5c0f\u578b\u539f\u578b\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u771f\u5b9e\u6f5c\u529b\u7684\u7406\u89e3\uff0c\u9700\u8981\u901a\u8fc7\u5546\u4e1a\u8bbe\u5907\u8fdb\u884c\u5b9e\u9645\u6027\u80fd\u8bc4\u4f30", "method": "\u4f7f\u7528\u5546\u4e1a\u8ba1\u7b97\u5728SRAM\u8bbe\u5907GSI APU\uff0c\u4e0eCPU\u548cGPU\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\uff1b\u63d0\u51fa\u5206\u6790\u6846\u67b6\u6a21\u578b\u6027\u80fd\u4ea4\u6362\uff1b\u63d0\u51fa\u4e09\u79cd\u4f18\u5316\u6280\u672f\uff1a\u901a\u4fe1\u611f\u77e5\u7684\u7f29\u51cf\u6620\u5c04\u3001\u5408\u5e76DMA\u548c\u5e7f\u64ad\u53cb\u597d\u6570\u636e\u5e03\u5c40", "result": "\u572810GB-200GB\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0c\u4f18\u5316\u540e\u7684\u8ba1\u7b97\u5728SRAM\u7cfb\u7edf\u68c0\u7d22\u901f\u5ea6\u6bd4\u4f18\u5316CPU\u57fa\u51c6\u63d0\u9ad84.8-6.6\u500d\uff0c\u7ed3\u679c\u751f\u6210\u5ef6\u8fdf\u63d0\u9ad81.1-1.8\u500d\uff0c\u6027\u80fd\u4e0eNVIDIA A6000 GPU\u76f8\u5f53\u4f46\u80fd\u6548\u63d0\u9ad854.4-117.9\u500d", "conclusion": "\u8ba1\u7b97\u5728SRAM\u6280\u672f\u5728\u590d\u6742\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u7ba1\u7406\u53ef\u5145\u5206\u53d1\u6325\u5176\u7ec6\u7c92\u5ea6\u5e76\u884c\u4f18\u52bf\uff0c\u4e3a\u8be5\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc"}}
{"id": "2509.05688", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.05688", "abs": "https://arxiv.org/abs/2509.05688", "authors": ["Kuan-Ting Lin", "Ching-Te Chiu", "Jheng-Yi Chang", "Shi-Zong Huang", "Yu-Ting Li"], "title": "High Utilization Energy-Aware Real-Time Inference Deep Convolutional Neural Network Accelerator", "comment": null, "summary": "Deep convolution Neural Network (DCNN) has been widely used in computer\nvision tasks. However, for edge devices even inference has too large\ncomputational complexity and data access amount. The inference latency of\nstate-of-the-art models are impractical for real-world applications. In this\npaper, we propose a high utilization energy-aware real-time inference deep\nconvolutional neural network accelerator, which improves the performance of the\ncurrent accelerators. First, we use the 1x1 size convolution kernel as the\nsmallest unit of the computing unit. Then we design suitable computing unit\nbased on the requirements of each model. Secondly, we use Reuse Feature SRAM to\nstore the output of the current layer in the chip and use the value as the\ninput of the next layer. Moreover, we import Output Reuse Strategy and Ring\nStream Dataflow to reduce the amount of data exchange between chips and DRAM.\nFinally, we present On-fly Pooling Module to let the calculation of the Pooling\nlayer directly complete in the chip. With the aid of the proposed method, the\nimplemented acceleration chip has an extremely high hardware utilization rate.\nWe reduce a generous amount of data transfer on the specific module, ECNN.\nCompared to the methods without reuse strategy, we can reduce 533 times of data\naccess amount. At the same time, we have enough computing power to perform\nreal-time execution of the existing image classification model, VGG16 and\nMobileNet. Compared with the design in VWA, we can speed up 7.52 times and have\n1.92x energy efficiency", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5229\u7528\u7387\u7684\u80fd\u6d88\u654f\u8ba1\u7b97\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u6570\u636e\u91cd\u7528\u7b56\u7565\u548c\u82af\u7247\u5185\u8ba1\u7b97\u4f18\u5316\uff0c\u5927\u5e45\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u548c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6570\u636e\u8bbf\u95ee\u91cf\u8fc7\u5927\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u7684\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u91c7\u75281x1\u5377\u79ef\u6838\u4f5c\u4e3a\u8ba1\u7b97\u5355\u5143\uff0c\u8bbe\u8ba1\u9002\u5408\u7684\u8ba1\u7b97\u5355\u5143\uff1b\u4f7f\u7528\u91cd\u7528\u7279\u5f81SRAM\u5b58\u50a8\u5f53\u524d\u5c42\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u5c42\u8f93\u5165\uff1b\u5bfc\u5165\u8f93\u51fa\u91cd\u7528\u7b56\u7565\u548c\u73af\u5f62\u6d41\u6570\u636e\u6d41\u51cf\u5c11\u82af\u7247\u4e0eDRAM\u7684\u6570\u636e\u4ea4\u6362\uff1b\u4f7f\u7528\u82af\u7247\u5185\u76f4\u63a5\u5b8c\u6210Pooling\u5c42\u8ba1\u7b97\u7684\u6a21\u5757\u3002", "result": "\u5b9e\u73b0\u7684\u52a0\u901f\u82af\u7247\u5177\u6709\u6781\u9ad8\u7684\u786c\u4ef6\u5229\u7528\u7387\uff0c\u5728ECNN\u6a21\u5757\u4e0a\u51cf\u5c11533\u500d\u7684\u6570\u636e\u8bbf\u95ee\u91cf\uff0c\u80fd\u591f\u5b9e\u65f6\u6267\u884cVGG16\u548cMobileNet\u6a21\u578b\uff0c\u4e0eVWA\u8bbe\u8ba1\u76f8\u6bd4\u901f\u5ea6\u63d0\u53477.52\u500d\uff0c\u80fd\u6548\u63d0\u53471.92\u500d\u3002", "conclusion": "\u8be5\u52a0\u901f\u5668\u8bbe\u8ba1\u901a\u8fc7\u6570\u636e\u91cd\u7528\u7b56\u7565\u548c\u82af\u7247\u5185\u8ba1\u7b97\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u80fd\u7684\u5b9e\u65f6\u6267\u884c\u3002"}}
{"id": "2509.05937", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.05937", "abs": "https://arxiv.org/abs/2509.05937", "authors": ["Wei-Hsing Huang", "Jianwei Jia", "Yuyao Kong", "Faaiq Waqar", "Tai-Hao Wen", "Meng-Fan Chang", "Shimeng Yu"], "title": "Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale Systems", "comment": null, "summary": "Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an\ninnovative architectural paradigm capable of replicating conventional deep\nneural network (DNN) capabilities while utilizing significantly reduced\nparameter counts through the employment of parameterized B-spline functions\nwith trainable coefficients. Nevertheless, the B-spline functional components\ninherent to KAN architectures introduce distinct hardware acceleration\ncomplexities. While B-spline function evaluation can be accomplished through\nlook-up table (LUT) implementations that directly encode functional mappings,\nthus minimizing computational overhead, such approaches continue to demand\nconsiderable circuit infrastructure, including LUTs, multiplexers, decoders,\nand related components. This work presents an algorithm-hardware co-design\napproach for KAN acceleration. At the algorithmic level, techniques include\nAlignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity\naware mapping strategy, and circuit-level techniques include N:1 Time\nModulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)\ncircuits. This work conducts evaluations on large-scale KAN networks to\nvalidate the proposed methodologies. Non-ideality factors, including partial\nsum deviations from process variations, have been evaluated with statistics\nmeasured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally\ndetermined KAN hyperparameters in conjunction with circuit optimizations\nfabricated at the 22nm technology node, despite the parameter count for\nlarge-scale tasks in this work increasing by 500Kx to 807Kx compared to\ntiny-scale tasks in previous work, the area overhead increases by only 28Kx to\n41Kx, with power consumption rising by merely 51x to 94x, while accuracy\ndegradation remains minimal at 0.11% to 0.23%, demonstrating the scaling\npotential of our proposed architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdKAN\u7f51\u7edc\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u3001\u7a00\u758f\u6620\u5c04\u548c\u6a21\u62df\u5b58\u5185\u8ba1\u7b97\u7b49\u6280\u672f\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u786c\u4ef6\u5f00\u9500\u3002", "motivation": "Kolmogorov-Arnold Networks (KAN) \u867d\u7136\u53c2\u6570\u6548\u7387\u9ad8\uff0c\u4f46\u5176B\u6837\u6761\u51fd\u6570\u7ec4\u4ef6\u5728\u786c\u4ef6\u52a0\u901f\u65b9\u9762\u5b58\u5728\u590d\u6742\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7535\u8def\u5b9e\u73b0\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528Alignment-Symmetry\u548cPowerGap\u91cf\u5316\u6280\u672f\u3001KAN\u7a00\u758f\u6620\u5c04\u7b56\u7565\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6a21\u62df\u5b58\u5185\u8ba1\u7b97(ACIM)\u7535\u8def\u7684N:1\u65f6\u95f4\u8c03\u5236\u52a8\u6001\u7535\u538b\u8f93\u5165\u751f\u6210\u5668\u3002", "result": "\u5728TSMC 22nm RRAM-ACIM\u539f\u578b\u82af\u7247\u4e0a\u9a8c\u8bc1\uff0c\u5927\u89c4\u6a21KAN\u7f51\u7edc\u53c2\u6570\u589e\u52a0500K-807K\u500d\u65f6\uff0c\u9762\u79ef\u5f00\u9500\u4ec5\u589e\u52a028K-41K\u500d\uff0c\u529f\u8017\u589e\u52a051-94\u500d\uff0c\u7cbe\u5ea6\u635f\u5931\u4ec50.11%-0.23%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u5c55\u793a\u4e86KAN\u67b6\u6784\u7684\u826f\u597d\u6269\u5c55\u6f5c\u529b\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u786c\u4ef6\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2509.06101", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.06101", "abs": "https://arxiv.org/abs/2509.06101", "authors": ["Fan Li", "Mimi Xie", "Yanan Guo", "Huize Li", "Xin Xin"], "title": "SCREME: A Scalable Framework for Resilient Memory Design", "comment": null, "summary": "The continuing advancement of memory technology has not only fueled a surge\nin performance, but also substantially exacerbate reliability challenges.\nTraditional solutions have primarily focused on improving the efficiency of\nprotection schemes, i.e., Error Correction Codes (ECC), under the assumption\nthat allocating additional memory space for parity data is always expensive and\ntherefore not a scalable solution.\n  We break the stereotype by proposing an orthogonal approach that provides\nadditional, cost-effective memory space for resilient memory design. In\nparticular, we recognize that ECC chips (used for parity storage) do not\nnecessarily require the same performance level as regular data chips. This\noffers two-fold benefits: First, the bandwidth originally provisioned for a\nregular-performance ECC chip can instead be used to accommodate multiple\nlow-performance chips. Second, the cost of ECC chips can be effectively\nreduced, as lower performance often correlates with lower expense. In addition,\nwe observe that server-class memory chips are often provisioned with ample, yet\nunderutilized I/O resources. This further offers the opportunity to repurpose\nthese resources to enable flexible on-DIMM interconnections. Based on the above\ntwo insights, we finally propose SCREME, a scalable memory framework leverages\ncost-effective, albeit slower, chips -- naturally produced during rapid\ntechnology evolution -- to meet the growing reliability demands driven by this\nevolution.", "AI": {"tldr": "SCREME\u6846\u67b6\u5229\u7528\u4f4e\u6210\u672c\u3001\u4f4e\u6027\u80fd\u7684ECC\u82af\u7247\u6765\u63d0\u4f9b\u66f4\u591a\u5185\u5b58\u7a7a\u95f4\u7528\u4e8e\u5bb9\u9519\u8bbe\u8ba1\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u5e26\u5bbd\u548c\u5229\u7528\u672a\u5145\u5206\u5229\u7528\u7684I/O\u8d44\u6e90\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5185\u5b58\u53ef\u9760\u6027\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5185\u5b58\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u4e5f\u52a0\u5267\u4e86\u53ef\u9760\u6027\u6311\u6218\u3002\u4f20\u7edfECC\u65b9\u6848\u5047\u8bbe\u5206\u914d\u989d\u5916\u5185\u5b58\u7a7a\u95f4\u7528\u4e8e\u5947\u5076\u6821\u9a8c\u6570\u636e\u603b\u662f\u6602\u8d35\u7684\uff0c\u8fd9\u79cd\u5047\u8bbe\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faSCREME\u6846\u67b6\uff0c\u5229\u7528ECC\u82af\u7247\u4e0d\u9700\u8981\u4e0e\u5e38\u89c4\u6570\u636e\u82af\u7247\u76f8\u540c\u6027\u80fd\u6c34\u5e73\u7684\u7279\u70b9\uff1a1\uff09\u5c06\u539f\u672c\u4e3a\u9ad8\u6027\u80fdECC\u82af\u7247\u63d0\u4f9b\u7684\u5e26\u5bbd\u7528\u4e8e\u5bb9\u7eb3\u591a\u4e2a\u4f4e\u6210\u672c\u82af\u7247\uff1b2\uff09\u5229\u7528\u670d\u52a1\u5668\u7ea7\u5185\u5b58\u82af\u7247\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684I/O\u8d44\u6e90\u5b9e\u73b0\u7075\u6d3b\u7684DIMM\u5185\u90e8\u8fde\u63a5\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u6210\u672c\u8f83\u4f4e\u4f46\u901f\u5ea6\u8f83\u6162\u7684\u82af\u7247\uff08\u6280\u672f\u6f14\u8fdb\u8fc7\u7a0b\u4e2d\u81ea\u7136\u4ea7\u751f\u7684\uff09\uff0c\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u5185\u5b58\u7a7a\u95f4\u7528\u4e8e\u5f39\u6027\u5185\u5b58\u8bbe\u8ba1\u3002", "conclusion": "SCREME\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5185\u5b58\u6846\u67b6\uff0c\u80fd\u591f\u6ee1\u8db3\u7531\u6280\u672f\u6f14\u8fdb\u9a71\u52a8\u7684\u65e5\u76ca\u589e\u957f\u7684\u5185\u5b58\u53ef\u9760\u6027\u9700\u6c42\uff0c\u6253\u7834\u4e86\u4f20\u7edfECC\u65b9\u6848\u7684\u6210\u672c\u9650\u5236\u5047\u8bbe\u3002"}}
{"id": "2509.06365", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.06365", "abs": "https://arxiv.org/abs/2509.06365", "authors": ["Omar Al Habsi", "Safa Mohammed Sali", "Anis Meribout", "Mahmoud Meribout", "Saif Almazrouei", "Mohamed Seghier"], "title": "Hardware Acceleration in Portable MRIs: State of the Art and Future Prospects", "comment": null, "summary": "There is a growing interest in portable MRI (pMRI) systems for point-of-care\nimaging, particularly in remote or resource-constrained environments. However,\nthe computational complexity of pMRI, especially in image reconstruction and\nmachine learning (ML) algorithms for enhanced imaging, presents significant\nchallenges. Such challenges can be potentially addressed by harnessing hardware\napplication solutions, though there is little focus in the current pMRI\nliterature on hardware acceleration. This paper bridges that gap by reviewing\nrecent developments in pMRI, emphasizing the role and impact of hardware\nacceleration to speed up image acquisition and reconstruction. Key technologies\nsuch as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays\n(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent\nperformance in terms of reconstruction speed and power consumption. This review\nalso highlights the promise of AI-powered reconstruction, open low-field pMRI\ndatasets, and innovative edge-based hardware solutions for the future of pMRI\ntechnology. Overall, hardware acceleration can enhance image quality, reduce\npower consumption, and increase portability for next-generation pMRI\ntechnology. To accelerate reproducible AI for portable MRI, we propose forming\na Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,\nretrospective multi-center testing, prospective reader and non-inferiority\ntrials) to provide standardized datasets, benchmarks, and regulator-ready\ntestbeds.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bc4\u8bba\u4e86\u786c\u4ef6\u52a0\u901f\u6280\u672f\u5728\u53ef\u7a7f\u6230MRI\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86GPU\u3001FPGA\u3001ASIC\u7b49\u6280\u672f\u5728\u52a0\u901f\u56fe\u50cf\u91cd\u5efa\u548c\u964d\u4f4e\u529f\u8017\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u4f4e\u573aMRI\u8054\u76df\u548c\u6807\u51c6\u5316\u6d4b\u8bd5\u6846\u67b6\u7684\u5efa\u8bae\u3002", "motivation": "\u53ef\u7a7f\u6230MRI\u7cfb\u7edf\u5728\u8fdc\u7a0b\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u56fe\u50cf\u91cd\u5efa\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u6027\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u5f53\u524d\u7814\u7a76\u5c11\u6709\u5173\u6ce8\u786c\u4ef6\u52a0\u901f\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u6700\u65b0\u53ef\u7a7f\u6230MRI\u6280\u672f\u53d1\u5c55\uff0c\u91cd\u70b9\u5206\u6790\u4e86GPU\u3001FPGA\u3001ASIC\u7b49\u786c\u4ef6\u52a0\u901f\u6280\u672f\u5728\u52a0\u901f\u56fe\u50cf\u83b7\u53d6\u548c\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u3002\u8fd8\u8ba8\u8bba\u4e86AI\u9a71\u52a8\u7684\u91cd\u5efa\u6280\u672f\u3001\u5f00\u653f\u4f4e\u573apMRI\u6570\u636e\u96c6\u548c\u8fb9\u7f18\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u786c\u4ef6\u52a0\u901f\u6280\u672f\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u56fe\u50cf\u91cd\u5efa\u901f\u5ea6\u3001\u964d\u4f4e\u529f\u8017\u548c\u589e\u5f3a\u7cfb\u7edf\u53ef\u7a7f\u6230\u6027\u3002\u7279\u522b\u662fGPU\u3001FPGA\u548cASIC\u5728\u6027\u80fd\u548c\u529f\u8017\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u786c\u4ef6\u52a0\u901f\u6280\u672f\u5bf9\u4e0b\u4e00\u4ee3\u53ef\u7a7f\u6230MRI\u6280\u672f\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3001\u964d\u4f4e\u529f\u8017\u5e76\u589e\u5f3a\u7cfb\u7edf\u53ef\u7a7f\u6230\u6027\u3002\u4e3a\u4fc3\u8fdbAI\u5728\u53ef\u7a7f\u6230MRI\u4e2d\u7684\u53ef\u590d\u73b0\u5e94\u7528\uff0c\u9700\u8981\u5efa\u7acb\u4f4e\u573aMRI\u8054\u76df\u548c\u6807\u51c6\u5316\u6d4b\u8bd5\u6846\u67b6\u3002"}}
{"id": "2509.06698", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.06698", "abs": "https://arxiv.org/abs/2509.06698", "authors": ["Leidy Mabel Alvero-Gonzalez", "Matias Miguez", "Eric Gutierrez", "Juan Sapriza", "Susana Pat\u00f3n", "David Atienza", "Jos\u00e9 Miranda"], "title": "VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal activity sensing", "comment": null, "summary": "Continuous monitoring of electrodermal activity (EDA) through wearable\ndevices has attracted much attention in recent times. However, the persistent\nchallenge demands analog front-end (AFE) systems with high sensitivity, low\npower consumption, and minimal calibration requirements to ensure practical\nusability in wearable technologies. In response to this challenge, this\nresearch introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog\nReadout tailored for continuous EDA sensing. The results show that our system\nachieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS\nrange and a negligible relative error of less than 0.0025% for\nfixed-resistance. Furthermore, the proposed system consumes only an average of\n2.3 uW based on post-layout validations and introduces a low noise\ncontribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.\nThis research aims to drive the evolution of wearable sensors characterized by\nseamless adaptability to diverse users, minimal power consumption, and\noutstanding noise resilience.", "AI": {"tldr": "\u63d0\u51faVCO-CARE\u7cfb\u7edf\uff0c\u4e00\u79cd\u57fa\u4e8e\u538b\u63a7\u632f\u8361\u5668\u7684\u6a21\u62df\u8bfb\u51fa\u7535\u8def\uff0c\u7528\u4e8e\u8fde\u7eedEDA\u76d1\u6d4b\uff0c\u5177\u6709\u9ad8\u7075\u654f\u5ea6(40pS)\u3001\u4f4e\u529f\u8017(2.3\u03bcW)\u548c\u4f4e\u566a\u58f0(0.8\u03bcVrms)\u7279\u6027", "motivation": "\u89e3\u51b3\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2dEDA\u76d1\u6d4b\u5bf9\u9ad8\u7075\u654f\u5ea6\u3001\u4f4e\u529f\u8017\u548c\u6700\u5c0f\u6821\u51c6\u9700\u6c42\u7684\u6a21\u62df\u524d\u7aef\u7cfb\u7edf\u7684\u6301\u7eed\u6311\u6218", "method": "\u5f00\u53d1\u57fa\u4e8e\u538b\u63a7\u632f\u8361\u5668\u7684\u6a21\u62df\u8bfb\u51fa\u7535\u8def(VCO-CARE)\uff0c\u4e13\u4e3a\u8fde\u7eedEDA\u4f20\u611f\u8bbe\u8ba1", "result": "\u7cfb\u7edf\u57280-20\u03bcS\u8303\u56f4\u5185\u8fbe\u523040pS\u7684\u5e73\u5747\u7075\u654f\u5ea6\uff0c\u56fa\u5b9a\u7535\u963b\u76f8\u5bf9\u8bef\u5dee\u5c0f\u4e8e0.0025%\uff0c\u529f\u8017\u4ec52.3\u03bcW\uff0c\u566a\u58f0\u8d21\u732e\u4ec50.8\u03bcVrms", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u5411\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u7528\u6237\u3001\u6700\u5c0f\u529f\u8017\u548c\u51fa\u8272\u566a\u58f0\u6297\u6270\u6027\u7684\u65b9\u5411\u53d1\u5c55"}}
