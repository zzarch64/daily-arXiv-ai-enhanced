{"id": "2512.00006", "categories": ["cs.AR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00006", "abs": "https://arxiv.org/abs/2512.00006", "authors": ["Yuqin Zhao", "Linghui Ye", "Haihang Xia", "Luke Seed", "Tiantai Deng"], "title": "VeriPy - A New Python-Based Approach for SDR Pipelined/Unrolled Hardware Accelerator Generation", "comment": "13 Pages, 16 figures, and 9 tables. Aim to submit to IEEE TCAD", "summary": "Software-defined radio (SDR) plays an important role in the communication field by providing a flexible and customized communication system for different purposes according to the needs. To enhance the performance of SDR applications, hardware accelerators have been widely deployed in recent years. In facing this obstacle, a necessity arises for a high-level synthesis (HLS) tool specifically designed for communication engineers without detailed hardware knowledge. To lower the barrier between SDR engineers and hardware development, this work proposed a Python-based HLS tool, VeriPy, which can generate both mainstream architecture for hardware accelerators in Verilog specifically for SDR designs including unrolled design and pipelined design, requiring no detailed digital hardware knowledge or Hardware Description Languages (HDL). Furthermore, VeriPy supports automatic testbench generation with random input stimulus, an extensible hardware library, performance and resource estimation, and offers strong optimisation potential at both the algorithmic and digital hardware levels. The generated hardware design by VeriPy can achieve up to 70% faster operating frequency compared to pragma-optimised Vivado HLS designs with a reasonably higher resource con-sumption while delivering comparable performance and resource consumption to hand-coded implementations. Regarding code complexity, VeriPy requires no pragmas, completely eliminating the need for low-level hardware knowledge. For straightforward algorithms, the input code length remains comparable to that of Vivado HLS.", "AI": {"tldr": "VeriPy\uff1a\u9762\u5411SDR\u5de5\u7a0b\u5e08\u7684Python HLS\u5de5\u5177\uff0c\u65e0\u9700\u786c\u4ef6\u77e5\u8bc6\u5373\u53ef\u751f\u6210Verilog\u786c\u4ef6\u52a0\u901f\u5668\u8bbe\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8eVivado HLS\uff0c\u63a5\u8fd1\u624b\u5de5\u7f16\u7801\u5b9e\u73b0", "motivation": "SDR\u5e94\u7528\u9700\u8981\u786c\u4ef6\u52a0\u901f\u5668\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u901a\u4fe1\u5de5\u7a0b\u5e08\u7f3a\u4e4f\u786c\u4ef6\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709HLS\u5de5\u5177\u95e8\u69db\u9ad8\uff0c\u9700\u8981\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\u548c\u5e95\u5c42\u786c\u4ef6\u77e5\u8bc6\uff0c\u963b\u788d\u4e86SDR\u5de5\u7a0b\u5e08\u4f7f\u7528\u786c\u4ef6\u52a0\u901f\u6280\u672f\u3002", "method": "\u5f00\u53d1\u57fa\u4e8ePython\u7684HLS\u5de5\u5177VeriPy\uff0c\u652f\u6301\u751f\u6210\u4e24\u79cd\u4e3b\u6d41\u786c\u4ef6\u52a0\u901f\u5668\u67b6\u6784\uff08\u5c55\u5f00\u8bbe\u8ba1\u548c\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff09\uff0c\u63d0\u4f9b\u81ea\u52a8\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\u3001\u53ef\u6269\u5c55\u786c\u4ef6\u5e93\u3001\u6027\u80fd\u8d44\u6e90\u4f30\u8ba1\uff0c\u5e76\u5728\u7b97\u6cd5\u548c\u786c\u4ef6\u5c42\u9762\u8fdb\u884c\u4f18\u5316\u3002", "result": "VeriPy\u751f\u6210\u7684\u786c\u4ef6\u8bbe\u8ba1\u76f8\u6bd4pragma\u4f18\u5316\u7684Vivado HLS\u8bbe\u8ba1\uff0c\u5de5\u4f5c\u9891\u7387\u6700\u9ad8\u63d0\u534770%\uff0c\u8d44\u6e90\u6d88\u8017\u5408\u7406\u589e\u52a0\uff1b\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u63a5\u8fd1\u624b\u5de5\u7f16\u7801\u5b9e\u73b0\u3002\u4ee3\u7801\u590d\u6742\u5ea6\u4f4e\uff0c\u65e0\u9700pragma\u548c\u786c\u4ef6\u77e5\u8bc6\u3002", "conclusion": "VeriPy\u6210\u529f\u964d\u4f4e\u4e86SDR\u5de5\u7a0b\u5e08\u4f7f\u7528\u786c\u4ef6\u52a0\u901f\u5668\u7684\u95e8\u69db\uff0c\u63d0\u4f9b\u9ad8\u6027\u80fd\u786c\u4ef6\u8bbe\u8ba1\u751f\u6210\u80fd\u529b\uff0c\u586b\u8865\u4e86\u901a\u4fe1\u9886\u57df\u4e13\u7528HLS\u5de5\u5177\u7684\u7a7a\u767d\u3002"}}
{"id": "2512.00016", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00016", "abs": "https://arxiv.org/abs/2512.00016", "authors": ["Mubarek Mohammed"], "title": "Architect in the Loop Agentic Hardware Design and Verification", "comment": null, "summary": "The ever increasing complexity of the hardware design process demands improved hardware design and verification methodologies. With the advent of generative AI various attempts have been made to automate parts of the design and verification process. Large language models (LLMs) as well as specialized models generate hdl and testbenches for small components, having a few leaf level components. However, there are only a few attempts to automate the entire processor design process. Hardware design demands hierarchical and modular design processes. We utilized this best practice systematically and effectively. We propose agentic automated processor design and verification with engineers in the loop. The agent with optional specification tries to break down the design into sub-components, generate HDL and cocotb tests, and verifies the components involving engineer guidance, especially during debugging and synthesis. We designed various digital systems using this approach. However, we selected two simple processors for demonstration purposes in this work. The first one is a LEGv8 like a simple processor verified, synthesized and programmed for the DE-10 Lite FPGA. The second one is a RISC-V like 32-bit processor designed and verified in similar manner and synthesized. However, it is not programmed into the DE-10 Lite. This process is accomplished usually using around a million inference tokens per processor, using a combination of reasoning (e.g gemini-pro) and non-reasoning models (eg. gpt-5-mini) based on the complexity of the task. This indicates that hardware design and verification experimentation can be done cost effectively without using any specialized hardware. The approach is scalable, we even attempted system-on-chip, which we want to experiment in our future work.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u667a\u80fd\u4f53\u4e0e\u5de5\u7a0b\u5e08\u534f\u540c\u7684\u5904\u7406\u5668\u81ea\u52a8\u5316\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u5c42\u751f\u6210HDL\u4ee3\u7801\u548c\u6d4b\u8bd5\uff0c\u6210\u529f\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e86\u4e24\u4e2a\u5904\u7406\u5668\uff08LEGv8\u548cRISC-V\uff09\uff0c\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u6269\u5c55\u3002", "motivation": "\u786c\u4ef6\u8bbe\u8ba1\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u6539\u8fdb\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u65b9\u6cd5\u3002\u73b0\u6709AI\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5c0f\u578b\u7ec4\u4ef6\uff0c\u7f3a\u4e4f\u5b8c\u6574\u7684\u5904\u7406\u5668\u8bbe\u8ba1\u81ea\u52a8\u5316\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u5206\u5c42\u6a21\u5757\u5316\u8bbe\u8ba1\u539f\u5219\uff0c\u5b9e\u73b0\u5904\u7406\u5668\u5168\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4f53\u4e0e\u5de5\u7a0b\u5e08\u534f\u540c\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u9a8c\u8bc1\u6846\u67b6\uff1a\u667a\u80fd\u4f53\u6839\u636e\u89c4\u8303\u5c06\u8bbe\u8ba1\u5206\u89e3\u4e3a\u5b50\u7ec4\u4ef6\uff0c\u751f\u6210HDL\u4ee3\u7801\u548ccocotb\u6d4b\u8bd5\uff0c\u5728\u8c03\u8bd5\u548c\u7efc\u5408\u9636\u6bb5\u5f15\u5165\u5de5\u7a0b\u5e08\u6307\u5bfc\u3002\u7ed3\u5408\u63a8\u7406\u6a21\u578b\uff08\u5982gemini-pro\uff09\u548c\u975e\u63a8\u7406\u6a21\u578b\uff08\u5982gpt-5-mini\uff09\uff0c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u9009\u62e9\u5408\u9002\u6a21\u578b\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e86\u4e24\u4e2a\u5904\u7406\u5668\uff1a1) LEGv8\u7c7b\u4f3c\u5904\u7406\u5668\uff0c\u5728DE-10 Lite FPGA\u4e0a\u9a8c\u8bc1\u3001\u7efc\u5408\u5e76\u7f16\u7a0b\uff1b2) RISC-V\u7c7b\u4f3c32\u4f4d\u5904\u7406\u5668\uff0c\u4ee5\u76f8\u540c\u65b9\u5f0f\u8bbe\u8ba1\u9a8c\u8bc1\u5e76\u7efc\u5408\u3002\u6bcf\u4e2a\u5904\u7406\u5668\u7ea6\u4f7f\u7528100\u4e07\u63a8\u7406token\uff0c\u6210\u672c\u6548\u76ca\u9ad8\u3002\u65b9\u6cd5\u53ef\u6269\u5c55\uff0c\u5df2\u5c1d\u8bd5\u7cfb\u7edf\u7ea7\u82af\u7247\u8bbe\u8ba1\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u4f53\u81ea\u52a8\u5316\u5904\u7406\u5668\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u53ef\u884c\uff0c\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u5373\u53ef\u4f4e\u6210\u672c\u8fdb\u884c\u786c\u4ef6\u8bbe\u8ba1\u5b9e\u9a8c\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u672a\u6765\u7cfb\u7edf\u7ea7\u82af\u7247\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.00017", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00017", "abs": "https://arxiv.org/abs/2512.00017", "authors": ["Kunlong Zhang", "Guiying Li", "Ning Lu", "Peng Yang", "Ke Tang"], "title": "Hardware-Aware DNN Compression for Homogeneous Edge Devices", "comment": "International Conference on Data-driven Optimization of Complex Systems 2025 Camera Ready", "summary": "Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Extensive experiments on multiple device types (Jetson Xavier NX and Jetson Nano) and task types (image classification with ResNet50, MobileNetV1, ResNet56, VGG16; object detection with YOLOv8n) demonstrate that HDAP consistently achieves lower average latency and competitive accuracy compared to state-of-the-art methods, with significant speedups (e.g., 2.86$\\times$ on ResNet50 at 1.0G FLOPs). HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices.", "AI": {"tldr": "HDAP\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u540c\u6784\u8fb9\u7f18\u8bbe\u5907\u7684\u786c\u4ef6\u611f\u77e5DNN\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u5907\u805a\u7c7b\u548c\u4ee3\u7406\u8bc4\u4f30\u6765\u89e3\u51b3\u8bbe\u5907\u6027\u80fd\u5dee\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u8bbe\u5907\u7684\u6700\u4f18\u5e73\u5747\u6027\u80fd\u3002", "motivation": "\u540c\u6784\u8fb9\u7f18\u8bbe\u5907\uff08\u76f8\u540c\u578b\u53f7\uff09\u5728\u5b9e\u9645\u90e8\u7f72\u540e\uff0c\u7531\u4e8e\u7528\u6237\u914d\u7f6e\u3001\u73af\u5883\u6761\u4ef6\u3001\u5236\u9020\u5dee\u5f02\u3001\u7535\u6c60\u9000\u5316\u7b49\u56e0\u7d20\uff0c\u6027\u80fd\u4f1a\u53d8\u5f97\u4e0d\u540c\u3002\u73b0\u6709\u7684DNN\u538b\u7f29\u65b9\u6cd5\u672a\u8003\u8651\u8fd9\u79cd\u60c5\u51b5\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u5728\u6240\u6709\u540c\u6784\u8bbe\u5907\u4e0a\u90fd\u6709\u826f\u597d\u7684\u538b\u7f29\u6548\u679c\u3002", "method": "\u63d0\u51faHDAP\u6846\u67b6\uff1a1\uff09\u5c06\u5927\u91cf\u540c\u6784\u8bbe\u5907\u805a\u7c7b\u6210\u51e0\u4e2a\u8bbe\u5907\u7c07\uff0c\u51cf\u5c11\u9700\u8981\u8bc4\u4f30\u7684\u8bbe\u5907\u6570\u91cf\uff1b2\uff09\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u8bc4\u4f30\u4ee3\u66ff\u5b9e\u65f6\u786c\u4ef6\u8bc4\u4f30\uff1b3\uff09\u76ee\u6807\u662f\u5b9e\u73b0\u538b\u7f29\u6a21\u578b\u5728\u6240\u6709\u8bbe\u5907\u4e0a\u7684\u6700\u4f18\u5e73\u5747\u6027\u80fd\u3002", "result": "\u5728\u591a\u79cd\u8bbe\u5907\u7c7b\u578b\uff08Jetson Xavier NX\u548cJetson Nano\uff09\u548c\u4efb\u52a1\u7c7b\u578b\uff08\u56fe\u50cf\u5206\u7c7b\u548c\u7269\u4f53\u68c0\u6d4b\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHDAP\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u80fd\u5b9e\u73b0\u66f4\u4f4e\u7684\u5e73\u5747\u5ef6\u8fdf\u548c\u7ade\u4e89\u6027\u51c6\u786e\u7387\uff0c\u663e\u8457\u52a0\u901f\uff08\u5982ResNet50\u57281.0G FLOPs\u4e0b\u8fbe\u52302.86\u500d\u52a0\u901f\uff09\u3002", "conclusion": "HDAP\u4e3a\u540c\u6784\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u7684DNN\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bbe\u5907\u6027\u80fd\u5dee\u5f02\u5e26\u6765\u7684\u538b\u7f29\u6311\u6218\u3002"}}
{"id": "2512.00020", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00020", "abs": "https://arxiv.org/abs/2512.00020", "authors": ["Guang Yang", "Wei Zheng", "Xiang Chen", "Dong Liang", "Peng Hu", "Yukui Yang", "Shaohang Peng", "Zhenghan Li", "Jiahui Feng", "Xiao Wei", "Kexin Sun", "Deyuan Ma", "Haotian Cheng", "Yiheng Shen", "Xing Hu", "Terry Yue Zhuo", "David Lo"], "title": "Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead", "comment": "WIP", "summary": "Code generation has emerged as a critical research area at the intersection of Software Engineering (SE) and Artificial Intelligence (AI), attracting significant attention from both academia and industry. Within this broader landscape, Verilog, as a representative hardware description language (HDL), plays a fundamental role in digital circuit design and verification, making its automated generation particularly significant for Electronic Design Automation (EDA). Consequently, recent research has increasingly focused on applying Large Language Models (LLMs) to Verilog code generation, particularly at the Register Transfer Level (RTL), exploring how these AI-driven techniques can be effectively integrated into hardware design workflows. Despite substantial research efforts have explored LLM applications in this domain, a comprehensive survey synthesizing these developments remains absent from the literature. This review fill addresses this gap by providing a systematic literature review of LLM-based methods for Verilog code generation, examining their effectiveness, limitations, and potential for advancing automated hardware design. The review encompasses research work from conferences and journals in the fields of SE, AI, and EDA, encompassing 70 papers published on venues, along with 32 high-quality preprint papers, bringing the total to 102 papers. By answering four key research questions, we aim to (1) identify the LLMs used for Verilog generation, (2) examine the datasets and metrics employed in evaluation, (3) categorize the techniques proposed for Verilog generation, and (4) analyze LLM alignment approaches for Verilog generation. Based on our findings, we have identified a series of limitations of existing studies. Finally, we have outlined a roadmap highlighting potential opportunities for future research endeavors in LLM-assisted hardware design.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728Verilog\u4ee3\u7801\u751f\u6210\u9886\u57df\u5e94\u7528\u7684\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86102\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u6280\u672f\u5206\u7c7b\uff0c\u5e76\u6307\u51fa\u4e86\u7814\u7a76\u5c40\u9650\u6027\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "Verilog\u4f5c\u4e3a\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\u5728\u6570\u5b57\u7535\u8def\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u548c\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u6210\u529f\u5e94\u7528\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5176\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u7814\u7a76\u5173\u6ce8LLM\u5728Verilog\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u53d1\u5c55\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u6536\u96c6\u4e86\u6765\u81ea\u8f6f\u4ef6\u5de5\u7a0b\u3001\u4eba\u5de5\u667a\u80fd\u548c\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u9886\u57df\u768470\u7bc7\u4f1a\u8bae/\u671f\u520a\u8bba\u6587\u548c32\u7bc7\u9ad8\u8d28\u91cf\u9884\u5370\u672c\u8bba\u6587\uff0c\u603b\u8ba1102\u7bc7\u3002\u901a\u8fc7\u56de\u7b54\u56db\u4e2a\u5173\u952e\u7814\u7a76\u95ee\u9898\u6765\u7ec4\u7ec7\u5206\u6790\u3002", "result": "\u7efc\u8ff0\u8bc6\u522b\u4e86\u7528\u4e8eVerilog\u751f\u6210\u7684LLM\u6a21\u578b\uff0c\u68c0\u67e5\u4e86\u8bc4\u4f30\u4e2d\u4f7f\u7528\u7684\u6570\u636e\u96c6\u548c\u6307\u6807\uff0c\u5bf9Verilog\u751f\u6210\u6280\u672f\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86LLM\u5bf9\u9f50\u65b9\u6cd5\u3002\u540c\u65f6\u6307\u51fa\u4e86\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u586b\u8865\u4e86LLM\u5728Verilog\u4ee3\u7801\u751f\u6210\u9886\u57df\u7cfb\u7edf\u6027\u7efc\u8ff0\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u73b0\u72b6\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u8def\u7ebf\u56fe\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u81ea\u52a8\u5316\u786c\u4ef6\u8bbe\u8ba1\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.00026", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00026", "abs": "https://arxiv.org/abs/2512.00026", "authors": ["Mahek Desai", "Rowena Quinn", "Marjan Asadinia"], "title": "ML-PCM : Machine Learning Technique for Write Optimization in Phase Change Memory (PCM)", "comment": null, "summary": "As transistor-based memory technologies like dynamic random access memory (DRAM) approach their scalability limits, the need to explore alternative storage solutions becomes increasingly urgent. Phase-change memory (PCM) has gained attention as a promising option due to its scalability, fast access speeds, and zero leakage power compared to conventional memory systems. However, despite these advantages, PCM faces several challenges that impede its broader adoption, particularly its limited lifespan due to material degradation during write operations, as well as the high energy demands of these processes. For PCM to become a viable storage alternative, enhancing its endurance and reducing the energy required for write operations are essential. This paper proposes the use of a neural network (NN) model to predict critical parameters such as write latency, energy consumption, and endurance by monitoring real-time operating conditions and device characteristics. These predictions are key to improving PCM performance and identifying optimal write settings, making PCM a more practical and efficient option for data storage in applications with frequent write operations. Our approach leads to significant improvements, with NN predictions achieving a Mean Absolute Percentage Error (MAPE) of 0.0073% for endurance, 0.23% for total write latency, and 4.92% for total write energy.", "AI": {"tldr": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9884\u6d4b\u76f8\u53d8\u5b58\u50a8\u5668\uff08PCM\uff09\u7684\u5199\u5165\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u8010\u4e45\u6027\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u5e76\u63d0\u9ad8\u5176\u4f5c\u4e3aDRAM\u66ff\u4ee3\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u968f\u7740DRAM\u7b49\u6676\u4f53\u7ba1\u5b58\u50a8\u6280\u672f\u63a5\u8fd1\u53ef\u6269\u5c55\u6027\u6781\u9650\uff0c\u9700\u8981\u63a2\u7d22\u66ff\u4ee3\u5b58\u50a8\u65b9\u6848\u3002\u76f8\u53d8\u5b58\u50a8\u5668\uff08PCM\uff09\u56e0\u53ef\u6269\u5c55\u6027\u3001\u5feb\u901f\u8bbf\u95ee\u901f\u5ea6\u548c\u96f6\u6cc4\u6f0f\u529f\u8017\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5176\u8010\u4e45\u6027\u6709\u9650\u548c\u5199\u5165\u80fd\u8017\u9ad8\u7684\u95ee\u9898\u963b\u788d\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u6d4b\u5b9e\u65f6\u8fd0\u884c\u6761\u4ef6\u548c\u8bbe\u5907\u7279\u6027\uff0c\u9884\u6d4bPCM\u7684\u5173\u952e\u53c2\u6570\uff08\u5199\u5165\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u8010\u4e45\u6027\uff09\uff0c\u4ece\u800c\u4f18\u5316\u5199\u5165\u8bbe\u7f6e\u5e76\u63d0\u5347\u6027\u80fd\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff1a\u8010\u4e45\u6027\u9884\u6d4b\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u4e3a0.0073%\uff0c\u603b\u5199\u5165\u5ef6\u8fdf\u4e3a0.23%\uff0c\u603b\u5199\u5165\u80fd\u8017\u4e3a4.92%\u3002", "conclusion": "\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u9884\u6d4b\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347PCM\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4f7f\u5176\u5728\u9891\u7e41\u5199\u5165\u7684\u5e94\u7528\u4e2d\u6210\u4e3a\u66f4\u5b9e\u7528\u3001\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5b58\u50a8\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.00028", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00028", "abs": "https://arxiv.org/abs/2512.00028", "authors": ["Na\u00efn Jonckers", "Toon Vinck", "Peter Karsmakers", "Jeffrey Prinzie"], "title": "Analysis of Single Event Induced Bit Faults in a Deep Neural Network Accelerator Pipeline", "comment": "Submitted to JINST in context of TWEPP 2025 proceedings", "summary": "In recent years, the increased interest and the growth in application domains of Artificial Intelligence (AI), and more specifically Deep Neural Networks (DNNs), has led to an extensive usage of domain specific DNN accelerator processors to improve the computational efficiency of DNN inference. However, like any digital circuit, these processors are prone to faults induced by radiation particles such as heavy ions, protons, etc., making their use in harsh radiation environments a challenge. This work presents an in-depth analysis of the impact of such faults on the computational pipeline of a Systolic Array based Deep Neural Network accelerator (SA-DNN accelerator) by means of a Register Transfer Level (RTL) Fault Injection (FI) simulation in order to improve the observability of each hardware block. From this analysis, we present the sensitivity to single bit faults of register groups in the pipeline for three different DNN workloads utilising two datasets, namely MNIST and CIFAR-10. These sensitivity figures are presented in terms of Fault Propagation Probability ($P(f_{non-crit})$) and False Classification Probability ($P(f_{crit})$) which respectively show the probability that an injected fault causes a non-critical error (numerical offset) or a critical error (classification fault). From these results, we devise a fault mitigation strategy to harden the SA-DNN accelerator in an efficient way, both in terms of area and power overhead.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u8f90\u5c04\u7c92\u5b50\u5f15\u8d77\u7684\u5355\u6bd4\u7279\u6545\u969c\u5bf9\u57fa\u4e8e\u8109\u52a8\u9635\u5217\u7684DNN\u52a0\u901f\u5668\u7684\u5f71\u54cd\uff0c\u901a\u8fc7RTL\u7ea7\u6545\u969c\u6ce8\u5165\u6a21\u62df\u8bc4\u4f30\u786c\u4ef6\u6a21\u5757\u654f\u611f\u6027\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u5bb9\u9519\u52a0\u56fa\u7b56\u7565\u3002", "motivation": "\u968f\u7740AI\u548cDNN\u5e94\u7528\u589e\u957f\uff0c\u4e13\u7528DNN\u52a0\u901f\u5668\u5728\u8f90\u5c04\u73af\u5883\uff08\u5982\u592a\u7a7a\u3001\u6838\u8bbe\u65bd\uff09\u4e2d\u9762\u4e34\u8f90\u5c04\u7c92\u5b50\u5f15\u53d1\u7684\u6545\u969c\u6311\u6218\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5bb9\u9519\u6027\u5e76\u8bbe\u8ba1\u9ad8\u6548\u52a0\u56fa\u65b9\u6848\u3002", "method": "\u91c7\u7528RTL\u7ea7\u6545\u969c\u6ce8\u5165\u6a21\u62df\uff0c\u5206\u6790\u8109\u52a8\u9635\u5217DNN\u52a0\u901f\u5668\u8ba1\u7b97\u6d41\u6c34\u7ebf\u4e2d\u5bc4\u5b58\u5668\u7ec4\u7684\u5355\u6bd4\u7279\u6545\u969c\u654f\u611f\u6027\uff0c\u4f7f\u7528MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u8bc4\u4f30\u4e09\u79cdDNN\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u63d0\u51fa\u4e86\u6545\u969c\u4f20\u64ad\u6982\u7387\u548c\u9519\u8bef\u5206\u7c7b\u6982\u7387\u4e24\u4e2a\u654f\u611f\u6027\u6307\u6807\uff0c\u8bc6\u522b\u4e86\u4e0d\u540c\u786c\u4ef6\u6a21\u5757\u7684\u6545\u969c\u654f\u611f\u6027\u5dee\u5f02\uff0c\u4e3a\u9488\u5bf9\u6027\u52a0\u56fa\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002", "conclusion": "\u57fa\u4e8e\u654f\u611f\u6027\u5206\u6790\u8bbe\u8ba1\u4e86\u517c\u987e\u9762\u79ef\u548c\u529f\u8017\u5f00\u9500\u7684\u9ad8\u6548\u5bb9\u9519\u52a0\u56fa\u7b56\u7565\uff0c\u4e3a\u8f90\u5c04\u73af\u5883\u4e0bDNN\u52a0\u901f\u5668\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00031", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00031", "abs": "https://arxiv.org/abs/2512.00031", "authors": ["Ravindra Ganti", "Steve Xu"], "title": "Hardware-Aware Neural Network Compilation with Learned Optimization: A RISC-V Accelerator Approach", "comment": "18 pages, 7 figures, 6 tables", "summary": "We present XgenSilicon ML Compiler, a fully automated end-to-end compilation framework that transforms high-level machine learning models into optimized RISC-V assembly code for custom ASIC accelerators. By unifying the system's cost model across software and hardware, the compiler achieves significant improvements in Power, Performance, and Area (PPA) metrics compared to standard off-the-shelf components and hand-designed chips through five key innovations: (1) a multi-algorithm auto-tuning framework with five search strategies (Bayesian Optimization, Genetic Algorithm, Simulated Annealing, Random Search, Grid Search) combined with a learned cost model, (2) an integrated quantization framework supporting extreme precisions from FP32 to Binary with full KL divergence calibration (2048-bin histogram optimization) and momentum-based QAT gradient updates, (3) hardware-aware validation ensuring 100 percent ISA compliance and memory constraint satisfaction, (4) dynamic shape support with multi-configuration specialization, and (5) advanced cache-aware cost modeling with multi-level cache hierarchy analysis. Our evaluation demonstrates that ASICs produced by this compiler achieve 2.5-4.5x better performance, 3-6x lower power consumption, and 40-60 percent area reduction compared to baseline implementations. The compiler supports more than 100 ONNX operators across 12 categories, implements advanced RISC-V Vector optimizations, and generates hardware-validated assembly code suitable for direct ASIC synthesis. All compilation steps are fully automated, requiring zero manual intervention from model input to ASIC-ready output.", "AI": {"tldr": "XgenSilicon ML\u7f16\u8bd1\u5668\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u7aef\u5230\u7aef\u7f16\u8bd1\u6846\u67b6\uff0c\u53ef\u5c06\u9ad8\u7ea7ML\u6a21\u578b\u8f6c\u6362\u4e3a\u9488\u5bf9\u5b9a\u5236ASIC\u52a0\u901f\u5668\u4f18\u5316\u7684RISC-V\u6c47\u7f16\u4ee3\u7801\uff0c\u901a\u8fc7\u4e94\u9879\u5173\u952e\u6280\u672f\u5b9e\u73b0\u663e\u8457\u7684PPA\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709ML\u6a21\u578b\u90e8\u7f72\u5230\u786c\u4ef6\u52a0\u901f\u5668\u65f6\uff0c\u901a\u5e38\u9700\u8981\u624b\u52a8\u4f18\u5316\u548c\u786c\u4ef6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8f6f\u4ef6-\u786c\u4ef6\u6210\u672c\u6a21\u578b\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5b9a\u5236ASIC\u7684\u6f5c\u529b\u3002", "method": "1) \u591a\u7b97\u6cd5\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\u7ed3\u5408\u5b66\u4e60\u6210\u672c\u6a21\u578b\uff1b2) \u96c6\u6210\u91cf\u5316\u6846\u67b6\u652f\u6301FP32\u5230\u4e8c\u8fdb\u5236\u7684\u6781\u7aef\u7cbe\u5ea6\uff1b3) \u786c\u4ef6\u611f\u77e5\u9a8c\u8bc1\u786e\u4fddISA\u5408\u89c4\u548c\u5185\u5b58\u7ea6\u675f\uff1b4) \u52a8\u6001\u5f62\u72b6\u652f\u6301\u4e0e\u591a\u914d\u7f6e\u4e13\u4e1a\u5316\uff1b5) \u9ad8\u7ea7\u7f13\u5b58\u611f\u77e5\u6210\u672c\u5efa\u6a21\u4e0e\u591a\u7ea7\u7f13\u5b58\u5c42\u6b21\u5206\u6790\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\uff0c\u751f\u6210\u7684ASIC\u6027\u80fd\u63d0\u53472.5-4.5\u500d\uff0c\u529f\u8017\u964d\u4f4e3-6\u500d\uff0c\u9762\u79ef\u51cf\u5c1140-60%\uff0c\u652f\u6301100\u591a\u4e2aONNX\u7b97\u5b50\uff0c\u5b9e\u73b0RISC-V\u5411\u91cf\u4f18\u5316\uff0c\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8eASIC\u7efc\u5408\u7684\u786c\u4ef6\u9a8c\u8bc1\u6c47\u7f16\u4ee3\u7801\u3002", "conclusion": "XgenSilicon ML\u7f16\u8bd1\u5668\u901a\u8fc7\u7edf\u4e00\u7684\u8f6f\u4ef6-\u786c\u4ef6\u6210\u672c\u6a21\u578b\u548c\u4e94\u9879\u5173\u952e\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4eceML\u6a21\u578b\u5230\u4f18\u5316ASIC\u7684\u5168\u81ea\u52a8\u7f16\u8bd1\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86PPA\u6307\u6807\uff0c\u4e3a\u5b9a\u5236AI\u52a0\u901f\u5668\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00032", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00032", "abs": "https://arxiv.org/abs/2512.00032", "authors": ["Giuseppe M. Sarda", "Nimish Shah", "Abubakr Nada", "Debjyoti Bhattacharjee", "Marian Verhelst"], "title": "Decoupled Control Flow and Data Access in RISC-V GPGPUs", "comment": null, "summary": "Vortex, a newly proposed open-source GPGPU platform based on the RISC-V ISA, offers a valid alternative for GPGPU research over the broadly-used modeling platforms based on commercial GPUs. Similarly to the push originating from the RISC-V movement for CPUs, Vortex can enable a myriad of fresh research directions for GPUs. However, as a young hardware platform, it currently lacks the performance competitiveness of commercial GPUs, which is crucial for widespread adoption. State-of-the-art GPUs, in fact, rely on complex architectural features, still unavailable in Vortex, to hide the micro-code overheads linked to control flow (CF) management and memory orchestration for data access. In particular, these components account for the majority of the dynamic instruction count in regular, memory-intensive kernels, such as linear algebra routines, which form the basis of many applications, including Machine Learning. To address these challenges with simple yet powerful micro-architecture modifications, this paper introduces decoupled CF and data access through 1.) a hardware CF manager to accelerate branching and predication in regular loop execution and 2.) decoupled memory streaming lanes to further hide memory latency with useful computation. The evaluation results for different kernels show 8$\\times$ faster execution, 10$\\times$ reduction in dynamic instruction count, and overall performance improvement from 0.35 to 1.63 $\\mathrm{GFLOP/s/mm^2}$. Thanks to these enhancements, Vortex can become an ideal playground to enable GPGPU research for the next generation of Machine Learning.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5f00\u6e90GPGPU\u5e73\u53f0Vortex\u7684\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u51fa\u901a\u8fc7\u89e3\u8026\u63a7\u5236\u6d41\u548c\u6570\u636e\u8bbf\u95ee\u7684\u5fae\u67b6\u6784\u6539\u8fdb\uff0c\u5305\u62ec\u786c\u4ef6\u63a7\u5236\u6d41\u7ba1\u7406\u5668\u548c\u89e3\u8026\u5185\u5b58\u6d41\u901a\u9053\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "Vortex\u4f5c\u4e3a\u57fa\u4e8eRISC-V\u7684\u5f00\u6e90GPGPU\u5e73\u53f0\uff0c\u4e3aGPGPU\u7814\u7a76\u63d0\u4f9b\u4e86\u5546\u4e1aGPU\u4e4b\u5916\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u76ee\u524d\u6027\u80fd\u4e0d\u53ca\u5546\u4e1aGPU\uff0c\u7279\u522b\u662f\u5728\u63a7\u5236\u6d41\u7ba1\u7406\u548c\u5185\u5b58\u8bbf\u95ee\u65b9\u9762\u5b58\u5728\u74f6\u9888\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u5b66\u4e60\u7b49\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5fae\u67b6\u6784\u6539\u8fdb\uff1a1\uff09\u786c\u4ef6\u63a7\u5236\u6d41\u7ba1\u7406\u5668\uff0c\u7528\u4e8e\u52a0\u901f\u5e38\u89c4\u5faa\u73af\u6267\u884c\u4e2d\u7684\u5206\u652f\u548c\u8c13\u8bcd\u64cd\u4f5c\uff1b2\uff09\u89e3\u8026\u5185\u5b58\u6d41\u901a\u9053\uff0c\u901a\u8fc7\u6709\u7528\u8ba1\u7b97\u8fdb\u4e00\u6b65\u9690\u85cf\u5185\u5b58\u5ef6\u8fdf\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff1a\u6267\u884c\u901f\u5ea6\u63d0\u53478\u500d\uff0c\u52a8\u6001\u6307\u4ee4\u6570\u51cf\u5c1110\u500d\uff0c\u6574\u4f53\u6027\u80fd\u4ece0.35\u63d0\u5347\u52301.63 GFLOP/s/mm\u00b2\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u7684\u5fae\u67b6\u6784\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86Vortex\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u6210\u4e3a\u4e0b\u4e00\u4ee3\u673a\u5668\u5b66\u4e60GPGPU\u7814\u7a76\u7684\u7406\u60f3\u5e73\u53f0\u3002"}}
{"id": "2512.00035", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2512.00035", "abs": "https://arxiv.org/abs/2512.00035", "authors": ["Mislav Has", "Tao Xiong", "Fehmi Ben Abdesslem", "Mario Ku\u0161ek"], "title": "WebAssembly on Resource-Constrained IoT Devices: Performance, Efficiency, and Portability", "comment": null, "summary": "The increasing heterogeneity of hardware and software in the Internet of Things (IoT) poses a major challenge for the portability, maintainability and deployment of software on devices with limited resources. WebAssembly (WASM), originally designed for the web, is increasingly recognized as a portable, secure and efficient runtime environment that can overcome these challenges. This paper explores the feasibility of using WASM in embedded IoT systems by evaluating its performance, memory footprint and energy consumption on three representative microcontrollers: the Raspberry Pi Pico, the ESP32 C6 and the nRF5340. Two lightweight WASM runtimes, WAMR and wasm3, are compared with the native C execution. The results show that while the native execution remains superior in terms of speed and energy efficiency, WASM offers acceptable trade-offs in return for cross-platform compatibility and sandbox execution. The results highlight that WASM is a viable option for embedded IoT applications when portability and security outweigh strict performance constraints, and that further runtime optimization could extend its practicality in this area.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86WebAssembly\u5728\u5d4c\u5165\u5f0f\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u7684\u6027\u80fd\u3001\u5185\u5b58\u5360\u7528\u548c\u80fd\u8017\uff0c\u53d1\u73b0\u867d\u7136\u539f\u751fC\u4ee3\u7801\u6027\u80fd\u66f4\u4f18\uff0c\u4f46WASM\u5728\u8de8\u5e73\u53f0\u517c\u5bb9\u6027\u548c\u5b89\u5168\u6c99\u7bb1\u6267\u884c\u65b9\u9762\u63d0\u4f9b\u4e86\u53ef\u63a5\u53d7\u7684\u6743\u8861\uff0c\u662f\u5d4c\u5165\u5f0f\u7269\u8054\u7f51\u5e94\u7528\u7684\u53ef\u884c\u9009\u62e9\u3002", "motivation": "\u7269\u8054\u7f51\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u5f02\u6784\u6027\u7ed9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u8f6f\u4ef6\u53ef\u79fb\u690d\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u90e8\u7f72\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002WebAssembly\u4f5c\u4e3a\u6700\u521d\u4e3aWeb\u8bbe\u8ba1\u7684\u53ef\u79fb\u690d\u3001\u5b89\u5168\u3001\u9ad8\u6548\u7684\u8fd0\u884c\u65f6\u73af\u5883\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u5fae\u63a7\u5236\u5668\uff08Raspberry Pi Pico\u3001ESP32 C6\u3001nRF5340\uff09\u4e0a\u8bc4\u4f30WASM\u7684\u6027\u80fd\u3001\u5185\u5b58\u5360\u7528\u548c\u80fd\u8017\u3002\u6bd4\u8f83\u4e24\u79cd\u8f7b\u91cf\u7ea7WASM\u8fd0\u884c\u65f6\uff08WAMR\u548cwasm3\uff09\u4e0e\u539f\u751fC\u6267\u884c\u7684\u5dee\u5f02\u3002", "result": "\u539f\u751f\u6267\u884c\u5728\u901f\u5ea6\u548c\u80fd\u6548\u65b9\u9762\u4ecd\u7136\u66f4\u4f18\uff0c\u4f46WASM\u5728\u8de8\u5e73\u53f0\u517c\u5bb9\u6027\u548c\u6c99\u7bb1\u6267\u884c\u65b9\u9762\u63d0\u4f9b\u4e86\u53ef\u63a5\u53d7\u7684\u6743\u8861\u3002WASM\u662f\u5d4c\u5165\u5f0f\u7269\u8054\u7f51\u5e94\u7528\u7684\u53ef\u884c\u9009\u62e9\uff0c\u7279\u522b\u662f\u5728\u53ef\u79fb\u690d\u6027\u548c\u5b89\u5168\u6027\u4f18\u5148\u4e8e\u4e25\u683c\u6027\u80fd\u7ea6\u675f\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "WebAssembly\u662f\u5d4c\u5165\u5f0f\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u53ef\u884c\u9009\u9879\uff0c\u5f53\u53ef\u79fb\u690d\u6027\u548c\u5b89\u5168\u6027\u6bd4\u4e25\u683c\u6027\u80fd\u7ea6\u675f\u66f4\u91cd\u8981\u65f6\u3002\u8fdb\u4e00\u6b65\u7684\u8fd0\u884c\u65f6\u4f18\u5316\u53ef\u4ee5\u6269\u5c55\u5176\u5728\u8be5\u9886\u57df\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.00038", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00038", "abs": "https://arxiv.org/abs/2512.00038", "authors": ["He Jiang", "Yi Guo", "Shikai Guo", "Huijiang Liu", "Xiaochen Li", "Ning Wang", "Zhixiong Di"], "title": "Critical Path Aware Timing-Driven Global Placement for Large-Scale Heterogeneous FPGAs", "comment": null, "summary": "Timing optimization during global placement is critical for achieving optimal circuit performance and remains a key challenge in modern Field Programmable Gate Array (FPGA) design. As FPGA designs scale and heterogeneous resources increase, dense interconnects introduce significant resistive and capacitive effects, making timing closure increasingly difficult. Existing methods face challenges in constructing accurate timing models due to multi-factor nonlinear constraints as well as load and crosstalk coupling effects arising in multi-pin driving scenarios. To address these challenges, we propose TD-Placer, a critical path aware, timing-driven global placement framework. It leverages graph-based representations to capture global net interactions and employs a nonlinear model to integrate diverse timing-related features for precise delay prediction, thereby improving the overall placement quality for FPGAs. TD-Placer adopts a quadratic placement objective that minimizes wirelength while incorporating a timing term constructed by a lightweight algorithm, enabling efficient and high-quality timing optimization. Regarding net-level timing contention, it also employs a finer-grained weighting scheme to facilitate smooth reduction of the Critical Path Delay (CPD). Extensive experiments were carried out on seven real-world open-source FPGA projects with LUT counts ranging from 60K to 400K. The results demonstrate that TD-Placer achieves an average 10% improvement in Worst Negative Slack (WNS) and a 5% reduction in CPD compared to the state-of-the-art method, with an average CPD comparable (*1.01) to the commercial AMD Vivado across five versions (2020.2-2024.2). Its code and dataset are publicly available.", "AI": {"tldr": "TD-Placer\u662f\u4e00\u4e2a\u9762\u5411FPGA\u7684\u65f6\u5e8f\u9a71\u52a8\u5168\u5c40\u5e03\u5c40\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u548c\u8f7b\u91cf\u7ea7\u7b97\u6cd5\u4f18\u5316\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\uff0c\u5728WNS\u548cCPD\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740FPGA\u8bbe\u8ba1\u89c4\u6a21\u6269\u5927\u548c\u5f02\u6784\u8d44\u6e90\u589e\u52a0\uff0c\u5bc6\u96c6\u4e92\u8fde\u5e26\u6765\u7684\u7535\u963b\u7535\u5bb9\u6548\u5e94\u4f7f\u65f6\u5e8f\u6536\u655b\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u56e0\u7d20\u975e\u7ebf\u6027\u7ea6\u675f\u3001\u8d1f\u8f7d\u548c\u4e32\u6270\u8026\u5408\u6548\u5e94\u4e0b\u96be\u4ee5\u6784\u5efa\u51c6\u786e\u7684\u65f6\u5e8f\u6a21\u578b\u3002", "method": "\u63d0\u51faTD-Placer\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u6355\u6349\u5168\u5c40\u7f51\u7edc\u4ea4\u4e92\uff1b2\uff09\u91c7\u7528\u975e\u7ebf\u6027\u6a21\u578b\u6574\u5408\u591a\u79cd\u65f6\u5e8f\u76f8\u5173\u7279\u5f81\u8fdb\u884c\u7cbe\u786e\u5ef6\u8fdf\u9884\u6d4b\uff1b3\uff09\u91c7\u7528\u4e8c\u6b21\u5e03\u5c40\u76ee\u6807\u6700\u5c0f\u5316\u7ebf\u957f\uff0c\u540c\u65f6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b97\u6cd5\u6784\u5efa\u65f6\u5e8f\u9879\uff1b4\uff09\u91c7\u7528\u7ec6\u7c92\u5ea6\u52a0\u6743\u65b9\u6848\u5904\u7406\u7f51\u7edc\u7ea7\u65f6\u5e8f\u7ade\u4e89\u3002", "result": "\u57287\u4e2a\u771f\u5b9eFPGA\u9879\u76ee\uff08LUT\u6570\u91cf60K-400K\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u76f8\u6bd4SOTA\u65b9\u6cd5\uff0c\u5e73\u5747WNS\u6539\u558410%\uff0cCPD\u51cf\u5c115%\uff1b\u5728\u4e94\u4e2aVivado\u7248\u672c\uff082020.2-2024.2\uff09\u4e0a\uff0c\u5e73\u5747CPD\u4e0e\u5546\u4e1a\u5de5\u5177\u76f8\u5f53\uff08*1.01\uff09\u3002", "conclusion": "TD-Placer\u901a\u8fc7\u6709\u6548\u7684\u65f6\u5e8f\u9a71\u52a8\u5168\u5c40\u5e03\u5c40\u6846\u67b6\uff0c\u663e\u8457\u6539\u5584\u4e86FPGA\u8bbe\u8ba1\u7684\u65f6\u5e8f\u6027\u80fd\uff0c\u5728\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u4f18\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2512.00044", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00044", "abs": "https://arxiv.org/abs/2512.00044", "authors": ["Junzhuo Zhou", "Ziwen Wang", "Haoxuan Xia", "Yuxin Yan", "Chengyu Zhu", "Ting-Jung Lin", "Wei Xing", "Lei He"], "title": "SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning", "comment": null, "summary": "Accurate setup/hold time characterization is crucial for modern chip timing closure, but its reliance on potentially millions of SPICE simulations across diverse process-voltagetemperature (PVT) corners creates a major bottleneck, often lasting weeks or months. Existing methods suffer from slow search convergence and inefficient exploration, especially in the multi-corner setting. We introduce SetupKit, a novel framework designed to break this bottleneck using statistical intelligence, circuit analysis and active learning (AL). SetupKit integrates three key innovations: BEIRA, a bias-enhanced interpolation search derived from statistical error modeling to accelerate convergence by overcoming stagnation issues, initial search interval estimation by circuit analysis and AL strategy using Gaussian Process. This AL component intelligently learns PVT-timing correlations, actively guiding the expensive simulations to the most informative corners, thus minimizing redundancy in multicorner characterization. Evaluated on industrial 22nm standard cells across 16 PVT corners, SetupKit demonstrates a significant 2.4x overall CPU time reduction (from 720 to 290 days on a single core) compared to standard practices, drastically cutting characterization time. SetupKit offers a principled, learningbased approach to library characterization, addressing a critical EDA challenge and paving the way for more intelligent simulation management.", "AI": {"tldr": "SetupKit\u662f\u4e00\u4e2a\u7528\u4e8e\u52a0\u901f\u82af\u7247\u65f6\u5e8f\u5e93\u8868\u5f81\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u667a\u80fd\u3001\u7535\u8def\u5206\u6790\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u5c06CPU\u65f6\u95f4\u4ece720\u5929\u51cf\u5c11\u5230290\u5929\uff082.4\u500d\u52a0\u901f\uff09\u3002", "motivation": "\u73b0\u4ee3\u82af\u7247\u65f6\u5e8f\u95ed\u5408\u9700\u8981\u51c6\u786e\u7684\u5efa\u7acb/\u4fdd\u6301\u65f6\u95f4\u8868\u5f81\uff0c\u4f46\u4f9d\u8d56\u6570\u767e\u4e07\u6b21SPICE\u4eff\u771f\u548c\u591a\u4e2aPVT\u89d2\u70b9\uff0c\u5bfc\u81f4\u6570\u5468\u751a\u81f3\u6570\u6708\u7684\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u6536\u655b\u6162\u4e14\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u7279\u522b\u662f\u5728\u591a\u89d2\u70b9\u8bbe\u7f6e\u4e0b\u3002", "method": "SetupKit\u6574\u5408\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) BEIRA - \u57fa\u4e8e\u7edf\u8ba1\u8bef\u5dee\u5efa\u6a21\u7684\u504f\u7f6e\u589e\u5f3a\u63d2\u503c\u641c\u7d22\uff0c\u52a0\u901f\u6536\u655b\u5e76\u514b\u670d\u505c\u6ede\u95ee\u9898\uff1b2) \u901a\u8fc7\u7535\u8def\u5206\u6790\u8fdb\u884c\u521d\u59cb\u641c\u7d22\u533a\u95f4\u4f30\u8ba1\uff1b3) \u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u667a\u80fd\u5b66\u4e60PVT-\u65f6\u5e8f\u76f8\u5173\u6027\uff0c\u5f15\u5bfc\u6602\u8d35\u4eff\u771f\u5230\u6700\u6709\u4fe1\u606f\u91cf\u7684\u89d2\u70b9\u3002", "result": "\u5728\u5de5\u4e1a22nm\u6807\u51c6\u5355\u5143\u5e93\u768416\u4e2aPVT\u89d2\u70b9\u4e0a\u8bc4\u4f30\uff0cSetupKit\u5b9e\u73b0\u4e862.4\u500d\u7684\u6574\u4f53CPU\u65f6\u95f4\u51cf\u5c11\uff08\u4ece\u5355\u6838720\u5929\u51cf\u5c11\u5230290\u5929\uff09\uff0c\u663e\u8457\u7f29\u77ed\u4e86\u8868\u5f81\u65f6\u95f4\u3002", "conclusion": "SetupKit\u4e3a\u5e93\u8868\u5f81\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u539f\u7406\u6027\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5173\u952e\u7684EDA\u6311\u6218\uff0c\u5e76\u4e3a\u66f4\u667a\u80fd\u7684\u4eff\u771f\u7ba1\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.00045", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00045", "abs": "https://arxiv.org/abs/2512.00045", "authors": ["Hung-Ming Huang", "Yu-Hsin Yang", "Fu-Chieh Chang", "Yun-Chia Hsu", "Yin-Yu Lin", "Ming-Fang Tsai", "Chun-Chih Yang", "Pei-Yuan Wu"], "title": "Assessing Large Language Models in Generating RTL Design Specifications", "comment": null, "summary": "As IC design grows more complex, automating comprehension and documentation of RTL code has become increasingly important. Engineers currently should manually interpret existing RTL code and write specifications, a slow and error-prone process. Although LLMs have been studied for generating RTL from specifications, automated specification generation remains underexplored, largely due to the lack of reliable evaluation methods. To address this gap, we investigate how prompting strategies affect RTL-to-specification quality and introduce metrics for faithfully evaluating generated specs. We also benchmark open-source and commercial LLMs, providing a foundation for more automated and efficient specification workflows in IC design.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u5229\u7528LLM\u4eceRTL\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u89c4\u683c\u8bf4\u660e\uff0c\u63d0\u51fa\u8bc4\u4f30\u6307\u6807\u5e76\u6bd4\u8f83\u4e0d\u540cLLM\u7684\u6027\u80fd", "motivation": "\u968f\u7740IC\u8bbe\u8ba1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u624b\u52a8\u89e3\u8bfbRTL\u4ee3\u7801\u5e76\u7f16\u5199\u89c4\u683c\u8bf4\u660e\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u7f13\u6162\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u4f7f\u7528LLM\u4ece\u89c4\u683c\u751f\u6210RTL\uff0c\u4f46\u4eceRTL\u81ea\u52a8\u751f\u6210\u89c4\u683c\u8bf4\u660e\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u4e3b\u8981\u7f3a\u4e4f\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u793a\u7b56\u7565\u5bf9RTL\u5230\u89c4\u683c\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5f15\u5165\u7528\u4e8e\u5fe0\u5b9e\u8bc4\u4f30\u751f\u6210\u89c4\u683c\u7684\u6307\u6807\uff0c\u5e76\u5bf9\u5f00\u6e90\u548c\u5546\u4e1aLLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u63d0\u4f9b\u4e86\u8bc4\u4f30\u751f\u6210\u89c4\u683c\u7684\u6307\u6807\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540cLLM\u5728RTL\u5230\u89c4\u683c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e3aIC\u8bbe\u8ba1\u4e2d\u66f4\u81ea\u52a8\u5316\u548c\u9ad8\u6548\u7684\u89c4\u683c\u5de5\u4f5c\u6d41\u7a0b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\u63d0\u793a\u7b56\u7565\u548c\u5f15\u5165\u8bc4\u4f30\u6307\u6807\uff0c\u4e3aRTL\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u89c4\u683c\u8bf4\u660e\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u63a8\u52a8\u4e86IC\u8bbe\u8ba1\u4e2d\u89c4\u683c\u5de5\u4f5c\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u548c\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2512.00053", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00053", "abs": "https://arxiv.org/abs/2512.00053", "authors": ["Nikhil Rout", "Blaise Tine"], "title": "A Configurable Mixed-Precision Fused Dot Product Unit for GPGPU Tensor Computation", "comment": "3 pages, 2 figures", "summary": "Efficient mixed-precision MMA operations are critical for accelerating Deep Learning workloads on GPGPUs. However, existing open-source RTL implementations of inner dot products rely on discrete arithmetic units, leading to suboptimal throughput and poor resource utilization. To address these challenges, we propose a scalable mixed-precision dot product unit that integrates floating-point and integer arithmetic pipelines within a singular fused architecture, implemented as part of the open-source RISC-V based Vortex GPGPU's Tensor Core Unit extension. Our design supports low-precision multiplication in (FP16/BF16/FP8/BF8/INT8/UINT4) formats and higher-precision accumulation in (FP32/INT32), with an extensible framework for adding and evaluating other custom representations in the future. Experimental results demonstrate 4-cycle operation latency at 306.6 MHz clock frequency on the AMD Xilinx Alveo U55C FPGA, delivering an ideal filled pipeline throughput of 9.812 GFLOPS in a 4-thread per warp configuration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6df7\u5408\u7cbe\u5ea6\u70b9\u79ef\u5355\u5143\uff0c\u5c06\u6d6e\u70b9\u548c\u6574\u6570\u7b97\u672f\u6d41\u6c34\u7ebf\u96c6\u6210\u5728\u5355\u4e00\u878d\u5408\u67b6\u6784\u4e2d\uff0c\u4f5c\u4e3a\u5f00\u6e90RISC-V Vortex GPGPU\u5f20\u91cf\u6838\u5fc3\u5355\u5143\u7684\u6269\u5c55\uff0c\u652f\u6301\u591a\u79cd\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u548c\u9ad8\u7cbe\u5ea6\u7d2f\u52a0\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90RTL\u5b9e\u73b0\u7684\u5185\u79ef\u64cd\u4f5c\u4f9d\u8d56\u79bb\u6563\u7b97\u672f\u5355\u5143\uff0c\u5bfc\u81f4\u541e\u5410\u91cf\u4e0d\u7406\u60f3\u548c\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6df7\u5408\u7cbe\u5ea6MMA\u64cd\u4f5c\u6765\u52a0\u901f\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6df7\u5408\u7cbe\u5ea6\u70b9\u79ef\u5355\u5143\uff0c\u5c06\u6d6e\u70b9\u548c\u6574\u6570\u7b97\u672f\u6d41\u6c34\u7ebf\u96c6\u6210\u5728\u5355\u4e00\u878d\u5408\u67b6\u6784\u4e2d\uff0c\u652f\u6301FP16/BF16/FP8/BF8/INT8/UINT4\u7b49\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u7684\u4e58\u6cd5\uff0c\u4ee5\u53caFP32/INT32\u7684\u9ad8\u7cbe\u5ea6\u7d2f\u52a0\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u4ee5\u652f\u6301\u672a\u6765\u81ea\u5b9a\u4e49\u8868\u793a\u3002", "result": "\u5728AMD Xilinx Alveo U55C FPGA\u4e0a\u5b9e\u73b0\u4e864\u5468\u671f\u64cd\u4f5c\u5ef6\u8fdf\u548c306.6 MHz\u65f6\u949f\u9891\u7387\uff0c\u5728\u6bcfwarp 4\u7ebf\u7a0b\u914d\u7f6e\u4e0b\u5b9e\u73b0\u4e869.812 GFLOPS\u7684\u7406\u60f3\u6d41\u6c34\u7ebf\u541e\u5410\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u878d\u5408\u67b6\u6784\u89e3\u51b3\u4e86\u73b0\u6709\u79bb\u6563\u7b97\u672f\u5355\u5143\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6df7\u5408\u7cbe\u5ea6\u70b9\u79ef\u5355\u5143\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u52a0\u901f\u6027\u80fd\u3002"}}
{"id": "2512.00055", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00055", "abs": "https://arxiv.org/abs/2512.00055", "authors": ["Sohaib Errabii", "Olivier Sentieys", "Marcello Traiola"], "title": "KAN-SAs: Efficient Acceleration of Kolmogorov-Arnold Networks on Systolic Arrays", "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have garnered significant attention for their promise of improved parameter efficiency and explainability compared to traditional Deep Neural Networks (DNNs). KANs' key innovation lies in the use of learnable non-linear activation functions, which are parametrized as splines. Splines are expressed as a linear combination of basis functions (B-splines). B-splines prove particularly challenging to accelerate due to their recursive definition. Systolic Array (SA)based architectures have shown great promise as DNN accelerators thanks to their energy efficiency and low latency. However, their suitability and efficiency in accelerating KANs have never been assessed. Thus, in this work, we explore the use of SA architecture to accelerate the KAN inference. We show that, while SAs can be used to accelerate part of the KAN inference, their utilization can be reduced to 30%. Hence, we propose KAN-SAs, a novel SA-based accelerator that leverages intrinsic properties of B-splines to enable efficient KAN inference. By including a nonrecursive B-spline implementation and leveraging the intrinsic KAN sparsity, KAN-SAs enhances conventional SAs, enabling efficient KAN inference, in addition to conventional DNNs. KAN-SAs achieves up to 100% SA utilization and up to 50% clock cycles reduction compared to conventional SAs of equivalent area, as shown by hardware synthesis results on a 28nm FD-SOI technology. We also evaluate different configurations of the accelerator on various KAN applications, confirming the improved efficiency of KAN inference provided by KAN-SAs.", "AI": {"tldr": "KAN-SAs\uff1a\u4e00\u79cd\u57fa\u4e8e\u8109\u52a8\u9635\u5217\u7684\u65b0\u578bKAN\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u975e\u9012\u5f52B\u6837\u6761\u5b9e\u73b0\u548c\u5229\u7528KAN\u7a00\u758f\u6027\uff0c\u76f8\u6bd4\u4f20\u7edfSA\u63d0\u5347100%\u5229\u7528\u7387\u548c\u51cf\u5c1150%\u65f6\u949f\u5468\u671f", "motivation": "KANs\u56e0\u5176\u53c2\u6570\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u6838\u5fc3\u7684B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u96be\u4ee5\u52a0\u901f\u3002\u8109\u52a8\u9635\u5217(SA)\u4f5c\u4e3a\u9ad8\u6548\u7684DNN\u52a0\u901f\u5668\uff0c\u5728KAN\u52a0\u901f\u65b9\u9762\u7684\u9002\u7528\u6027\u548c\u6548\u7387\u4ece\u672a\u88ab\u8bc4\u4f30\u3002", "method": "\u63d0\u51faKAN-SAs\u52a0\u901f\u5668\uff1a1) \u91c7\u7528\u975e\u9012\u5f52B\u6837\u6761\u5b9e\u73b0\uff1b2) \u5229\u7528KAN\u56fa\u6709\u7684\u7a00\u758f\u6027\uff1b3) \u5728\u4f20\u7edfSA\u57fa\u7840\u4e0a\u589e\u5f3a\u4ee5\u652f\u6301\u9ad8\u6548KAN\u63a8\u7406\u3002", "result": "\u572828nm FD-SOI\u5de5\u827a\u4e0a\u7684\u786c\u4ef6\u7efc\u5408\u7ed3\u679c\u663e\u793a\uff1a\u76f8\u6bd4\u540c\u7b49\u9762\u79ef\u7684\u4f20\u7edfSA\uff0cKAN-SAs\u5b9e\u73b0100% SA\u5229\u7528\u7387\uff0c\u65f6\u949f\u5468\u671f\u51cf\u5c1150%\u3002\u5728\u4e0d\u540cKAN\u5e94\u7528\u4e0a\u7684\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5176\u6548\u7387\u63d0\u5347\u3002", "conclusion": "KAN-SAs\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9B\u6837\u6761\u7279\u6027\u548cKAN\u7a00\u758f\u6027\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86KAN\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u4f20\u7edfDNN\u7684\u52a0\u901f\u80fd\u529b\uff0c\u4e3aKAN\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00059", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00059", "abs": "https://arxiv.org/abs/2512.00059", "authors": ["Swastik Bhattacharya", "Sanjay Das", "Anand Menon", "Shamik Kundu", "Arnab Raha", "Kanad Basu"], "title": "SafeCiM: Investigating Resilience of Hybrid Floating-Point Compute-in-Memory Deep Learning Accelerators", "comment": null, "summary": "Deep Neural Networks (DNNs) continue to grow in complexity with Large Language Models (LLMs) incorporating vast numbers of parameters. Handling these parameters efficiently in traditional accelerators is limited by data-transmission bottlenecks, motivating Compute-in-Memory (CiM) architectures that integrate computation within or near memory to reduce data movement. Recent work has explored CiM designs using Floating-Point (FP) and Integer (INT) operations. FP computations typically deliver higher output quality due to their wider dynamic range and precision, benefiting precision-sensitive Generative AI applications. These include models such as LLMs, thus driving advancements in FP-CiM accelerators. However, the vulnerability of FP-CiM to hardware faults remains underexplored, posing a major reliability concern in mission-critical settings. To address this gap, we systematically analyze hardware fault effects in FP-CiM by introducing bit-flip faults at key computational stages, including digital multipliers, CiM memory cells, and digital adder trees. Experiments with Convolutional Neural Networks (CNNs) such as AlexNet and state-of-the-art LLMs including LLaMA-3.2-1B and Qwen-0.3B-Base reveal how faults at each stage affect inference accuracy. Notably, a single adder fault can reduce LLM accuracy to 0%. Based on these insights, we propose a fault-resilient design, SafeCiM, that mitigates fault impact far better than a naive FP-CiM with a pre-alignment stage. For example, with 4096 MAC units, SafeCiM reduces accuracy degradation by up to 49x for a single adder fault compared to the baseline FP-CiM architecture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6d6e\u70b9\u8ba1\u7b97\u5185\u5b58\uff08FP-CiM\uff09\u67b6\u6784\u4e2d\u7684\u786c\u4ef6\u6545\u969c\u5f71\u54cd\uff0c\u9488\u5bf9LLM\u7b49\u751f\u6210\u5f0fAI\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSafeCiM\u7684\u5bb9\u9519\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6545\u969c\u5bf9\u63a8\u7406\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740LLM\u7b49\u590d\u6742DNN\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u4e0d\u65ad\u589e\u957f\uff0c\u4f20\u7edf\u52a0\u901f\u5668\u9762\u4e34\u6570\u636e\u4f20\u8f93\u74f6\u9888\uff0c\u8ba1\u7b97\u5185\u5b58\uff08CiM\uff09\u67b6\u6784\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\u3002\u6d6e\u70b9\u8fd0\u7b97\uff08FP\uff09\u56e0\u52a8\u6001\u8303\u56f4\u548c\u7cbe\u5ea6\u4f18\u52bf\u66f4\u9002\u5408\u751f\u6210\u5f0fAI\u5e94\u7528\uff0c\u4f46FP-CiM\u5bf9\u786c\u4ef6\u6545\u969c\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u8fd9\u5728\u5173\u952e\u4efb\u52a1\u73af\u5883\u4e2d\u6784\u6210\u91cd\u5927\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790FP-CiM\u4e2d\u7684\u786c\u4ef6\u6545\u969c\u5f71\u54cd\uff0c\u5728\u5173\u952e\u8ba1\u7b97\u9636\u6bb5\uff08\u6570\u5b57\u4e58\u6cd5\u5668\u3001CiM\u5b58\u50a8\u5355\u5143\u3001\u6570\u5b57\u52a0\u6cd5\u5668\u6811\uff09\u5f15\u5165\u6bd4\u7279\u7ffb\u8f6c\u6545\u969c\u3002\u4f7f\u7528CNN\uff08\u5982AlexNet\uff09\u548c\u5148\u8fdbLLM\uff08\u5982LLaMA-3.2-1B\u548cQwen-0.3B-Base\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5404\u9636\u6bb5\u6545\u969c\u5bf9\u63a8\u7406\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u540d\u4e3aSafeCiM\u7684\u6545\u969c\u5f39\u6027\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5355\u4e2a\u52a0\u6cd5\u5668\u6545\u969c\u53ef\u4f7fLLM\u7cbe\u5ea6\u964d\u81f30%\u3002\u63d0\u51fa\u7684SafeCiM\u8bbe\u8ba1\u76f8\u6bd4\u57fa\u7840FP-CiM\u67b6\u6784\u663e\u8457\u964d\u4f4e\u4e86\u6545\u969c\u5f71\u54cd\uff0c\u4f8b\u5982\u57284096\u4e2aMAC\u5355\u5143\u914d\u7f6e\u4e0b\uff0c\u5bf9\u5355\u4e2a\u52a0\u6cd5\u5668\u6545\u969c\u7684\u7cbe\u5ea6\u4e0b\u964d\u51cf\u5c11\u4e8649\u500d\u3002", "conclusion": "FP-CiM\u67b6\u6784\u5bf9\u786c\u4ef6\u6545\u969c\u9ad8\u5ea6\u654f\u611f\uff0c\u7279\u522b\u662f\u5728\u52a0\u6cd5\u5668\u9636\u6bb5\u3002SafeCiM\u8bbe\u8ba1\u901a\u8fc7\u6709\u6548\u7684\u6545\u969c\u7f13\u89e3\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86FP-CiM\u5728\u5173\u952e\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u5e94\u7528\u7684\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00070", "categories": ["cs.AR", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00070", "abs": "https://arxiv.org/abs/2512.00070", "authors": ["Sungyu Jeong", "Minsu Kim", "Byungsub Kim"], "title": "A CNN-Based Technique to Assist Layout-to-Generator Conversion for Analog Circuits", "comment": null, "summary": "We propose a technique to assist in converting a reference layout of an analog circuit into the procedural layout generator by efficiently reusing available generators for sub-cell creation. The proposed convolutional neural network (CNN) model automatically detects sub-cells that can be generated by available generator scripts in the library, and suggests using them in the hierarchically correct places of the generator software. In experiments, the CNN model examined sub-cells of a high-speed wireline receiver that has a total of 4,885 sub-cell instances including different 145 sub-cell designs. The CNN model classified the sub-cell instances into 51 generatable and one not-generatable classes. One not-generatable class indicates that no available generator can generate the classified sub-cell. The CNN model achieved 99.3% precision in examining the 145 different sub-cell designs. The CNN model greatly reduced the examination time to 18 seconds from 88 minutes required in manual examination. Also, the proposed CNN model could correctly classify unfamiliar sub-cells that are very different from the training dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6a21\u62df\u7535\u8def\u53c2\u8003\u5e03\u5c40\u4e2d\u53ef\u7531\u73b0\u6709\u751f\u6210\u5668\u811a\u672c\u751f\u6210\u7684\u5b50\u5355\u5143\uff0c\u663e\u8457\u63d0\u9ad8\u5e03\u5c40\u751f\u6210\u6548\u7387", "motivation": "\u4f20\u7edf\u624b\u52a8\u68c0\u67e5\u6a21\u62df\u7535\u8def\u5e03\u5c40\u4e2d\u54ea\u4e9b\u5b50\u5355\u5143\u53ef\u7531\u73b0\u6709\u751f\u6210\u5668\u751f\u6210\u975e\u5e38\u8017\u65f6\uff0888\u5206\u949f\uff09\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u53c2\u8003\u5e03\u5c40\u4e2d\u7684\u5b50\u5355\u5143\uff0c\u8bc6\u522b\u54ea\u4e9b\u53ef\u7531\u5e93\u4e2d\u73b0\u6709\u751f\u6210\u5668\u811a\u672c\u751f\u6210\uff0c\u5e76\u5728\u751f\u6210\u5668\u8f6f\u4ef6\u4e2d\u5206\u5c42\u6b63\u786e\u4f4d\u7f6e\u5efa\u8bae\u4f7f\u7528", "result": "CNN\u6a21\u578b\u5728\u9ad8\u901f\u6709\u7ebf\u63a5\u6536\u5668\u7535\u8def\uff08\u5305\u542b4,885\u4e2a\u5b50\u5355\u5143\u5b9e\u4f8b\uff0c145\u79cd\u4e0d\u540c\u8bbe\u8ba1\uff09\u4e0a\u5b9e\u73b099.3%\u7684\u7cbe\u786e\u5ea6\uff0c\u5c06\u68c0\u67e5\u65f6\u95f4\u4ece88\u5206\u949f\u51cf\u5c11\u523018\u79d2\uff0c\u5e76\u80fd\u6b63\u786e\u5206\u7c7b\u4e0e\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u5f88\u5927\u7684\u964c\u751f\u5b50\u5355\u5143", "conclusion": "\u63d0\u51fa\u7684CNN\u65b9\u6cd5\u80fd\u6709\u6548\u81ea\u52a8\u5316\u5b50\u5355\u5143\u751f\u6210\u5668\u8bc6\u522b\u8fc7\u7a0b\uff0c\u5927\u5e45\u63d0\u9ad8\u6a21\u62df\u7535\u8def\u5e03\u5c40\u751f\u6210\u6548\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2512.00079", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00079", "abs": "https://arxiv.org/abs/2512.00079", "authors": ["Bin Sun", "Rengang Zhang", "Zhiteng Chao", "Zizhen Liu", "Jianan Mu", "Jing Ye", "Huawei Li"], "title": "InF-ATPG: Intelligent FFR-Driven ATPG with Advanced Circuit Representation Guided Reinforcement Learning", "comment": "9 pages,6 figures", "summary": "Automatic test pattern generation (ATPG) is a crucial process in integrated circuit (IC) design and testing, responsible for efficiently generating test patterns. As semiconductor technology progresses, traditional ATPG struggles with long execution times to achieve the expected fault coverage, which impacts the time-to-market of chips. Recent machine learning techniques, like reinforcement learning (RL) and graph neural networks (GNNs), show promise but face issues such as reward delay in RL models and inadequate circuit representation in GNN-based methods. In this paper, we propose InF-ATPG, an intelligent FFR-driven ATPG framework that overcomes these challenges by using advanced circuit representation to guide RL. By partitioning circuits into fanout-free regions (FFRs) and incorporating ATPG-specific features into a novel QGNN architecture, InF-ATPG enhances test pattern generation efficiency. Experimental results show InF-ATPG reduces backtracks by 55.06\\% on average compared to traditional methods and 38.31\\% compared to the machine learning approach, while also improving fault coverage.", "AI": {"tldr": "InF-ATPG\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u667a\u80fd\u6d4b\u8bd5\u6a21\u5f0f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7FFR\u5206\u533a\u548cQGNN\u67b6\u6784\uff0c\u663e\u8457\u51cf\u5c11\u56de\u6eaf\u6b21\u6570\u5e76\u63d0\u9ad8\u6545\u969c\u8986\u76d6\u7387\u3002", "motivation": "\u968f\u7740\u534a\u5bfc\u4f53\u6280\u672f\u8fdb\u6b65\uff0c\u4f20\u7edfATPG\u65b9\u6cd5\u751f\u6210\u6d4b\u8bd5\u6a21\u5f0f\u65f6\u95f4\u957f\uff0c\u5f71\u54cd\u82af\u7247\u4e0a\u5e02\u65f6\u95f4\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5982\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u5956\u52b1\u5ef6\u8fdf\u548c\u7535\u8def\u8868\u793a\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faInF-ATPG\u6846\u67b6\uff1a1) \u5c06\u7535\u8def\u5212\u5206\u4e3a\u6247\u51fa\u81ea\u7531\u533a\u57df(FFR)\uff1b2) \u5c06ATPG\u7279\u5b9a\u7279\u5f81\u878d\u5165\u65b0\u9896\u7684QGNN\u67b6\u6784\uff1b3) \u4f7f\u7528\u6539\u8fdb\u7684\u7535\u8def\u8868\u793a\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\u5e73\u5747\u51cf\u5c1155.06%\u56de\u6eaf\u6b21\u6570\uff0c\u4e0e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\u51cf\u5c1138.31%\u56de\u6eaf\u6b21\u6570\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6545\u969c\u8986\u76d6\u7387\u3002", "conclusion": "InF-ATPG\u901a\u8fc7\u5148\u8fdb\u7684\u7535\u8def\u8868\u793a\u548cFFR\u9a71\u52a8\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfATPG\u548c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u6a21\u5f0f\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2512.00083", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00083", "abs": "https://arxiv.org/abs/2512.00083", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Wei Zhang"], "title": "LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling", "comment": "Accepted to ICPP 2025", "summary": "Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.\n  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms.", "AI": {"tldr": "LLaMCAT\uff1a\u9488\u5bf9LLM\u63a8\u7406\u7684LLC\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7MSHR\u611f\u77e5\u7684\u7f13\u5b58\u4ef2\u88c1\u548c\u7ebf\u7a0b\u8282\u6d41\u89e3\u51b3KV Cache\u8bbf\u95ee\u4e2d\u7684\u5e26\u5bbd\u74f6\u9888\u548c\u7f13\u5b58\u505c\u987f\u95ee\u9898\uff0c\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u5b9e\u73b01.26-1.58\u500d\u52a0\u901f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u65f6\u5bf9\u5185\u5b58\u7cfb\u7edf\u63d0\u51fa\u4e86\u5de8\u5927\u6311\u6218\uff0c\u7279\u522b\u662fKV Cache\u8bbf\u95ee\u5bfc\u81f4\u7684\u5e26\u5bbd\u9700\u6c42\u548c\u7f13\u5b58\u505c\u987f\u95ee\u9898\u3002\u73b0\u6709\u5de5\u4f5c\u672a\u5145\u5206\u9488\u5bf9LLM\u89e3\u7801\u7279\u6709\u7684MSHR\u7ade\u4e89\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51faLLaMCAT\u65b9\u6cd5\uff1a1\uff09\u7ed3\u5408MSHR\u611f\u77e5\u548c\u8d1f\u8f7d\u5747\u8861\u611f\u77e5\u7684\u7f13\u5b58\u4ef2\u88c1\uff1b2\uff09\u7ebf\u7a0b\u8282\u6d41\u6280\u672f\uff1b3\uff09\u6df7\u5408\u4eff\u771f\u6846\u67b6\uff08\u5206\u6790\u6a21\u578b+\u5468\u671f\u7ea7\u6a21\u62df\u5668+\u5185\u5b58\u8ffd\u8e2a\uff09\uff0c\u5e73\u8861\u67b6\u6784\u7ec6\u8282\u548c\u6548\u7387\u3002", "result": "\u5728miss\u5904\u7406\u541e\u5410\u91cf\u4e3a\u4e3b\u8981\u74f6\u9888\u65f6\uff0c\u5e73\u5747\u52a0\u901f1.26\u500d\uff1b\u5728\u7f13\u5b58\u5927\u5c0f\u4e5f\u53d7\u9650\u65f6\uff0c\u76f8\u6bd4\u672a\u4f18\u5316\u7248\u672c\u52a0\u901f1.58\u500d\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\uff08dyncta\uff09\u63d0\u53471.26\u500d\u3002\u9996\u6b21\u9488\u5bf9LLM\u89e3\u7801\u7279\u6709\u7684MSHR\u7ade\u4e89\u95ee\u9898\u3002", "conclusion": "LLaMCAT\u662f\u9996\u4e2a\u9488\u5bf9LLM\u89e3\u7801\u7279\u5b9aMSHR\u7ade\u4e89\u7684\u4f18\u5316\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684LLM\u63a8\u7406\u52a0\u901f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u7a7a\u767d\u3002"}}
{"id": "2512.00096", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.00096", "abs": "https://arxiv.org/abs/2512.00096", "authors": ["Mahdi Aghaei", "Saba Ebrahimi", "Mohammad Saleh Arafati", "Elham Cheshmikhani", "Dara Rahmati", "Saeid Gorgin", "Jungrae Kim"], "title": "Modeling and Simulation Frameworks for Processing-in-Memory Architectures", "comment": null, "summary": "Processing-in-Memory (PIM) has emerged as a promising computing paradigm to address the memory wall and the fundamental bottleneck of the von Neumann architecture by reducing costly data movement between memory and processing units. As with any engineering challenge, identifying the most effective solutions requires thorough exploration of diverse architectural proposals, device technologies, and application domains. In this context, simulation plays a critical role in enabling researchers to evaluate, compare, and refine PIM designs prior to fabrication. Over the past decade, a variety of PIM simulators have been introduced, spanning low-level device models, architectural frameworks, and application-oriented environments. These tools differ significantly in fidelity, scalability, supported memory/compute technologies, and benchmark compatibility. Understanding these trade-offs is essential for researchers to select appropriate simulators that accurately map and validate their research efforts. This chapter provides a comprehensive overview of PIM simulation methodologies and tools. We categorize simulators according to abstraction levels, design objectives, and evaluation metrics, highlighting representative examples. To improve accessibility, some content may appear in multiple contexts to guide readers with different backgrounds. We also survey benchmark suites commonly employed in PIM studies and discuss open challenges in simulation methodology, paving the way for more reliable, scalable, and efficient PIM modeling.", "AI": {"tldr": "\u672c\u7ae0\u5168\u9762\u7efc\u8ff0\u4e86\u5b58\u5185\u8ba1\u7b97\uff08PIM\uff09\u4eff\u771f\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u5bf9\u73b0\u6709\u6a21\u62df\u5668\u6309\u62bd\u8c61\u5c42\u6b21\u3001\u8bbe\u8ba1\u76ee\u6807\u548c\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u548c\u4eff\u771f\u65b9\u6cd5\u5b66\u7684\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u5b58\u5185\u8ba1\u7b97\u4f5c\u4e3a\u7a81\u7834\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u74f6\u9888\u7684\u65b0\u5174\u8ba1\u7b97\u8303\u5f0f\uff0c\u9700\u8981\u6709\u6548\u7684\u4eff\u771f\u5de5\u5177\u6765\u8bc4\u4f30\u548c\u4f18\u5316\u8bbe\u8ba1\u65b9\u6848\u3002\u73b0\u6709PIM\u6a21\u62df\u5668\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u3001\u652f\u6301\u6280\u672f\u7b49\u65b9\u9762\u5dee\u5f02\u663e\u8457\uff0c\u7814\u7a76\u4eba\u5458\u9700\u8981\u7cfb\u7edf\u4e86\u89e3\u8fd9\u4e9b\u6743\u8861\u4ee5\u9009\u62e9\u5408\u9002\u7684\u4eff\u771f\u5de5\u5177\u3002", "method": "\u672c\u7ae0\u91c7\u7528\u5206\u7c7b\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c06PIM\u6a21\u62df\u5668\u6309\u62bd\u8c61\u5c42\u6b21\uff08\u4ece\u5e95\u5c42\u5668\u4ef6\u6a21\u578b\u5230\u5e94\u7528\u5c42\u73af\u5883\uff09\u3001\u8bbe\u8ba1\u76ee\u6807\u548c\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u5206\u7c7b\uff0c\u7a81\u51fa\u4ee3\u8868\u6027\u5b9e\u4f8b\u3002\u4e3a\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\uff0c\u90e8\u5206\u5185\u5bb9\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u91cd\u590d\u51fa\u73b0\u4ee5\u6307\u5bfc\u4e0d\u540c\u80cc\u666f\u7684\u8bfb\u8005\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86PIM\u4eff\u771f\u5de5\u5177\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5404\u7c7b\u6a21\u62df\u5668\u7684\u7279\u70b9\u548c\u5e94\u7528\u573a\u666f\uff0c\u8c03\u67e5\u4e86PIM\u7814\u7a76\u4e2d\u5e38\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5e76\u6307\u51fa\u4e86\u4eff\u771f\u65b9\u6cd5\u5b66\u4e2d\u5b58\u5728\u7684\u5f00\u653e\u6311\u6218\u3002", "conclusion": "PIM\u4eff\u771f\u5728\u5b58\u5185\u8ba1\u7b97\u7814\u7a76\u4e2d\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u672c\u7ae0\u7684\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u9009\u62e9\u5408\u9002\u4eff\u771f\u5de5\u5177\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u9700\u8981\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684PIM\u5efa\u6a21\u65b9\u6cd5\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2512.00112", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00112", "abs": "https://arxiv.org/abs/2512.00112", "authors": ["Elham Cheshmikhani", "Hamed Farbeh"], "title": "An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache", "comment": null, "summary": "Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.\n  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u6a21\u578b\u6765\u786e\u5b9a\u7f13\u5b58\u6807\u7b7e\u5206\u533a\u7684\u6700\u4f18\u5206\u5272\u70b9k\uff0c\u4ee5\u6700\u5927\u5316\u6807\u7b7e\u8bfb\u53d6\u51cf\u5c11\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u7814\u7a76\u4e2dk\u503c\u9009\u62e9\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u5173\u8054\u7f13\u5b58\u5bf9\u5904\u7406\u5668\u6027\u80fd\u548c\u80fd\u8017\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u7f13\u5b58\u6807\u7b7e\u9635\u5217\u4f5c\u4e3a\u786c\u4ef6\u7ba1\u7406\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u65e2\u662f\u80fd\u8017\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u4e5f\u5bb9\u6613\u53d7\u5230\u6545\u969c\u5f71\u54cd\u3002\u6807\u7b7e\u5206\u533a\u6280\u672f\u867d\u7136\u88ab\u5e7f\u6cdb\u4f7f\u7528\u6765\u964d\u4f4e\u6807\u7b7e\u8bbf\u95ee\u80fd\u8017\u548c\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u5173\u952e\u53c2\u6570k\uff08\u6807\u7b7e\u5206\u5272\u70b9\uff09\u7684\u9009\u62e9\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u901a\u5e38\u51ed\u76f4\u89c9\u3001\u968f\u673a\u6216\u7ecf\u9a8c\u786e\u5b9a\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u4e14\u65e0\u6cd5\u8de8\u914d\u7f6e\u901a\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790\u63a8\u5bfc\u51fa\u57fa\u4e8e\u7f13\u5b58\u914d\u7f6e\u53c2\u6570\u786e\u5b9a\u6700\u4f18\u5206\u5272\u70b9k\u7684\u6570\u5b66\u516c\u5f0f\u3002\u8be5\u516c\u5f0f\u662f\u51f8\u51fd\u6570\u4e14\u53ef\u5fae\u5206\uff0c\u80fd\u591f\u7cbe\u786e\u91cf\u5316\u4efb\u610fk\u503c\u548c\u914d\u7f6e\u4e0b\u7684\u6807\u7b7e\u5206\u533a\u6548\u7387\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u7f13\u5b58\u8bbe\u8ba1\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5206\u6790\u6a21\u578b\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6807\u7b7e\u5206\u533a\u6548\u7387\u548c\u6700\u4f18k\u503c\u3002\u6a21\u578b\u663e\u793a\u9009\u62e9\u8fc7\u5927\u6216\u8fc7\u5c0f\u7684k\u503c\u90fd\u4f1a\u663e\u8457\u964d\u4f4e\u6807\u7b7e\u5206\u533a\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u6790\u6a21\u578b\u4f7f\u8bbe\u8ba1\u8005\u548c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5373\u65f6\u8ba1\u7b97\u6700\u4f18\u6807\u7b7e\u5206\u5272\u70b9\uff0c\u5e76\u51c6\u786e\u4f30\u8ba1\u6807\u7b7e\u8bfb\u53d6\u51cf\u5c11\u91cf\uff0c\u4e3a\u7f13\u5b58\u6807\u7b7e\u5206\u533a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2512.00113", "categories": ["cs.AR", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.00113", "abs": "https://arxiv.org/abs/2512.00113", "authors": ["Amirreza Yousefzadeh"], "title": "From RISC-V Cores to Neuromorphic Arrays: A Tutorial on Building Scalable Digital Neuromorphic Processors", "comment": null, "summary": "Digital neuromorphic processors are emerging as a promising computing substrate for low-power, always-on EdgeAI applications. In this tutorial paper, we outline the main architectural design principles behind fully digital neuromorphic processors and illustrate them using the SENECA platform as a running example. Starting from a flexible array of tiny RISC-V processing cores connected by a simple Network-on-Chip (NoC), we show how to progressively evolve the architecture: from a baseline event-driven implementation of fully connected networks, to versions with dedicated Neural Processing Elements (NPEs) and a loop controller that offloads fine-grained control from the general-purpose cores. Along the way, we discuss software and mapping techniques such as spike grouping, event-driven depth-first convolution for convolutional networks, and hard-attention style processing for high-resolution event-based vision. The focus is on architectural trade-offs, performance and energy bottlenecks, and on leveraging flexibility to incrementally add domain-specific acceleration. This paper assumes familiarity with basic neuromorphic concepts (spikes, event-driven computation, sparse activation) and deep neural network workloads. It does not present new experimental results; instead, it synthesizes and contextualizes findings previously reported in our SENECA publications to provide a coherent, step-by-step architectural perspective for students and practitioners who wish to design their own digital neuromorphic processors.", "AI": {"tldr": "\u672c\u6587\u4ee5SENECA\u5e73\u53f0\u4e3a\u4f8b\uff0c\u7cfb\u7edf\u4ecb\u7ecd\u4e86\u5168\u6570\u5b57\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u7684\u67b6\u6784\u8bbe\u8ba1\u539f\u5219\uff0c\u4ece\u57fa\u7840RISC-V\u6838\u5fc3\u9635\u5217\u9010\u6b65\u6f14\u8fdb\u5230\u4e13\u7528\u795e\u7ecf\u5904\u7406\u5355\u5143\uff0c\u91cd\u70b9\u8ba8\u8bba\u67b6\u6784\u6743\u8861\u3001\u6027\u80fd\u74f6\u9888\u548c\u8f6f\u4ef6\u6620\u5c04\u6280\u672f\u3002", "motivation": "\u6570\u5b57\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u5728\u4f4e\u529f\u8017\u3001\u5e38\u5f00\u8fb9\u7f18AI\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u67b6\u6784\u8bbe\u8ba1\u6307\u5bfc\u3002\u672c\u6587\u65e8\u5728\u4e3a\u5e0c\u671b\u8bbe\u8ba1\u81ea\u5df1\u6570\u5b57\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u7684\u5b66\u751f\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u8fde\u8d2f\u7684\u3001\u9010\u6b65\u7684\u67b6\u6784\u89c6\u89d2\u3002", "method": "\u4ee5SENECA\u5e73\u53f0\u4e3a\u6848\u4f8b\uff0c\u4ece\u7075\u6d3b\u7684RISC-V\u5904\u7406\u6838\u5fc3\u9635\u5217\u5f00\u59cb\uff0c\u9010\u6b65\u6f14\u8fdb\u67b6\u6784\uff1a\u4ece\u4e8b\u4ef6\u9a71\u52a8\u7684\u5168\u8fde\u63a5\u7f51\u7edc\u5b9e\u73b0\uff0c\u5230\u6dfb\u52a0\u4e13\u7528\u795e\u7ecf\u5904\u7406\u5355\u5143\u548c\u5faa\u73af\u63a7\u5236\u5668\uff0c\u540c\u65f6\u8ba8\u8bba\u5c16\u5cf0\u5206\u7ec4\u3001\u4e8b\u4ef6\u9a71\u52a8\u6df1\u5ea6\u4f18\u5148\u5377\u79ef\u7b49\u8f6f\u4ef6\u6620\u5c04\u6280\u672f\u3002", "result": "\u672c\u6587\u672a\u5448\u73b0\u65b0\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u800c\u662f\u7efc\u5408\u4e86\u5148\u524dSENECA\u51fa\u7248\u7269\u4e2d\u7684\u53d1\u73b0\uff0c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u67b6\u6784\u8bbe\u8ba1\u89c6\u89d2\uff0c\u91cd\u70b9\u5173\u6ce8\u67b6\u6784\u6743\u8861\u3001\u6027\u80fd\u74f6\u9888\u548c\u80fd\u91cf\u74f6\u9888\u5206\u6790\u3002", "conclusion": "\u6570\u5b57\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u8bbe\u8ba1\u9700\u8981\u5e73\u8861\u7075\u6d3b\u6027\u548c\u4e13\u7528\u52a0\u901f\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u67b6\u6784\u6f14\u8fdb\u548c\u4f18\u5316\u7684\u8f6f\u4ef6\u6620\u5c04\u6280\u672f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8fb9\u7f18AI\u5904\u7406\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u6846\u67b6\u3002"}}
{"id": "2512.00138", "categories": ["cs.AR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00138", "abs": "https://arxiv.org/abs/2512.00138", "authors": ["Yuyang Li", "Swasthik Muloor", "Jack Laudati", "Nickolas Dematteis", "Yidam Park", "Hana Kim", "Nathan Chang", "Inhee Lee"], "title": "Ternary-Input Binary-Weight CNN Accelerator Design for Miniature Object Classification System with Query-Driven Spatial DVS", "comment": "6 pages.12 figures & 2 table", "summary": "Miniature imaging systems are essential for space-constrained applications but are limited by memory and power constraints. While machine learning can reduce data size by extracting key features, its high energy demands often exceed the capacity of small batteries. This paper presents a CNN hardware accelerator optimized for object classification in miniature imaging systems. It processes data from a spatial Dynamic Vision Sensor (DVS), reconfigurable to a temporal DVS via pixel sharing, minimizing sensor area. By using ternary DVS outputs and a ternary-input, binary-weight neural network, the design reduces computation and memory needs. Fabricated in 28 nm CMOS, the accelerator cuts data size by 81% and MAC operations by 27%. It achieves 440 ms inference time at just 1.6 mW power consumption, improving the Figure-of-Merit (FoM) by 7.3x over prior CNN accelerators for miniature systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5fae\u578b\u6210\u50cf\u7cfb\u7edf\u7684CNN\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u91c7\u7528\u4e09\u503cDVS\u8f93\u51fa\u548c\u4e09\u5143\u8f93\u5165-\u4e8c\u503c\u6743\u91cd\u795e\u7ecf\u7f51\u7edc\uff0c\u663e\u8457\u964d\u4f4e\u6570\u636e\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u5fae\u578b\u6210\u50cf\u7cfb\u7edf\u53d7\u9650\u4e8e\u5185\u5b58\u548c\u529f\u8017\u7ea6\u675f\uff0c\u800c\u673a\u5668\u5b66\u4e60\u867d\u7136\u80fd\u51cf\u5c11\u6570\u636e\u5927\u5c0f\uff0c\u4f46\u5176\u9ad8\u80fd\u8017\u9700\u6c42\u5f80\u5f80\u8d85\u8fc7\u5c0f\u578b\u7535\u6c60\u7684\u5bb9\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u4f4e\u529f\u8017\u7684\u786c\u4ef6\u52a0\u901f\u5668\u3002", "method": "\u8bbe\u8ba1CNN\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5904\u7406\u6765\u81ea\u7a7a\u95f4\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668(DVS)\u7684\u6570\u636e\uff0c\u53ef\u901a\u8fc7\u50cf\u7d20\u5171\u4eab\u91cd\u65b0\u914d\u7f6e\u4e3a\u65f6\u95f4DVS\u4ee5\u51cf\u5c11\u4f20\u611f\u5668\u9762\u79ef\u3002\u91c7\u7528\u4e09\u503cDVS\u8f93\u51fa\u548c\u4e09\u5143\u8f93\u5165-\u4e8c\u503c\u6743\u91cd\u795e\u7ecf\u7f51\u7edc\uff0c\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002", "result": "\u572828nm CMOS\u5de5\u827a\u4e0b\u5236\u9020\uff0c\u6570\u636e\u5927\u5c0f\u51cf\u5c1181%\uff0cMAC\u64cd\u4f5c\u51cf\u5c1127%\u3002\u5728\u4ec51.6mW\u529f\u8017\u4e0b\u5b9e\u73b0440ms\u63a8\u7406\u65f6\u95f4\uff0c\u76f8\u6bd4\u5148\u524d\u5fae\u578b\u7cfb\u7edfCNN\u52a0\u901f\u5668\u7684\u54c1\u8d28\u56e0\u6570(FoM)\u63d0\u53477.3\u500d\u3002", "conclusion": "\u8be5\u786c\u4ef6\u52a0\u901f\u5668\u901a\u8fc7\u521b\u65b0\u7684\u4f20\u611f\u5668\u8bbe\u8ba1\u548c\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u65b9\u6cd5\uff0c\u4e3a\u5fae\u578b\u6210\u50cf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u7269\u4f53\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u6bd4\u3002"}}
{"id": "2512.00186", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.00186", "abs": "https://arxiv.org/abs/2512.00186", "authors": ["Seyed Hadi Mirfarshbafan", "Nicolas Filliol", "Oscar Casta\u00f1eda", "Christoph Studer"], "title": "Variable Point: A Number Format for Area- and Energy-Efficient Multiplication of High-Dynamic-Range Numbers", "comment": "Presented at the 59th Asilomar Conference on Signals, Systems, and Computers", "summary": "Fixed-point number representation is commonly employed in digital VLSI designs that have stringent hardware efficiency constraints. However, fixed-point numbers cover a relatively small dynamic range for a given bitwidth. In contrast, floating-point numbers offer a larger dynamic range at the cost of increased hardware complexity. In this paper, we propose a novel number format called variable-point (VP). VP numbers cover a larger dynamic range than fixed-point numbers with similar bitwidth, without notably increasing hardware complexity -- this allows for a more efficient representation of signals with high dynamic range. To demonstrate the efficacy of the proposed VP number format, we consider a matrix-vector multiplication engine for spatial equalization in multi-antenna wireless communication systems involving high-dynamic-range signals. Through post-layout VLSI implementation results, we demonstrate that the proposed VP-based design achieves 20% and 10% area and power savings, respectively, compared to a fully optimized fixed-point design, without incurring any noticeable performance degradation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u6570\u5b57\u683c\u5f0f\u2014\u2014\u53ef\u53d8\u70b9(VP)\uff0c\u5728\u4fdd\u6301\u786c\u4ef6\u590d\u6742\u5ea6\u4e0d\u663e\u8457\u589e\u52a0\u7684\u540c\u65f6\uff0c\u6bd4\u5b9a\u70b9\u6570\u63d0\u4f9b\u66f4\u5927\u7684\u52a8\u6001\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u9ad8\u52a8\u6001\u8303\u56f4\u4fe1\u53f7\u5904\u7406\u3002", "motivation": "\u5b9a\u70b9\u6570\u8868\u793a\u5728\u786c\u4ef6\u6548\u7387\u8981\u6c42\u9ad8\u7684VLSI\u8bbe\u8ba1\u4e2d\u5e38\u7528\uff0c\u4f46\u5176\u52a8\u6001\u8303\u56f4\u6709\u9650\uff1b\u6d6e\u70b9\u6570\u867d\u63d0\u4f9b\u66f4\u5927\u52a8\u6001\u8303\u56f4\u4f46\u786c\u4ef6\u590d\u6742\u5ea6\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u6298\u4e2d\u65b9\u6848\uff0c\u65e2\u80fd\u6269\u5927\u52a8\u6001\u8303\u56f4\u53c8\u4e0d\u663e\u8457\u589e\u52a0\u786c\u4ef6\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u53ef\u53d8\u70b9(VP)\u6570\u5b57\u683c\u5f0f\uff0c\u5728\u7c7b\u4f3c\u4f4d\u5bbd\u4e0b\u6bd4\u5b9a\u70b9\u6570\u63d0\u4f9b\u66f4\u5927\u52a8\u6001\u8303\u56f4\u3002\u901a\u8fc7\u591a\u5929\u7ebf\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u7a7a\u95f4\u5747\u8861\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u5f15\u64ce\u6765\u9a8c\u8bc1VP\u683c\u5f0f\u7684\u6709\u6548\u6027\u3002", "result": "\u540e\u5e03\u5c40VLSI\u5b9e\u73b0\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eVP\u7684\u8bbe\u8ba1\u76f8\u6bd4\u5b8c\u5168\u4f18\u5316\u7684\u5b9a\u70b9\u8bbe\u8ba1\uff0c\u9762\u79ef\u8282\u770120%\uff0c\u529f\u8017\u8282\u770110%\uff0c\u4e14\u6ca1\u6709\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "VP\u6570\u5b57\u683c\u5f0f\u4e3a\u9ad8\u52a8\u6001\u8303\u56f4\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u786c\u4ef6\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u9762\u79ef\u548c\u529f\u8017\u3002"}}
{"id": "2512.00335", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00335", "abs": "https://arxiv.org/abs/2512.00335", "authors": ["Takuto Ando", "Yu Eto", "Ayumu Takeuchi", "Yasuhiko Nakashima"], "title": "Efficient Kernel Mapping and Comprehensive System Evaluation of LLM Acceleration on a CGLA", "comment": "This paper is published at IEEE Access", "summary": "Large Language Models (LLMs) demand substantial computational resources, resulting in high energy consumption on GPUs. To address this challenge, we focus on Coarse-Grained Reconfigurable Arrays (CGRAs) as an effective alternative that provides a trade-off between energy efficiency and programmability. This paper presents the first comprehensive, end-to-end evaluation of a non-AI-specialized Coarse-Grained Linear Array (CGLA) accelerator for the state-of-the-art Qwen LLM family. The architecture has a general-purpose, task-agnostic design, yet its flexible instruction set allows for domain-specific adaptations. This flexibility enables the architecture to achieve high efficiency for sustainable LLM inference. We assess the performance of our architecture on an FPGA prototype using the widely adopted llama.cpp framework. We then project its potential as a 28nm ASIC and compare it against a high-performance GPU (NVIDIA RTX 4090) and an edge AI device (NVIDIA Jetson AGX Orin). While GPUs exhibit lower latency, our non-AI-specific accelerator achieves higher energy efficiency, improving the Power-Delay Product (PDP) by up to 44.4x and 13.6x compared with the RTX 4090 and Jetson, respectively. Similarly, it reduces the Energy-Delay Product (EDP) by up to 11.5x compared to the high-performance GPU, demonstrating a favorable performance-energy trade-off. Critically, our system-level analysis identifies host-accelerator data transfer as the primary performance bottleneck, a factor often overlooked in kernel-level studies. These findings provide design guidance for next-generation LLM accelerators. This work validates CGRAs as a suitable platform for LLM inference in power-constrained environments, without being confined to specific algorithms.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u975eAI\u4e13\u7528\u7c97\u7c92\u5ea6\u7ebf\u6027\u9635\u5217(CGLA)\u52a0\u901f\u5668\u5728Qwen\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u6027\u80fd\uff0c\u76f8\u6bd4GPU\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u80fd\u6548\u6bd4", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u5927\uff0cGPU\u80fd\u8017\u9ad8\uff0c\u9700\u8981\u5bfb\u627e\u80fd\u6548\u4e0e\u53ef\u7f16\u7a0b\u6027\u5e73\u8861\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u4f7f\u7528\u975eAI\u4e13\u7528\u7c97\u7c92\u5ea6\u7ebf\u6027\u9635\u5217(CGLA)\u52a0\u901f\u5668\uff0c\u901a\u8fc7FPGA\u539f\u578b\u5b9e\u73b0\uff0c\u57fa\u4e8ellama.cpp\u6846\u67b6\u8bc4\u4f30\uff0c\u5e76\u6295\u5f71\u523028nm ASIC\u6027\u80fd", "result": "\u76f8\u6bd4RTX 4090\u548cJetson AGX Orin\uff0cCGLA\u52a0\u901f\u5668\u5206\u522b\u63d0\u5347\u529f\u7387\u5ef6\u8fdf\u79ef44.4\u500d\u548c13.6\u500d\uff0c\u964d\u4f4e\u80fd\u91cf\u5ef6\u8fdf\u79ef11.5\u500d", "conclusion": "\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\u662f\u529f\u7387\u53d7\u9650\u73af\u5883\u4e0bLLM\u63a8\u7406\u7684\u5408\u9002\u5e73\u53f0\uff0c\u65e0\u9700\u5c40\u9650\u4e8e\u7279\u5b9a\u7b97\u6cd5\uff0c\u4e3b\u673a-\u52a0\u901f\u5668\u6570\u636e\u4f20\u8f93\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888"}}
{"id": "2512.00441", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.00441", "abs": "https://arxiv.org/abs/2512.00441", "authors": ["Amogh K M", "Sunita M S"], "title": "A Novel 8T SRAM-Based In-Memory Computing Architecture for MAC-Derived Logical Functions", "comment": "6 pages, 6 figures, Accepted at 39th VLSID 2026 conference", "summary": "This paper presents an in-memory computing (IMC) architecture developed on an 8x8 array of 8T SRAM cells. This architecture enables both multi-bit parallel Multiply-Accumulate (MAC) operations and standard memory processing through charge-sharing on dedicated read bit-lines. By leveraging the maturity of SRAM technology, this work introduces an 8T SRAM-based IMC architecture that decouples read and write paths, thereby overcoming the reliability limitations of prior 6T SRAM designs. A novel analog-to-digital decoding scheme converts the MAC voltage output into digital counts, which are subsequently interpreted to realize fundamental logic functions including AND/NAND, NOR/OR, XOR/XNOR, and 1-bit addition within the same array. Simulated in a 90 nm CMOS process at 1.8 V supply voltage, the proposed design achieves 8-bit MAC and logical operations at a frequency of 142.85 MHz, with a latency of 0.7 ns and energy consumption of 56.56 fJ/bit per MAC operation and throughput of 15.8 M operations/s.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e8T SRAM\u9635\u5217\u7684\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\uff0c\u652f\u6301\u591a\u6bd4\u7279\u5e76\u884cMAC\u8fd0\u7b97\u548c\u6807\u51c6\u5b58\u50a8\u5904\u7406\uff0c\u901a\u8fc7\u7535\u8377\u5171\u4eab\u548c\u65b0\u578b\u89e3\u7801\u65b9\u6848\u5b9e\u73b0\u903b\u8f91\u8fd0\u7b97", "motivation": "\u4f20\u7edf6T SRAM\u8bbe\u8ba1\u5b58\u5728\u53ef\u9760\u6027\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u6267\u884c\u5185\u5b58\u8ba1\u7b97\u53c8\u80fd\u4fdd\u6301\u6807\u51c6\u5b58\u50a8\u529f\u80fd\u7684\u9ad8\u6027\u80fd\u67b6\u6784", "method": "\u4f7f\u75288x8 8T SRAM\u9635\u5217\uff0c\u901a\u8fc7\u4e13\u7528\u8bfb\u4f4d\u7ebf\u7535\u8377\u5171\u4eab\u5b9e\u73b0MAC\u8fd0\u7b97\uff0c\u91c7\u7528\u65b0\u578b\u6a21\u6570\u89e3\u7801\u65b9\u6848\u5c06\u7535\u538b\u8f93\u51fa\u8f6c\u6362\u4e3a\u6570\u5b57\u8ba1\u6570\uff0c\u652f\u6301\u591a\u79cd\u903b\u8f91\u8fd0\u7b97", "result": "\u572890nm CMOS\u5de5\u827a\u30011.8V\u7535\u538b\u4e0b\uff0c\u5b9e\u73b0142.85MHz\u9891\u7387\u30010.7ns\u5ef6\u8fdf\u300156.56fJ/bit\u80fd\u8017\u76848\u4f4dMAC\u8fd0\u7b97\uff0c\u541e\u5410\u91cf\u8fbe15.8M\u64cd\u4f5c/\u79d2", "conclusion": "8T SRAM\u67b6\u6784\u6210\u529f\u514b\u670d\u4e866T\u8bbe\u8ba1\u7684\u53ef\u9760\u6027\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u8ba1\u7b97\u548c\u903b\u8f91\u8fd0\u7b97\u96c6\u6210\uff0c\u4e3aSRAM\u6280\u672f\u7684\u5185\u5b58\u8ba1\u7b97\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2512.00487", "categories": ["cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.00487", "abs": "https://arxiv.org/abs/2512.00487", "authors": ["Yuhao Gu", "Zhongchun Zheng", "Nong Xiao", "Yutong Lu", "Xianwei Zhang"], "title": "Partial Cross-Compilation and Mixed Execution for Accelerating Dynamic Binary Translation", "comment": null, "summary": "With the growing diversity of instruction set architectures (ISAs), cross-ISA program execution has become common. Dynamic binary translation (DBT) is the main solution but suffers from poor performance. Cross-compilation avoids emulation costs but is constrained by an \"all-or-nothing\" model-programs are either fully cross-compiled or entirely emulated. Complete cross-compilation is often unfeasible due to ISA-specific code or missing dependencies, leaving programs with high emulation overhead.\n  We propose a hybrid execution system that combines compilation and emulation, featuring a selective function offloading mechanism. This mechanism establishes cross-environment calling channels, offloading eligible functions to the host for native execution to reduce DBT overhead. Key optimizations address offloading costs, enabling efficient hybrid operation. Built on LLVM and QEMU, the system works automatically for both applications and libraries. Evaluations show it achieves up to 13x speedups over existing DBT, with strong practical value.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u6267\u884c\u7cfb\u7edf\uff0c\u7ed3\u5408\u7f16\u8bd1\u4e0e\u4eff\u771f\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u51fd\u6570\u5378\u8f7d\u673a\u5236\u51cf\u5c11\u52a8\u6001\u4e8c\u8fdb\u5236\u7ffb\u8bd1\u5f00\u9500\uff0c\u5b9e\u73b0\u6700\u9ad813\u500d\u52a0\u901f", "motivation": "\u968f\u7740\u6307\u4ee4\u96c6\u67b6\u6784\u591a\u6837\u5316\uff0c\u8de8ISA\u7a0b\u5e8f\u6267\u884c\u53d8\u5f97\u666e\u904d\u3002\u52a8\u6001\u4e8c\u8fdb\u5236\u7ffb\u8bd1\u6027\u80fd\u5dee\uff0c\u4ea4\u53c9\u7f16\u8bd1\u53d7\u9650\u4e8e\"\u5168\u6709\u6216\u5168\u65e0\"\u6a21\u5f0f\uff0c\u5b8c\u6574\u4ea4\u53c9\u7f16\u8bd1\u5e38\u4e0d\u53ef\u884c\uff0c\u5bfc\u81f4\u9ad8\u4eff\u771f\u5f00\u9500", "method": "\u63d0\u51fa\u6df7\u5408\u6267\u884c\u7cfb\u7edf\uff0c\u7ed3\u5408\u7f16\u8bd1\u4e0e\u4eff\u771f\uff0c\u91c7\u7528\u9009\u62e9\u6027\u51fd\u6570\u5378\u8f7d\u673a\u5236\u5efa\u7acb\u8de8\u73af\u5883\u8c03\u7528\u901a\u9053\uff0c\u5c06\u7b26\u5408\u6761\u4ef6\u7684\u51fd\u6570\u5378\u8f7d\u5230\u4e3b\u673a\u8fdb\u884c\u539f\u751f\u6267\u884c\u4ee5\u51cf\u5c11DBT\u5f00\u9500\uff0c\u5e76\u4f18\u5316\u5378\u8f7d\u6210\u672c\u3002\u57fa\u4e8eLLVM\u548cQEMU\u6784\u5efa\uff0c\u81ea\u52a8\u9002\u7528\u4e8e\u5e94\u7528\u7a0b\u5e8f\u548c\u5e93", "result": "\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u76f8\u6bd4\u73b0\u6709DBT\u5b9e\u73b0\u6700\u9ad813\u500d\u52a0\u901f\uff0c\u5177\u6709\u5f3a\u5927\u5b9e\u7528\u4ef7\u503c", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6267\u884c\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u8de8ISA\u6267\u884c\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u51fd\u6570\u5378\u8f7d\u673a\u5236\u5728\u7f16\u8bd1\u4e0e\u4eff\u771f\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd"}}
{"id": "2512.00974", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.00974", "abs": "https://arxiv.org/abs/2512.00974", "authors": ["Aradhya Chakrabarti"], "title": "A WASM-Subset Stack Architecture for Low-cost FPGAs using Open-Source EDA Flows", "comment": "6 pages, 5 figures. Source code available at https://github.com/TimeATronics/wasm_cpu", "summary": "Soft-core processors on resource-constrained FPGAs often suffer from low code density and reliance on proprietary toolchains. This paper details the design, implementation, and evaluation of a 32-bit dual-stack microprocessor architecture optimized for low-cost, resource-constrained Field-Programmable Gate Arrays (FPGAs). Implemented on the Gowin GW1NR-9 (Tang Nano 9K), the processor utilizes an instruction set architecture (ISA) inspired from a subset of the WebAssembly (WASM) specification to achieve high code density. Unlike traditional soft-cores that often rely on proprietary vendor toolchains and opaque IP blocks, this design is synthesized and routed utilizing an open-source flow, providing transparency and portability. The architecture features a dual-stack model (Data and Return), executing directly from SPI Flash via an Execute-in-Place (XIP) mechanism to conserve scarce Block RAM on the intended target device. An analysis of the trade-offs involved in stack depth parametrization is presented, demonstrating that an 8-entry distributed RAM implementation provides a balance between logic resource utilization ($\\sim 80\\%$) and routing congestion. Furthermore, timing hazards in single-cycle stack operations are identified and resolved through a refined Finite State Machine (FSM) design. The system achieves a stable operating frequency of 27 MHz, limited by Flash latency, and successfully executes simple applications including a single and multi-digit infix calculator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8d44\u6e90\u53d7\u9650FPGA\u4f18\u5316\u768432\u4f4d\u53cc\u6808\u5fae\u5904\u7406\u5668\u67b6\u6784\uff0c\u91c7\u7528WebAssembly\u5b50\u96c6ISA\u5b9e\u73b0\u9ad8\u4ee3\u7801\u5bc6\u5ea6\uff0c\u4f7f\u7528\u5f00\u6e90\u5de5\u5177\u94fe\uff0c\u901a\u8fc7XIP\u673a\u5236\u76f4\u63a5\u4eceSPI Flash\u6267\u884c\uff0c\u5728Gowin GW1NR-9 FPGA\u4e0a\u5b9e\u73b027MHz\u7a33\u5b9a\u8fd0\u884c\u3002", "motivation": "\u4f20\u7edf\u8f6f\u6838\u5904\u7406\u5668\u5728\u8d44\u6e90\u53d7\u9650FPGA\u4e0a\u5b58\u5728\u4ee3\u7801\u5bc6\u5ea6\u4f4e\u3001\u4f9d\u8d56\u4e13\u6709\u5de5\u5177\u94fe\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u4f18\u5316\u7684\u5fae\u5904\u7406\u5668\u67b6\u6784\uff0c\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u4f9b\u9ad8\u4ee3\u7801\u5bc6\u5ea6\u548c\u5f00\u6e90\u5de5\u5177\u94fe\u652f\u6301\u3002", "method": "\u8bbe\u8ba132\u4f4d\u53cc\u6808\u5fae\u5904\u7406\u5668\u67b6\u6784\uff08\u6570\u636e\u6808\u548c\u8fd4\u56de\u6808\uff09\uff0c\u91c7\u7528WebAssembly\u5b50\u96c6\u4f5c\u4e3a\u6307\u4ee4\u96c6\u67b6\u6784\uff0c\u4f7f\u7528\u5f00\u6e90\u7efc\u5408\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4eceSPI Flash\u7684XIP\u6267\u884c\u673a\u5236\uff0c\u5206\u6790\u6808\u6df1\u5ea6\u53c2\u6570\u5316\u6743\u8861\uff0c\u4f18\u5316FSM\u8bbe\u8ba1\u89e3\u51b3\u65f6\u5e8f\u5192\u9669\u3002", "result": "\u5728Gowin GW1NR-9 FPGA\u4e0a\u5b9e\u73b0\uff0c8\u9879\u5206\u5e03\u5f0fRAM\u914d\u7f6e\u5728\u903b\u8f91\u8d44\u6e90\u5229\u7528\u7387\uff08\u7ea680%\uff09\u548c\u5e03\u7ebf\u62e5\u585e\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u7a33\u5b9a\u8fd0\u884c\u9891\u738727MHz\uff08\u53d7Flash\u5ef6\u8fdf\u9650\u5236\uff09\uff0c\u6210\u529f\u6267\u884c\u5355/\u591a\u4f4d\u4e2d\u7f00\u8ba1\u7b97\u5668\u7b49\u7b80\u5355\u5e94\u7528\u3002", "conclusion": "\u8be5\u53cc\u6808\u5fae\u5904\u7406\u5668\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650FPGA\u4e0a\u8f6f\u6838\u5904\u7406\u5668\u7684\u4ee3\u7801\u5bc6\u5ea6\u548c\u5de5\u5177\u94fe\u4f9d\u8d56\u95ee\u9898\uff0c\u901a\u8fc7\u5f00\u6e90\u6d41\u7a0b\u548c\u4f18\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5b9e\u7528\u6027\u80fd\uff0c\u4e3a\u4f4e\u6210\u672c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.01193", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01193", "abs": "https://arxiv.org/abs/2512.01193", "authors": ["Masoud Rahimi", "S\u00e9bastien Le Beux"], "title": "Leveraging Recurrent Patterns in Graph Accelerators", "comment": "Accepted at DATE 2026", "summary": "Graph accelerators have emerged as a promising solution for processing large-scale sparse graphs, leveraging the in-situ compu-tation of ReRAM-based crossbars to maximize computational efficiency. However, existing designs suffer from memristor access overhead due to the large number of graph partitions. This leads to increased execution time, higher energy consumption, and re-duced circuit lifetime. This paper proposes a graph processing method that minimizes memristor write operations by identifying frequent subgraph patterns and assigning them to graph engines, referred to as static, allowing most subgraphs to be processed without a need for crossbar reconfiguration. Experimental results show speed up to 2.38x speedup and 7.23x energy savings com-pared to state-of-the-art accelerators. Furthermore, our method extends the circuit lifetime by 2x compared to state-of-the-art ReRAM graph accelerators.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eReRAM\u7684\u56fe\u52a0\u901f\u5668\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u9891\u7e41\u5b50\u56fe\u6a21\u5f0f\u5e76\u9759\u6001\u5206\u914d\u7ed9\u8ba1\u7b97\u5f15\u64ce\uff0c\u51cf\u5c11\u5185\u5b58\u5199\u5165\u64cd\u4f5c\uff0c\u63d0\u5347\u6027\u80fd\u3001\u964d\u4f4e\u80fd\u8017\u5e76\u5ef6\u957f\u7535\u8def\u5bff\u547d", "motivation": "\u73b0\u6709\u57fa\u4e8eReRAM\u7684\u56fe\u52a0\u901f\u5668\u5728\u5904\u7406\u5927\u89c4\u6a21\u7a00\u758f\u56fe\u65f6\uff0c\u7531\u4e8e\u5927\u91cf\u56fe\u5206\u533a\u5bfc\u81f4\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\u5927\uff0c\u589e\u52a0\u4e86\u6267\u884c\u65f6\u95f4\u3001\u80fd\u8017\uff0c\u5e76\u7f29\u77ed\u4e86\u7535\u8def\u5bff\u547d", "method": "\u8bc6\u522b\u9891\u7e41\u51fa\u73b0\u7684\u5b50\u56fe\u6a21\u5f0f\uff0c\u5c06\u5176\u9759\u6001\u5206\u914d\u7ed9\u56fe\u8ba1\u7b97\u5f15\u64ce\uff0c\u4f7f\u5927\u591a\u6570\u5b50\u56fe\u5904\u7406\u65e0\u9700\u91cd\u65b0\u914d\u7f6e\u4ea4\u53c9\u5f00\u5173\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u5185\u5b58\u5199\u5165\u64cd\u4f5c", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u52a0\u901f\u5668\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad82.38\u500d\u7684\u52a0\u901f\u548c7.23\u500d\u7684\u80fd\u8017\u8282\u7701\uff0c\u540c\u65f6\u5c06\u7535\u8def\u5bff\u547d\u5ef6\u957f\u4e862\u500d", "conclusion": "\u901a\u8fc7\u51cf\u5c11\u5185\u5b58\u5199\u5165\u64cd\u4f5c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5904\u7406\u6027\u80fd\u3001\u964d\u4f4e\u4e86\u80fd\u8017\uff0c\u5e76\u6709\u6548\u5ef6\u957f\u4e86ReRAM\u56fe\u52a0\u901f\u5668\u7684\u7535\u8def\u5bff\u547d"}}
{"id": "2512.01463", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01463", "abs": "https://arxiv.org/abs/2512.01463", "authors": ["Jan-Frederik Schulte", "Benjamin Ramhorst", "Chang Sun", "Jovan Mitrevski", "Nicol\u00f2 Ghielmetti", "Enrico Lupi", "Dimitrios Danopoulos", "Vladimir Loncar", "Javier Duarte", "David Burnette", "Lauri Laatu", "Stylianos Tzelepis", "Konstantinos Axiotis", "Quentin Berthet", "Haoyan Wang", "Paul White", "Suleyman Demirsoy", "Marco Colombo", "Thea Aarrestad", "Sioni Summers", "Maurizio Pierini", "Giuseppe Di Guglielmo", "Jennifer Ngadiuba", "Javier Campos", "Ben Hawks", "Abhijith Gandrakota", "Farah Fahim", "Nhan Tran", "George Constantinides", "Zhiqiang Que", "Wayne Luk", "Alexander Tapper", "Duc Hoang", "Noah Paladino", "Philip Harris", "Bo-Cheng Lai", "Manuel Valentin", "Ryan Forelli", "Seda Ogrenci", "Lino Gerlach", "Rian Flynn", "Mia Liu", "Daniel Diaz", "Elham Khoda", "Melissa Quinnan", "Russell Solares", "Santosh Parajuli", "Mark Neubauer", "Christian Herwig", "Ho Fung Tsoi", "Dylan Rankin", "Shih-Chieh Hsu", "Scott Hauck"], "title": "hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware", "comment": null, "summary": "We present hls4ml, a free and open-source platform that translates machine learning (ML) models from modern deep learning frameworks into high-level synthesis (HLS) code that can be integrated into full designs for field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs). With its flexible and modular design, hls4ml supports a large number of deep learning frameworks and can target HLS compilers from several vendors, including Vitis HLS, Intel oneAPI and Catapult HLS. Together with a wider eco-system for software-hardware co-design, hls4ml has enabled the acceleration of ML inference in a wide range of commercial and scientific applications where low latency, resource usage, and power consumption are critical. In this paper, we describe the structure and functionality of the hls4ml platform. The overarching design considerations for the generated HLS code are discussed, together with selected performance results.", "AI": {"tldr": "hls4ml\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u53ef\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8f6c\u6362\u4e3aHLS\u4ee3\u7801\uff0c\u7528\u4e8eFPGA/ASIC\u90e8\u7f72\uff0c\u652f\u6301\u591a\u79cd\u6846\u67b6\u548c\u7f16\u8bd1\u5668\uff0c\u9002\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u5e94\u7528\u3002", "motivation": "\u5728\u5546\u4e1a\u548c\u79d1\u5b66\u5e94\u7528\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u63a8\u7406\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u8d44\u6e90\u4f7f\u7528\u548c\u4f4e\u529f\u8017\uff0c\u800c\u4f20\u7edf\u8f6f\u4ef6\u5b9e\u73b0\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u3002\u9700\u8981\u5c06ML\u6a21\u578b\u9ad8\u6548\u90e8\u7f72\u5230FPGA\u6216ASIC\u786c\u4ef6\u4e0a\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u7075\u6d3b\u7684hls4ml\u5e73\u53f0\uff0c\u80fd\u591f\u4ece\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08\u5982TensorFlow\u3001PyTorch\u7b49\uff09\u7ffb\u8bd1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u7ea7\u7efc\u5408\uff08HLS\uff09\u4ee3\u7801\uff0c\u652f\u6301\u591a\u79cdHLS\u7f16\u8bd1\u5668\uff08Vitis HLS\u3001Intel oneAPI\u3001Catapult HLS\uff09\u3002", "result": "hls4ml\u652f\u6301\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u548cHLS\u7f16\u8bd1\u5668\uff0c\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5e7f\u6cdb\u7684\u5546\u4e1a\u548c\u79d1\u5b66\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86ML\u63a8\u7406\u7684\u786c\u4ef6\u52a0\u901f\uff0c\u6ee1\u8db3\u4e86\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u8d44\u6e90\u4f7f\u7528\u548c\u4f4e\u529f\u8017\u7684\u9700\u6c42\u3002", "conclusion": "hls4ml\u662f\u4e00\u4e2a\u6709\u6548\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u5e73\u53f0\uff0c\u80fd\u591f\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9ad8\u6548\u90e8\u7f72\u5230FPGA/ASIC\u786c\u4ef6\u4e0a\uff0c\u4e3a\u9700\u8981\u9ad8\u6027\u80fd\u3001\u4f4e\u529f\u8017ML\u63a8\u7406\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.01541", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01541", "abs": "https://arxiv.org/abs/2512.01541", "authors": ["Hwayong Nam", "Seungmin Baek", "Jumin Kim", "Michael Jaemin Kim", "Jung Ho Ahn"], "title": "RoMe: Row Granularity Access Memory System for Large Language Models", "comment": "15 pages, 14 figures, accepted at HPCA 2026", "summary": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.", "AI": {"tldr": "RoMe\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u5185\u5b58\u67b6\u6784\uff0c\u901a\u8fc7\u4ee5\u884c\u7c92\u5ea6\u8bbf\u95eeDRAM\u3001\u7b80\u5316\u5185\u5b58\u63a5\u53e3\u8bbe\u8ba1\uff0c\u5728\u6700\u5c0f\u786c\u4ef6\u5f00\u9500\u4e0b\u63d0\u5347\u5e26\u5bbd12.5%", "motivation": "\u73b0\u4ee3HBM\u5185\u5b58\u7cfb\u7edf\u4fdd\u6301\u7f13\u5b58\u884c\u7c92\u5ea6\u8bbf\u95ee\uff0c\u5f15\u5165\u4e86bank\u7ec4\u548c\u4f2a\u901a\u9053\u7b49\u590d\u6742\u7ed3\u6784\uff0c\u589e\u52a0\u4e86\u5185\u5b58\u63a7\u5236\u5668\u8c03\u5ea6\u590d\u6742\u5ea6\u3002\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u8fde\u7eed\u6570\u636e\u5757\u4f20\u8f93\uff08KB\u5230MB\u7ea7\uff09\uff0c\u4f46\u5728\u4f20\u7edfHBM\u7cfb\u7edf\u4e2d\u88ab\u5206\u5272\u6210\u6570\u767e\u4e2a32B\u7f13\u5b58\u884c\u4e8b\u52a1\uff0c\u5bfc\u81f4\u8c03\u5ea6\u6548\u7387\u4f4e\u4e0b", "method": "RoMe\u4ee5\u884c\u7c92\u5ea6\u8bbf\u95eeDRAM\uff0c\u4ece\u5185\u5b58\u63a5\u53e3\u4e2d\u79fb\u9664\u5217\u3001bank\u7ec4\u548c\u4f2a\u901a\u9053\uff0c\u7b80\u5316\u5185\u5b58\u8c03\u5ea6\u903b\u8f91\u3002\u91ca\u653e\u7684\u5f15\u811a\u88ab\u805a\u5408\u5f62\u6210\u989d\u5916\u901a\u9053\uff0c\u5728\u6700\u5c0f\u989d\u5916\u5f15\u811a\u5f00\u9500\u4e0b\u589e\u52a0\u5e26\u5bbd", "result": "RoMe\u5728\u6700\u5c0f\u786c\u4ef6\u5f00\u9500\u4e0b\u5b9e\u73b0\u4e8612.5%\u7684\u5e26\u5bbd\u63d0\u5347\uff0c\u663e\u8457\u7b80\u5316\u4e86\u5185\u5b58\u8c03\u5ea6\u903b\u8f91\uff0c\u4e3a\u4e0b\u4e00\u4ee3HBM\u5185\u5b58\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848", "conclusion": "RoMe\u5c55\u793a\u4e86\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5de5\u4f5c\u8d1f\u8f7d\u53ef\u4ee5\u663e\u8457\u7b80\u5316\u5185\u5b58\u8c03\u5ea6\u903b\u8f91\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6700\u5c0f\u786c\u4ef6\u5f00\u9500\u589e\u52a0\u5e26\u5bbd\u7684\u4e0b\u4e00\u4ee3HBM\u5185\u5b58\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5"}}
{"id": "2512.01644", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01644", "abs": "https://arxiv.org/abs/2512.01644", "authors": ["Haonan Wang", "Xuxin Xiao", "Mingyu Yan", "Zhuoyuan Zhu", "Dengke Han", "Duo Wang", "Wenming Li", "Xiaochun Ye", "Cunchen Hu", "Hongyang Chen", "Guangyu Sun"], "title": "A Systematic Characterization of LLM Inference on GPUs", "comment": null, "summary": "This work presents a systematic characterization of Large Language Model (LLM) inference to address fragmented understanding. Through comprehensive experiments, we establish a four-dimensional analytical framework: (1) Two-Phase Heterogeneity Observation; (2) Microarchitectural Root Cause Analysis; (3) System Scaling Principles; and (4) Emerging Paradigm Boundaries. Our investigation progresses systematically from observation to foresight: identifying performance phenomena, revealing hardware causes, validating system behavior, and exploring new paradigms. This study not only consolidates a reliable empirical foundation for existing research but also provides new discoveries and practical optimization guidance for LLM inference.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u56db\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u4ece\u89c2\u5bdf\u5230\u9884\u6d4b\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\u548c\u4f18\u5316\u6307\u5bfc\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u7406\u89e3\u8f83\u4e3a\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u3002\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\uff0c\u4ece\u73b0\u8c61\u89c2\u5bdf\u5230\u6839\u672c\u539f\u56e0\u5206\u6790\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u53ef\u9760\u7684\u5b9e\u8bc1\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u5efa\u7acb\u56db\u7ef4\u5206\u6790\u6846\u67b6\uff1a1) \u4e24\u9636\u6bb5\u5f02\u6784\u6027\u89c2\u5bdf\uff1b2) \u5fae\u67b6\u6784\u6839\u672c\u539f\u56e0\u5206\u6790\uff1b3) \u7cfb\u7edf\u6269\u5c55\u539f\u5219\uff1b4) \u65b0\u5174\u8303\u5f0f\u8fb9\u754c\u3002\u7814\u7a76\u4ece\u89c2\u5bdf\u5230\u9884\u6d4b\u7cfb\u7edf\u63a8\u8fdb\uff1a\u8bc6\u522b\u6027\u80fd\u73b0\u8c61\u3001\u63ed\u793a\u786c\u4ef6\u539f\u56e0\u3001\u9a8c\u8bc1\u7cfb\u7edf\u884c\u4e3a\u3001\u63a2\u7d22\u65b0\u8303\u5f0f\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u9760\u7684LLM\u63a8\u7406\u5b9e\u8bc1\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u53d1\u73b0\u548c\u5b9e\u7528\u7684\u4f18\u5316\u6307\u5bfc\u3002\u7814\u7a76\u4e0d\u4ec5\u5de9\u56fa\u4e86\u73b0\u6709\u7814\u7a76\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u8fd8\u4e3aLLM\u63a8\u7406\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790LLM\u63a8\u7406\uff0c\u5efa\u7acb\u4e86\u4ece\u89c2\u5bdf\u5230\u9884\u6d4b\u7684\u56db\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b9e\u8bc1\u57fa\u7840\u3001\u65b0\u7684\u53d1\u73b0\u548c\u5b9e\u7528\u7684\u4f18\u5316\u6307\u5bfc\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7cfb\u7edf\u6027\u5206\u6790\u7684\u7a7a\u767d\u3002"}}
