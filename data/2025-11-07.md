<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies](https://arxiv.org/abs/2511.03944)
*Tong Zhang,Vikram Sharma Mailthody,Fei Sun,Linsen Ma,Chris J. Newburn,Teresa Zhang,Yang Liu,Jiangpeng Li,Hao Zhong,Wen-Mei Hwu*

Main category: cs.AR

TL;DR: 本文重新审视了经典的五分钟规则，通过整合主机成本、DRAM带宽/容量限制以及基于物理的SSD性能模型，提出了一个约束和负载感知的框架，发现现代AI平台下DRAM到闪存的缓存阈值从分钟级降至秒级。


<details>
  <summary>Details</summary>
Motivation: 传统的五分钟规则仅基于存储-内存经济学，忽略了主机成本、可行性限制和工作负载行为。随着现代AI平台（特别是GPU主机与高性能SSD配对）的发展，需要从第一性原理重新审视这一规则。

Method: 从第一性原理出发，整合主机成本、DRAM带宽/容量限制、基于物理的SSD性能和成本模型，并嵌入到约束和负载感知的框架中。开发了MQSim-Next SSD模拟器进行验证和敏感性分析。

Result: 对于现代AI平台，特别是GPU主机与为细粒度随机访问设计的高IOPS SSD配对时，DRAM到闪存的缓存阈值从分钟级崩溃到几秒钟。这重新定义了NAND闪存作为活跃数据层的角色。

Conclusion: 将经典启发式方法转变为可操作的、可行性感知的分析和配置框架，为AI时代内存层次结构的进一步研究奠定了基础，并展示了由此产生的软件系统设计空间。

Abstract: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a
simple, storage-memory-economics-based heuristic for deciding when data should
live in DRAM rather than on storage. Subsequent revisits to the rule largely
retained that economics-only view, leaving host costs, feasibility limits, and
workload behavior out of scope. This paper revisits the rule from first
principles, integrating host costs, DRAM bandwidth/capacity, and
physics-grounded models of SSD performance and cost, and then embedding these
elements in a constraint- and workload-aware framework that yields actionable
provisioning guidance. We show that, for modern AI platforms, especially
GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained
random access, the DRAM-to-flash caching threshold collapses from minutes to a
few seconds. This shift reframes NAND flash memory as an active data tier and
exposes a broad research space across the hardware-software stack. We further
introduce MQSim-Next, a calibrated SSD simulator that supports validation and
sensitivity analysis and facilitates future architectural and system research.
Finally, we present two concrete case studies that showcase the software system
design space opened by such memory hierarchy paradigm shift. Overall, we turn a
classical heuristic into an actionable, feasibility-aware analysis and
provisioning framework and set the stage for further research on AI-era memory
hierarchy.

</details>


### [2] [PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration](https://arxiv.org/abs/2511.04036)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: 提出了一种基于3D堆叠芯粒的大语言模型推理加速器，采用非易失性内存计算处理单元和硅光子互连技术解决通信瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理中的通信瓶颈问题，提升计算效率和可扩展性

Method: 使用3D堆叠芯粒架构，集成非易失性内存计算处理单元和硅光子互连的IPCN网络，开发了专门的LLM映射方案优化硬件调度和工作负载映射

Result: 相比Nvidia A100实现3.95倍加速和30倍效率提升；通过芯粒聚类和功率门控方案后，相比Nvidia H100在相似吞吐量下实现57倍效率提升

Conclusion: 该3D堆叠芯粒架构能有效解决LLM推理的通信瓶颈，显著提升计算效率和可扩展性，为更大模型提供支持

Abstract: This paper presents a 3D-stacked chiplets based large language model (LLM)
inference accelerator, consisting of non-volatile in-memory-computing
processing elements (PEs) and Inter-PE Computational Network (IPCN),
interconnected via silicon photonic to effectively address the communication
bottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling
and workload mapping. Simulation results show it achieves $3.95\times$ speedup
and $30\times$ efficiency improvement over the Nvidia A100 before chiplet
clustering and power gating scheme (CCPG). Additionally, the system achieves
further scalability and efficiency improvement with the implementation of CCPG
to accommodate larger models, attaining $57\times$ efficiency improvement over
Nvidia H100 at similar throughput.

</details>


### [3] [Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs](https://arxiv.org/abs/2511.04104)
*Chao Guo,Jiahe Xu,Moshe Zukerman*

Main category: cs.AR

TL;DR: 硬件解耦将数据中心资源从传统服务器集群转变为统一资源池，本文综述了其动机、最新进展、研究挑战和机遇，并指出该技术可能重塑整个数据中心生态系统。


<details>
  <summary>Details</summary>
Motivation: 硬件解耦旨在解决传统服务器架构的资源利用率低、扩展性差等问题，通过将计算、存储、内存等资源解耦为独立池，实现更灵活的资源分配和更高的资源利用率。

Method: 本文采用综述分析方法，结合数值研究，系统梳理硬件解耦的动机、进展、挑战和机遇，重点关注以往研究较少涉及的方面。

Result: 研究表明硬件解耦在产业界和学术界都取得了显著进展，但面临诸多技术挑战，包括资源调度、应用设计、硬件配置、冷却和电力系统优化等方面。

Conclusion: 硬件解耦有潜力重塑整个数据中心生态系统，但需要解决相关技术挑战，未来研究应关注系统层面的优化和跨领域协同设计。

Abstract: Hardware disaggregation seeks to transform Data Center (DC) resources from
traditional server fleets into unified resource pools. Despite existing
challenges that may hinder its full realization, significant progress has been
made in both industry and academia. In this article, we provide an overview of
the motivations and recent advancements in hardware disaggregation. We further
discuss the research challenges and opportunities associated with disaggregated
architectures, focusing on aspects that have received limited attention. We
argue that hardware disaggregation has the potential to reshape the entire DC
ecosystem, impacting application design, resource scheduling, hardware
configuration, cooling, and power system optimization. Additionally, we present
a numerical study to illustrate several key aspects of these challenges.

</details>


### [4] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: AIM是一个针对高性能SRAM存内计算(PIM)的软硬件协同设计框架，通过架构级IR-drop缓解技术，在7nm 256-TOPS PIM芯片上实现了69.2%的IR-drop缓解、2.29倍的能效提升和1.152倍的加速。


<details>
  <summary>Details</summary>
Motivation: 随着SRAM存内计算追求更高性能和更复杂电路设计，IR-drop问题日益严重，传统电路级缓解方法资源密集且会牺牲PPA(功耗、性能、面积)，需要架构级的解决方案。

Method: 提出AIM软硬件协同设计：1)利用PIM的位串行和原位数据流特性建立工作负载与IR-drop的直接关联(Rtog和HR)；2)通过LHR和WDS进行架构级IR-drop缓解探索；3)开发IR-Booster动态调整机制，结合软件HR信息和硬件IR-drop监控来调整PIM宏的电压-频率对；4)提出HR感知的任务映射方法。

Result: 在7nm 256-TOPS PIM芯片的后端仿真中，AIM实现了69.2%的IR-drop缓解、2.29倍的能效提升和1.152倍的加速。

Conclusion: AIM通过软硬件协同设计成功解决了高性能PIM中的IR-drop问题，在保持计算精度的同时显著提升了能效和性能，为未来高性能存内计算系统提供了有效的架构级IR-drop缓解方案。

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


### [5] [Scalable and Efficient Intra- and Inter-node Interconnection Networks for Post-Exascale Supercomputers and Data centers](https://arxiv.org/abs/2511.04677)
*Joaquin Tarraga-Moreno,Daniel Barley,Francisco J. Andujar Munoz,Jesus Escudero-Sahuquillo,Holger Froning,Pedro Javier Garcia,Francisco J. Quiles,Jose Duato*

Main category: cs.AR

TL;DR: 现代超级计算机和数据中心正朝着异构和紧密集成的架构发展，以支持数据密集型应用，但加速器数量的增加导致了通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI、科学模拟和大规模分析等数据密集型应用的快速增长，推动了对异构架构的需求，以减少数据移动并提高计算效率。

Method: 结合强大的CPU和加速器，并采用新兴的高带宽内存和存储技术。

Result: 随着每个节点中加速器数量的增加，在节点内部和节点之间出现了通信瓶颈，特别是在网络资源被异构组件共享时。

Conclusion: 异构架构虽然提高了计算效率，但通信瓶颈成为需要解决的关键挑战。

Abstract: The rapid growth of data-intensive applications such as generative AI,
scientific simulations, and large-scale analytics is driving modern
supercomputers and data centers toward increasingly heterogeneous and tightly
integrated architectures. These systems combine powerful CPUs and accelerators
with emerging high-bandwidth memory and storage technologies to reduce data
movement and improve computational efficiency. However, as the number of
accelerators per node increases, communication bottlenecks emerge both within
and between nodes, particularly when network resources are shared among
heterogeneous components.

</details>
