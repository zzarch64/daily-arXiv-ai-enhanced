{"id": "2601.14260", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14260", "abs": "https://arxiv.org/abs/2601.14260", "authors": ["Xiaoxuan Yang", "Peilin Chen", "Tergel Molom-Ochir", "Yiran Chen"], "title": "End-to-End Transformer Acceleration Through Processing-in-Memory Architectures", "comment": "ICM 2025", "summary": "Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models."}
{"id": "2601.14347", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14347", "abs": "https://arxiv.org/abs/2601.14347", "authors": ["George Rafael Gourdoumanis", "Fotoini Oikonomou", "Maria Pantazi-Kypraiou", "Pavlos Stoikos", "Olympia Axelou", "Athanasios Tziouvaras", "Georgios Karakonstantis", "Tahani Aladwani", "Christos Anagnostopoulos", "Yixian Shen", "Anuj Pathania", "Alberto Garcia-Ortiz", "George Floros"], "title": "Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability", "comment": "DATE 2026", "summary": "As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis."}
{"id": "2601.15151", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.15151", "abs": "https://arxiv.org/abs/2601.15151", "authors": ["Jean Bruant", "Pierre-Henri Horrein", "Olivier Muller", "Frédéric Pétrot"], "title": "Pipeline Automation Framework for Reusable High-throughput Network Applications on FPGA", "comment": "29 pages, 10 listings, 5 tables", "summary": "In a context of ever-growing worldwide communication traffic, cloud service providers aim at deploying scalable infrastructures to address heterogeneous needs. Part of the network infrastructure, FPGAs are tailored to guarantee low-latency and high-throughput packet processing. However, slowness of the hardware design process impairs FPGA ability to be part of an agile infrastructure under constant evolution, from incident response to long-term transformation. Deploying and maintaining network functionalities across a wide variety of FPGAs raises the need to fine-tune hardware designs for several FPGA targets. To address this issue, we introduce PAF, an open-source architectural parameterization framework based on a pipeline-oriented design methodology. PAF (Pipeline Automation Framework) implementation is based on Chisel, a Scala-embedded Hardware Construction Language (HCL), that we leverage to interface with circuit elaboration. Applied to industrial network packet classification systems, PAF demonstrates efficient parameterization abilities, enabling to reuse and optimize the same pipelined design on several FPGAs. In addition, PAF focuses the pipeline description on the architectural intent, incidentally reducing the number of lines of code to express complex functionalities. Finally, PAF confirms that automation does not imply any loss of tight control on the architecture by achieving on par performance and resource usage with equivalent exhaustively described implementations."}
