<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism](https://arxiv.org/abs/2510.10225)
*Jialin Sun,Yuchen Hu,Dean You,Yushu Du,Hui Wang,Xinwei Fang,Weiwei Shan,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: ISAAC是一个基于大语言模型的CPU验证框架，通过多代理激励生成和FPGA并行仿真，显著提升验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统CPU验证方法存在瓶颈：前端激励生成缺乏微架构感知导致测试质量低，后端仿真基础设施即使使用FPGA加速也会因长时间测试而停滞，反馈延迟且调试周期长。

Method: ISAAC采用全栈LLM辅助验证框架：前端使用注入微架构知识和历史bug模式的多代理激励引擎生成针对性测试；后端引入轻量级前向快照机制和解耦的协同仿真架构，使单个指令集仿真器可并行驱动多个被测设计。

Result: 在成熟CPU验证中，相比软件RTL仿真实现最高17,536倍加速，并检测到多个先前未知的bug。

Conclusion: ISAAC通过消除长尾测试瓶颈和利用FPGA并行性，显著提高了仿真吞吐量，为CPU验证提供了高效解决方案。

Abstract: Functional verification is a critical bottleneck in integrated circuit
development, with CPU verification being especially time-intensive and
labour-consuming. Industrial practice relies on differential testing for CPU
verification, yet faces bottlenecks at nearly each stage of the framework
pipeline: front-end stimulus generation lacks micro-architectural awareness,
yielding low-quality and redundant tests that impede coverage closure and miss
corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA
acceleration, often stalls on long-running tests and offers limited visibility,
delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a
full-stack, Large Language Model (LLM)-aided CPU verification framework with
FPGA parallelism, from bug categorisation and stimulus generation to simulation
infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's
front-end, infused with micro-architectural knowledge and historical bug
patterns, generating highly targeted tests that rapidly achieve coverage goals
and capture elusive corner cases. In ISAAC's back-end, we introduce a
lightweight forward-snapshot mechanism and a decoupled co-simulation
architecture between the Instruction Set Simulator (ISS) and the Design Under
Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By
eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the
simulation throughput is significantly improved. As a demonstration, we used
ISAAC to verify a mature CPU that has undergone multiple successful tape-outs.
Results show up to 17,536x speed-up over software RTL simulation, while
detecting several previously unknown bugs, two of which are reported in this
paper.

</details>


### [2] [ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration](https://arxiv.org/abs/2510.10623)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: ADiP是一种新型自适应精度脉动阵列架构，专门为高效矩阵乘法加速而设计，支持多种计算模式和精度调整，显著提升Transformer模型的计算效率和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer模型对矩阵乘法有巨大需求，需要高效加速来应对其内存和计算要求。量化技术可以减少内存使用，但需要可重构架构来动态调整精度以实现更高效的计算。

Method: 提出ADiP架构，包含NxN自适应精度处理单元和共享累加器，支持对称单矩阵乘法和不对称多矩阵乘法，能够适应8bitx8bit、8bitx4bit、8bitx2bit等不同精度。

Result: 在22nm商用技术下，ADiP实现了高达4倍的计算吞吐量提升。在GPT-2 Medium、BERT Large和BitNet-1.58B模型上，延迟改善最高达53.6%，BitNet-1.58B MHA工作负载的能效改善达24.4%。64x64规模下峰值吞吐量分别为8.192 TOPS、16.384 TOPS和32.768 TOPS。

Conclusion: ADiP架构通过自适应精度和多种计算模式，有效提升了矩阵乘法的计算效率和能效，特别适用于现代AI中的Transformer模型加速。

Abstract: Transformers are at the core of modern AI nowadays. They rely heavily on
matrix multiplication and require efficient acceleration due to their
substantial memory and computational requirements. Quantization plays a vital
role in reducing memory usage, and can be exploited for computations by
designing reconfigurable architectures that enhance matrix multiplication by
dynamically adjusting the precision. This paper proposes ADiP, a novel
adaptive-precision systolic array architecture designed for efficient matrix
multiplication acceleration.The proposed architecture consists of NxN
adaptive-precision processing elements (PEs) and shared accumulators. ADiP
supports multiple computation modes, including symmetric single-matrix
multiplication as well as asymmetric multi-matrix multiplication with a shared
input matrix, thereby improving data-reuse and PE utilization. In addition,
ADiP maximizes the computational density by adapting to different precisions,
such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed
for ADiP architecture, including latency and throughput for versatile
architecture configurations. A comprehensive hardware design space exploration
is demonstrated using 22nm commercial technology, achieving up to a 4x higher
computational throughput. Furthermore, ADiP is evaluated on different
transformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,
delivering latency improvement up to 53.6%, and energy improvement up to 24.4%
for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a
peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,
8bitx4bit, and 8bitx2bit operations, respectively.

</details>


### [3] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: Bhasha-Rupantarika是一个通过算法-硬件协同设计的高效多语言翻译系统，专为资源受限环境优化。该系统在FPGA上部署超低精度量化模型（FP8/INT8/INT4/FP4），实现了4.1倍模型压缩和4.2倍推理加速，特别适用于IoT设备的实时部署。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限环境（如IoT设备）中多语言翻译系统的部署需求，需要开发轻量高效的解决方案。传统模型在计算资源和存储方面要求较高，难以在边缘设备上实时运行。

Method: 采用算法-硬件协同设计方法，研究在FPGA加速器上部署超低精度量化模型（FP8、INT8、INT4、FP4）。系统支持印度语言与国际语言之间的双向翻译，特别关注低资源语言环境。

Result: FP4量化实现4.1倍模型大小缩减和4.2倍推理速度提升，吞吐量达66 tokens/s（提升4.8倍）。FPGA部署减少1.96倍LUTs和1.65倍FFs，相比OPU和HPTA分别提升2.2倍和4.6倍吞吐量。

Conclusion: 该研究证明了超低精度量化在FPGA上的有效性，为可部署的多语言AI系统提供了可行的解决方案。代码和数据集已公开，便于研究人员快速集成和进一步开发。

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


### [4] [FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash](https://arxiv.org/abs/2510.10872)
*Sumukh Pinge,Ashkan Moradifirouzabadi,Keming Fan,Prasanna Venkatesan Ravindran,Tanvir H. Pantha,Po-Kai Hsu,Zheyu Li,Weihong Xu,Zihan Xia,Flavio Ponzina,Winston Chern,Taeyoung Song,Priyankka Ravikumar,Mengkun Tian,Lance Fernandes,Huy Tran,Hari Jayasankar,Hang Chen,Chinsung Park,Amrit Garlapati,Kijoon Kim,Jongho Woo,Suhwan Lim,Kwangsoo Kim,Wanki Kim,Daewon Ha,Duygu Kuzum,Shimeng Yu,Sourav Dutta,Asif Khan,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: 提出了一种基于3D铁电NAND存储器和超维计算架构的质谱数据搜索方法，实现了43倍加速和21倍能效提升


<details>
  <summary>Details</summary>
Motivation: 质谱数据快速增长至数百TB规模，传统处理器难以高效处理大规模库搜索，需要新的存储计算架构

Method: 结合3D铁电NAND存储器和超维计算，采用双边界近似匹配距离度量，并行化向量计算

Result: 相比最先进的3D NAND方法，实现了43倍速度提升和21倍能效提升，同时保持相当的准确性

Conclusion: 该存储计算架构为大规模质谱数据搜索提供了高效的解决方案

Abstract: The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of
terabytes, poses significant challenges for efficient, large-scale library
search - a critical component for drug discovery. Traditional processors
struggle to handle this data volume efficiently, making in-storage computing
(ISP) a promising alternative. This work introduces an ISP architecture
leveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly
higher density, faster speeds, and lower voltage requirements compared to
traditional NAND flash. Despite its superior density, the NAND structure has
not been widely utilized in ISP applications due to limited throughput
associated with row-by-row reads from serially connected cells. To overcome
these limitations, we integrate hyperdimensional computing (HDC), a
brain-inspired paradigm that enables highly parallel processing with simple
operations and strong error tolerance. By combining HDC with the proposed
dual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND
structure, we parallelize vector computations to enable efficient MS spectral
library search, achieving 43x speedup and 21x higher energy efficiency over
state-of-the-art 3D NAND methods, while maintaining comparable accuracy.

</details>


### [5] [Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs](https://arxiv.org/abs/2510.11192)
*João Paulo Cardoso de Lima,Marc Dietrich,Jeronimo Castrillon,Asif Ali Khan*

Main category: cs.AR

TL;DR: 提出自动化框架，通过新颖的映射和调度策略在存内计算加速器上加速稀疏大语言模型推理，利用块对角稀疏性提高阵列利用率超过50%，减少4倍以上内存占用和浮点运算。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏性使大语言模型能在资源受限系统上部署，但传统冯·诺依曼架构上推理成本高昂。存内计算架构通过内存中直接计算缓解此问题，但稀疏矩阵在CIM阵列上的简单映射会导致阵列利用率低和计算效率下降。

Method: 开发自动化框架，采用新颖的映射和调度策略，利用块对角稀疏性来优化稀疏LLM在CIM加速器上的部署。

Result: CIM阵列利用率提高超过50%，内存占用和浮点运算数量均减少4倍以上。

Conclusion: 该框架成功解决了稀疏LLM在CIM加速器上的效率问题，显著提升了推理性能。

Abstract: Structured sparsity enables deploying large language models (LLMs) on
resource-constrained systems. Approaches like dense-to-sparse fine-tuning are
particularly compelling, achieving remarkable structured sparsity by reducing
the model size by over 6.7x, while still maintaining acceptable accuracy.
Despite this reduction, LLM inference, especially the decode stage being
inherently memory-bound, is extremely expensive on conventional Von-Neumann
architectures. Compute-in-memory (CIM) architectures mitigate this by
performing computations directly in memory, and when paired with sparse LLMs,
enable storing and computing the entire model in memory, eliminating the data
movement on the off-chip bus and improving efficiency. Nonetheless, naively
mapping sparse matrices onto CIM arrays leads to poor array utilization and
diminished computational efficiency. In this paper, we present an automated
framework with novel mapping and scheduling strategies to accelerate sparse LLM
inference on CIM accelerators. By exploiting block-diagonal sparsity, our
approach improves CIM array utilization by over 50%, achieving more than 4x
reduction in both memory footprint and the number of required floating-point
operations.

</details>
