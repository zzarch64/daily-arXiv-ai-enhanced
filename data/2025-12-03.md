<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis](https://arxiv.org/abs/2512.02189)
*Aaron Jarmusch,Sunita Chandrasekaran*

Main category: cs.AR

TL;DR: 开发开源微基准测试套件，系统评估NVIDIA Blackwell (B200) GPU架构创新，相比H200实现1.56倍混合精度吞吐提升和42%能效改进，内存访问延迟降低58%


<details>
  <summary>Details</summary>
Motivation: GPU架构快速发展以满足百亿亿次计算和机器学习需求，但架构创新的性能影响在不同工作负载中理解不足。Blackwell架构引入多项创新（第五代张量核心、张量内存、解压缩引擎等），但系统量化这些改进的方法滞后于硬件开发周期

Method: 开发开源微基准测试套件，系统评估Blackwell GPU并与H200比较，分析内存子系统、张量核心流水线和浮点精度（FP32到FP4），评估密集/稀疏GEMM、Transformer推理和训练工作负载

Result: B200张量核心增强实现1.56倍混合精度吞吐提升和42%能效改进；内存分析显示缓存未命中时内存访问延迟降低58%，从根本上改变了最优算法设计策略

Conclusion: 开源微基准套件为应用开发者提供实用洞察，帮助充分利用现代GPU架构特性，支持明智的架构决策并指导未来GPU设计方向

Abstract: As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.
  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.

</details>


### [2] [Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras](https://arxiv.org/abs/2512.02346)
*Hongyang Shang,An Guo,Shuai Dong,Junyi Yang,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 提出NM-TOS近内存架构，通过8T SRAM单元和流水线优化，显著降低事件相机角点检测的延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 事件相机在边缘设备上的角点检测算法（如TOS）存在高延迟问题，限制了其高速低功耗优势的发挥

Method: 设计近内存架构NM-TOS，采用读写解耦的8T SRAM单元，通过流水线优化补丁更新速度，结合硬件-软件协同优化的外围电路和DVFS技术

Result: 相比传统数字实现，在1.2V下延迟降低24.7倍/能耗降低1.2倍，在0.6V下延迟降低1.93倍/能耗降低6.6倍；蒙特卡洛仿真显示电路鲁棒性良好

Conclusion: NM-TOS架构有效解决了事件相机角点检测在边缘设备上的延迟问题，在保持检测精度的同时显著提升了能效

Abstract: Event-based Cameras (EBCs) are widely utilized in surveillance and autonomous driving applications due to their high speed and low power consumption. Corners are essential low-level features in event-driven computer vision, and novel algorithms utilizing event-based representations, such as Threshold-Ordinal Surface (TOS), have been developed for corner detection. However, the implementation of these algorithms on resource-constrained edge devices is hindered by significant latency, undermining the advantages of EBCs. To address this challenge, a near-memory architecture for efficient TOS updates (NM-TOS) is proposed. This architecture employs a read-write decoupled 8T SRAM cell and optimizes patch update speed through pipelining. Hardware-software co-optimized peripheral circuits and dynamic voltage and frequency scaling (DVFS) enable power and latency reductions. Compared to traditional digital implementations, our architecture reduces latency/energy by 24.7x/1.2x at Vdd = 1.2 V or 1.93x/6.6x at Vdd = 0.6 V based on 65nm CMOS process. Monte Carlo simulations confirm robust circuit operation, demonstrating zero bit error rate at operating voltages above 0.62 V, with only 0.2% at 0.61 V and 2.5% at 0.6 V. Corner detection evaluation using precision-recall area under curve (AUC) metrics reveals minor AUC reductions of 0.027 and 0.015 at 0.6 V for two popular EBC datasets.

</details>


### [3] [Monomorphism-based CGRA Mapping via Space and Time Decoupling](https://arxiv.org/abs/2512.02859)
*Cristian Tirelli,Rodrigo Otoni,Laura Pozzi*

Main category: cs.AR

TL;DR: 提出一种新的CGRA映射方法，通过解耦时间和空间维度分别探索，显著提升编译速度，特别在大规模CGRA上效果明显。


<details>
  <summary>Details</summary>
Motivation: 现有CGRA编译技术存在可扩展性问题，难以将代码映射到大规模CGRA上，需要更高效的编译方法。

Method: 采用解耦时间和空间维度的映射方法：首先使用SMT公式遍历时间维度，然后基于单态搜索寻找有效的空间解决方案。

Result: 在保持与最先进技术相同映射质量的同时，显著减少编译时间，特别是在大规模CGRA上效果显著。在20×20 CGRA上实现约10^5倍的平均编译加速。

Conclusion: 提出的解耦时间-空间维度映射方法有效解决了CGRA编译的可扩展性问题，在大规模CGRA上实现了显著的编译速度提升。

Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) provide flexibility and energy efficiency in accelerating compute-intensive loops. Existing compilation techniques often struggle with scalability, unable to map code onto large CGRAs. To address this, we propose a novel approach to the mapping problem where the time and space dimensions are decoupled and explored separately. We leverage an SMT formulation to traverse the time dimension first, and then perform a monomorphism-based search to find a valid spatial solution. Experimental results show that our approach achieves the same mapping quality of state-of-the-art techniques while significantly reducing compilation time, with this reduction being particularly tangible when compiling for large CGRAs. We achieve approximately $10^5\times$ average compilation speedup for the benchmarks evaluated on a $20\times 20$ CGRA.

</details>


### [4] [SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures](https://arxiv.org/abs/2512.02875)
*Cristian Tirelli,Lorenzo Ferretti,Laura Pozzi*

Main category: cs.AR

TL;DR: SAT-MapIt：使用SAT求解器为CGRA进行映射编译，相比现有方法在47.72%的基准测试中表现更好


<details>
  <summary>Details</summary>
Motivation: CGRA（粗粒度可重构阵列）是新兴的低功耗架构，用于加速计算密集型循环。但加速效果严重依赖于映射质量，现有技术使用模调度和图算法（如最大团枚举），存在改进空间。

Method: 提出基于SAT（可满足性）的映射方法：1）引入核移动性调度（KMS）作为特殊调度方案；2）结合数据流图和CGRA架构信息生成布尔约束；3）使用SAT求解器高效搜索解空间；4）采用迭代过程，如果当前II（迭代间隔）无解则增加II重新求解。

Result: SAT-MapIt在47.72%的基准测试中优于现有技术：有时找到更低的II，有时在现有方法无法找到映射的情况下找到有效映射。

Conclusion: 基于SAT的映射方法比现有图算法更有效地探索解空间，为CGRA编译提供了更优的映射解决方案。

Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.
  We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.
  Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.

</details>


### [5] [Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver](https://arxiv.org/abs/2512.02884)
*Cristian Tirelli,Laura Pozzi*

Main category: cs.AR

TL;DR: 提出基于SAT的CGRA映射方法，通过Kernel Mobility Schedule编码所有可能映射，显著减少编译时间并提高映射质量


<details>
  <summary>Details</summary>
Motivation: CGRA作为协处理器加速计算密集型循环，现有编译技术使用模调度最小化迭代间隔，但仍有改进空间，需要更高效的编译方法

Method: 使用可满足性(SAT)公式化映射问题，引入Kernel Mobility Schedule编码给定数据流图和迭代间隔的所有可能映射，结合CGRA架构信息生成约束条件

Result: 实验结果表明，该方法不仅平均减少编译时间，而且相比现有技术获得更高质量的映射

Conclusion: 提出的基于SAT的编译方法能有效找到任意拓扑结构的最低迭代间隔，在编译时间和映射质量方面优于现有技术

Abstract: Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.

</details>
