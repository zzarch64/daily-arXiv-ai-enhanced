<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Efficient Deployment of CNN Models on Multiple In-Memory Computing Units](https://arxiv.org/abs/2511.04682)
*Eleni Bougioukou,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出了Load-Balance-Longest-Path (LBLP)算法，用于在具有多个处理单元的存内计算模拟器上优化CNN模型的部署，以提高处理速率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 存内计算(IMC)通过减少数据移动瓶颈和利用内存计算的并行性来加速深度学习，但在多处理单元IMC硬件上高效部署CNN需要先进的任务分配策略。

Method: 使用IMC模拟器(IMCE)和多处理单元，开发了LBLP算法，动态地将CNN节点分配给可用处理单元，以最大化处理速率并最小化延迟。

Result: 实验结果表明，LBLP算法在多个CNN模型上相比其他调度策略表现更优，能够有效提高计算效率。

Conclusion: LBLP算法为IMC硬件上的CNN部署提供了一种有效的任务分配策略，能够显著提升处理性能并降低延迟。

Abstract: In-Memory Computing (IMC) represents a paradigm shift in deep learning
acceleration by mitigating data movement bottlenecks and leveraging the
inherent parallelism of memory-based computations. The efficient deployment of
Convolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use
of advanced task allocation strategies for achieving maximum computational
efficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple
Processing Units (PUs) for investigating how the deployment of a CNN model in a
multi-processing system affects its performance, in terms of processing rate
and latency. For that purpose, we introduce the Load-Balance-Longest-Path
(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE
PUs, for maximizing the processing rate and minimizing latency due to efficient
resources utilization. We are benchmarking LBLP against other alternative
scheduling strategies for a number of CNN models and experimental results
demonstrate the effectiveness of the proposed algorithm.

</details>


### [2] [RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression](https://arxiv.org/abs/2511.04684)
*Yuchao Qin,Anjunyi Fan,Bonan Yan*

Main category: cs.AR

TL;DR: RAS是一个硬件加速系统，通过集成rANS算法到无损压缩流水线中，解决了概率模型方法计算速度慢的问题，实现了显著的编码和解码加速。


<details>
  <summary>Details</summary>
Motivation: 数据中心处理大量数据需要高效的无损压缩，但基于概率模型的新方法通常计算速度较慢，需要硬件加速解决方案。

Method: RAS将rANS核心与概率生成器耦合，使用BF16格式存储分布并转换为固定点域，采用两阶段rANS更新和字节级重归一化，预测引导的解码路径缩小CDF搜索窗口，多通道组织扩展吞吐量。

Result: 在图像工作负载上，RTL模拟原型相比Python rANS基线实现了121.2倍编码和70.9倍解码加速，平均解码二分搜索步骤从7.00减少到3.15（约减少55%）。

Conclusion: RAS与神经概率模型配对时，比传统编解码器保持更高的压缩比，并优于CPU/GPU rANS实现，为快速神经无损压缩提供了实用方法。

Abstract: Data centers handle vast volumes of data that require efficient lossless
compression, yet emerging probabilistic models based methods are often
computationally slow. To address this, we introduce RAS, the Range Asymmetric
Numeral System Acceleration System, a hardware architecture that integrates the
rANS algorithm into a lossless compression pipeline and eliminates key
bottlenecks. RAS couples an rANS core with a probabilistic generator, storing
distributions in BF16 format and converting them once into a fixed-point domain
shared by a unified division/modulo datapath. A two-stage rANS update with
byte-level re-normalization reduces logic cost and memory traffic, while a
prediction-guided decoding path speculatively narrows the cumulative
distribution function (CDF) search window and safely falls back to maintain
bit-exactness. A multi-lane organization scales throughput and enables
fine-grained clock gating for efficient scheduling. On image workloads, our
RTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a
Python rANS baseline, reducing average decoder binary-search steps from 7.00 to
3.15 (approximately 55% fewer). When paired with neural probability models, RAS
sustains higher compression ratios than classical codecs and outperforms
CPU/GPU rANS implementations, offering a practical approach to fast neural
lossless compression.

</details>


### [3] [Eliminating the Hidden Cost of Zone Management in ZNS SSDs](https://arxiv.org/abs/2511.04687)
*Teona Bagashvili,Tarikul Islam Papon,Subhadeep Sarkar,Manos Athanassoulis*

Main category: cs.AR

TL;DR: SilentZNS是一种新的ZNS SSD区域映射和管理方法，通过动态分配资源解决传统ZNS实现中的设备级写入放大、磨损增加和主机I/O干扰问题。


<details>
  <summary>Details</summary>
Motivation: 传统ZNS SSD实现存在设备级写入放大(DLWA)、增加磨损以及与主机I/O干扰的问题，主要原因是固定物理区域和全区域操作导致过多物理写入。

Method: 提出SilentZNS方法，采用灵活的区域分配方案，动态分配可用资源到区域，避免传统逻辑到物理区域映射的限制，允许任意块集合分配给区域，同时确保磨损均衡和读取性能。

Result: SilentZNS可消除高达20倍的虚拟写入负担，减少86%的设备级写入放大(在10%区域占用时)，降低76.9%的整体磨损，工作负载执行速度提升高达3.7倍。

Conclusion: SilentZNS通过创新的区域映射和管理方法，有效解决了ZNS SSD的关键限制，显著提升了性能和耐用性。

Abstract: Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput
and low-latency storage by eliminating device-side garbage collection. They
expose storage as append-only zones that give the host applications direct
control over data placement. However, current ZNS implementations suffer from
(a) device-level write amplification (DLWA), (b) increased wear, and (c)
interference with host I/O due to zone mapping and management. We identify two
primary design decisions as the main cause: (i) fixed physical zones and (ii)
full-zone operations that lead to excessive physical writes. We propose
SilentZNS, a new zone mapping and management approach that addresses the
aforementioned limitations by on-the-fly allocating available resources to
zones, while minimizing wear, maintaining parallelism, and avoiding unnecessary
writes at the device-level. SilentZNS is a flexible zone allocation scheme that
departs from the traditional logical-to-physical zone mapping and allows for
arbitrary collections of blocks to be assigned to a zone. We add the necessary
constraints to ensure wear-leveling and state-of-the-art read performance, and
use only the required blocks to avoid dummy writes during zone reset. We
implement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that
it eliminates the undue burden of dummy writes by up to 20x, leading to lower
DLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up
to 3.7x faster workload execution.

</details>


### [4] [SMART-WRITE: Adaptive Learning-based Write Energy Optimization for Phase Change Memory](https://arxiv.org/abs/2511.04713)
*Mahek Desai,Rowena Quinn,Marjan Asadinia*

Main category: cs.AR

TL;DR: 提出SMART-WRITE方法，结合神经网络和强化学习动态优化相变存储器的写入能耗和性能，相比基准模型可减少63%写入能耗并提升51%性能。


<details>
  <summary>Details</summary>
Motivation: DRAM等传统存储器面临可扩展性限制，相变存储器(PCM)虽具潜力但存在写入寿命有限和能耗高的问题，需要提升其耐用性和降低写入能耗。

Method: 集成神经网络和强化学习，NN模型监控实时运行条件和设备特性确定最优写入参数，RL模型动态调整参数以优化PCM能耗。

Result: 通过实时调整PCM写入参数，相比基准和先前模型，写入能耗降低63%，性能提升51%。

Conclusion: SMART-WRITE方法有效解决了PCM的写入能耗和性能问题，为PCM作为数据存储替代方案提供了可行路径。

Abstract: As dynamic random access memory (DRAM) and other current transistor-based
memories approach their scalability limits, the search for alternative storage
methods becomes increasingly urgent. Phase-change memory (PCM) emerges as a
promising candidate due to its scalability, fast access time, and zero leakage
power compared to many existing memory technologies. However, PCM has
significant drawbacks that currently hinder its viability as a replacement. PCM
cells suffer from a limited lifespan because write operations degrade the
physical material, and these operations consume a considerable amount of
energy. For PCM to be a practical option for data storage-which involves
frequent write operations-its cell endurance must be enhanced, and write energy
must be reduced. In this paper, we propose SMART-WRITE, a method that
integrates neural networks (NN) and reinforcement learning (RL) to dynamically
optimize write energy and improve performance. The NN model monitors real-time
operating conditions and device characteristics to determine optimal write
parameters, while the RL model dynamically adjusts these parameters to further
optimize PCM's energy consumption. By continuously adjusting PCM write
parameters based on real-time system conditions, SMART-WRITE reduces write
energy consumption by up to 63% and improves performance by up to 51% compared
to the baseline and previous models.

</details>


### [5] [MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars](https://arxiv.org/abs/2511.04798)
*Matheus Farias,Wanghley Martins,H. T. Kung*

Main category: cs.AR

TL;DR: MDM是一种后训练DNN权重映射技术，通过优化有源忆阻器位置来减少忆阻计算内存交叉阵列中的寄生电阻非理想性，提高模拟精度。


<details>
  <summary>Details</summary>
Motivation: 寄生电阻限制了交叉阵列效率，需要将DNN矩阵映射到小尺寸交叉阵列瓦片中，这降低了CIM加速效果并增加了数字同步、ADC转换、延迟和芯片面积的开销。

Method: 利用比特级结构化稀疏性，从密度较高的低阶侧输入激活，并根据曼哈顿距离重新排列行，将有源单元重新定位到受寄生电阻影响较小的区域。

Result: 在ImageNet-1k上的DNN模型中，MDM将非理想性因子降低高达46%，在ResNets中模拟失真下的准确率平均提高3.6%。

Conclusion: MDM提供了一种轻量级、空间感知的方法来扩展CIM DNN加速器。

Abstract: Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)
weight mapping technique for memristive bit-sliced compute-in-memory (CIM)
crossbars that reduces parasitic resistance (PR) nonidealities.
  PR limits crossbar efficiency by mapping DNN matrices into small crossbar
tiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring
digital synchronization before the next layer. At this granularity, designers
either deploy many small crossbars in parallel or reuse a few sequentially-both
increasing analog-to-digital conversions, latency, I/O pressure, and chip area.
  MDM alleviates PR effects by optimizing active-memristor placement.
Exploiting bit-level structured sparsity, it feeds activations from the denser
low-order side and reorders rows according to the Manhattan distance,
relocating active cells toward regions less affected by PR and thus lowering
the nonideality factor (NF).
  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and
improves accuracy under analog distortion by an average of 3.6% in ResNets.
Overall, it provides a lightweight, spatially informed method for scaling CIM
DNN accelerators.

</details>


### [6] [MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for Neural Network Inference](https://arxiv.org/abs/2511.05321)
*Maximilian Kirschner,Konstantin Dudzik,Ben Krusekamp,Jürgen Becker*

Main category: cs.AR

TL;DR: 提出了一种新的硬件架构，通过多核向量处理器和本地暂存内存来解决AI加速器在实时系统中性能和可预测性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 实时系统（如自动驾驶）越来越多地采用神经网络，需要兼具高性能和可预测时序行为的硬件。现有实时硬件资源有限，而现代AI加速器由于内存干扰缺乏可预测性。

Method: 设计多核向量处理器架构，每个核心配备本地暂存内存，由中央管理核心按照静态调度表协调共享外部内存访问。

Result: 与单核大向量寄存器基线架构相比，配置更多小核心的变体由于有效内存带宽增加和时钟频率更高，实现了更好的性能。执行时间波动非常低，证明了平台的时间可预测性。

Conclusion: 该架构成功弥合了性能和可预测性之间的差距，为实时系统中的神经网络应用提供了理想的硬件解决方案。

Abstract: Real-time systems, particularly those used in domains like automated driving,
are increasingly adopting neural networks. From this trend arises the need for
high-performance hardware exhibiting predictable timing behavior. While
state-of-the-art real-time hardware often suffers from limited memory and
compute resources, modern AI accelerators typically lack the crucial
predictability due to memory interference.
  We present a new hardware architecture to bridge this gap between performance
and predictability. The architecture features a multi-core vector processor
with predictable cores, each equipped with local scratchpad memories. A central
management core orchestrates access to shared external memory following a
statically determined schedule.
  To evaluate the proposed hardware architecture, we analyze different variants
of our parameterized design. We compare these variants to a baseline
architecture consisting of a single-core vector processor with large vector
registers. We find that configurations with a larger number of smaller cores
achieve better performance due to increased effective memory bandwidth and
higher clock frequencies. Crucially for real-time systems, execution time
fluctuation remains very low, demonstrating the platform's time predictability.

</details>
