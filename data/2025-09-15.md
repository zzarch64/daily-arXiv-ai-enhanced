<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Towards An Approach to Identify Divergences in Hardware Designs for HPC Workloads](https://arxiv.org/abs/2509.09774)
*Doru Thom Popovici,Mario Vega,Angelos Ioannou,Fabien Chaix,Dania Mosuli,Blair Reasoner,Tan Nguyen,Xiaokun Yang,John Shalf*

Main category: cs.AR

TL;DR: 提出了一种分层分解数学核到基本原语的方法，在不同编程环境中实现这些原语并组装算法，通过自动化方法比较硬件设计效率和资源使用


<details>
  <summary>Details</summary>
Motivation: 传统硬件加速器开发需要低层Verilog编程和大量手动优化，虽然出现了高级硬件设计工具，但生成的硬件可能不如专家设计优化，需要理解效率差距的来源

Method: 将数学核（如傅里叶变换、矩阵乘法、QR分解）分层分解为通用构建块/原语，在不同编程环境中实现这些原语并组装算法，采用自动化方法分析可达到的频率和所需资源

Result: 通过在各个层级进行实验，可以提供更公平的设计比较，并为工具开发者和硬件设计者提供更好的实践指导

Conclusion: 该方法论有助于识别硬件设计中的效率问题，为改进高级硬件设计工具和优化实践提供了有价值的见解

Abstract: Developing efficient hardware accelerators for mathematical kernels used in
scientific applications and machine learning has traditionally been a
labor-intensive task. These accelerators typically require low-level
programming in Verilog or other hardware description languages, along with
significant manual optimization effort. Recently, to alleviate this challenge,
high-level hardware design tools like Chisel and High-Level Synthesis have
emerged. However, as with any compiler, some of the generated hardware may be
suboptimal compared to expert-crafted designs. Understanding where these
inefficiencies arise is crucial, as it provides valuable insights for both
users and tool developers. In this paper, we propose a methodology to
hierarchically decompose mathematical kernels - such as Fourier transforms,
matrix multiplication, and QR factorization - into a set of common building
blocks or primitives. Then the primitives are implemented in the different
programming environments, and the larger algorithms get assembled. Furthermore,
we employ an automatic approach to investigate the achievable frequency and
required resources. Performing this experimentation at each level will provide
fairer comparisons between designs and offer guidance for both tool developers
and hardware designers to adopt better practices.

</details>


### [2] [Finesse: An Agile Design Framework for Pairing-based Cryptography via Software/Hardware Co-Design](https://arxiv.org/abs/2509.10051)
*Tianwei Pan,Tianao Dai,Jianlei Yang,Hongbin Jing,Yang Su,Zeyu Hao,Xiaotao Jia,Chunming Hu,Weisheng Zhao*

Main category: cs.AR

TL;DR: Finesse是一个基于协同设计方法的敏捷设计框架，用于配对密码学加速器，通过编译器驱动的协同优化和模块化设计流程，显著缩短设计周期并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统配对密码学加速器设计方法面临设计周期长、性能与灵活性难以平衡、架构探索支持不足等挑战，需要新的设计框架来解决这些问题。

Method: 采用协同设计方法，通过专用编译器和多粒度硬件模拟器驱动的协同优化循环，结合模块化设计流程和通用抽象，支持不同曲线族和硬件架构。

Result: 编译时间缩短至分钟级，在流行曲线上实现34倍吞吐量提升和6.2倍面积效率提升，相比非灵活ASIC设计也有3倍吞吐量和3.2倍面积效率优势。

Conclusion: Finesse框架成功解决了配对密码学加速器设计中的关键挑战，提供了灵活性、高效性和快速原型开发能力，为未来密码学硬件设计提供了有效解决方案。

Abstract: Pairing-based cryptography (PBC) is crucial in modern cryptographic
applications. With the rapid advancement of adversarial research and the
growing diversity of application requirements, PBC accelerators need regular
updates in algorithms, parameter configurations, and hardware design. However,
traditional design methodologies face significant challenges, including
prolonged design cycles, difficulties in balancing performance and flexibility,
and insufficient support for potential architectural exploration.
  To address these challenges, we introduce Finesse, an agile design framework
based on co-design methodology. Finesse leverages a co-optimization cycle
driven by a specialized compiler and a multi-granularity hardware simulator,
enabling both optimized performance metrics and effective design space
exploration. Furthermore, Finesse adopts a modular design flow to significantly
shorten design cycles, while its versatile abstraction ensures flexibility
across various curve families and hardware architectures.
  Finesse offers flexibility, efficiency, and rapid prototyping, comparing with
previous frameworks. With compilation times reduced to minutes, Finesse enables
faster iteration cycles and streamlined hardware-software co-design.
Experiments on popular curves demonstrate its effectiveness, achieving
$34\times$ improvement in throughput and $6.2\times$ increase in area
efficiency compared to previous flexible frameworks, while outperforming
state-of-the-art non-flexible ASIC designs with a $3\times$ gain in throughput
and $3.2\times$ improvement in area efficiency.

</details>


### [3] [MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging Bit-Slice-enabled Sparsity and Repetitiveness](https://arxiv.org/abs/2509.10372)
*Huizheng Wang,Zichuan Wang,Zhiheng Yue,Yousheng Long,Taiquan Wei,Jianxun Yang,Yang Wang,Chao Li,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: MCBP是一种基于比特粒度的算法-硬件协同设计，通过利用比特切片的重复杂性和稀疏性来加速大语言模型推理，在计算和内存效率方面都有显著提升


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时面临GEMM操作、权重访问和KV缓存访问的低效问题，现有Transformer加速器难以同时优化计算和内存效率，需要一种新的协同设计方法

Method: 提出三种关键技术：1) BS重复杂性启发的计算减少(BRCR) - 通过比特切片向量间的冗余消除GEMM计算；2) BS稀疏性启发的双状态编码(BSTC) - 利用高阶比特切片权重的稀疏性减少权重访问；3) 比特粒度渐进预测(BGPP) - 基于早期终止的比特粒度预测减少KV缓存访问

Result: 在26个基准测试中，MCBP相比Nvidia A100 GPU实现了9.43倍加速和31.1倍能效提升；相比SOTA Transformer加速器，分别比Spatten、FACT和SOFA节省35倍、5.2倍和3.2倍能耗

Conclusion: MCBP通过比特粒度的算法-硬件协同设计，有效解决了LLM推理中的计算和内存瓶颈，为实时场景下的大语言模型推理提供了高效的加速解决方案

Abstract: Large language models (LLMs) face significant inference latency due to
inefficiencies in GEMM operations, weight access, and KV cache access,
especially in real-time scenarios. This highlights the need for a versatile
compute-memory efficient accelerator. Unfortunately, existing Transformer
accelerators struggle to address both aspects simultaneously, as they focus on
value-level processing, missing fine-grained opportunities to optimize
computation and memory collaboratively. This paper introduces MCBP, a
bit-grained compute-memory efficient algorithm-hardware co-design that
leverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM
inference. MCBP features three key innovations: 1) BS-repetitiveness-enabled
computation reduction (BRCR), which eliminates redundant GEMM computations via
leveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state
coding (BSTC), which reduces weight access via exploiting significant sparsity
in high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),
which reduces KV cache access by leveraging early-termination-based bit-grained
prediction. These techniques, supported by custom accelerator designs,
effectively alleviate the burden in GEMM, weight access, and KV cache access.
Extensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up
and 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA
Transformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than
Spatten, FACT and SOFA, respectively.

</details>


### [4] [TurboFuzz: FPGA Accelerated Hardware Fuzzing for Processor Agile Verification](https://arxiv.org/abs/2509.10400)
*Yang Zhong,Haoran Wu,Xueqi Li,Sa Wang,David Boland,Yungang Bao,Kan Shi*

Main category: cs.AR

TL;DR: TurboFuzz是一个端到端的硬件加速验证框架，将整个测试生成-仿真-覆盖率反馈循环在单个FPGA上实现，相比软件模糊测试器在相同时间内获得2.23倍覆盖率，检测真实问题时性能提升571倍


<details>
  <summary>Details</summary>
Motivation: 现代处理器设计复杂性和新兴ISA（如RISC-V）对敏捷高效验证方法的需求增加，传统仿真方法性能差、测试用例质量不足，硬件加速方案存在通信开销大、测试模式生成效率低等问题

Method: 在单个FPGA上实现完整的测试生成-仿真-覆盖率反馈循环，通过优化的测试用例控制流、高效的种子间调度和混合模糊器集成来提高测试质量，采用反馈驱动的生成机制加速覆盖率收敛

Result: 在相同时间预算内比基于软件的模糊测试器多获得2.23倍覆盖率，检测真实世界问题时性能提升高达571倍，同时保持完全可见性和调试能力，面积开销适中

Conclusion: TurboFuzz提供了一个高效的硬件加速验证解决方案，显著提升了处理器验证的覆盖率和性能，同时保持了良好的调试能力

Abstract: Verification is a critical process for ensuring the correctness of modern
processors. The increasing complexity of processor designs and the emergence of
new instruction set architectures (ISAs) like RISC-V have created demands for
more agile and efficient verification methodologies, particularly regarding
verification efficiency and faster coverage convergence. While simulation-based
approaches now attempt to incorporate advanced software testing techniques such
as fuzzing to improve coverage, they face significant limitations when applied
to processor verification, notably poor performance and inadequate test case
quality. Hardware-accelerated solutions using FPGA or ASIC platforms have tried
to address these issues, yet they struggle with challenges including host-FPGA
communication overhead, inefficient test pattern generation, and suboptimal
implementation of the entire multi-step verification process.
  In this paper, we present TurboFuzz, an end-to-end hardware-accelerated
verification framework that implements the entire Test
Generation-Simulation-Coverage Feedback loop on a single FPGA for modern
processor verification. TurboFuzz enhances test quality through optimized test
case (seed) control flow, efficient inter-seed scheduling, and hybrid fuzzer
integration, thereby improving coverage and execution efficiency. Additionally,
it employs a feedback-driven generation mechanism to accelerate coverage
convergence. Experimental results show that TurboFuzz achieves up to 2.23x more
coverage collection than software-based fuzzers within the same time budget,
and up to 571x performance speedup when detecting real-world issues, while
maintaining full visibility and debugging capabilities with moderate area
overhead.

</details>
