<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Privacy-Preserving Performance Profiling of In-The-Wild GPUs](https://arxiv.org/abs/2509.21762)
*Ian McDougall,Michael Davies,Rahul Chatterjee,Somesh Jha,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 本文提出了一种行星级实时GPU性能分析系统，解决了大规模GPU部署中的性能数据收集问题，同时确保零性能影响和用户隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着GPU应用和硬件复杂度的增加，制造商需要从真实部署环境中收集全面的性能特征数据来优化芯片设计和应用性能，但现有工具只能提供单个用户的数据，缺乏大规模部署的全局视角。

Method: 开发了一个行星级实时GPU性能分析系统，解决了三个关键问题：零性能影响的用户体验、保护用户隐私（第三方无法获知用户运行的应用）、在数千GPU上有效收集数据并关联到具体应用。

Result: 在模拟10万规模GPU部署环境中运行Torchbench套件应用，系统成功解决了所有三个核心问题，实现了大规模GPU性能数据的有效收集。

Conclusion: 该系统能够在大规模GPU部署中实现实时性能分析，同时保证零性能影响和用户隐私保护，为GPU芯片设计和应用优化提供了有价值的数据支持。

Abstract: GPUs are the dominant platform for many important applications today
including deep learning, accelerated computing, and scientific simulation.
However, as the complexity of both applications and hardware increases, GPU
chip manufacturers face a significant challenge: how to gather comprehensive
performance characteristics and value profiles from GPUs deployed in real-world
scenarios. Such data, encompassing the types of kernels executed and the time
spent in each, is crucial for optimizing chip design and enhancing application
performance. Unfortunately, despite the availability of low-level tools like
NSYS and NCU, current methodologies fall short, offering data collection
capabilities only on an individual user basis rather than a broader, more
informative fleet-wide scale. This paper takes on the problem of realizing a
system that allows planet-scale real-time GPU performance profiling of
low-level hardware characteristics. The three fundamental problems we solve
are: i) user experience of achieving this with no slowdown; ii) preserving user
privacy, so that no 3rd party is aware of what applications any user runs; iii)
efficacy in showing we are able to collect data and assign it applications even
when run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,
running applications from the Torchbench suite, showing our system addresses
all 3 problems.

</details>


### [2] [NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction](https://arxiv.org/abs/2509.22410)
*Shayne Wadle,Yanxin Zhang,Vikas Singh,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 提出基于深度学习的高保真微处理器仿真框架，可在现有硬件上评估未来处理器设计，实现5 MIPS仿真速度，仅产生0.1%性能开销。


<details>
  <summary>Details</summary>
Motivation: 传统微处理器设计评估受限于缓慢的周期精确仿真器和缺乏代表性的基准测试，需要更高效的仿真方法。

Method: 使用深度学习模型，基于微架构无关特征预测周期级性能，结合轻量级硬件追踪收集器和采样策略。

Result: 在商用GPU上实现5 MIPS仿真速度，仅0.1%性能开销；专用Neutrino加速器性能提升85倍；支持大规模硬件A/B测试。

Conclusion: 该框架能够在真实应用环境中进行准确性能分析和大规模硬件评估，显著提升微处理器设计效率。

Abstract: The evaluation of new microprocessor designs is constrained by slow,
cycle-accurate simulators that rely on unrepresentative benchmark traces. This
paper introduces a novel deep learning framework for high-fidelity,
``in-the-wild'' simulation on production hardware. Our core contribution is a
DL model trained on microarchitecture-independent features to predict
cycle-level performance for hypothetical processor designs. This unique
approach allows the model to be deployed on existing silicon to evaluate future
hardware. We propose a complete system featuring a lightweight hardware trace
collector and a principled sampling strategy to minimize user impact. This
system achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a
mere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip
accelerator improves performance by 85x over the GPU. We demonstrate that this
framework enables accurate performance analysis and large-scale hardware A/B
testing on a massive scale using real-world applications.

</details>


### [3] [AxLLM: accelerator architecture for large language models with computation reuse capability](https://arxiv.org/abs/2509.22512)
*Soroush Ahadi,Mehdi Modarressi,Masoud Daneshtalab*

Main category: cs.AR

TL;DR: AxLLM是一种针对量化LLM的硬件加速器架构，通过消除冗余计算实现高效部署，可减少90%计算量，能耗降低28%，速度提升1.7倍


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要大量计算和内存资源，部署效率面临挑战。量化不仅能减小模型尺寸，还能增加参数局部性，为计算重用创造机会

Method: 提出AxLLM硬件加速器架构，采用新颖的冗余消除技术，缓存并重用重复权重值的乘法结果。架构具有双乘法和重用流水线，支持基础模型和LoRA微调模型

Result: 实验结果显示AxLLM可减少高达90%的计算量，能耗降低28%，速度比基线执行快1.7倍

Conclusion: AxLLM是专门硬件上加速LLM的可扩展高效解决方案，无需改变参数、重新训练或离线预处理

Abstract: Large language models demand massive computational power and memory
resources, posing significant challenges for efficient deployment. While
quantization has been widely explored to reduce model size and computation,
this paper demonstrates an additional benefit: quantization increases parameter
locality, creating opportunities for computation reuse. Building on this
insight, we propose AxLLM, a hardware accelerator architecture designed for
quantized models. Axllm introduces a novel redundancy elimination technique
that caches and reuses multiplication results for repeated weight values,
substantially reducing redundant operations. The architecture features dual
multiply and reuse pipelines, efficiently supporting both base models and LoRA
fine-tuned models without altering parameters, retraining, or requiring offline
preprocessing. Experimental results show that AxLLM achieves up to 90%
reduction in computations, delivering 28% lower energy consumption and a 1.7x
speedup over baseline execution. These results highlight Axllm as a scalable
and efficient solution for accelerating LLMs on specialized hardware.

</details>
