{"id": "2512.22131", "categories": ["cs.AR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.22131", "abs": "https://arxiv.org/abs/2512.22131", "authors": ["Sheng Lu", "Qianhou Qu", "Sungyong Jung", "Qilian Liang", "Chenyun Pan"], "title": "An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator", "comment": null, "summary": "Stochastic computing (SC) offers significant reductions in hardware complexity for traditional convolutional neural networks (CNNs), but stochastic computing neural networks (SCNNs) still suffer from high resource usage due to components such as stochastic number generators (SNGs) and accumulative parallel counters (APCs), which limit performance. This paper introduces a novel SCNN architecture based on reconfigurable field-effect transistors (RFETs), whose device-level reconfigurability enables the design of highly efficient and compact SNGs, APCs, and other core modules. A dedicated SCNN accelerator architecture is also developed for system-level simulation. Using publicly available open-source standard cell libraries, experimental results show that the proposed RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u91cd\u6784\u573a\u6548\u5e94\u6676\u4f53\u7ba1(RFET)\u7684\u968f\u673a\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc(SCNN)\u67b6\u6784\uff0c\u901a\u8fc7\u5668\u4ef6\u7ea7\u53ef\u91cd\u6784\u6027\u8bbe\u8ba1\u9ad8\u6548\u7d27\u51d1\u7684\u968f\u673a\u6570\u751f\u6210\u5668\u548c\u7d2f\u52a0\u5e76\u884c\u8ba1\u6570\u5668\uff0c\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u968f\u673a\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u80fd\u964d\u4f4e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u786c\u4ef6\u590d\u6742\u5ea6\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u968f\u673a\u6570\u751f\u6210\u5668\u548c\u7d2f\u52a0\u5e76\u884c\u8ba1\u6570\u5668\u7b49\u9ad8\u8d44\u6e90\u6d88\u8017\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u5229\u7528RFET\u7684\u5668\u4ef6\u7ea7\u53ef\u91cd\u6784\u7279\u6027\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7d27\u51d1\u7684\u968f\u673a\u6570\u751f\u6210\u5668\u3001\u7d2f\u52a0\u5e76\u884c\u8ba1\u6570\u5668\u7b49\u6838\u5fc3\u6a21\u5757\uff0c\u5e76\u5f00\u53d1\u4e13\u7528\u7684SCNN\u52a0\u901f\u5668\u67b6\u6784\u8fdb\u884c\u7cfb\u7edf\u7ea7\u4eff\u771f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u76f8\u540c\u6280\u672f\u8282\u70b9\u4e0b\uff0c\u57fa\u4e8eRFET\u7684SCNN\u52a0\u901f\u5668\u76f8\u6bd4FinFET\u8bbe\u8ba1\u5728\u9762\u79ef\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u5747\u5b9e\u73b0\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "RFET\u6280\u672f\u4e3a\u968f\u673a\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u786c\u4ef6\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u5668\u4ef6\u7ea7\u53ef\u91cd\u6784\u6027\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u7d27\u51d1\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u8bbe\u8ba1\u3002"}}
{"id": "2512.22435", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22435", "abs": "https://arxiv.org/abs/2512.22435", "authors": ["Zining Wang", "Jian Gao", "Weimin Fu", "Xiaolong Guo", "Xuan Zhang"], "title": "AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience", "comment": null, "summary": "Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\\times$ overall pass rate, a 48$\\times$ Pass@1, and a 4$\\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.", "AI": {"tldr": "AnalogSAGE\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u81ea\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u548c\u4e09\u9636\u6bb5\u667a\u80fd\u4f53\u63a2\u7d22\u5b9e\u73b0\u6a21\u62df\u7535\u8def\u81ea\u52a8\u5316\u8bbe\u8ba1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u901a\u8fc7\u7387\u548c\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u4e25\u91cd\u4f9d\u8d56\u4eba\u7c7b\u7ecf\u9a8c\u548c\u76f4\u89c9\uff0c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u63d0\u793a\u9a71\u52a8\u7684\u7f51\u8868\u751f\u6210\u6216\u9884\u5b9a\u4e49\u62d3\u6251\u6a21\u677f\uff0c\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u89c4\u683c\u8981\u6c42\u3002", "method": "\u63d0\u51faAnalogSAGE\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u5206\u5c42\u8bb0\u5fc6\u5c42\u534f\u8c03\u4e09\u9636\u6bb5\u667a\u80fd\u4f53\u63a2\u7d22\uff0c\u5229\u7528\u4eff\u771f\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u652f\u6301\u53ef\u91cd\u73b0\u6027\u548c\u901a\u7528\u6027\u3002", "result": "\u5728SKY130 PDK\u548cngspice\u73af\u5883\u4e0b\uff0cAnalogSAGE\u76f8\u6bd4\u73b0\u6709\u6846\u67b6\u5b9e\u73b0\u4e8610\u500d\u6574\u4f53\u901a\u8fc7\u7387\u300148\u500dPass@1\uff0c\u5e76\u5c06\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u51cf\u5c114\u500d\u3002", "conclusion": "\u5206\u5c42\u8bb0\u5fc6\u548c\u57fa\u4e8e\u4eff\u771f\u7684\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u53ef\u9760\u6027\u548c\u81ea\u4e3b\u6027\uff0c\u4e3a\u590d\u6742\u89c4\u683c\u9a71\u52a8\u7684\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.23062", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.23062", "abs": "https://arxiv.org/abs/2512.23062", "authors": ["Soham Pramanik", "Vimal William", "Arnab Raha", "Debayan Das", "Amitava Mukherjee", "Janet L. Paluh"], "title": "TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators", "comment": null, "summary": "The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.", "AI": {"tldr": "TYTAN\u662f\u4e00\u79cd\u57fa\u4e8e\u6cf0\u52d2\u7ea7\u6570\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u5f15\u64ce\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u786c\u4ef6\u8bbe\u8ba1\u548c\u52a8\u6001\u8fd1\u4f3c\u7b97\u6cd5\u4f18\u5316\u8fb9\u7f18AI\u63a8\u7406\u4e2d\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u5b9e\u73b02\u500d\u6027\u80fd\u63d0\u5347\u300156%\u529f\u8017\u964d\u4f4e\u548c35\u500d\u9762\u79ef\u51cf\u5c11\u3002", "motivation": "\u8fb9\u7f18AI\u90e8\u7f72\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u548c\u80fd\u8017\u7b49\u8d44\u6e90\u7ea6\u675f\uff0c\u7279\u522b\u662fGEMM\u548c\u6fc0\u6d3b\u51fd\u6570\u7b49\u9ad8\u529f\u8017\u64cd\u4f5c\u9700\u8981\u4f18\u5316\u3002\u9700\u8981\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u7684\u9886\u57df\u7279\u5b9a\u67b6\u6784\u6765\u63d0\u5347AI\u63a8\u7406\u7684\u52a0\u901f\u548c\u80fd\u6548\u3002", "method": "\u63d0\u51faTYTAN\u7cfb\u7edf\uff0c\u5305\u542b\u53ef\u91cd\u6784\u786c\u4ef6\u8bbe\u8ba1\u548c\u4e13\u95e8\u7b97\u6cd5\uff0c\u901a\u8fc7\u6cf0\u52d2\u7ea7\u6570\u52a8\u6001\u4f30\u8ba1\u6bcf\u4e2a\u6fc0\u6d3b\u51fd\u6570\u6240\u9700\u8fd1\u4f3c\uff0c\u5b9e\u73b0\u6700\u5c0f\u5316\u4e0e\u57fa\u51c6\u7cbe\u5ea6\u7684\u504f\u5dee\u3002", "result": "\u5728Silvaco FreePDK45\u5de5\u827a\u8282\u70b9\u4e0a\u9a8c\u8bc1\uff0c\u65f6\u949f\u9891\u7387>950MHz\uff0c\u76f8\u6bd4NVDLA\u57fa\u51c6\u5b9e\u73b0\uff1a\u6027\u80fd\u63d0\u5347\u7ea62\u500d\uff0c\u529f\u8017\u964d\u4f4e56%\uff0c\u9762\u79ef\u51cf\u5c1135\u500d\u3002", "conclusion": "TYTAN\u80fd\u6709\u6548\u652f\u6301\u8fb9\u7f18AI\u63a8\u7406\u7684\u52a0\u901f\u548c\u80fd\u6548\u4f18\u5316\uff0c\u901a\u8fc7\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u4e86\u6fc0\u6d3b\u51fd\u6570\u8ba1\u7b97\u7684\u9ad8\u529f\u8017\u95ee\u9898\u3002"}}
