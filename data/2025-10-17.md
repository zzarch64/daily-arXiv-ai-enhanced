<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [DIAMOND: Systolic Array Acceleration of Sparse Matrix Multiplication for Quantum Simulation](https://arxiv.org/abs/2510.14172)
*Yuchao Su,Srikar Chundury,Jiajia Li,Frank Mueller*

Main category: cs.AR

TL;DR: 提出了首个针对对角线优化的量子模拟加速器\name，通过利用哈密顿矩阵中常见的对角线结构，采用重构的脉动阵列数据流将稀疏矩阵转换为密集计算，显著提升了经典哈密顿模拟的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 哈密顿模拟是量子计算中的关键工作负载，但由于希尔伯特空间维度随量子比特数指数增长，矩阵指数运算变得计算昂贵。现有加速器主要针对机器学习工作负载设计，无法有效处理哈密顿模拟中常见的结构化对角线稀疏模式。

Method: 利用哈密顿矩阵中常见的对角线结构，采用重构的脉动阵列数据流，将稀疏矩阵转换为密集计算，提高利用率和性能。

Result: 在HamLib多样化基准测试中，\name相比SIGMA、外积算法和Gustavson算法分别实现了10.26倍、33.58倍和53.15倍的平均性能提升，峰值加速比达127.03倍，同时能耗平均降低471.55倍，最高降低4630.58倍。

Conclusion: \name通过专门针对哈密顿模拟中的对角线稀疏模式进行优化，显著提升了经典哈密顿模拟的可扩展性和能效，为量子系统研究和量子设备验证提供了高效的解决方案。

Abstract: Hamiltonian simulation is a key workload in quantum computing, enabling the
study of complex quantum systems and serving as a critical tool for classical
verification of quantum devices. However, it is computationally challenging
because the Hilbert space dimension grows exponentially with the number of
qubits. The growing dimensions make matrix exponentiation, the key kernel in
Hamiltonian simulations, increasingly expensive. Matrix exponentiation is
typically approximated by the Taylor series, which contains a series of matrix
multiplications. Since Hermitian operators are often sparse, sparse matrix
multiplication accelerators are essential for improving the scalability of
classical Hamiltonian simulation. Yet, existing accelerators are primarily
designed for machine learning workloads and tuned to their characteristic
sparsity patterns, which differ fundamentally from those in Hamiltonian
simulations that are often dominated by structured diagonals.
  In this work, we present \name, the first diagonal-optimized quantum
simulation accelerator. It exploits the diagonal structure commonly found in
problem-Hamiltonian (Hermitian) matrices and leverages a restructured systolic
array dataflow to transform diagonally sparse matrices into dense computations,
enabling high utilization and performance. Through detailed cycle-level
simulation of diverse benchmarks in HamLib, \name{} demonstrates average
performance improvements of $10.26\times$, $33.58\times$, and $53.15\times$
over SIGMA, Outer Product, and Gustavson's algorithm, respectively, with peak
speedups up to $127.03\times$ while reducing energy consumption by an average
of $471.55\times$ and up to $4630.58\times$ compared to SIGMA.

</details>


### [2] [Computing-In-Memory Aware Model Adaption For Edge Devices](https://arxiv.org/abs/2510.14379)
*Ming-Han Lin,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: 提出两阶段CIM感知模型适配方法，通过模型压缩、资源重分配和量化感知训练，解决CIM宏尺寸和ADC精度限制，提升吞吐量和精度。


<details>
  <summary>Details</summary>
Motivation: CIM宏在深度学习加速中面临宏尺寸限制和ADC精度问题，导致吞吐量和精度瓶颈，需要优化模型适配方法。

Method: 第一阶段：基于层重要性和宏尺寸约束压缩模型并重分配资源；第二阶段：量化感知训练，包含部分和量化及ADC精度考虑。

Result: CIM阵列利用率达90%，支持256个字线并发激活，压缩率达93%，精度与先前方法相当。

Conclusion: 该方法有效提升CIM宏的资源利用率和性能，同时保持模型精度。

Abstract: Computing-in-Memory (CIM) macros have gained popularity for deep learning
acceleration due to their highly parallel computation and low power
consumption. However, limited macro size and ADC precision introduce throughput
and accuracy bottlenecks. This paper proposes a two-stage CIM-aware model
adaptation process. The first stage compresses the model and reallocates
resources based on layer importance and macro size constraints, reducing model
weight loading latency while improving resource utilization and maintaining
accuracy. The second stage performs quantization-aware training, incorporating
partial sum quantization and ADC precision to mitigate quantization errors in
inference. The proposed approach enhances CIM array utilization to 90\%,
enables concurrent activation of up to 256 word lines, and achieves up to 93\%
compression, all while preserving accuracy comparable to previous methods.

</details>


### [3] [Low Power Vision Transformer Accelerator with Hardware-Aware Pruning and Optimized Dataflow](https://arxiv.org/abs/2510.14393)
*Ching-Lin Hsiung,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: 提出了一种低功耗视觉Transformer加速器，通过算法-硬件协同设计优化，针对短token长度的视觉Transformer中FFN成为计算瓶颈的问题，采用动态token剪枝、ReLU替换GELU和动态FFN2剪枝等技术，显著减少计算量和权重，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer加速器主要优化自注意力机制，但对于token长度较短的视觉Transformer，前馈网络(FFN)往往成为主要计算瓶颈，需要专门优化。

Method: 采用算法-硬件协同设计：1)硬件友好的动态token剪枝；2)用ReLU替换GELU激活函数；3)动态FFN2剪枝；4)硬件采用行向数据流和输出导向数据访问，消除数据转置。

Result: 实现了61.5%的计算操作减少和59.3%的FFN2权重减少，精度损失小于2%。在28nm工艺下，设计占用496.4K门和232KB SRAM，峰值吞吐量1024 GOPS@1GHz，能效2.31 TOPS/W，面积效率858.61 GOPS/mm²。

Conclusion: 该工作证明了针对视觉Transformer中FFN瓶颈的优化策略的有效性，通过算法-硬件协同设计实现了高性能、低功耗的加速器解决方案。

Abstract: Current transformer accelerators primarily focus on optimizing self-attention
due to its quadratic complexity. However, this focus is less relevant for
vision transformers with short token lengths, where the Feed-Forward Network
(FFN) tends to be the dominant computational bottleneck. This paper presents a
low power Vision Transformer accelerator, optimized through algorithm-hardware
co-design. The model complexity is reduced using hardware-friendly dynamic
token pruning without introducing complex mechanisms. Sparsity is further
improved by replacing GELU with ReLU activations and employing dynamic FFN2
pruning, achieving a 61.5\% reduction in operations and a 59.3\% reduction in
FFN2 weights, with an accuracy loss of less than 2\%. The hardware adopts a
row-wise dataflow with output-oriented data access to eliminate data
transposition, and supports dynamic operations with minimal area overhead.
Implemented in TSMC's 28nm CMOS technology, our design occupies 496.4K gates
and includes a 232KB SRAM buffer, achieving a peak throughput of 1024 GOPS at
1GHz, with an energy efficiency of 2.31 TOPS/W and an area efficiency of 858.61
GOPS/mm2.

</details>


### [4] [ColumnDisturb: Understanding Column-based Read Disturbance in Real DRAM Chips and Implications for Future Systems](https://arxiv.org/abs/2510.14750)
*İsmail Emir Yüksel,Ataberk Olgun,F. Nisa Bostancı,Haocong Luo,A. Giray Yağlıkçı,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文发现并实验验证了DRAM芯片中广泛存在的列干扰现象ColumnDisturb，该现象通过重复打开或保持DRAM行激活，能够通过列（位线）干扰共享相同列的DRAM单元，导致比特翻转。


<details>
  <summary>Details</summary>
Motivation: 研究DRAM芯片中除了已知的RowHammer/RowPress行干扰之外的新干扰现象，揭示列干扰对DRAM可靠性的影响及其随技术节点缩小而加剧的趋势。

Method: 使用216个DDR4和4个HBM2芯片进行严格实验表征，在多种操作条件下测试ColumnDisturb特性，获得27个关键实验观察结果。

Result: ColumnDisturb影响所有三大制造商的芯片，技术节点越小影响越严重；在标准DDR4刷新窗口内即可引发比特翻转；影响的行数比保留故障多198倍。

Conclusion: ColumnDisturb现象广泛存在且随技术发展加剧，对基于保留时间异质性的刷新机制构成严重挑战，需要在未来DRAM设计中加以考虑。

Abstract: We experimentally demonstrate a new widespread read disturbance phenomenon,
ColumnDisturb, in real commodity DRAM chips. By repeatedly opening or keeping a
DRAM row (aggressor row) open, we show that it is possible to disturb DRAM
cells through a DRAM column (i.e., bitline) and induce bitflips in DRAM cells
sharing the same columns as the aggressor row (across multiple DRAM subarrays).
With ColumnDisturb, the activation of a single row concurrently disturbs cells
across as many as three subarrays (e.g., 3072 rows) as opposed to
RowHammer/RowPress, which affect only a few neighboring rows of the aggressor
row in a single subarray. We rigorously characterize ColumnDisturb and its
characteristics under various operational conditions using 216 DDR4 and 4 HBM2
chips from three major manufacturers. Among our 27 key experimental
observations, we highlight two major results and their implications.
  First, ColumnDisturb affects chips from all three major manufacturers and
worsens as DRAM technology scales down to smaller node sizes (e.g., the minimum
time to induce the first ColumnDisturb bitflip reduces by up to 5.06x). We
observe that, in existing DRAM chips, ColumnDisturb induces bitflips within a
standard DDR4 refresh window (e.g., in 63.6 ms) in multiple cells. We predict
that, as DRAM technology node size reduces, ColumnDisturb would worsen in
future DRAM chips, likely causing many more bitflips in the standard refresh
window. Second, ColumnDisturb induces bitflips in many (up to 198x) more rows
than retention failures. Therefore, ColumnDisturb has strong implications for
retention-aware refresh mechanisms that leverage the heterogeneity in cell
retention times: our detailed analyses show that ColumnDisturb greatly reduces
the benefits of such mechanisms.

</details>
