{"id": "2510.13147", "categories": ["cs.AR", "cs.LG", "cs.PF", "C.1.4"], "pdf": "https://arxiv.org/pdf/2510.13147", "abs": "https://arxiv.org/abs/2510.13147", "authors": ["Faraz Tahmasebi", "Michael Pelluer", "Hyoukjun Kwon"], "title": "D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations", "comment": "12 pages, 13 figures", "summary": "The computation and memory costs of large language models kept increasing\nover last decade, which reached over the scale of 1T parameters. To address the\nchallenges from the large scale models, model compression techniques such as\nlow-rank decomposition have been explored. Previous model decomposition works\nhave focused on weight decomposition to avoid costly runtime decomposition,\nwhose latency often significantly exceeds the benefits from decomposition\n(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K\nsequence length with activation decomposition compared to no decomposition). In\nthis work, we debunk such observations and report that the input decomposition\ncan be significantly beneficial with a proper choice of decomposition algorithm\nand hardware support. We adopt progressive decomposition algorithm, Lanczos\nalgorithm, and design a co-accelerator architecture for the decomposition\nalgorithm. To address the memory- boundness of the decomposition operation, we\nintroduce a novel compute replication methodology that moves the op- eration\ntoward compute-bound region, which enables 6.2x speedup in our evaluation. We\nalso develop an output shape- preserving computation scheme that eliminates\ndecomposi- tion costs in consecutive layers. To compensate model quality loss\nfrom compression, we introduce a multi-track decom- position approach that\nseparately handles outlier channels for high accuracy and low perplexity with\nminimal compu- tational costs. Combined together, our accelerator, D-com,\nprovides 22% end-to-end latency improvements compared to A100 GPU at the cost\nof small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faD-com\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u8f93\u5165\u5206\u89e3\u3001\u6e10\u8fdb\u5206\u89e3\u7b97\u6cd5\u548c\u534f\u540c\u52a0\u901f\u5668\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5206\u89e3\u8fc7\u7a0b\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8622%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u6539\u8fdb\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u6301\u7eed\u589e\u957f\uff0c\u4f20\u7edf\u6743\u91cd\u5206\u89e3\u65b9\u6cd5\u5bfc\u81f4\u8fd0\u884c\u65f6\u5206\u89e3\u5ef6\u8fdf\u8fc7\u9ad8\uff0c\u8d85\u8fc7\u4e86\u5206\u89e3\u5e26\u6765\u7684\u6536\u76ca\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5206\u89e3\u7b97\u6cd5\uff08Lanczos\u7b97\u6cd5\uff09\uff0c\u8bbe\u8ba1\u534f\u540c\u52a0\u901f\u5668\u67b6\u6784\uff0c\u5f15\u5165\u8ba1\u7b97\u590d\u5236\u65b9\u6cd5\u89e3\u51b3\u5185\u5b58\u74f6\u9888\uff0c\u5f00\u53d1\u8f93\u51fa\u5f62\u72b6\u4fdd\u6301\u8ba1\u7b97\u65b9\u6848\uff0c\u5e76\u4f7f\u7528\u591a\u901a\u9053\u5206\u89e3\u65b9\u6cd5\u5904\u7406\u5f02\u5e38\u901a\u9053\u3002", "result": "\u5b9e\u73b0\u4e866.2\u500d\u52a0\u901f\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u6bd4A100 GPU\u63d0\u534722%\uff0c\u6a21\u578b\u8d28\u91cf\u635f\u5931\u8f83\u5c0f\uff08\u5728AI2\u63a8\u7406\u6311\u6218\u4efb\u52a1\u4e0a\u4ec53%\uff09\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u5206\u89e3\u7b97\u6cd5\u548c\u786c\u4ef6\u652f\u6301\uff0c\u8f93\u5165\u5206\u89e3\u53ef\u4ee5\u663e\u8457\u6709\u76ca\uff0cD-com\u52a0\u901f\u5668\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.13362", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.13362", "abs": "https://arxiv.org/abs/2510.13362", "authors": ["Angelos Athanasiadis", "Nikolaos Tampouratzis", "Ioannis Papaefstathiou"], "title": "Energy-Efficient FPGA Framework for Non-Quantized Convolutional Neural Networks", "comment": "9th International Workoshop on Microsystems, International Hellenic\n  University", "summary": "The growing demand for real-time processing in artificial intelligence\napplications, particularly those involving Convolutional Neural Networks\n(CNNs), has highlighted the need for efficient computational solutions.\nConventional processors, very often, fall short in balancing performance, power\nconsumption, and latency, especially in embedded systems and edge computing\nplatforms. Field-Programmable Gate Arrays (FPGAs) offer a promising\nalternative, combining high performance with energy efficiency and\nreconfigurability. The presented framework addresses the complex and demanding\ncomputations of CNNs on FPGAs maintaining full precision in all neural network\nparameters. Specifically, our framework is based on Darknet which is very\nwidely used for the design of CNNs and allows the designer, by using a similar\ninput to that given to Darknet, to efficiently implement a CNN in a\nheterogeneous system comprising of CPUs and FPGAs. When compared with the FPGA\nframeworks that support quantization, our solution aims to offer similar\nperformance and/or energy efficiency without any degradation on the NN\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eDarknet\u7684FPGA\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4fdd\u6301\u795e\u7ecf\u7f51\u7edc\u5168\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u5b9e\u73b0CNN\u8ba1\u7b97\uff0c\u5728CPU-FPGA\u5f02\u6784\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u4e0e\u91cf\u5316\u65b9\u6848\u76f8\u5f53\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "AI\u5e94\u7528\u4e2dCNN\u7684\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f20\u7edf\u5904\u7406\u5668\u5728\u6027\u80fd\u3001\u529f\u8017\u548c\u5ef6\u8fdf\u65b9\u9762\u96be\u4ee5\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548c\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\u4e2d\u3002FPGA\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u9ad8\u80fd\u6548\u548c\u53ef\u91cd\u6784\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684Darknet\u6846\u67b6\u5f00\u53d1\uff0c\u5141\u8bb8\u8bbe\u8ba1\u8005\u4f7f\u7528\u7c7b\u4f3cDarknet\u7684\u8f93\u5165\uff0c\u5728CPU-FPGA\u5f02\u6784\u7cfb\u7edf\u4e2d\u9ad8\u6548\u5b9e\u73b0CNN\uff0c\u540c\u65f6\u4fdd\u6301\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u7684\u5168\u7cbe\u5ea6\u3002", "result": "\u4e0e\u652f\u6301\u91cf\u5316\u7684FPGA\u6846\u67b6\u76f8\u6bd4\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u65e8\u5728\u63d0\u4f9b\u76f8\u4f3c\u7684\u6027\u80fd\u548c/\u6216\u80fd\u6548\uff0c\u800c\u4e0d\u4f1a\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aCNN\u5728FPGA\u4e0a\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5168\u7cbe\u5ea6\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6027\u80fd\u548c\u80fd\u6548\u5e73\u8861\u3002"}}
{"id": "2510.13401", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13401", "abs": "https://arxiv.org/abs/2510.13401", "authors": ["Jude Haris", "Jos\u00e9 Cano"], "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs", "comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025", "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).", "AI": {"tldr": "\u63d0\u51faF-BFQ\u52a0\u901f\u5668\uff0c\u652f\u6301\u52a8\u6001\u5207\u6362\u4e24\u79cdBFP\u91cf\u5316\u53d8\u4f53\uff0c\u5728AMD Kria\u677f\u4e0a\u5b9e\u73b01.4\u500d\u63a8\u7406\u52a0\u901f\uff0c\u8fbe\u52305.2 tokens/\u79d2\u7684\u6027\u80fd", "motivation": "LLM\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u9700\u8981\u91cf\u5316\u6280\u672f\uff0c\u4f46\u6df7\u5408BFP\u91cf\u5316\u9700\u8981\u4e13\u7528\u52a0\u901f\u5668\u652f\u6301\u4e0d\u540c\u91cf\u5316\u53d8\u4f53\u800c\u65e0\u9700\u91cd\u65b0\u914d\u7f6e", "method": "\u8bbe\u8ba1\u7075\u6d3b\u7684\u5757\u6d6e\u70b9\u91cf\u5316(F-BFQ)\u52a0\u901f\u5668\uff0c\u80fd\u591f\u52a8\u6001\u5207\u6362\u4e24\u79cdBFP\u91cf\u5316\u53d8\u4f53\u5e76\u6267\u884c\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c", "result": "\u5728AMD Kria\u677f\u4e0a\u90e8\u7f72\uff0c\u76f8\u6bd4Arm NEON CPU\u6267\u884c\u5e73\u5747\u51cf\u5c111.4\u500d\u63a8\u7406\u65f6\u95f4\uff0c\u8fbe\u52305.2 tokens/\u79d2\uff08\u7ea63.9 words/\u79d2\uff09", "conclusion": "F-BFQ\u52a0\u901f\u5668\u6709\u6548\u652f\u6301\u6df7\u5408BFP\u91cf\u5316LLM\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
