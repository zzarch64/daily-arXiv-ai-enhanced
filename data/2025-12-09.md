<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 10]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Compass: Mapping Space Exploration for Multi-Chiplet Accelerators Targeting LLM Inference Serving Workloads](https://arxiv.org/abs/2512.06093)
*Boyu Li,Zongwei Zhu,Yi Xiong,Qianyue Cao,Jiawei Geng,Xiaonan Zhang,Xi Li*

Main category: cs.AR

TL;DR: Compass框架通过解耦微批次和层的映射编码方案，结合遗传算法搜索，为LLM推理服务在多芯片加速器上实现高效映射，相比现有方法平均EDP降低63.12%


<details>
  <summary>Details</summary>
Motivation: 现有映射空间探索主要针对传统CNN/Transformer工作负载，无法充分支持真实LLM推理服务中混合请求类型和可变序列长度的动态行为，需要新的解决方案

Method: 1) 提出基于计算执行图的映射编码方案，解耦微批次和层，实现异构芯片上的细粒度执行控制；2) 开发Compass框架，集成评估引擎和基于遗传算法的映射生成引擎

Result: 相比最先进方法，Compass框架平均实现63.12%的EDP（能耗延迟积）降低

Conclusion: 提出的映射编码方案和Compass框架能有效解决LLM推理服务在多芯片加速器上的映射问题，显著提升能效

Abstract: Large Language Models (LLMs) impose massive computational demands, driving the need for scalable multi-chiplet accelerators. However, existing mapping space exploration efforts for such accelerators primarily focus on traditional CNN/Transformer workloads and fail to adequately support the dynamic behaviors of mixed request types and variable sequence lengths in real-world LLM inference serving. To bridge this gap, we first propose a computation execution graph-based mapping encoding scheme that decouples micro-batches and layers, enabling fine-grained execution control on heterogeneous chiplets and flexibly representing various parallelism strategies. Second, building upon this scheme, we develop the Compass framework, which integrates an evaluation engine and a genetic algorithm-based mapping generation engine to achieve efficient mapping search. Compared to state-of-the-art works, our solution achieves an average EDP reduction of 63.12%.

</details>


### [2] [Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures](https://arxiv.org/abs/2512.06113)
*Bin Xu,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AR

TL;DR: MERINDA是一个FPGA加速的模型恢复框架，通过流式数据流管道重构计算，相比GPU方案解决了迭代依赖、内核启动开销等问题，在代表性MR工作负载上比FPGA基准方案减少最多6.3倍周期。


<details>
  <summary>Details</summary>
Motivation: 模型恢复（MR）是物理AI和实时数字孪生的核心原语，但GPU执行MR效率低下，主要问题包括：迭代依赖、内核启动开销、内存带宽未充分利用以及高数据移动延迟。

Method: MERINDA将计算重构为流式数据流管道，利用BRAM分块、定点内核、并发使用LUT结构和进位链加法器来利用片上局部性，实现细粒度空间并行，同时最小化片外流量。

Result: 在代表性MR工作负载上，MERINDA比基于FPGA的LTC基准方案减少了最多6.3倍的周期，为时间关键的物理系统实现了实时性能。

Conclusion: MERINDA的硬件感知公式消除了同步瓶颈，在MR的迭代更新中维持高吞吐量，为物理AI和数字孪生应用提供了高效的FPGA加速解决方案。

Abstract: Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.

</details>


### [3] [From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators](https://arxiv.org/abs/2512.06177)
*Jiahan Xie,Evan Williams,Adrian Sampson*

Main category: cs.AR

TL;DR: 开发了一个端到端的开源编译器工具链，能够从PyTorch编写的ML模型生成可综合的SystemVerilog代码


<details>
  <summary>Details</summary>
Motivation: 为机器学习硬件加速提供开源解决方案，替代闭源的商业工具如Vitis HLS，降低硬件设计门槛

Method: 基于Allo加速器设计语言、Calyx硬件中间表示和LLVM的CIRCT项目构建工具链，实现内存分区等编译器优化

Result: 能够有效生成优化的FPGA可实现的硬件设计，性能与闭源工业级工具Vitis HLS相当

Conclusion: 提供了一个完整的开源ML到硬件编译器解决方案，证明了开源工具在硬件设计领域的可行性

Abstract: We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.

</details>


### [4] [SparsePixels: Efficient Convolution for Sparse Data on FPGAs](https://arxiv.org/abs/2512.06208)
*Ho Fung Tsoi,Dylan Rankin,Vladimir Loncar,Philip Harris*

Main category: cs.AR

TL;DR: SparsePixels：针对空间稀疏图像数据的FPGA高效卷积框架，通过只计算活跃像素实现73倍推理加速


<details>
  <summary>Details</summary>
Motivation: 标准CNN在FPGA上推理时，由于需要对每个输入像素进行密集卷积，导致高延迟和长启动间隔。许多图像数据具有空间稀疏性，语义信息只占一小部分像素，大部分计算浪费在空区域上。

Method: 提出SparsePixels框架，实现特殊类别的CNN，只选择性地保留和计算活跃像素子集，忽略其余像素。开发了支持构建稀疏CNN的库（包含量化感知训练）和用于FPGA部署的HLS实现。

Result: 在中微子物理数据集中，标准CNN（4k参数）推理延迟为48.665μs，而稀疏CNN（计算少于1%输入像素）实现73倍加速至0.665μs，资源利用率在片上预算内，仅损失少量性能。在其他稀疏图像数据集上也展示出至少一个数量级的加速。

Conclusion: 该框架为现代实验（如CERN大型强子对撞机的触发和数据采集系统）中的快速高效数据读出算法开发提供支持，通过利用图像的空间稀疏性实现微秒级推理延迟。

Abstract: Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $μ$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\times 73$ inference speedup to 0.665 $μ$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.

</details>


### [5] [A 33.6-136.2 TOPS/W Nonlinear Analog Computing-In-Memory Macro for Multi-bit LSTM Accelerator in 65 nm CMOS](https://arxiv.org/abs/2512.06362)
*Junyi Yang,Xinyu Luo,Ye Ke,Zheng Wang,Hongyang Shang,Shuai Dong,Zhengnan Fu,Xiaofeng Yang,Hongjie Liu,Arindam Basu*

Main category: cs.AR

TL;DR: 提出一种结合可重构非线性内存ADC的LSTM加速器，直接在模拟域计算非线性激活，显著提升能效和面积效率


<details>
  <summary>Details</summary>
Motivation: 传统模拟内存计算加速器在处理RNN/LSTM时，由于大量非线性操作需要数字处理，限制了能效提升

Method: 采用可重构1-5位非线性内存ADC，包含：1)双9T位单元支持有符号输入和三元权重；2)RUDC技术提升读取动态范围；3)双电源6T-SRAM阵列减少位单元数量和延迟

Result: 5位NLIM ADC平均误差<1 LSB，对温度变化鲁棒；在12类关键词检测任务中达到92.0%片上推理准确率，系统级能效提升2.2倍，面积效率提升1.6倍

Conclusion: 提出的LSTM加速器通过模拟域直接计算非线性激活，显著提升了能效和面积效率，为RNN加速器设计提供了新方向

Abstract: The energy efficiency of analog computing-in-memory (ACIM) accelerator for recurrent neural networks, particularly long short-term memory (LSTM) network, is limited by the high proportion of nonlinear (NL) operations typically executed digitally. To address this, we propose an LSTM accelerator incorporating an ACIM macro with reconfigurable (1-5 bit) nonlinear in-memory (NLIM) analog-to-digital converter (ADC) to compute NL activations directly in the analog domain using: 1) a dual 9T bitcell with decoupled read/write paths for signed inputs and ternary weight operations; 2) a read-word-line underdrive Cascode (RUDC) technique achieving 2.8X higher read-bitline dynamic range than single-transistor designs (1.4X better over conventional Cascode structure with 7X lower current variation); 3) a dual-supply 6T-SRAM array for efficient multi-bit weight operations and reducing both bitcell count (7.8X) and latency (4X) for 5-bit weight operations. We experimentally demonstrate 5-bit NLIM ADC for approximating NL activations in LSTM cells, achieving average error <1 LSB. Simulation confirms the robustness of NLIM ADC against temperature variations thanks to the replica bias strategy. Our design achieves 92.0% on-chip inference accuracy for a 12-class keyword-spotting task while demonstrating 2.2X higher system-level normalized energy efficiency and 1.6X better normalized area efficiency than state-of-the-art works. The results combine physical measurements of a macro unit-accounting for the majority of LSTM operations (99% linear and 80% nonlinear operations)-with simulations of the remaining components, including additional LSTM and fully connected layers.

</details>


### [6] [Approximate Multiplier Induced Error Propagation in Deep Neural Networks](https://arxiv.org/abs/2512.06537)
*A. M. H. H. Alahakoon,Hassaan Saadat,Darshana Jayasinghe,Sri Parameswaran*

Main category: cs.AR

TL;DR: 提出一个分析框架，将近似乘法器的统计误差特性与DNN精度损失关联起来，证明误差均值是主要影响因素，并通过FPGA实现验证。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络依赖密集算术运算，近似乘法器可降低能耗，但缺乏对其误差分布如何影响DNN精度的严格数学分析。

Method: 建立分析框架，将AxM的统计误差矩与GEMM中的失真关联，使用Frobenius范数推导闭式表达式，并通过误差注入和FPGA实现验证。

Result: 预测的失真与观测到的精度下降强相关，误差可配置的AxM在FPGA上的案例研究进一步证实了分析趋势。

Conclusion: 该框架为行为或硬件级仿真提供了轻量级替代方案，能够快速估计AxM对DNN推理质量的影响。

Abstract: Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.

</details>


### [7] [ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design](https://arxiv.org/abs/2512.06854)
*Qijun Zhang,Yao Lu,Mengming Li,Shang Liu,Zhiyao Xie*

Main category: cs.AR

TL;DR: ArchPower是首个用于架构级处理器功耗建模的开源数据集，包含200个CPU数据样本，涵盖25种CPU配置和8种工作负载，提供超过100个架构特征和细粒度功耗标签。


<details>
  <summary>Details</summary>
Motivation: 当前CPU功耗评估需要耗时的IC实现流程，早期设计阶段的传统功耗模型不准确，而基于ML的架构级功耗模型面临数据可用性挑战，缺乏开源数据集，现有数据集难以反映真实CPU设计场景。

Method: 通过复杂且真实的设计流程收集CPU架构信息作为特征，使用模拟功耗作为标签。数据集包含200个CPU数据样本，来自25种不同CPU配置执行8种工作负载，每个样本有超过100个架构特征（硬件和事件参数）。

Result: 创建了首个开源架构级处理器功耗建模数据集ArchPower，提供细粒度功耗信息：总设计功耗和11个组件的功耗，每个功耗值进一步分解为组合逻辑、时序逻辑、存储器和时钟功耗四个细粒度组。

Conclusion: ArchPower填补了架构级处理器功耗建模领域开源数据集的空白，为ML-based功耗模型研究提供了高质量、细粒度的真实数据支持，有助于加速CPU设计流程。

Abstract: Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.

</details>


### [8] [DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management](https://arxiv.org/abs/2512.07312)
*Zhongchun Zhou,Chengtao Lai,Yuhang Gu,Wei Zhang*

Main category: cs.AR

TL;DR: 该论文提出了一种面向AI加速器的共享系统级缓存设计，通过应用感知的管理策略（包括缓存替换、死块预测和旁路决策）来简化编程，相比传统缓存架构获得了最高1.8倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速采用，AI加速器设计变得越来越复杂，特别是具有深度层次化暂存器内存（SPMs）及其异步管理的架构增加了软件开发难度。作者探索相反的设计方向：采用共享系统级缓存和基于应用感知的管理策略，以保持编程简单性。

Method: 提出多核AI加速器架构，配备共享系统级缓存，利用软件栈中的数据流信息指导缓存替换（包括死块预测），结合旁路决策和缓解缓存颠簸的机制。通过周期精确模拟器评估，并建立考虑实际重叠行为的分析模型来扩展到更大规模工作负载。

Result: 相比传统缓存架构获得最高1.80倍的性能加速。旁路和颠簸缓解策略能有效处理有无核间数据共享的场景。RTL实现显示在15nm工艺下面积为0.064mm²，可运行在2GHz时钟频率。

Conclusion: 共享缓存设计展示了简化AI加速器开发的潜力，通过应用感知的管理策略在保持编程简单性的同时获得显著性能提升，为未来AI加速器系统开发提供了新方向。

Abstract: The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.
  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.
  Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.

</details>


### [9] [aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \& Software Formal Verification](https://arxiv.org/abs/2512.07520)
*Noé Amiot,Quentin L. Meunier,Karine Heydemann,Emmanuelle Encrenaz*

Main category: cs.AR

TL;DR: aLEAKator是一个开源框架，用于自动形式化验证掩码加密硬件加速器和CPU软件实现，支持多种泄漏模型和信号粒度，无需目标CPU架构的先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法存在局限性：要么只能处理小型硬件模块或CPU上的小规模程序（如S盒），要么受限于特定泄漏模型，或者需要硬件特定的先验知识。在考虑毛刺、转换和CPU微架构特性等高级泄漏模型时，验证掩码实现的安全性仍然是一个重大挑战。

Method: 提出aLEAKator框架，采用混合域仿真方法，能够精确建模和验证各种（包括鲁棒和宽松的）1-探测泄漏模型，支持可变信号粒度而不限于1位线。该框架还支持查找表存在时的验证，且不需要目标CPU架构的先验知识。

Result: aLEAKator通过现有工具和实际测量进行了验证，并提供了创新性结果，如在各种CPU上验证完整的一阶掩码AES实现。

Conclusion: aLEAKator是一个有效的自动化验证框架，能够解决掩码硬件和软件实现安全验证中的关键挑战，特别是在高级泄漏模型下，为实际加密实现的安全性评估提供了实用工具。

Abstract: Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs

</details>


### [10] [Análisis de rendimiento y eficiencia energética en el cluster Raspberry Pi Cronos](https://arxiv.org/abs/2512.07622)
*Martha Semken,Mariano Vargas,Ignacio Tula,Giuliana Zorzoli,Andrés Rojas Paredes*

Main category: cs.AR

TL;DR: 评估基于Raspberry Pi的教育集群Cronos的计算性能和能效，使用HPL基准测试，分析不同节点配置下的可扩展性、稳定性和功耗。


<details>
  <summary>Details</summary>
Motivation: 为教育研究环境设计和评估低成本ARM集群，分析其在计算密集型工作负载下的性能、稳定性和能效表现。

Method: 使用High Performance Linpack (HPL)基准测试，在配置Slurm资源管理和Open MPI并行通信的环境中，对不同节点配置（包括Raspberry Pi4和3b）进行实验测试。

Result: 6个Raspberry Pi4节点的同构配置达到6.91 GFLOPS性能；异构节点（包含Pi3b）会降低稳定性和效率；测量了系统总功耗并计算了性能功耗比(GFLOPS/W)。

Conclusion: 该研究为教育研究环境中低成本ARM集群的设计、评估和使用提供了具体贡献，展示了其在计算性能和能效方面的表现。

Abstract: This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.

</details>
