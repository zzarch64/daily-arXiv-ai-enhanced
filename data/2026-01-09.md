<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Memory-Guided Unified Hardware Accelerator for Mixed-Precision Scientific Computing](https://arxiv.org/abs/2601.04476)
*Chuanzhen Wang,Leo Zhang,Eric Liu*

Main category: cs.AR

TL;DR: 提出Memory-Guided Unified Hardware Accelerator框架，通过内存引导的精度选择、经验驱动的位宽管理和课程学习自动发现稀疏模式，在统一平台上高效处理有限元方法、脉冲神经网络和稀疏计算。


<details>
  <summary>Details</summary>
Motivation: 现有专用加速器存在三个根本限制：有限元方法缺乏全面的舍入误差分析且使用固定精度策略；脉冲神经网络加速器无法处理非脉冲操作且位宽随网络深度增加而扩大；FPGA张量加速器仅优化密集计算且需要手动配置稀疏模式。

Method: 提出Memory-Guided Unified Hardware Accelerator框架，包含三个增强模块：1) 内存引导的精度选择克服固定精度限制；2) 经验驱动的位宽管理和动态并行适配增强脉冲神经网络加速；3) 课程学习自动发现稀疏模式。

Result: 在FEniCS、COMSOL、ANSYS基准测试，MNIST、CIFAR-10、CIFAR-100、DVS-Gesture数据集和COCO 2017上实验显示：数值精度提高2.8%，吞吐量增加47%，能耗降低34%，相比专用加速器吞吐量提升45-65%。

Conclusion: 该工作实现了在单一平台上统一处理有限元方法、脉冲神经网络和稀疏计算，消除了单独单元间的数据传输开销，为混合精度科学计算提供了高效解决方案。

Abstract: Recent hardware acceleration advances have enabled powerful specialized accelerators for finite element computations, spiking neural network inference, and sparse tensor operations. However, existing approaches face fundamental limitations: (1) finite element methods lack comprehensive rounding error analysis for reduced-precision implementations and use fixed precision assignment strategies that cannot adapt to varying numerical conditioning; (2) spiking neural network accelerators cannot handle non-spike operations and suffer from bit-width escalation as network depth increases; and (3) FPGA tensor accelerators optimize only for dense computations while requiring manual configuration for each sparsity pattern. To address these challenges, we introduce \textbf{Memory-Guided Unified Hardware Accelerator for Mixed-Precision Scientific Computing}, a novel framework that integrates three enhanced modules with memory-guided adaptation for efficient mixed-workload processing on unified platforms. Our approach employs memory-guided precision selection to overcome fixed precision limitations, integrates experience-driven bit-width management and dynamic parallelism adaptation for enhanced spiking neural network acceleration, and introduces curriculum learning for automatic sparsity pattern discovery. Extensive experiments on FEniCS, COMSOL, ANSYS benchmarks, MNIST, CIFAR-10, CIFAR-100, DVS-Gesture datasets, and COCO 2017 demonstrate 2.8\% improvement in numerical accuracy, 47\% throughput increase, 34\% energy reduction, and 45-65\% throughput improvement compared to specialized accelerators. Our work enables unified processing of finite element methods, spiking neural networks, and sparse computations on a single platform while eliminating data transfer overhead between separate units.

</details>


### [2] [MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration](https://arxiv.org/abs/2601.04801)
*Lei Xu,Shanshan Wang,Chenglong Xiao*

Main category: cs.AR

TL;DR: 提出MPM-LLM4DSE框架，结合多模态预测模型和LLM优化器，显著提升HLS设计空间探索性能


<details>
  <summary>Details</summary>
Motivation: 现有GNN预测方法无法充分捕捉行为描述的丰富语义特征，传统多目标优化算法缺乏对pragma指令如何影响QoR的领域知识考虑

Method: 提出MPM-LLM4DSE框架：1) 多模态预测模型融合行为描述和控制数据流图特征；2) 使用LLM作为优化器，配合专门设计的提示工程方法，包含pragma对QoR影响分析

Result: 多模态预测模型比现有最佳方法ProgSG提升高达10.25倍；在DSE任务中，LLM4DSE比先前方法平均性能提升39.90%

Conclusion: 提出的MPM-LLM4DSE框架有效解决了HLS DSE中的预测精度和优化效率问题，验证了多模态特征融合和LLM引导优化的有效性

Abstract: High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE.

</details>


### [3] [Challenges and Research Directions for Large Language Model Inference Hardware](https://arxiv.org/abs/2601.05047)
*Xiaoyu Ma,David Patterson*

Main category: cs.AR

TL;DR: 论文分析了LLM推理的挑战，指出内存和互连是主要瓶颈而非计算，提出了四种架构研究机会来解决这些问题


<details>
  <summary>Details</summary>
Motivation: LLM推理面临独特挑战：自回归解码阶段使推理与训练根本不同，且受AI趋势影响，内存和互连成为主要瓶颈而非计算能力

Method: 提出四种架构研究机会：1) 高带宽闪存实现10倍内存容量和HBM级带宽；2) 近内存处理；3) 3D内存-逻辑堆叠实现高内存带宽；4) 低延迟互连加速通信

Result: 虽然主要关注数据中心AI，但论文也评估了这些架构方案在移动设备上的适用性

Conclusion: 需要新的架构创新来解决LLM推理中的内存和互连瓶颈，提出的四种研究方向有望显著提升推理效率

Abstract: Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.

</details>
