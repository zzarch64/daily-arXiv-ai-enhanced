<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for CGRAs](https://arxiv.org/abs/2509.13557)
*Zesong Jiang,Yuqi Sun,Qing Zhong,Mahathi Krishna,Deepak Patil,Cheng Tan,Sriram Krishnamoorthy,Jeff Zhang*

Main category: cs.AR

TL;DR: MACO是一个基于多智能体大语言模型的开源框架，用于CGRA的硬件/软件协同设计，通过LLM推理自动生成和优化CGRA架构，显著减少人工设计工作量。


<details>
  <summary>Details</summary>
Motivation: CGRA设计面临设计空间巨大、架构参数独立以及人工设计耗时等挑战，而大语言模型的快速发展为自动化这一过程提供了新机遇。

Method: 采用多智能体LLM框架，通过四个阶段进行HW/SW协同设计：硬件/软件协同设计、设计错误纠正、最佳设计选择、评估与反馈，并引入LLM自学习机制选择最优CGRA。

Result: 实验结果表明，MACO能高效生成高质量的CGRA架构，在性能、功耗和面积方面优于现有LLM方法和人工设计。

Conclusion: 该框架展示了多智能体LLM在CGRA设计中的巨大潜力，能够显著减少人工设计工作量，为实际CGRA设计提供了有效的自动化解决方案。

Abstract: Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing
architecture that can deliver high-performance, energy-efficient acceleration
across diverse domains. By supporting reconfiguration at the functional unit
level, CGRAs efficiently adapt to varying computational patterns and optimize
resource utilization. However, designing CGRAs is highly challenging due to the
vast design space, independent architectural parameters, and the time-consuming
nature of manual design. Fortunately, the rapid advancement of large language
models (LLMs) presents new opportunities to automate this process.
  In this work, we propose MACO -- an open-source multi-agent LLM-based
framework for Hardware/Software (HW/SW) co-design of CGRAs. The framework
employs LLM reasoning to generate CGRAs across four stages: HW/SW co-design,
Design error correction, Best design selection, and Evaluation & Feedback.
Furthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent
reasoning and feedback to achieve higher PPA (that is, power, performance, and
area) design points for a given domain. In addition, we introduce an LLM
self-learning mechanism that employs LLM-driven decision making to select the
optimal CGRA to accelerate the design process.
  We evaluate the framework with state-of-the-art LLM-based methods and manual
CGRA design, in terms of performance, power consumption, and area. Experimental
results show that MACO efficiently generates high-quality CGRA architectures,
significantly reducing manual design effort and demonstrating the potential of
our framework for real-world CGRA design.

</details>


### [2] [StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs](https://arxiv.org/abs/2509.13694)
*Hanchen Ye,Deming Chen*

Main category: cs.AR

TL;DR: StreamTensor是一个编译器框架，通过引入迭代张量类型系统自动构建和优化基于流的数据流加速器，在FPGA上实现比现有FPGA LLM加速器和GPU更低的延迟和更高的能效


<details>
  <summary>Details</summary>
Motivation: 解决深度学习工作负载在数据流架构上执行时的内存瓶颈问题，现有方法在处理内核间相关性、外部内存访问管理和缓冲区优化方面存在困难

Method: 提出StreamTensor编译器框架，引入新颖的迭代张量类型系统来显式编码流布局，通过系统探索三个层次化设计空间（张量分块、内核融合和资源分配）来平衡计算强度、内存效率和数据流

Result: 在FPGA上的大型语言模型评估中，相比最先进的FPGA LLM加速器和GPU，实现了0.76倍和0.64倍的延迟降低，以及相比GPU高达1.99倍的能效提升

Conclusion: StreamTensor为基于数据流的可扩展深度学习加速提供了一种有前景的方法

Abstract: Efficient execution of deep learning workloads on dataflow architectures is
crucial for overcoming memory bottlenecks and maximizing performance. While
streaming intermediate results between computation kernels can significantly
improve efficiency, existing approaches struggle with inter-kernel
correlations, external memory access management, and buffer optimization. In
this work, we propose StreamTensor, a compiler framework that automatically
constructs and optimizes stream-based dataflow accelerators. StreamTensor
introduces a novel iterative tensor type system to explicitly encode stream
layouts, enabling seamless kernel fusion, buffer allocation, and memory
optimization. By systematically exploring three hierarchical design spaces,
including tensor tiling, kernel fusion, and resource allocation, StreamTensor
balances computational intensity, memory efficiency, and data streaming to
maximize performance. Based on FPGA evaluations on Large Language Models (LLM),
StreamTensor achieves up to 0.76x and 0.64x lower latency compared to the
state-of-the-art FPGA LLM accelerators and GPUs, and up to 1.99x higher energy
efficiency compared to GPUs, making it a promising approach for scalable
dataflow-based deep learning acceleration.

</details>


### [3] [CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration](https://arxiv.org/abs/2509.13710)
*Hongyi Li,Songchen Ma,Huanyu Qu,Weihao Zhang,Jia Chen,Junfeng Lin,Fengbin Tu,Rong Zhao*

Main category: cs.AR

TL;DR: CompAir是一种新型混合PIM架构，通过DRAM-PIM和SRAM-PIM的混合键合集成，结合创新的片上网络设计，为大语言模型推理提供了高效能、低能耗的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算和能耗需求巨大，传统PIM架构在灵活性、性能和成本效率方面难以平衡LLM的动态内存计算模式和算子多样性需求。

Method: 提出CompAir混合PIM架构，集成DRAM-PIM和SRAM-PIM，开发CompAir-NoC片上网络进行非线性运算，并设计分层指令集架构确保灵活性和可编程性。

Result: 相比最先进的全PIM架构，CompAir在预填充和解码阶段分别实现1.83-7.98倍和1.95-6.28倍的性能提升，相比混合A100和HBM-PIM系统能耗降低3.52倍。

Conclusion: 这是首个系统探索混合DRAM-PIM和SRAM-PIM架构并具备网络内计算能力的工作，为LLM提供了高效解决方案。

Abstract: The rapid advancement of Large Language Models (LLMs) has revolutionized
various aspects of human life, yet their immense computational and energy
demands pose significant challenges for efficient inference. The memory wall,
the growing processor-memory speed disparity, remains a critical bottleneck for
LLM. Process-In-Memory (PIM) architectures overcome limitations by co-locating
compute units with memory, leveraging 5-20$\times$ higher internal bandwidth
and enabling greater energy efficiency than GPUs. However, existing PIMs
struggle to balance flexibility, performance, and cost-efficiency for LLMs'
dynamic memory-compute patterns and operator diversity. DRAM-PIM suffers from
inter-bank communication overhead despite its vector parallelism. SRAM-PIM
offers sub-10ns latency for matrix operation but is constrained by limited
capacity. This work introduces CompAir, a novel PIM architecture that
integrates DRAM-PIM and SRAM-PIM with hybrid bonding, enabling efficient linear
computations while unlocking multi-granularity data pathways. We further
develop CompAir-NoC, an advanced network-on-chip with an embedded arithmetic
logic unit that performs non-linear operations during data movement,
simultaneously reducing communication overhead and area cost. Finally, we
develop a hierarchical Instruction Set Architecture that ensures both
flexibility and programmability of the hybrid PIM. Experimental results
demonstrate that CompAir achieves 1.83-7.98$\times$ prefill and
1.95-6.28$\times$ decode improvement over the current state-of-the-art fully
PIM architecture. Compared to the hybrid A100 and HBM-PIM system, CompAir
achieves 3.52$\times$ energy consumption reduction with comparable throughput.
This work represents the first systematic exploration of hybrid DRAM-PIM and
SRAM-PIM architectures with in-network computation capabilities, offering a
high-efficiency solution for LLM.

</details>


### [4] [TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary LLM Inference On Edge](https://arxiv.org/abs/2509.13765)
*Zhirui Huang,Rui Ma,Shijie Cao,Ran Shu,Ian Wang,Ting Cao,Chixiao Chen,Yongqiang Xiong*

Main category: cs.AR

TL;DR: TENET是一个针对三元量化LLM推理的稀疏感知LUT中心架构，通过算法、计算和内存协同优化，在FPGA和ASIC平台上分别实现了4.3倍和21.1倍的能效提升，以及2.7倍的端到端推理延迟加速。


<details>
  <summary>Details</summary>
Motivation: 传统GPU平台无法充分利用三元量化的优势，缺乏对三元算术和内存专门化的原生支持，在低批次实时场景下利用率严重不足。

Method: 提出稀疏感知LUT中心架构TENET，包括稀疏三元LUT核心优化三元混合精度GEMM、动态激活N:M稀疏性利用token内稀疏性、基于LUT的64B:80B三元权重解压缩模块，以及线性投影感知稀疏注意力数据流。

Result: TENET-FPGA和TENET-ASIC相比A100 GPU分别实现4.3倍和21.1倍的能效提升，TENET-ASIC在端到端推理延迟上平均加速2.7倍。

Conclusion: TENET架构通过协同优化算法、计算和内存，有效解决了三元量化LLM在传统硬件上的效率问题，为实时推理部署提供了高效的硬件解决方案。

Abstract: Ternary quantization has emerged as a powerful technique for reducing both
computational and memory footprint of large language models (LLM), enabling
efficient real-time inference deployment without significantly compromising
model accuracy. Conventional LLM inference platforms (e.g GPUs) cannot
capitalize on its benefits, as they (i) lack native support for ternary
arithmetic and memory specialization and (ii) remain severely under-utilized in
low-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware
LUT-centric architecture that co-optimizes algorithm, compute, and memory for
ternary LLM inference. To maximize the efficiency of Ternary Linear layer,
TENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary
mixed-precision GEMM using a symmetric precompute lookup table. It also
features Dynamic Activation N:M Sparsity to exploit the sparsity within the
activation of each token. Additionally, we propose a LUT-based 64B:80B ternary
weight decompression module to fully exploit the memory efficiency of ternary
values. At the system level, we design a heterogeneous TENET accelerator with
full programmability that integrates STL cores with high-precision cores. An
associated Linear-Projection-aware Sparse Attention dataflow is introduced to
optimize memory access and hardware utilization. We implement TENET accelerator
prototype on both FPGA and ASIC platforms. Experiments across various model
sizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy
efficiency by 4.3$\times$ and 21.1$\times$, respectively, compared to the A100
GPU. Furthermore, TENET-ASIC achieves a 2.7$\times$ average speedup compared to
the A100 GPU in end-to-end inference latency.

</details>


### [5] [An RDMA-First Object Storage System with SmartNIC Offload](https://arxiv.org/abs/2509.13997)
*Yu Zhu,Aditya Dhakal,Pedro Bruel,Gourav Rattihalli,Yunming Xiao,Johann Lombardi,Dejan Milojicic*

Main category: cs.AR

TL;DR: ROS2是一个基于RDMA的对象存储系统，将DAOS客户端卸载到NVIDIA BlueField-3 SmartNIC上，通过分离控制平面和数据平面来提升AI训练中的存储性能。


<details>
  <summary>Details</summary>
Motivation: AI训练和推理需要持续、细粒度的I/O操作，传统基于TCP的存储路径存在性能瓶颈，需要重新审视POSIX兼容的对象存储方案来支持GPU中心化流水线。

Method: 设计ROS2系统，采用RDMA优先的架构，将DAOS客户端卸载到SmartNIC，保持控制平面（gRPC）和数据平面（UCX/libfabric over RDMA或TCP）分离，消除数据路径中的主机干预。

Result: 实验显示RDMA在服务器级CPU上始终优于TCP，无论是大顺序I/O还是小随机I/O。SmartNIC上的RDMA驱动客户端性能与主机相当，而TCP在SmartNIC上性能较差。

Conclusion: RDMA优先、SmartNIC卸载的对象存储栈是现代LLM训练环境中扩展数据交付的实用基础，未来可集成GPU直接放置功能。

Abstract: AI training and inference impose sustained, fine-grain I/O that stresses
host-mediated, TCP-based storage paths. Motivated by kernel-bypass networking
and user-space storage stacks, we revisit POSIX-compatible object storage for
GPU-centric pipelines. We present ROS2, an RDMA-first object storage system
design that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while
leaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a
lightweight control plane (gRPC for namespace and capability exchange) from a
high-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host
mediation from the data path.
  Using FIO/DFS across local and remote configurations, we find that on
server-grade CPUs RDMA consistently outperforms TCP for both large sequential
and small random I/O. When the RDMA-driven DAOS client is offloaded to
BlueField-3, end-to-end performance is comparable to the host, demonstrating
that SmartNIC offload preserves RDMA efficiency while enabling DPU-resident
features such as multi-tenant isolation and inline services (e.g.,
encryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags
host performance, underscoring the importance of RDMA for offloaded
deployments.
  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded
object-storage stack is a practical foundation for scaling data delivery in
modern LLM training environments; integrating optional GPU-direct placement for
LLM tasks is left for future work.

</details>


### [6] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: TRRIP是一种软硬件协同设计方法，通过编译器分析代码温度（热/冷）并指导硬件优化指令缓存替换策略，在移动CPU上实现26.5%的L2指令MPKI降低和3.9%的性能提升


<details>
  <summary>Details</summary>
Motivation: 移动CPU软件复杂的运行时行为导致指令重用距离增大，传统硬件中心的指令缓存管理方法不足，前端停顿严重造成CPU资源浪费，代码复杂度增长快于片上内存容量增长

Method: 编译器分析代码温度并分类转换，通过操作系统接口提供代码温度信息给硬件；硬件轻量级扩展利用温度属性优化缓存替换策略，减少热代码的驱逐率

Result: L2指令MPKI降低26.5%，在已使用PGO优化的移动代码基础上实现3.9%的几何平均加速比

Conclusion: TRRIP软硬件协同设计方法能有效优化移动系统的指令缓存管理，在严格的功能要求下具有实际部署可行性

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>
