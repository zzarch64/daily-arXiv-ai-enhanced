{"id": "2512.20073", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20073", "abs": "https://arxiv.org/abs/2512.20073", "authors": ["Hongyang Shang", "Shuai Dong", "Ye Ke", "Arindam Basu"], "title": "3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras", "comment": null, "summary": "This work proposes a 3D Stack In-Sensor-Computing (3DS-ISC) architecture for efficient event-based vision processing. A real-time normalization method using an exponential decay function is introduced to construct the time-surface, reducing hardware usage while preserving temporal information. The circuit design utilizes the leakage characterization of Dynamic Random Access Memory(DRAM) for timestamp normalization. Custom interdigitated metal-oxide-metal capacitor (MOMCAP) is used to store the charge and low leakage switch (LL switch) is used to extend the effective charge storage time. The 3DS-ISC architecture integrates sensing, memory, and computation to overcome the memory wall problem, reducing power, latency, and reducing area by 69x, 2.2x and 1.9x, respectively, compared with its 2D counterpart. Moreover, compared to works using a 16-bit SRAM to store timestamps, the ISC analog array can reduce power consumption by three orders of magnitude. In real computer vision (CV) tasks, we applied the spatial-temporal correlation filter (STCF) for denoise, and 3D-ISC achieved almost equivalent accuracy compared to the digital implementation using high precision timestamps. As for the image classification, time-surface constructed by 3D-ISC is used as the input of GoogleNet, achieving 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, and 97% on DVS128 Gesture, comparable with state-of-the-art results on each dataset. Additionally, the 3D-ISC method is also applied to image reconstruction using the DAVIS240C dataset, achieving the highest average SSIM (0.62) among three methods. This work establishes a foundation for real-time, resource-efficient event-based processing and points to future integration of advanced computational circuits for broader applications.", "AI": {"tldr": "\u63d0\u51fa3D\u5806\u53e0\u4f20\u611f\u5668\u5185\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u4e8b\u4ef6\u89c6\u89c9\u5904\u7406\uff0c\u901a\u8fc7\u6cc4\u6f0fDRAM\u7279\u6027\u5b9e\u73b0\u65f6\u95f4\u8868\u9762\u5f52\u4e00\u5316\uff0c\u76f8\u6bd42D\u67b6\u6784\u5927\u5e45\u964d\u4f4e\u529f\u8017\u3001\u5ef6\u8fdf\u548c\u9762\u79ef", "motivation": "\u4f20\u7edf\u4e8b\u4ef6\u89c6\u89c9\u5904\u7406\u9762\u4e34\u5185\u5b58\u5899\u95ee\u9898\uff0c\u9700\u8981\u9891\u7e41\u5728\u4f20\u611f\u5668\u548c\u5904\u7406\u5668\u95f4\u4f20\u8f93\u6570\u636e\uff0c\u5bfc\u81f4\u9ad8\u529f\u8017\u548c\u5ef6\u8fdf\u3002\u9700\u8981\u4e00\u79cd\u96c6\u6210\u611f\u77e5\u3001\u5b58\u50a8\u548c\u8ba1\u7b97\u7684\u67b6\u6784\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236", "method": "\u63d0\u51fa3D\u5806\u53e0\u4f20\u611f\u5668\u5185\u8ba1\u7b97\u67b6\u6784\uff0c\u5229\u7528DRAM\u6cc4\u6f0f\u7279\u6027\u5b9e\u73b0\u6307\u6570\u8870\u51cf\u65f6\u95f4\u5f52\u4e00\u5316\uff0c\u4f7f\u7528\u5b9a\u5236\u91d1\u5c5e-\u6c27\u5316\u7269-\u91d1\u5c5e\u7535\u5bb9\u5b58\u50a8\u7535\u8377\uff0c\u4f4e\u6cc4\u6f0f\u5f00\u5173\u5ef6\u957f\u7535\u8377\u5b58\u50a8\u65f6\u95f4", "result": "\u76f8\u6bd42D\u67b6\u6784\uff0c\u529f\u8017\u964d\u4f4e69\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e2.2\u500d\uff0c\u9762\u79ef\u51cf\u5c111.9\u500d\u3002\u76f8\u6bd416\u4f4dSRAM\u5b58\u50a8\u65f6\u95f4\u6233\uff0c\u529f\u8017\u964d\u4f4e\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "conclusion": "3D-ISC\u67b6\u6784\u4e3a\u5b9e\u65f6\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u4e8b\u4ef6\u5904\u7406\u5960\u5b9a\u57fa\u7840\uff0c\u672a\u6765\u53ef\u96c6\u6210\u66f4\u5148\u8fdb\u8ba1\u7b97\u7535\u8def\u6269\u5c55\u5e94\u7528\u8303\u56f4"}}
{"id": "2512.20198", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.20198", "abs": "https://arxiv.org/abs/2512.20198", "authors": ["Huizheng Wang", "Taiquan Wei", "Hongbin Wang", "Zichuan Wang", "Xinru Tang", "Zhiheng Yue", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling", "comment": "Accepted for publication in IEEE Transactions on Computers", "summary": "Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\\times$ speedup and 71.2$\\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\\times$ energy and 27.1$\\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\\times$ throughput improvement.", "AI": {"tldr": "STAR\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u957f\u5e8f\u5217\u5e76\u884c\u5904\u7406\u7684\u8de8\u9636\u6bb5\u534f\u540c\u7a00\u758f\u52a0\u901f\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9884\u6d4b\u7a00\u758f\u6027\u3001\u5206\u5e03\u5f0f\u6392\u5e8f\u548c\u534f\u8c03\u5206\u5757\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u7a00\u758f\u52a0\u901f\u5668\u5728\u957f\u5e8f\u5217\u5e76\u884c\u5904\u7406\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u91c7\u7528\u9636\u6bb5\u9694\u79bb\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u8de8\u9636\u6bb5\u534f\u540c\u7684\u673a\u4f1a\u3002\u91cd\u65b0\u5ba1\u89c6\u7aef\u5230\u7aef\u7a00\u758f\u52a0\u901f\u6d41\u7a0b\uff0c\u53d1\u73b0\u8de8\u9636\u6bb5\u534f\u8c03\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u548c\u5185\u5b58\u8bbf\u95ee\u3002", "method": "\u63d0\u51faSTAR\u8de8\u9636\u6bb5\u534f\u540c\u8bbe\u8ba1\uff1a1) \u57fa\u4e8e\u524d\u5bfc\u96f6\u7684\u7a00\u758f\u9884\u6d4b\uff0c\u4f7f\u7528\u5bf9\u6570\u57df\u4ec5\u52a0\u6cd5\u64cd\u4f5c\u6700\u5c0f\u5316\u9884\u6d4b\u5f00\u9500\uff1b2) \u5206\u5e03\u5f0f\u6392\u5e8f\u548c\u6392\u5e8f\u66f4\u65b0FlashAttention\u673a\u5236\uff1b3) \u534f\u8c03\u5206\u5757\u7b56\u7565\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u9636\u6bb5\u4ea4\u4e92\uff1b4) \u4e13\u7528STAR\u52a0\u901f\u5668\u67b6\u6784\uff1b5) \u591a\u6838\u7a7a\u95f4\u67b6\u6784\u90e8\u7f72\u4f18\u5316\u6570\u636e\u6d41\u548c\u6267\u884c\u7f16\u6392\u3002", "result": "STAR\u52a0\u901f\u5668\u76f8\u6bd4A100\u5b9e\u73b09.2\u500d\u52a0\u901f\u548c71.2\u500d\u80fd\u6548\u63d0\u5347\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u52a0\u901f\u566816.1\u500d\u80fd\u6548\u548c27.1\u500d\u9762\u79ef\u6548\u7387\u3002\u7a7a\u95f4\u67b6\u6784\u7248\u672c\u76f8\u6bd4\u57fa\u7ebf\u8bbe\u8ba1\u5b9e\u73b020.1\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "\u8de8\u9636\u6bb5\u534f\u540c\u8bbe\u8ba1\u662f\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u957f\u5e8f\u5217\u5e76\u884c\u5904\u7406\u4e2d\u7a00\u758f\u52a0\u901f\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\uff0cSTAR\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86Transformer\u63a8\u7406\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u3002"}}
{"id": "2512.20495", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20495", "abs": "https://arxiv.org/abs/2512.20495", "authors": ["He Zhu", "Zheng Liu", "Xingyang Li", "Anbang Wu", "Jieru Zhao", "Fangxin Liu", "Yiming Gan", "Jingwen Leng", "Yu Feng"], "title": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization", "comment": null, "summary": "3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.\n  We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.", "AI": {"tldr": "Nebula\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u534f\u540c\u6e32\u67d3\u7684\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u4f20\u8f93\u4e2d\u95f4\u7ed3\u679c\u800c\u975e\u89c6\u9891\uff0c\u5927\u5e45\u964d\u4f4e\u4e91\u5ba2\u6237\u7aef\u901a\u4fe1\u5e26\u5bbd\uff0c\u5e76\u63d0\u5347VR\u6e32\u67d3\u6027\u80fd\u3002", "motivation": "\u5f53\u524d3D\u9ad8\u65af\u6cfc\u6e85\u67b6\u6784\u8bbe\u8ba1\u5ffd\u7565\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5bf9\u5927\u89c4\u6a21\u573a\u666f\u8106\u5f31\uff1b\u540c\u65f6VR\u5e26\u5bbd\u9700\u6c42\u4f7f\u5f97\u4e91\u7aef\u65e0\u6cd5\u9ad8\u6548\u4f20\u8f93\u9ad8\u4fdd\u771f\u6d41\u7545\u5185\u5bb9\u3002", "method": "1) \u6d41\u5f0f\u4f20\u8f93LoD\u641c\u7d22\u540e\u7684\u4e2d\u95f4\u7ed3\u679c\u800c\u975e\u89c6\u9891\uff1b2) \u4e91\u7aef\u5f15\u5165\u65f6\u95f4\u611f\u77e5\u7684LoD\u641c\u7d22\uff0c\u5229\u7528\u5e27\u95f4\u65f6\u95f4\u4e00\u81f4\u6027\u51cf\u5c11\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\uff1b3) \u5ba2\u6237\u7aef\u63d0\u51fa\u65b0\u9896\u7684\u7acb\u4f53\u5149\u6805\u5316\uff0c\u8ba9\u53cc\u773c\u5171\u4eab\u8ba1\u7b97\uff1b4) \u6700\u5c0f\u786c\u4ef6\u589e\u5f3a\u3002", "result": "\u76f8\u6bd4\u6709\u635f\u89c6\u9891\u6d41\uff0cNebula\u5b9e\u73b0\u4e862.7\u500d\u7684\u8fd0\u52a8\u5230\u5149\u5b50\u5ef6\u8fdf\u52a0\u901f\uff0c\u5e76\u51cf\u5c11\u4e861925%\u7684\u5e26\u5bbd\u9700\u6c42\u3002", "conclusion": "Nebula\u901a\u8fc7\u521b\u65b0\u7684\u534f\u540c\u6e32\u67d3\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u7684\u6269\u5c55\u6027\u548cVR\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cfVR\u5185\u5bb9\u4f20\u8f93\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.20571", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20571", "abs": "https://arxiv.org/abs/2512.20571", "authors": ["Brennan Romero", "D. G. Perera"], "title": "Composing Mini Oscilloscope on Embedded Systems", "comment": "22 pages, 11 figures", "summary": "In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.", "AI": {"tldr": "\u4f7f\u7528Nuvoton NUC-140\u5d4c\u5165\u5f0f\u5e73\u53f0\u5b9e\u73b0\u57fa\u672c\u793a\u6ce2\u5668\u529f\u80fd\uff0c\u5305\u62ec\u81ea\u52a8/\u8fb9\u6cbf\u89e6\u53d1/\u5355\u6b21\u6a21\u5f0f\u3001\u6ce2\u5f62\u7f29\u653e\u548c\u63a2\u5934\u6821\u51c6\uff0c\u8fbe\u5230\u5546\u7528\u793a\u6ce2\u566890%\u5e38\u7528\u529f\u80fd", "motivation": "\u5229\u7528\u4f4e\u6210\u672c\u5d4c\u5165\u5f0f\u5e73\u53f0(NUC-140)\u590d\u73b0\u5e38\u89c4\u793a\u6ce2\u5668\u7684\u57fa\u672c\u529f\u80fd\uff0c\u4e3a\u7535\u5b50\u8c03\u8bd5\u63d0\u4f9b\u7ecf\u6d4e\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u8bbe\u8ba1\u5b9a\u5236\u5b50\u677f\u8fde\u63a5NUC-140\u5e73\u53f0\uff0c\u96c6\u6210\u4e24\u4e2aBNC\u63a2\u5934\u63a5\u53e3\u3001\u4e5d\u952e\u952e\u76d8\u548c\u6821\u51c6\u4fe1\u53f7\uff0c\u5229\u7528\u5f00\u53d1\u677fLCD\u663e\u793a\u6ce2\u5f62\uff0c\u5b9e\u73b0\u591a\u79cd\u89e6\u53d1\u6a21\u5f0f\u548c\u7f29\u653e\u529f\u80fd", "result": "\u7cfb\u7edf\u6210\u529f\u5b9e\u73b090%\u5546\u7528\u793a\u6ce2\u5668\u5e38\u7528\u529f\u80fd\uff0c\u5305\u62ec\u81ea\u52a8/\u8fb9\u6cbf\u89e6\u53d1/\u5355\u6b21\u6a21\u5f0f\u3001\u5782\u76f4\u6c34\u5e73\u7f29\u653e\u3001\u63a2\u5934\u6821\u51c6\uff0c\u6210\u4e3a\u6709\u6548\u7684\u8c03\u8bd5\u5de5\u5177", "conclusion": "\u57fa\u4e8eNUC-140\u7684\u793a\u6ce2\u5668\u7cfb\u7edf\u662f\u7ecf\u6d4e\u5b9e\u7528\u7684\u8c03\u8bd5\u5de5\u5177\uff0c\u590d\u73b0\u4e86\u5927\u90e8\u5206\u5546\u7528\u793a\u6ce2\u5668\u6838\u5fc3\u529f\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u6210\u672c\u5d4c\u5165\u5f0f\u5e73\u53f0\u5b9e\u73b0\u4e13\u4e1a\u4eea\u5668\u7684\u53ef\u884c\u6027"}}
