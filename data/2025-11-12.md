<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing](https://arxiv.org/abs/2511.07665)
*Yuzhe Fu,Changchun Zhou,Hancheng Ye,Bowen Duan,Qiyu Huang,Chiyue Wei,Cong Guo,Hai "Helen'' Li,Yiran Chen*

Main category: cs.AR

TL;DR: FractalCloud是一种受分形启发的硬件架构，用于高效处理大规模3D点云，通过形状感知分区和块并行操作解决了现有加速器在处理大规模点云时的计算复杂度和内存访问问题。


<details>
  <summary>Details</summary>
Motivation: 随着点云神经网络处理大规模点云（数十万个点）的需求增加，全对全计算和全局内存访问导致O(n²)的计算复杂度和内存流量，现有加速器主要针对小规模工作负载优化，无法有效扩展。

Method: 提出FractalCloud架构，包含两个关键优化：1）协同设计的Fractal方法实现形状感知和硬件友好的分区；2）块并行点操作，分解并并行化所有点操作。专用硬件设计支持片上分形和灵活并行处理。

Result: 在28nm技术下实现，核心面积1.5mm²，相比最先进的加速器实现了21.7倍加速和27倍能耗降低，同时保持网络精度。

Conclusion: FractalCloud展示了在大规模点云神经网络推理中的可扩展性和高效性，为大规模3D点云处理提供了有效的硬件解决方案。

Abstract: Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.

</details>


### [2] [PIMfused: Near-Bank DRAM-PIM with Fused-layer Dataflow for CNN Data Transfer Optimization](https://arxiv.org/abs/2511.07985)
*Simei Yang,Xinyu Shi,Lu Zhao,Yunyu Ling,Quanjun Wang,Francky Catthoor*

Main category: cs.AR

TL;DR: PIMfused通过硬件-软件协同设计，在近内存处理架构中实现融合层数据流，优化CNN执行性能，显著减少跨bank数据传输开销。


<details>
  <summary>Details</summary>
Motivation: 传统逐层数据流在DRAM-PIM架构中会导致跨bank数据传输，成为CNN加速的性能瓶颈。

Method: 提出PIMfused硬件-软件协同设计方案，采用融合层数据流，打破bank间数据依赖，优化跨bank数据传输。

Result: 在4-bank PIMcores配置下，相比GDDR6-AiM基线，内存周期减少至30.6%，能耗降至83.4%，面积降至76.5%。

Conclusion: PIMfused通过融合层数据流有效解决了DRAM-PIM架构中跨bank数据传输瓶颈，实现了显著的PPA提升。

Abstract: Near-bank Processing-in-Memory (PIM) architectures integrate processing cores (PIMcores) close to DRAM banks to mitigate the high cost of off-chip memory accesses. When accelerating convolutional neural network (CNN) on DRAM-PIM, performance is often constrained by cross-bank (or cross-PIMcore) data transfers, which are induced by the conventional layer-by-layer dataflow that enforces inter-bank (or inter-PIMcore) dependencies across successive CNN layers. To address this challenge, we propose PIMfused, a hardware-software co-design that enables fused-layer dataflow for end-to-end CNN execution in near-bank DRAM-PIM. By adopting fused-layer dataflow, PIMfused improves data reuse and, more importantly, breaks inter-bank data dependencies, thereby optimizing cross-bank data transfers without sacrificing bank-level parallelism. We study the impact of buffer sizes and PIMcore parallelism (1-bank vs. 4-bank) on PIMfused using end-to-end ResNet18. We present three key takeaways and show that with 4-bank PIMcores, PIMfused achieves overall PPA gains over a GDDR6-AiM-like baseline, cutting memory cycles to 30.6%, energy to 83.4%, and area to 76.5%.

</details>


### [3] [Re$^{\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating](https://arxiv.org/abs/2511.08054)
*Yunqi Shi,Xi Lin,Zhiang Wang,Siyuan Xu,Shixiong Kai,Yao Lai,Chengrui Gao,Ke Xue,Mingxuan Yuan,Chao Qian,Zhi-Hua Zhou*

Main category: cs.AR

TL;DR: Re$^{\text{2}}$MaP通过递归原型构建和基于树的重新布局，生成专家级宏单元布局，显著改善了时序指标WNS和TNS。


<details>
  <summary>Details</summary>
Motivation: 现有宏单元布局方法在时序优化方面存在不足，需要一种能够同时考虑线长、数据流和设计约束的自动化布局方法。

Method: 采用多级宏分组和PPA感知单元聚类，构建统一连接矩阵；使用DREAMPlace构建混合尺寸布局原型；提出ABPlace椭圆布局优化方法；设计基于树的重新布局过程。

Result: 相比最先进的学术布局器Hier-RTLMP，WNS提升最高22.22%（平均10.26%），TNS提升最高97.91%（平均33.97%）。

Conclusion: Re$^{\text{2}}$MaP方法在时序、功耗、DRC违规和运行时间等多个指标上均优于现有方法，证明了递归原型构建和基于树重新布局的有效性。

Abstract: This work introduces the Re$^{\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.

</details>


### [4] [BDD2Seq: Enabling Scalable Reversible-Circuit Synthesis via Graph-to-Sequence Learning](https://arxiv.org/abs/2511.08315)
*Mingkai Miao,Jianheng Tang,Guangyu Hu,Hongce Zhang*

Main category: cs.AR

TL;DR: BDD2Seq是一个图到序列框架，使用图神经网络编码器和指针网络解码器来预测BDD变量排序，显著降低了量子计算中的量子成本和合成时间。


<details>
  <summary>Details</summary>
Motivation: 在基于BDD的可逆电路合成中，变量排序直接影响BDD节点数量和量子成本等关键指标。由于寻找最优变量排序是NP完全问题，现有启发式方法在电路复杂度增加时性能下降。

Method: 提出BDD2Seq框架，将电路网表视为图，使用图神经网络编码器学习结构依赖关系，指针网络解码器预测变量排序，并采用多样化波束搜索提高结果质量。

Result: 在三个公开基准测试上的实验表明，BDD2Seq相比现代启发式算法实现了约1.4倍的量子成本降低和3.7倍的合成速度提升。

Conclusion: 这是首个使用基于图的生成模型和多样性解码来解决BDD可逆电路合成中变量排序问题的工作，显著提升了合成效率。

Abstract: Binary Decision Diagrams (BDDs) are instrumental in many electronic design automation (EDA) tasks thanks to their compact representation of Boolean functions. In BDD-based reversible-circuit synthesis, which is critical for quantum computing, the chosen variable ordering governs the number of BDD nodes and thus the key metrics of resource consumption, such as Quantum Cost. Because finding an optimal variable ordering for BDDs is an NP-complete problem, existing heuristics often degrade as circuit complexity grows. We introduce BDD2Seq, a graph-to-sequence framework that couples a Graph Neural Network encoder with a Pointer-Network decoder and Diverse Beam Search to predict high-quality orderings. By treating the circuit netlist as a graph, BDD2Seq learns structural dependencies that conventional heuristics overlooked, yielding smaller BDDs and faster synthesis. Extensive experiments on three public benchmarks show that BDD2Seq achieves around 1.4 times lower Quantum Cost and 3.7 times faster synthesis than modern heuristic algorithms. To the best of our knowledge, this is the first work to tackle the variable-ordering problem in BDD-based reversible-circuit synthesis with a graph-based generative model and diversity-promoting decoding.

</details>


### [5] [DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator](https://arxiv.org/abs/2511.08395)
*Xingyu Liu,Jiawei Liang,Yipu Zhang,Linfeng Du,Chaofang Ma,Hui Yu,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: 提出基于FPGA的硬件高效RBD加速器，包含三个关键创新：精度感知量化框架、除法延迟优化和模块间DSP复用方法，在多种机器人类型上实现8倍吞吐量提升和7.4倍延迟降低。


<details>
  <summary>Details</summary>
Motivation: 为高自由度机器人系统开发硬件高效的RBD（刚体动力学）加速器，解决现有加速器在性能和资源利用方面的限制。

Method: 1. 精度感知量化框架减少DSP需求同时保持运动精度；2. 质量矩阵求逆算法中的除法延迟优化；3. 模块间DSP复用方法提高DSP利用率。

Result: 相比最先进的RBD加速器，在多种机器人类型上实现最高8倍吞吐量提升和7.4倍延迟降低。

Conclusion: 该工作证明了其在高自由度机器人系统中的有效性和可扩展性，为机器人控制硬件加速提供了创新解决方案。

Abstract: We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.

</details>


### [6] [CO2-Meter: A Comprehensive Carbon Footprint Estimator for LLMs on Edge Devices](https://arxiv.org/abs/2511.08575)
*Zhenxiao Fu,Chen Fan,Lei Jiang*

Main category: cs.AR

TL;DR: CO2-Meter是一个用于估计LLM边缘推理中运营碳和嵌入碳的统一框架，通过考虑外围能耗、预填充/解码阶段差异和SoC设计复杂性，提供比现有方法更准确的碳足迹评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM碳足迹估算方法不完整，忽视了外围设备能耗、预填充和解码阶段的不同行为以及SoC设计的复杂性，导致在边缘设备部署LLM时的碳挑战评估不足。

Method: 提出CO2-Meter框架，包括：(1)基于方程的外围能耗模型和数据集；(2)使用GNN预测器和阶段特定LLM能耗数据；(3)SoC瓶颈分析的单元级嵌入碳模型；(4)验证显示优于现有方法的准确性。

Result: CO2-Meter在准确性上优于现有方法，案例研究证明了其在识别碳热点和指导边缘平台可持续LLM设计方面的有效性。

Conclusion: CO2-Meter为LLM边缘推理提供了全面的碳足迹评估框架，能够有效识别碳热点并指导可持续设计决策。

Abstract: LLMs have transformed NLP, yet deploying them on edge devices poses great carbon challenges. Prior estimators remain incomplete, neglecting peripheral energy use, distinct prefill/decode behaviors, and SoC design complexity. This paper presents CO2-Meter, a unified framework for estimating operational and embodied carbon in LLM edge inference. Contributions include: (1) equation-based peripheral energy models and datasets; (2) a GNN-based predictor with phase-specific LLM energy data; (3) a unit-level embodied carbon model for SoC bottleneck analysis; and (4) validation showing superior accuracy over prior methods. Case studies show CO2-Meter's effectiveness in identifying carbon hotspots and guiding sustainable LLM design on edge platforms. Source code: https://github.com/fuzhenxiao/CO2-Meter

</details>
