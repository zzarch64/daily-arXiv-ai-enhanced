<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15166)
*Tanner Andrulis,Michael Gilbert,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: FFM是首个能快速在张量代数工作负载的融合映射空间中寻找最优映射的映射器，通过剪枝和部分映射组合解决融合映射搜索的指数级复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 现有映射器无法在可行时间内找到融合映射的最优解，因为融合映射的搜索空间随计算步骤数量呈指数增长，限制了张量代数加速器的性能潜力评估。

Method: FFM通过剪枝永远不会成为最优映射的部分映射子集来缩小搜索空间，然后通过连接部分映射来构建最优融合映射，实现近似线性的运行时扩展。

Result: FFM在Transformer等任务中寻找最优映射的速度比现有最优方法快1000倍以上，且尽管映射空间大小呈指数增长，FFM的运行时却近似线性扩展。

Conclusion: FFM首次实现了在融合映射空间中快速寻找最优映射，解决了张量代数加速器映射搜索的关键瓶颈，为加速器架构评估提供了高效工具。

Abstract: The latency and energy of tensor algebra accelerators depend on how data movement and operations are scheduled (i.e., mapped) onto accelerators, so determining the potential of an accelerator architecture requires both a performance model and a mapper to search for the optimal mapping. A key optimization that the mapper must explore is fusion, meaning holding data on-chip between computation steps, which has been shown to reduce energy and latency by reducing DRAM accesses. However, prior mappers cannot find optimal mappings with fusion (i.e., fused mappings) in a feasible runtime because the number of fused mappings to search increases exponentially with the number of workload computation steps.
  In this paper, we introduce the Fast and Fusiest Mapper (FFM), the first mapper to quickly find optimal mappings in a comprehensive fused mapspace for tensor algebra workloads. FFM shrinks the search space by pruning subsets of mappings (i.e., partial mappings) that are shown to never be a part of optimal mappings, quickly eliminating all suboptimal mappings with those partial mappings as subsets. Then FFM joins partial mappings to construct optimal fused mappings. We evaluate FFM and show that, although the mapspace size grows exponentially with the number of computation steps, FFM's runtime scales approximately linearly. FFM is orders of magnitude faster ($>1000\times$) than prior state-of-the-art approaches at finding optimal mappings for Transformers.

</details>


### [2] [The Turbo-Charged Mapper: Fast and Optimal Mapping for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15172)
*Michael Gilbert,Tanner Andrulis,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: TCM是一个保证找到最优映射的快速映射器，通过定义数据放置概念和剪枝策略，将搜索空间减少32个数量级，能在1分钟内找到最优映射，而现有方法即使运行10小时也无法找到最优解。


<details>
  <summary>Details</summary>
Motivation: 现有加速器映射优化方法使用启发式或元启发式算法，无法保证找到最优映射，这阻碍了硬件评估的准确性，因为无法区分性能差异是源于硬件改进还是次优映射。

Method: 提出Turbo-Charged Mapper (TCM)，定义新的数据放置概念，与数据流概念结合，识别并剪枝冗余和次优映射，将搜索空间减少高达32个数量级。

Result: TCM能在1分钟内找到最优映射，而现有方法即使运行10小时（1000倍时间）也无法找到最优解，其能量延迟乘积比最优解高21%。

Conclusion: TCM是首个能在可行时间内进行完整映射空间搜索并保证找到最优映射的映射器，为准确的硬件评估和设计提供了关键工具。

Abstract: The energy and latency of an accelerator running a deep neural network (DNN) depend on how the computation and data movement are scheduled in the accelerator (i.e., mapping). Optimizing mappings is essential to evaluating and designing accelerators. However, the space of mappings is large, and prior works can not guarantee finding optimal mappings because they use heuristics or metaheuristics to narrow down the space. These limitations preclude proper hardware evaluation, since designers can not tell whether performance differences are due to changes in hardware or suboptimal mapping.
  To address this challenge, we propose the Turbo-Charged Mapper (TCM), a fast mapper that is guaranteed to find optimal mappings. The key to our approach is that we define a new concept in mapping, called dataplacement, which, like the prior concept of dataflow, allows for clear analysis and comparison of mappings. Through it, we identify multiple opportunities to prune redundant and suboptimal mappings, reducing search space by up to 32 orders of magnitude.
  Leveraging these insights, TCM can perform full mapspace searches, making it the first mapper that can find optimal mappings in feasible runtime. Compared to prior mappers, we show that TCM can find optimal mappings quickly (less than a minute), while prior works can not find optimal mappings (energy-delay-product $21\%$ higher than optimal) even when given $1000\times$ the runtime ($>10$ hours).

</details>


### [3] [Human-AI Interaction: Evaluating LLM Reasoning on Digital Logic Circuit included Graph Problems, in terms of creativity in design and analysis](https://arxiv.org/abs/2602.15336)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: 评估GPT、Gemini和Claude在数字逻辑问题上的表现，发现模型在技术正确性上存在严重缺陷，但学生却常认为其回答有帮助，揭示了感知有用性与实际正确性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地被本科生用作按需导师，需要评估它们在电路和图表相关的数字逻辑问题上的可靠性，因为这些领域对准确性要求高，错误可能强化学生的误解。

Method: 采用人机交互研究，让24名学生比较三种主流LLM（GPT、Gemini、Claude）在10个本科数字逻辑问题上的表现，同时进行独立的法官评估，使用严格正确性标准对比官方答案。

Result: 对于最复杂的时序问题（Q1-Q7），所有模型都无法匹配官方答案，尽管它们生成了自信、结构良好的解释，学生常给予积极评价。模型倾向于使用标准教科书模板，难以准确转换电路结构为状态演化和时序行为。

Conclusion: 在没有验证框架的情况下，LLMs在核心数字逻辑主题上可能不可靠，可能无意中强化本科教学中的误解，需要开发验证工具来确保教育应用中的可靠性。

Abstract: Large Language Models (LLMs) are increasingly used by undergraduate students as on-demand tutors, yet their reliability on circuit- and diagram-based digital logic problems remains unclear. We present a human- AI study evaluating three widely used LLMs (GPT, Gemini, and Claude) on 10 undergraduate-level digital logic questions spanning non-standard counters, JK-based state transitions, timing diagrams, frequency division, and finite-state machines. Twenty-four students performed pairwise model comparisons, providing per-question judgments on (i) preferred model, (ii) perceived correctness, (iii) consistency, (iv) verbosity, and (v) confidence, along with global ratings of overall model quality, satisfaction across multiple dimensions (e.g., accuracy and clarity), and perceived mental effort required to verify answers. To benchmark technical validity, we applied an independent judge-based evaluation against official solutions for all ten questions, using strict correctness criteria. Results reveal a consistent gap between perceived helpfulness and formal correctness: for the most sequentially demanding problems (Q1- Q7), none of the evaluated LLMs matched the official answers, despite producing confident, well-structured explanations that students often rated favorably. Error analysis indicates that models frequently default to canonical textbook templates (e.g., standard ripple counters) and struggle to translate circuit structure into exact state evolution and timing behavior. These findings suggest that, without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction.

</details>


### [4] [Iterative LLM-Based Assertion Generation Using Syntax-Semantic Representations for Functional Coverage-Guided Verification](https://arxiv.org/abs/2602.15388)
*Yonghao Wang,Jiaxin Zhou,Yang Yin,Hongqin Lyu,Zhiteng Chao,Wenchao Ding,Jing Ye,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: CoverAssert是一个基于LLM的迭代框架，通过功能覆盖率反馈循环优化SystemVerilog断言生成，显著提升覆盖率指标。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成SystemVerilog断言的技术面临关键挑战：LLM对IC设计理解不足导致单次生成的断言质量差，且缺乏有效的功能覆盖率验证和反馈机制。

Method: 提出CoverAssert框架，通过聚类LLM生成断言的语义特征和AST提取的结构特征，将其映射回规格说明以分析功能覆盖率质量，并基于覆盖率构建反馈循环指导LLM优先处理未覆盖的功能点。

Result: 在四个开源设计上的实验表明，CoverAssert与最先进的生成器AssertLLM和Spec2Assertion集成后，平均分支覆盖率提升9.57%，语句覆盖率提升9.64%，翻转覆盖率提升15.69%。

Conclusion: CoverAssert通过轻量级的断言-规格匹配机制和基于功能覆盖率的反馈循环，有效解决了LLM生成SystemVerilog断言的质量问题，显著提升了断言覆盖率。

Abstract: While leveraging LLMs to automatically generate SystemVerilog assertions (SVAs) from natural language specifications holds great potential, existing techniques face a key challenge: LLMs often lack sufficient understanding of IC design, leading to poor assertion quality in a single pass. Therefore, verifying whether the generated assertions effectively cover the functional specifications and designing feedback mechanisms based on this coverage remain significant hurdles. To address these limitations, this paper introduces CoverAssert, a novel iterative framework for optimizing SVA generation with LLMs. The core contribution is a lightweight mechanism for matching generated assertions with specific functional descriptions in the specifications. CoverAssert achieves this by clustering the joint representations of semantic features of LLM-generated assertions and structural features extracted from abstract syntax trees (ASTs) about signals related to assertions, and then mapping them back to the specifications to analyze functional coverage quality. Leveraging this capability, CoverAssert constructs a feedback loop based on functional coverage to guide LLMs in prioritizing uncovered functional points, thereby iteratively improving assertion quality. Experimental evaluations on four open-source designs demonstrate that integrating CoverAssert with state-of-the-art generators, AssertLLM and Spec2Assertion, achieves average improvements of 9.57 % in branch coverage, 9.64 % in statement coverage, and 15.69 % in toggle coverage.

</details>
