{"id": "2512.18152", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.18152", "abs": "https://arxiv.org/abs/2512.18152", "authors": ["Rui Xie", "Yunhua Fang", "Asad Ul Haq", "Linsen Ma", "Sanchari Sen", "Swagath Venkataramani", "Liu Liu", "Tong Zhang"], "title": "Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference", "comment": null, "summary": "LLM inference is increasingly memory bound, and HBM cost per GB dominates system cost. Current HBM stacks include short on-die ECC that tightens binning, raises price, and fixes reliability policy inside the device. This paper asks whether a system can tolerate a much higher raw HBM bit error rate and still keep end-to-end correctness and throughput, without changing the HBM PHY or the fixed 32 B transaction size. We propose REACH, a controller managed ECC design that keeps the HBM link and 32 B transfers unchanged. REACH uses a two level Reed-Solomon scheme: each 32 B chunk uses an inner code to check and correct most faults locally, while chunks that cannot be fixed are marked as erasures. An outer code spans kilobytes and runs in erasure only mode, repairing only flagged chunks and avoiding the expensive locator step. For small random writes, REACH updates outer parity with differential parity to avoid recomputing parity over the whole span, and an optional importance adaptive bit plane policy can protect only critical fields such as BF16 exponents to reduce ECC work and traffic. On three LLMs at 8K context, REACH keeps about 79 percent of on-die ECC throughput at zero BER and remains qualified up to a raw BER of 1e-3, extending tolerable device error rates by about three orders of magnitude while keeping tokens per second nearly flat. In ASAP7, a full REACH controller occupies 15.2 mm2 and consumes 17.5 W at 3.56 TB/s, and it reduces ECC area by 11.6x and power by about 60 percent compared to a naive long Reed-Solomon baseline. By moving strong ECC into the controller, REACH turns long code reliability into a system choice that can enable lower cost HBM under the same standard interface.", "AI": {"tldr": "REACH\u63d0\u51fa\u63a7\u5236\u5668\u7ba1\u7406\u7684ECC\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4e24\u7ea7Reed-Solomon\u7f16\u7801\u65b9\u6848\uff0c\u5728\u4e0d\u6539\u53d8HBM\u7269\u7406\u63a5\u53e3\u548c32B\u4f20\u8f93\u5927\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u53ef\u5bb9\u5fcd\u7684\u539f\u59cb\u8bef\u7801\u7387\u63d0\u9ad8\u7ea6\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301LLM\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u5f53\u524dHBM\u4e2d\u7684\u7247\u4e0aECC\u8bbe\u8ba1\u9650\u5236\u4e86\u82af\u7247\u7b5b\u9009\uff0c\u63d0\u9ad8\u4e86\u6210\u672c\uff0c\u5e76\u5c06\u53ef\u9760\u6027\u7b56\u7565\u56fa\u5316\u5728\u8bbe\u5907\u5185\u90e8\u3002HBM\u6210\u672c\u5360\u7cfb\u7edf\u6210\u672c\u4e3b\u5bfc\u5730\u4f4d\uff0c\u800cLLM\u63a8\u7406\u65e5\u76ca\u53d7\u5185\u5b58\u9650\u5236\u3002\u9700\u8981\u63a2\u7d22\u7cfb\u7edf\u662f\u5426\u80fd\u5bb9\u5fcd\u66f4\u9ad8\u7684\u539f\u59cbHBM\u8bef\u7801\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u7aef\u5230\u7aef\u6b63\u786e\u6027\u548c\u541e\u5410\u91cf\u3002", "method": "REACH\u91c7\u7528\u4e24\u7ea7Reed-Solomon\u7f16\u7801\u65b9\u6848\uff1a1) \u5185\u5c42\u7801\u5bf9\u6bcf\u4e2a32B\u5757\u8fdb\u884c\u672c\u5730\u68c0\u9519\u548c\u7ea0\u9519\uff1b2) \u5916\u5c42\u7801\u8de8\u8d8aKB\u7ea7\u8303\u56f4\uff0c\u4ec5\u4ee5\u64e6\u9664\u6a21\u5f0f\u8fd0\u884c\uff0c\u4fee\u590d\u6807\u8bb0\u4e3a\u64e6\u9664\u7684\u5757\uff0c\u907f\u514d\u6602\u8d35\u7684\u5b9a\u4f4d\u6b65\u9aa4\u3002\u9488\u5bf9\u5c0f\u968f\u673a\u5199\u5165\uff0c\u4f7f\u7528\u5dee\u5206\u5947\u5076\u6821\u9a8c\u66f4\u65b0\u5916\u5c42\u5947\u5076\u6821\u9a8c\uff0c\u907f\u514d\u91cd\u65b0\u8ba1\u7b97\u6574\u4e2a\u8de8\u5ea6\u7684\u5947\u5076\u6821\u9a8c\u3002\u53ef\u9009\u7684\u91cd\u8981\u6027\u81ea\u9002\u5e94\u4f4d\u5e73\u9762\u7b56\u7565\u53ef\u4ec5\u4fdd\u62a4\u5173\u952e\u5b57\u6bb5\uff08\u5982BF16\u6307\u6570\uff09\u4ee5\u51cf\u5c11ECC\u5de5\u4f5c\u548c\u6d41\u91cf\u3002", "result": "\u57288K\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u4e09\u4e2aLLM\u4e0a\uff0cREACH\u5728\u96f6\u8bef\u7801\u7387\u4e0b\u4fdd\u6301\u7ea679%\u7684\u7247\u4e0aECC\u541e\u5410\u91cf\uff0c\u5728\u539f\u59cb\u8bef\u7801\u7387\u9ad8\u8fbe1e-3\u65f6\u4ecd\u4fdd\u6301\u5408\u683c\uff0c\u5c06\u53ef\u5bb9\u5fcd\u8bbe\u5907\u8bef\u7801\u7387\u6269\u5c55\u7ea6\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u6bcf\u79d2\u4ee4\u724c\u6570\u51e0\u4e4e\u4e0d\u53d8\u3002\u5728ASAP7\u5de5\u827a\u4e2d\uff0c\u5b8c\u6574REACH\u63a7\u5236\u5668\u5360\u752815.2 mm\u00b2\uff0c\u57283.56 TB/s\u4e0b\u529f\u801717.5 W\uff0c\u4e0e\u6734\u7d20\u7684\u957fReed-Solomon\u57fa\u7ebf\u76f8\u6bd4\uff0cECC\u9762\u79ef\u51cf\u5c1111.6\u500d\uff0c\u529f\u8017\u964d\u4f4e\u7ea660%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5f3aECC\u79fb\u81f3\u63a7\u5236\u5668\uff0cREACH\u5c06\u957f\u7801\u53ef\u9760\u6027\u8f6c\u53d8\u4e3a\u7cfb\u7edf\u9009\u62e9\uff0c\u53ef\u5728\u76f8\u540c\u6807\u51c6\u63a5\u53e3\u4e0b\u5b9e\u73b0\u66f4\u4f4e\u6210\u672c\u7684HBM\u3002\u8fd9\u4e3a\u5728\u4e0d\u6539\u53d8HBM\u7269\u7406\u63a5\u53e3\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u964d\u4f4e\u6210\u672c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.18158", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.18158", "abs": "https://arxiv.org/abs/2512.18158", "authors": ["Tsung-Han Lu", "Zheyu Li", "Minxuan Zhou", "Tajana Rosing"], "title": "PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM", "comment": null, "summary": "All-pairs shortest paths (APSP) is a fundamental algorithm used for routing, logistics, and network analysis, but the cubic time complexity and heavy data movement of the canonical Floyd-Warshall (FW) algorithm severely limits its scalability on conventional CPUs or GPUs. In this paper, we propose PIM-FW, a novel co-designed hardware architecture and dataflow that leverages processing in and near memory architecture designed to accelerate blocked FW algorithm on an HBM3 stack. To enable fine-grained parallelism, we propose a massively parallel array of specialized bit-serial bank PE and channel PE designed to accelerate the core min-plus operations. Our novel dataflow complements this hardware, employing an interleaved mapping policy for superior load balancing and hybrid in and near memory computing model for efficient computation and reduction. The novel in-bank computing approach allows all distance updates to be performed and stored in memory bank, a key contribution is that eliminates the data movement bottleneck inherent in GPU-based approaches. We implement a full software and hardware co-design using a cycle-accurate simulator to simulate an 8-channel, 4-Hi HBM3 PIM stack on real road-network traces. Experimental results show that, for a 8192 x 8192 graph, PIM-FW achieves a 18.7x speedup in end-to-end execution, and consumes 3200x less DRAM energy compared to a state-of-the-art GPU-only Floyd-Warshall.", "AI": {"tldr": "PIM-FW\uff1a\u4e00\u79cd\u57fa\u4e8eHBM3\u5185\u5b58\u5904\u7406\u67b6\u6784\u7684Floyd-Warshall\u7b97\u6cd5\u52a0\u901f\u65b9\u6848\uff0c\u901a\u8fc7\u5185\u5b58\u5185\u8ba1\u7b97\u6d88\u9664\u6570\u636e\u79fb\u52a8\u74f6\u9888\uff0c\u76f8\u6bd4GPU\u65b9\u6848\u5b9e\u73b018.7\u500d\u52a0\u901f\u548c3200\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfFloyd-Warshall\u7b97\u6cd5\u5728CPU/GPU\u4e0a\u5b58\u5728\u7acb\u65b9\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5927\u91cf\u6570\u636e\u79fb\u52a8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8def\u7531\u3001\u7269\u6d41\u548c\u7f51\u7edc\u5206\u6790\u7b49\u5e94\u7528\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faPIM-FW\u786c\u4ef6\u67b6\u6784\u4e0e\u6570\u636e\u6d41\u534f\u540c\u8bbe\u8ba1\uff1a1\uff09\u57fa\u4e8eHBM3\u5185\u5b58\u6808\u7684\u5904\u7406\u8fd1\u5185\u5b58\u67b6\u6784\uff1b2\uff09\u5927\u89c4\u6a21\u5e76\u884c\u6bd4\u7279\u4e32\u884cbank PE\u548cchannel PE\u52a0\u901fmin-plus\u64cd\u4f5c\uff1b3\uff09\u4ea4\u9519\u6620\u5c04\u7b56\u7565\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff1b4\uff09\u6df7\u5408\u5185\u5b58\u5185\u548c\u8fd1\u5185\u5b58\u8ba1\u7b97\u6a21\u578b\uff1b5\uff09\u5728\u5185\u5b58bank\u5185\u5b8c\u6210\u6240\u6709\u8ddd\u79bb\u66f4\u65b0\u548c\u5b58\u50a8\u3002", "result": "\u57288192\u00d78192\u56fe\u4e0a\uff0cPIM-FW\u76f8\u6bd4\u6700\u5148\u8fdb\u7684GPU-only Floyd-Warshall\u5b9e\u73b0\uff1a\u7aef\u5230\u7aef\u6267\u884c\u901f\u5ea6\u63d0\u534718.7\u500d\uff0cDRAM\u80fd\u8017\u964d\u4f4e3200\u500d\u3002", "conclusion": "PIM-FW\u901a\u8fc7\u5185\u5b58\u5185\u8ba1\u7b97\u6709\u6548\u89e3\u51b3\u4e86Floyd-Warshall\u7b97\u6cd5\u7684\u6570\u636e\u79fb\u52a8\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2512.18300", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.18300", "abs": "https://arxiv.org/abs/2512.18300", "authors": ["Suhas Vittal", "Moinuddin Qureshi"], "title": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism", "comment": "Accepted to HPCA 2026", "summary": "This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.\n  Our paper proposes {\\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\\% on average and up-to 8.5\\%. BARD requires only 8 bytes of SRAM per LLC slice.", "AI": {"tldr": "\u63d0\u51faBARD\uff08Bank-Aware Replacement Decisions\uff09\u7f13\u5b58\u66ff\u6362\u7b56\u7565\uff0c\u901a\u8fc7\u8003\u8651DDR5 DRAM\u5199\u5165\u5ef6\u8fdf\u7684\u5dee\u5f02\u6027\uff0c\u4f18\u5316\u5199\u5165\u6d41\u4ee5\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "DDR5 DRAM\u4e2d\uff0c\u7531\u4e8e\u7247\u4e0aECC\u7684\u5b58\u5728\uff0c\u5199\u5165\u64cd\u4f5c\u65f6\u95f4\u5dee\u5f02\u663e\u8457\uff081x\u523024x\u4e0d\u7b49\uff09\u3002\u5f53DRAM\u6267\u884c\u5199\u5165\u65f6\u65e0\u6cd5\u670d\u52a1\u8bfb\u53d6\u8bf7\u6c42\uff0c\u5bfc\u81f4\u8bfb\u53d6\u5ef6\u8fdf\u589e\u52a0\u548c\u6027\u80fd\u4e0b\u964d\u3002\u5f53\u524d\u7cfb\u7edf\u7684\u5199\u5165\u6d41\u7531\u7f13\u5b58\u66ff\u6362\u7b56\u7565\u51b3\u5b9a\uff0c\u4f46\u8be5\u7b56\u7565\u4e0d\u4e86\u89e3DRAM\u5199\u5165\u7684\u53ef\u53d8\u5ef6\u8fdf\u7279\u6027\u3002", "method": "\u63d0\u51faBARD\u7b56\u7565\uff0c\u4fee\u6539\u7f13\u5b58\u66ff\u6362\u7b56\u7565\u4ee5\u4f18\u5148\u5904\u7406\u6ca1\u6709\u5f85\u5904\u7406\u5199\u5165\u7684bank\u4e2d\u7684\u810f\u6570\u636e\u884c\u3002\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1aBARD-E\uff08\u57fa\u4e8e\u9a71\u9010\uff09\u548cBARD-C\uff08\u57fa\u4e8e\u6e05\u7406\uff09\uff0c\u4ee5\u53ca\u6df7\u5408\u7b56\u7565BARD-H\u3002BARD\u4ec5\u9700\u6bcf\u4e2aLLC\u5207\u72478\u5b57\u8282SRAM\u5f00\u9500\u3002", "result": "\u5728SPEC2017\u3001LIGRA\u3001STREAM\u548cGoogle\u670d\u52a1\u5668\u8ddf\u8e2a\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cBARD-H\u5e73\u5747\u63d0\u5347\u6027\u80fd4.3%\uff0c\u6700\u9ad8\u53ef\u8fbe8.5%\u3002", "conclusion": "\u901a\u8fc7\u4fee\u6539\u7f13\u5b58\u66ff\u6362\u7b56\u7565\u4ee5\u8003\u8651DRAM\u5199\u5165\u5ef6\u8fdf\u7684\u5dee\u5f02\u6027\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347DDR5\u7cfb\u7edf\u6027\u80fd\uff0cBARD-H\u6df7\u5408\u7b56\u7565\u5728\u6027\u80fd\u548c\u5b9e\u73b0\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2512.18459", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.18459", "abs": "https://arxiv.org/abs/2512.18459", "authors": ["Akul Malhotra", "Sumeet Kumar Gupta"], "title": "Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework", "comment": null, "summary": "The deployment of deep neural networks (DNNs) on compute-in-memory (CiM) accelerators offers significant energy savings and speed-up by reducing data movement during inference. However, the reliability of CiM-based systems is challenged by stuck-at faults (SAFs) in memory cells, which corrupt stored weights and lead to accuracy degradation. While closest value mapping (CVM) has been shown to partially mitigate these effects for multibit DNNs deployed on bit-sliced crossbars, its fault tolerance is often insufficient under high SAF rates or for complex tasks. In this work, we propose two training-free weight transformation techniques, sign-flip and bit-flip, that enhance SAF tolerance in multi-bit DNNs deployed on bit-sliced crossbar arrays. Sign-flip operates at the weight-column level by selecting between a weight and its negation, whereas bit-flip provides finer granularity by selectively inverting individual bit slices. Both methods expand the search space for fault-aware mappings, operate synergistically with CVM, and require no retraining or additional memory. To enable scalability, we introduce a look-up-table (LUT)-based framework that accelerates the computation of optimal transformations and supports rapid evaluation across models and fault rates. Extensive experiments on ResNet-18, ResNet-50, and ViT models with CIFAR-100 and ImageNet demonstrate that the proposed techniques recover most of the accuracy lost under SAF injection. Hardware analysis shows that these methods incur negligible overhead, with sign-flip leading to negligible energy, latency, and area cost, and bit-flip providing higher fault resilience with modest overheads. These results establish sign-flip and bit-flip as practical and scalable SAF-mitigation strategies for CiM-based DNN accelerators.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6743\u91cd\u53d8\u6362\u6280\u672f\uff08sign-flip\u548cbit-flip\uff09\uff0c\u589e\u5f3a\u591a\u6bd4\u7279DNN\u5728\u6bd4\u7279\u5207\u7247\u4ea4\u53c9\u9635\u5217\u4e2d\u7684\u7c98\u6ede\u6545\u969c\u5bb9\u5fcd\u5ea6\uff0c\u4e0eCVM\u534f\u540c\u5de5\u4f5c\uff0c\u65e0\u9700\u91cd\u8bad\u7ec3\u6216\u989d\u5916\u5185\u5b58\u3002", "motivation": "\u8ba1\u7b97\u5185\u5b58\uff08CiM\uff09\u52a0\u901f\u5668\u90e8\u7f72DNN\u80fd\u663e\u8457\u8282\u80fd\u52a0\u901f\uff0c\u4f46\u5185\u5b58\u5355\u5143\u7684\u7c98\u6ede\u6545\u969c\uff08SAFs\uff09\u4f1a\u635f\u574f\u5b58\u50a8\u6743\u91cd\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002\u73b0\u6709\u7684\u6700\u8fd1\u503c\u6620\u5c04\uff08CVM\uff09\u65b9\u6cd5\u5728\u9ad8SAF\u7387\u6216\u590d\u6742\u4efb\u52a1\u4e0b\u5bb9\u9519\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6743\u91cd\u53d8\u6362\u6280\u672f\uff1a1) sign-flip\u5728\u6743\u91cd\u5217\u7ea7\u522b\u9009\u62e9\u6743\u91cd\u6216\u5176\u8d1f\u503c\uff1b2) bit-flip\u901a\u8fc7\u9009\u62e9\u6027\u53cd\u8f6c\u5355\u4e2a\u6bd4\u7279\u5207\u7247\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u4e0eCVM\u534f\u540c\u5de5\u4f5c\uff0c\u65e0\u9700\u91cd\u8bad\u7ec3\u3002\u5f15\u5165\u57fa\u4e8e\u67e5\u627e\u8868\uff08LUT\uff09\u7684\u6846\u67b6\u52a0\u901f\u6700\u4f18\u53d8\u6362\u8ba1\u7b97\u3002", "result": "\u5728ResNet-18\u3001ResNet-50\u548cViT\u6a21\u578b\u4e0a\u4f7f\u7528CIFAR-100\u548cImageNet\u8fdb\u884c\u5b9e\u9a8c\uff0c\u663e\u793a\u63d0\u51fa\u7684\u6280\u672f\u80fd\u6062\u590dSAF\u6ce8\u5165\u4e0b\u7684\u5927\u90e8\u5206\u7cbe\u5ea6\u635f\u5931\u3002\u786c\u4ef6\u5206\u6790\u8868\u660esign-flip\u5e26\u6765\u53ef\u5ffd\u7565\u7684\u5f00\u9500\uff0cbit-flip\u4ee5\u9002\u5ea6\u5f00\u9500\u63d0\u4f9b\u66f4\u9ad8\u5bb9\u9519\u80fd\u529b\u3002", "conclusion": "sign-flip\u548cbit-flip\u662f\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684SAF\u7f13\u89e3\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u57fa\u4e8eCiM\u7684DNN\u52a0\u901f\u5668\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u5185\u5b58\u7c98\u6ede\u6545\u969c\u95ee\u9898\u3002"}}
{"id": "2512.19304", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.19304", "abs": "https://arxiv.org/abs/2512.19304", "authors": ["Emir Devlet Ert\u00f6rer", "Cem \u00dcnsalan"], "title": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA", "comment": "13 pages, 1 figure", "summary": "Binary neural networks provide a promising solution for low-power, high-speed inference by replacing expensive floating-point operations with bitwise logic. This makes them well-suited for deployment on resource-constrained platforms such as FPGAs. In this study, we present a fully custom BNN inference accelerator for handwritten digit recognition, implemented entirely in Verilog without the use of high-level synthesis tools. The design targets the Xilinx Artix-7 FPGA and achieves real-time classification at 80\\,MHz with low power consumption and predictable timing. Simulation results demonstrate 84\\% accuracy on the MNIST test set and highlight the advantages of manual HDL design for transparent, efficient, and flexible BNN deployment in embedded systems. The complete project including training scripts and Verilog source code are available at GitHub repo for reproducibility and future development.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5b8c\u5168\u5b9a\u5236\u7684BNN\u63a8\u7406\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u624b\u5199\u6570\u5b57\u8bc6\u522b\uff0c\u5728Xilinx Artix-7 FPGA\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u5206\u7c7b\uff0c\u8fbe\u523084%\u7684MNIST\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "motivation": "\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\uff08BNN\uff09\u901a\u8fc7\u7528\u4f4d\u8fd0\u7b97\u66ff\u4ee3\u6d6e\u70b9\u8fd0\u7b97\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\uff08\u5982FPGA\uff09\u63d0\u4f9b\u4e86\u4f4e\u529f\u8017\u3001\u9ad8\u901f\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u5b8c\u5168\u5b9a\u5236\u7684BNN\u52a0\u901f\u5668\uff0c\u65e0\u9700\u9ad8\u7ea7\u7efc\u5408\u5de5\u5177\uff0c\u5b9e\u73b0\u900f\u660e\u3001\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5d4c\u5165\u5f0f\u90e8\u7f72\u3002", "method": "\u5b8c\u5168\u4f7f\u7528Verilog\u624b\u52a8\u8bbe\u8ba1BNN\u63a8\u7406\u52a0\u901f\u5668\uff0c\u4e0d\u4f7f\u7528\u9ad8\u7ea7\u7efc\u5408\u5de5\u5177\u3002\u9488\u5bf9Xilinx Artix-7 FPGA\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b080MHz\u7684\u5b9e\u65f6\u5206\u7c7b\u3002\u8bbe\u8ba1\u5305\u62ec\u5b8c\u6574\u7684\u8bad\u7ec3\u811a\u672c\u548cVerilog\u6e90\u4ee3\u7801\u3002", "result": "\u5728MNIST\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523084%\u7684\u51c6\u786e\u7387\uff0c\u5728Xilinx Artix-7 FPGA\u4e0a\u5b9e\u73b080MHz\u7684\u5b9e\u65f6\u5206\u7c7b\uff0c\u5177\u6709\u4f4e\u529f\u8017\u548c\u53ef\u9884\u6d4b\u7684\u65f6\u5e8f\u7279\u6027\u3002\u5b8c\u6574\u9879\u76ee\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "\u624b\u52a8HDL\u8bbe\u8ba1\u4e3aBNN\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u3002\u5f00\u6e90\u4ee3\u7801\u4fc3\u8fdb\u4e86\u53ef\u91cd\u590d\u6027\u548c\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2512.19445", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.19445", "abs": "https://arxiv.org/abs/2512.19445", "authors": ["Guan-Cheng Chen", "Chieh-Lin Tsai", "Pei-Hsuan Tsai", "Yuan-Hao Chang"], "title": "Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory", "comment": null, "summary": "Compute-In-Memory (CIM) systems, particularly those utilizing ReRAM and memristive technologies, offer a promising path toward energy-efficient neural network computation. However, conventional quantization and compression techniques often fail to fully optimize performance and efficiency in these architectures. In this work, we present a structured quantization method that combines sensitivity analysis with mixed-precision strategies to enhance weight storage and computational performance on ReRAM-based CIM systems. Our approach improves ReRAM Crossbar utilization, significantly reducing power consumption, latency, and computational load, while maintaining high accuracy. Experimental results show 86.33% accuracy at 70% compression, alongside a 40% reduction in power consumption, demonstrating the method's effectiveness for power-constrained applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u654f\u611f\u5ea6\u5206\u6790\u548c\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565\u7684\u7ed3\u6784\u5316\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316ReRAM\u5b58\u5185\u8ba1\u7b97\u7cfb\u7edf\u7684\u6743\u91cd\u5b58\u50a8\u548c\u8ba1\u7b97\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u529f\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u538b\u7f29\u6280\u672f\u5728ReRAM\u5b58\u5185\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u65e0\u6cd5\u5145\u5206\u4f18\u5316\u6027\u80fd\u548c\u6548\u7387\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347ReRAM\u4ea4\u53c9\u9635\u5217\u5229\u7528\u7387\u5e76\u964d\u4f4e\u529f\u8017\u3002", "method": "\u7ed3\u5408\u654f\u611f\u5ea6\u5206\u6790\u548c\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565\u7684\u7ed3\u6784\u5316\u91cf\u5316\u65b9\u6cd5\uff0c\u4f18\u5316\u6743\u91cd\u5b58\u50a8\u548c\u8ba1\u7b97\u6027\u80fd\uff0c\u63d0\u9ad8ReRAM\u4ea4\u53c9\u9635\u5217\u5229\u7528\u7387\u3002", "result": "\u572870%\u538b\u7f29\u7387\u4e0b\u8fbe\u523086.33%\u51c6\u786e\u7387\uff0c\u529f\u8017\u964d\u4f4e40%\uff0c\u663e\u8457\u51cf\u5c11\u5ef6\u8fdf\u548c\u8ba1\u7b97\u8d1f\u8f7d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347ReRAM\u5b58\u5185\u8ba1\u7b97\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u9002\u7528\u4e8e\u529f\u8017\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u3002"}}
