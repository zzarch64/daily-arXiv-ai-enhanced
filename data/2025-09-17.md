<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [A Scalable Architecture for Efficient Multi-bit Fully Homomorphic Encryption](https://arxiv.org/abs/2509.12676)
*Jiaao Ma,Ceyu Xu,Lisa Wu Wills*

Main category: cs.AR

TL;DR: Taurus是一个硬件加速器，通过支持10位密文、优化FFT单元和内存带宽，显著提升了多比特TFHE的计算效率，在隐私保护计算中实现了突破性性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决多比特TFHE在云计算隐私保护中的效率瓶颈，特别是其较窄的数值表示范围限制了在需要更宽数值表示的应用中的采用。

Method: 设计Taurus硬件加速器，支持10位密文，采用新型FFT单元，通过密钥重用策略优化内存带宽，并提出带有操作去重功能的编译器来改善内存利用率。

Result: Taurus相比CPU实现2600倍加速，相比GPU实现1200倍加速，比现有最优TFHE加速器快7倍，并首次实现了GPT-2等大语言模型的隐私保护推理。

Conclusion: Taurus的突破性性能提升使得隐私保护计算在云环境中更加实用和可扩展，为安全数据处理开辟了新途径。

Abstract: In the era of cloud computing, privacy-preserving computation offloading is
crucial for safeguarding sensitive data. Fully Homomorphic Encryption (FHE)
enables secure processing of encrypted data, but the inherent computational
complexity of FHE operations introduces significant computational overhead on
the server side. FHE schemes often face a tradeoff between efficiency and
versatility. While the CKKS scheme is highly efficient for polynomial
operations, it lacks the flexibility of the binary TFHE (Torus-FHE) scheme,
which offers greater versatility but at the cost of efficiency. The recent
multi-bit TFHE extension offers greater flexibility and performance by
supporting native non-polynomial operations and efficient integer processing.
However, current implementations of multi-bit TFHE are constrained by its
narrower numeric representation, which prevents its adoption in applications
requiring wider numeric representations.
  To address this challenge, we introduce Taurus, a hardware accelerator
designed to enhance the efficiency of multi-bit TFHE computations. Taurus
supports ciphertexts up to 10 bits by leveraging novel FFT units and optimizing
memory bandwidth through key reuse strategies. We also propose a compiler with
operation deduplication to improve memory utilization. Our experiment results
demonstrate that Taurus achieves up to 2600x speedup over a CPU, 1200x speedup
over a GPU, and up to 7x faster compared to the previous state-of-the-art TFHE
accelerator. Moreover, Taurus is the first accelerator to demonstrate
privacy-preserving inference with large language models such as GPT-2. These
advancements enable more practical and scalable applications of
privacy-preserving computation in cloud environments.

</details>


### [2] [HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large Language Models Inference](https://arxiv.org/abs/2509.12993)
*Cenlin Duan,Jianlei Yang,Rubing Yang,Yikun Wang,Yiou Wang,Lingkun Long,Yingjie Qi,Xiaolin He,Ao Zhou,Xueyan Wang,Weisheng Zhao*

Main category: cs.AR

TL;DR: HPIM是一种针对大语言模型推理的内存中心异构内存处理加速器，通过SRAM-PIM和HBM-PIM子系统协同工作，实现22.8倍于A100 GPU的性能提升


<details>
  <summary>Details</summary>
Motivation: 传统GPU等计算中心加速器在处理大语言模型时存在严重的资源利用不足和内存带宽瓶颈问题，特别是在自回归解码阶段

Method: 采用软硬件协同设计方法，结合专用编译器框架和异构硬件架构，根据操作特性智能分配工作负载：延迟敏感的注意力操作映射到SRAM-PIM，权重密集的GEMV计算分配到HBM-PIM

Result: 通过精确周期模拟器评估，HPIM相比NVIDIA A100 GPU实现了最高22.8倍的加速，且优于同期其他基于PIM的加速器

Conclusion: HPIM作为一个高度实用和可扩展的解决方案，在大规模LLM推理加速方面展现出巨大潜力

Abstract: The deployment of large language models (LLMs) presents significant
challenges due to their enormous memory footprints, low arithmetic intensity,
and stringent latency requirements, particularly during the autoregressive
decoding stage. Traditional compute-centric accelerators, such as GPUs, suffer
from severe resource underutilization and memory bandwidth bottlenecks in these
memory-bound workloads. To overcome these fundamental limitations, we propose
HPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)
accelerator that integrates SRAM-PIM and HBM-PIM subsystems designed
specifically for LLM inference. HPIM employs a software-hardware co-design
approach that combines a specialized compiler framework with a heterogeneous
hardware architecture. It intelligently partitions workloads based on their
characteristics: latency-critical attention operations are mapped to the
SRAM-PIM subsystem to exploit its ultra-low latency and high computational
flexibility, while weight-intensive GEMV computations are assigned to the
HBM-PIM subsystem to leverage its high internal bandwidth and large storage
capacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy
across SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,
thereby significantly mitigating serial dependency of the autoregressive
decoding stage. Comprehensive evaluations using a cycle-accurate simulator
demonstrate that HPIM significantly outperforms state-of-the-art accelerators,
achieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.
Moreover, HPIM exhibits superior performance over contemporary PIM-based
accelerators, highlighting its potential as a highly practical and scalable
solution for accelerating large-scale LLM inference.

</details>


### [3] [Orthrus: Dual-Loop Automated Framework for System-Technology Co-Optimization](https://arxiv.org/abs/2509.13029)
*Yi Ren,Baokang Peng,Chenhao Xue,Kairong Guo,Yukun Wang,Guoyao Cheng,Yibo Lin,Lining Zhang,Guangyu Sun*

Main category: cs.AR

TL;DR: Orthrus是一个双循环自动化框架，通过系统级和技术级协同优化，在7nm技术上实现了12.5%的延迟降低和61.4%的功耗节省


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律收益递减，系统技术协同优化(STCO)成为维持VLSI行业扩展趋势的有前景方法，但现有研究缺乏有效的STCO方法来解决设计层次间的信息鸿沟和跨层设计空间的探索

Method: 提出Orthrus双循环框架：系统级使用新颖机制优先优化关键标准单元，通过贝叶斯优化探索帕累托前沿的法线方向指导技术级优化；技术级利用系统感知洞察优化标准单元库，采用神经网络辅助的增强差分进化算法优化技术参数

Result: 在7nm技术上的实验结果表明，与基线方法相比，在等功耗下实现12.5%的延迟降低，在等延迟下实现61.4%的功耗节省，建立了新的STCO帕累托前沿

Conclusion: Orthrus框架成功解决了STCO中的信息鸿沟和设计空间探索挑战，通过系统级和技术级的协同优化实现了显著的性能提升

Abstract: With the diminishing return from Moore's Law, system-technology
co-optimization (STCO) has emerged as a promising approach to sustain the
scaling trends in the VLSI industry. By bridging the gap between system
requirements and technology innovations, STCO enables customized optimizations
for application-driven system architectures. However, existing research lacks
sufficient discussion on efficient STCO methodologies, particularly in
addressing the information gap across design hierarchies and navigating the
expansive cross-layer design space. To address these challenges, this paper
presents Orthrus, a dual-loop automated framework that synergizes system-level
and technology-level optimizations. At the system level, Orthrus employs a
novel mechanism to prioritize the optimization of critical standard cells using
system-level statistics. It also guides technology-level optimization via the
normal directions of the Pareto frontier efficiently explored by Bayesian
optimization. At the technology level, Orthrus leverages system-aware insights
to optimize standard cell libraries. It employs a neural network-assisted
enhanced differential evolution algorithm to efficiently optimize technology
parameters. Experimental results on 7nm technology demonstrate that Orthrus
achieves 12.5% delay reduction at iso-power and 61.4% power savings at
iso-delay over the baseline approaches, establishing new Pareto frontiers in
STCO.

</details>
