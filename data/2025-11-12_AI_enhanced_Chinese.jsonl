{"id": "2511.07665", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07665", "abs": "https://arxiv.org/abs/2511.07665", "authors": ["Yuzhe Fu", "Changchun Zhou", "Hancheng Ye", "Bowen Duan", "Qiyu Huang", "Chiyue Wei", "Cong Guo", "Hai \"Helen'' Li", "Yiran Chen"], "title": "FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing", "comment": "Accepted for publication in HPCA2026. Codes will be released later", "summary": "Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.", "AI": {"tldr": "FractalCloud\u662f\u4e00\u79cd\u53d7\u5206\u5f62\u542f\u53d1\u7684\u786c\u4ef6\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a213D\u70b9\u4e91\uff0c\u901a\u8fc7\u5f62\u72b6\u611f\u77e5\u5206\u533a\u548c\u5757\u5e76\u884c\u64cd\u4f5c\u89e3\u51b3\u4e86\u73b0\u6709\u52a0\u901f\u5668\u5728\u5904\u7406\u5927\u89c4\u6a21\u70b9\u4e91\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u8bbf\u95ee\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u70b9\u4e91\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5927\u89c4\u6a21\u70b9\u4e91\uff08\u6570\u5341\u4e07\u4e2a\u70b9\uff09\u7684\u9700\u6c42\u589e\u52a0\uff0c\u5168\u5bf9\u5168\u8ba1\u7b97\u548c\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\u5bfc\u81f4O(n\u00b2)\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d41\u91cf\uff0c\u73b0\u6709\u52a0\u901f\u5668\u4e3b\u8981\u9488\u5bf9\u5c0f\u89c4\u6a21\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u6269\u5c55\u3002", "method": "\u63d0\u51faFractalCloud\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u4f18\u5316\uff1a1\uff09\u534f\u540c\u8bbe\u8ba1\u7684Fractal\u65b9\u6cd5\u5b9e\u73b0\u5f62\u72b6\u611f\u77e5\u548c\u786c\u4ef6\u53cb\u597d\u7684\u5206\u533a\uff1b2\uff09\u5757\u5e76\u884c\u70b9\u64cd\u4f5c\uff0c\u5206\u89e3\u5e76\u5e76\u884c\u5316\u6240\u6709\u70b9\u64cd\u4f5c\u3002\u4e13\u7528\u786c\u4ef6\u8bbe\u8ba1\u652f\u6301\u7247\u4e0a\u5206\u5f62\u548c\u7075\u6d3b\u5e76\u884c\u5904\u7406\u3002", "result": "\u572828nm\u6280\u672f\u4e0b\u5b9e\u73b0\uff0c\u6838\u5fc3\u9762\u79ef1.5mm\u00b2\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u52a0\u901f\u5668\u5b9e\u73b0\u4e8621.7\u500d\u52a0\u901f\u548c27\u500d\u80fd\u8017\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u7f51\u7edc\u7cbe\u5ea6\u3002", "conclusion": "FractalCloud\u5c55\u793a\u4e86\u5728\u5927\u89c4\u6a21\u70b9\u4e91\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u5927\u89c4\u6a213D\u70b9\u4e91\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.07985", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.07985", "abs": "https://arxiv.org/abs/2511.07985", "authors": ["Simei Yang", "Xinyu Shi", "Lu Zhao", "Yunyu Ling", "Quanjun Wang", "Francky Catthoor"], "title": "PIMfused: Near-Bank DRAM-PIM with Fused-layer Dataflow for CNN Data Transfer Optimization", "comment": "6 pages", "summary": "Near-bank Processing-in-Memory (PIM) architectures integrate processing cores (PIMcores) close to DRAM banks to mitigate the high cost of off-chip memory accesses. When accelerating convolutional neural network (CNN) on DRAM-PIM, performance is often constrained by cross-bank (or cross-PIMcore) data transfers, which are induced by the conventional layer-by-layer dataflow that enforces inter-bank (or inter-PIMcore) dependencies across successive CNN layers. To address this challenge, we propose PIMfused, a hardware-software co-design that enables fused-layer dataflow for end-to-end CNN execution in near-bank DRAM-PIM. By adopting fused-layer dataflow, PIMfused improves data reuse and, more importantly, breaks inter-bank data dependencies, thereby optimizing cross-bank data transfers without sacrificing bank-level parallelism. We study the impact of buffer sizes and PIMcore parallelism (1-bank vs. 4-bank) on PIMfused using end-to-end ResNet18. We present three key takeaways and show that with 4-bank PIMcores, PIMfused achieves overall PPA gains over a GDDR6-AiM-like baseline, cutting memory cycles to 30.6%, energy to 83.4%, and area to 76.5%.", "AI": {"tldr": "PIMfused\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u8fd1\u5185\u5b58\u5904\u7406\u67b6\u6784\u4e2d\u5b9e\u73b0\u878d\u5408\u5c42\u6570\u636e\u6d41\uff0c\u4f18\u5316CNN\u6267\u884c\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u8de8bank\u6570\u636e\u4f20\u8f93\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u9010\u5c42\u6570\u636e\u6d41\u5728DRAM-PIM\u67b6\u6784\u4e2d\u4f1a\u5bfc\u81f4\u8de8bank\u6570\u636e\u4f20\u8f93\uff0c\u6210\u4e3aCNN\u52a0\u901f\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faPIMfused\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6848\uff0c\u91c7\u7528\u878d\u5408\u5c42\u6570\u636e\u6d41\uff0c\u6253\u7834bank\u95f4\u6570\u636e\u4f9d\u8d56\uff0c\u4f18\u5316\u8de8bank\u6570\u636e\u4f20\u8f93\u3002", "result": "\u57284-bank PIMcores\u914d\u7f6e\u4e0b\uff0c\u76f8\u6bd4GDDR6-AiM\u57fa\u7ebf\uff0c\u5185\u5b58\u5468\u671f\u51cf\u5c11\u81f330.6%\uff0c\u80fd\u8017\u964d\u81f383.4%\uff0c\u9762\u79ef\u964d\u81f376.5%\u3002", "conclusion": "PIMfused\u901a\u8fc7\u878d\u5408\u5c42\u6570\u636e\u6d41\u6709\u6548\u89e3\u51b3\u4e86DRAM-PIM\u67b6\u6784\u4e2d\u8de8bank\u6570\u636e\u4f20\u8f93\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684PPA\u63d0\u5347\u3002"}}
{"id": "2511.08054", "categories": ["cs.AR", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08054", "abs": "https://arxiv.org/abs/2511.08054", "authors": ["Yunqi Shi", "Xi Lin", "Zhiang Wang", "Siyuan Xu", "Shixiong Kai", "Yao Lai", "Chengrui Gao", "Ke Xue", "Mingxuan Yuan", "Chao Qian", "Zhi-Hua Zhou"], "title": "Re$^{\\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating", "comment": "IEEE Transactions on Comupter-Aided Design under review", "summary": "This work introduces the Re$^{\\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.", "AI": {"tldr": "Re$^{\\text{2}}$MaP\u901a\u8fc7\u9012\u5f52\u539f\u578b\u6784\u5efa\u548c\u57fa\u4e8e\u6811\u7684\u91cd\u65b0\u5e03\u5c40\uff0c\u751f\u6210\u4e13\u5bb6\u7ea7\u5b8f\u5355\u5143\u5e03\u5c40\uff0c\u663e\u8457\u6539\u5584\u4e86\u65f6\u5e8f\u6307\u6807WNS\u548cTNS\u3002", "motivation": "\u73b0\u6709\u5b8f\u5355\u5143\u5e03\u5c40\u65b9\u6cd5\u5728\u65f6\u5e8f\u4f18\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8003\u8651\u7ebf\u957f\u3001\u6570\u636e\u6d41\u548c\u8bbe\u8ba1\u7ea6\u675f\u7684\u81ea\u52a8\u5316\u5e03\u5c40\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u5b8f\u5206\u7ec4\u548cPPA\u611f\u77e5\u5355\u5143\u805a\u7c7b\uff0c\u6784\u5efa\u7edf\u4e00\u8fde\u63a5\u77e9\u9635\uff1b\u4f7f\u7528DREAMPlace\u6784\u5efa\u6df7\u5408\u5c3a\u5bf8\u5e03\u5c40\u539f\u578b\uff1b\u63d0\u51faABPlace\u692d\u5706\u5e03\u5c40\u4f18\u5316\u65b9\u6cd5\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u6811\u7684\u91cd\u65b0\u5e03\u5c40\u8fc7\u7a0b\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5b66\u672f\u5e03\u5c40\u5668Hier-RTLMP\uff0cWNS\u63d0\u5347\u6700\u9ad822.22%\uff08\u5e73\u574710.26%\uff09\uff0cTNS\u63d0\u5347\u6700\u9ad897.91%\uff08\u5e73\u574733.97%\uff09\u3002", "conclusion": "Re$^{\\text{2}}$MaP\u65b9\u6cd5\u5728\u65f6\u5e8f\u3001\u529f\u8017\u3001DRC\u8fdd\u89c4\u548c\u8fd0\u884c\u65f6\u95f4\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u9012\u5f52\u539f\u578b\u6784\u5efa\u548c\u57fa\u4e8e\u6811\u91cd\u65b0\u5e03\u5c40\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.08315", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08315", "abs": "https://arxiv.org/abs/2511.08315", "authors": ["Mingkai Miao", "Jianheng Tang", "Guangyu Hu", "Hongce Zhang"], "title": "BDD2Seq: Enabling Scalable Reversible-Circuit Synthesis via Graph-to-Sequence Learning", "comment": null, "summary": "Binary Decision Diagrams (BDDs) are instrumental in many electronic design automation (EDA) tasks thanks to their compact representation of Boolean functions. In BDD-based reversible-circuit synthesis, which is critical for quantum computing, the chosen variable ordering governs the number of BDD nodes and thus the key metrics of resource consumption, such as Quantum Cost. Because finding an optimal variable ordering for BDDs is an NP-complete problem, existing heuristics often degrade as circuit complexity grows. We introduce BDD2Seq, a graph-to-sequence framework that couples a Graph Neural Network encoder with a Pointer-Network decoder and Diverse Beam Search to predict high-quality orderings. By treating the circuit netlist as a graph, BDD2Seq learns structural dependencies that conventional heuristics overlooked, yielding smaller BDDs and faster synthesis. Extensive experiments on three public benchmarks show that BDD2Seq achieves around 1.4 times lower Quantum Cost and 3.7 times faster synthesis than modern heuristic algorithms. To the best of our knowledge, this is the first work to tackle the variable-ordering problem in BDD-based reversible-circuit synthesis with a graph-based generative model and diversity-promoting decoding.", "AI": {"tldr": "BDD2Seq\u662f\u4e00\u4e2a\u56fe\u5230\u5e8f\u5217\u6846\u67b6\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u5668\u548c\u6307\u9488\u7f51\u7edc\u89e3\u7801\u5668\u6765\u9884\u6d4bBDD\u53d8\u91cf\u6392\u5e8f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u91cf\u5b50\u6210\u672c\u548c\u5408\u6210\u65f6\u95f4\u3002", "motivation": "\u5728\u57fa\u4e8eBDD\u7684\u53ef\u9006\u7535\u8def\u5408\u6210\u4e2d\uff0c\u53d8\u91cf\u6392\u5e8f\u76f4\u63a5\u5f71\u54cdBDD\u8282\u70b9\u6570\u91cf\u548c\u91cf\u5b50\u6210\u672c\u7b49\u5173\u952e\u6307\u6807\u3002\u7531\u4e8e\u5bfb\u627e\u6700\u4f18\u53d8\u91cf\u6392\u5e8f\u662fNP\u5b8c\u5168\u95ee\u9898\uff0c\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u7535\u8def\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faBDD2Seq\u6846\u67b6\uff0c\u5c06\u7535\u8def\u7f51\u8868\u89c6\u4e3a\u56fe\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u5668\u5b66\u4e60\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u6307\u9488\u7f51\u7edc\u89e3\u7801\u5668\u9884\u6d4b\u53d8\u91cf\u6392\u5e8f\uff0c\u5e76\u91c7\u7528\u591a\u6837\u5316\u6ce2\u675f\u641c\u7d22\u63d0\u9ad8\u7ed3\u679c\u8d28\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBDD2Seq\u76f8\u6bd4\u73b0\u4ee3\u542f\u53d1\u5f0f\u7b97\u6cd5\u5b9e\u73b0\u4e86\u7ea61.4\u500d\u7684\u91cf\u5b50\u6210\u672c\u964d\u4f4e\u548c3.7\u500d\u7684\u5408\u6210\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u751f\u6210\u6a21\u578b\u548c\u591a\u6837\u6027\u89e3\u7801\u6765\u89e3\u51b3BDD\u53ef\u9006\u7535\u8def\u5408\u6210\u4e2d\u53d8\u91cf\u6392\u5e8f\u95ee\u9898\u7684\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u6548\u7387\u3002"}}
{"id": "2511.08395", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.08395", "abs": "https://arxiv.org/abs/2511.08395", "authors": ["Xingyu Liu", "Jiawei Liang", "Yipu Zhang", "Linfeng Du", "Chaofang Ma", "Hui Yu", "Jiang Xu", "Wei Zhang"], "title": "DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator", "comment": null, "summary": "We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eFPGA\u7684\u786c\u4ef6\u9ad8\u6548RBD\u52a0\u901f\u5668\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u7cbe\u5ea6\u611f\u77e5\u91cf\u5316\u6846\u67b6\u3001\u9664\u6cd5\u5ef6\u8fdf\u4f18\u5316\u548c\u6a21\u5757\u95f4DSP\u590d\u7528\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u673a\u5668\u4eba\u7c7b\u578b\u4e0a\u5b9e\u73b08\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c7.4\u500d\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u4e3a\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u786c\u4ef6\u9ad8\u6548\u7684RBD\uff08\u521a\u4f53\u52a8\u529b\u5b66\uff09\u52a0\u901f\u5668\uff0c\u89e3\u51b3\u73b0\u6709\u52a0\u901f\u5668\u5728\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "1. \u7cbe\u5ea6\u611f\u77e5\u91cf\u5316\u6846\u67b6\u51cf\u5c11DSP\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u7cbe\u5ea6\uff1b2. \u8d28\u91cf\u77e9\u9635\u6c42\u9006\u7b97\u6cd5\u4e2d\u7684\u9664\u6cd5\u5ef6\u8fdf\u4f18\u5316\uff1b3. \u6a21\u5757\u95f4DSP\u590d\u7528\u65b9\u6cd5\u63d0\u9ad8DSP\u5229\u7528\u7387\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684RBD\u52a0\u901f\u5668\uff0c\u5728\u591a\u79cd\u673a\u5668\u4eba\u7c7b\u578b\u4e0a\u5b9e\u73b0\u6700\u9ad88\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c7.4\u500d\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.08575", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.08575", "abs": "https://arxiv.org/abs/2511.08575", "authors": ["Zhenxiao Fu", "Chen Fan", "Lei Jiang"], "title": "CO2-Meter: A Comprehensive Carbon Footprint Estimator for LLMs on Edge Devices", "comment": null, "summary": "LLMs have transformed NLP, yet deploying them on edge devices poses great carbon challenges. Prior estimators remain incomplete, neglecting peripheral energy use, distinct prefill/decode behaviors, and SoC design complexity. This paper presents CO2-Meter, a unified framework for estimating operational and embodied carbon in LLM edge inference. Contributions include: (1) equation-based peripheral energy models and datasets; (2) a GNN-based predictor with phase-specific LLM energy data; (3) a unit-level embodied carbon model for SoC bottleneck analysis; and (4) validation showing superior accuracy over prior methods. Case studies show CO2-Meter's effectiveness in identifying carbon hotspots and guiding sustainable LLM design on edge platforms. Source code: https://github.com/fuzhenxiao/CO2-Meter", "AI": {"tldr": "CO2-Meter\u662f\u4e00\u4e2a\u7528\u4e8e\u4f30\u8ba1LLM\u8fb9\u7f18\u63a8\u7406\u4e2d\u8fd0\u8425\u78b3\u548c\u5d4c\u5165\u78b3\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u5916\u56f4\u80fd\u8017\u3001\u9884\u586b\u5145/\u89e3\u7801\u9636\u6bb5\u5dee\u5f02\u548cSoC\u8bbe\u8ba1\u590d\u6742\u6027\uff0c\u63d0\u4f9b\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u7684\u78b3\u8db3\u8ff9\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709LLM\u78b3\u8db3\u8ff9\u4f30\u7b97\u65b9\u6cd5\u4e0d\u5b8c\u6574\uff0c\u5ffd\u89c6\u4e86\u5916\u56f4\u8bbe\u5907\u80fd\u8017\u3001\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u4e0d\u540c\u884c\u4e3a\u4ee5\u53caSoC\u8bbe\u8ba1\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72LLM\u65f6\u7684\u78b3\u6311\u6218\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCO2-Meter\u6846\u67b6\uff0c\u5305\u62ec\uff1a(1)\u57fa\u4e8e\u65b9\u7a0b\u7684\u5916\u56f4\u80fd\u8017\u6a21\u578b\u548c\u6570\u636e\u96c6\uff1b(2)\u4f7f\u7528GNN\u9884\u6d4b\u5668\u548c\u9636\u6bb5\u7279\u5b9aLLM\u80fd\u8017\u6570\u636e\uff1b(3)SoC\u74f6\u9888\u5206\u6790\u7684\u5355\u5143\u7ea7\u5d4c\u5165\u78b3\u6a21\u578b\uff1b(4)\u9a8c\u8bc1\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "result": "CO2-Meter\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u5728\u8bc6\u522b\u78b3\u70ed\u70b9\u548c\u6307\u5bfc\u8fb9\u7f18\u5e73\u53f0\u53ef\u6301\u7eedLLM\u8bbe\u8ba1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "CO2-Meter\u4e3aLLM\u8fb9\u7f18\u63a8\u7406\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u78b3\u8db3\u8ff9\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u78b3\u70ed\u70b9\u5e76\u6307\u5bfc\u53ef\u6301\u7eed\u8bbe\u8ba1\u51b3\u7b56\u3002"}}
