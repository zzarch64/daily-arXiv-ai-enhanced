{"id": "2509.13557", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13557", "abs": "https://arxiv.org/abs/2509.13557", "authors": ["Zesong Jiang", "Yuqi Sun", "Qing Zhong", "Mahathi Krishna", "Deepak Patil", "Cheng Tan", "Sriram Krishnamoorthy", "Jeff Zhang"], "title": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for CGRAs", "comment": null, "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."}
{"id": "2509.13694", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13694", "abs": "https://arxiv.org/abs/2509.13694", "authors": ["Hanchen Ye", "Deming Chen"], "title": "StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs", "comment": "Accepted by MICRO'25", "summary": "Efficient execution of deep learning workloads on dataflow architectures is\ncrucial for overcoming memory bottlenecks and maximizing performance. While\nstreaming intermediate results between computation kernels can significantly\nimprove efficiency, existing approaches struggle with inter-kernel\ncorrelations, external memory access management, and buffer optimization. In\nthis work, we propose StreamTensor, a compiler framework that automatically\nconstructs and optimizes stream-based dataflow accelerators. StreamTensor\nintroduces a novel iterative tensor type system to explicitly encode stream\nlayouts, enabling seamless kernel fusion, buffer allocation, and memory\noptimization. By systematically exploring three hierarchical design spaces,\nincluding tensor tiling, kernel fusion, and resource allocation, StreamTensor\nbalances computational intensity, memory efficiency, and data streaming to\nmaximize performance. Based on FPGA evaluations on Large Language Models (LLM),\nStreamTensor achieves up to 0.76x and 0.64x lower latency compared to the\nstate-of-the-art FPGA LLM accelerators and GPUs, and up to 1.99x higher energy\nefficiency compared to GPUs, making it a promising approach for scalable\ndataflow-based deep learning acceleration."}
{"id": "2509.13710", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13710", "abs": "https://arxiv.org/abs/2509.13710", "authors": ["Hongyi Li", "Songchen Ma", "Huanyu Qu", "Weihao Zhang", "Jia Chen", "Junfeng Lin", "Fengbin Tu", "Rong Zhao"], "title": "CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized\nvarious aspects of human life, yet their immense computational and energy\ndemands pose significant challenges for efficient inference. The memory wall,\nthe growing processor-memory speed disparity, remains a critical bottleneck for\nLLM. Process-In-Memory (PIM) architectures overcome limitations by co-locating\ncompute units with memory, leveraging 5-20$\\times$ higher internal bandwidth\nand enabling greater energy efficiency than GPUs. However, existing PIMs\nstruggle to balance flexibility, performance, and cost-efficiency for LLMs'\ndynamic memory-compute patterns and operator diversity. DRAM-PIM suffers from\ninter-bank communication overhead despite its vector parallelism. SRAM-PIM\noffers sub-10ns latency for matrix operation but is constrained by limited\ncapacity. This work introduces CompAir, a novel PIM architecture that\nintegrates DRAM-PIM and SRAM-PIM with hybrid bonding, enabling efficient linear\ncomputations while unlocking multi-granularity data pathways. We further\ndevelop CompAir-NoC, an advanced network-on-chip with an embedded arithmetic\nlogic unit that performs non-linear operations during data movement,\nsimultaneously reducing communication overhead and area cost. Finally, we\ndevelop a hierarchical Instruction Set Architecture that ensures both\nflexibility and programmability of the hybrid PIM. Experimental results\ndemonstrate that CompAir achieves 1.83-7.98$\\times$ prefill and\n1.95-6.28$\\times$ decode improvement over the current state-of-the-art fully\nPIM architecture. Compared to the hybrid A100 and HBM-PIM system, CompAir\nachieves 3.52$\\times$ energy consumption reduction with comparable throughput.\nThis work represents the first systematic exploration of hybrid DRAM-PIM and\nSRAM-PIM architectures with in-network computation capabilities, offering a\nhigh-efficiency solution for LLM."}
{"id": "2509.13765", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13765", "abs": "https://arxiv.org/abs/2509.13765", "authors": ["Zhirui Huang", "Rui Ma", "Shijie Cao", "Ran Shu", "Ian Wang", "Ting Cao", "Chixiao Chen", "Yongqiang Xiong"], "title": "TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary LLM Inference On Edge", "comment": null, "summary": "Ternary quantization has emerged as a powerful technique for reducing both\ncomputational and memory footprint of large language models (LLM), enabling\nefficient real-time inference deployment without significantly compromising\nmodel accuracy. Conventional LLM inference platforms (e.g GPUs) cannot\ncapitalize on its benefits, as they (i) lack native support for ternary\narithmetic and memory specialization and (ii) remain severely under-utilized in\nlow-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware\nLUT-centric architecture that co-optimizes algorithm, compute, and memory for\nternary LLM inference. To maximize the efficiency of Ternary Linear layer,\nTENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary\nmixed-precision GEMM using a symmetric precompute lookup table. It also\nfeatures Dynamic Activation N:M Sparsity to exploit the sparsity within the\nactivation of each token. Additionally, we propose a LUT-based 64B:80B ternary\nweight decompression module to fully exploit the memory efficiency of ternary\nvalues. At the system level, we design a heterogeneous TENET accelerator with\nfull programmability that integrates STL cores with high-precision cores. An\nassociated Linear-Projection-aware Sparse Attention dataflow is introduced to\noptimize memory access and hardware utilization. We implement TENET accelerator\nprototype on both FPGA and ASIC platforms. Experiments across various model\nsizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy\nefficiency by 4.3$\\times$ and 21.1$\\times$, respectively, compared to the A100\nGPU. Furthermore, TENET-ASIC achieves a 2.7$\\times$ average speedup compared to\nthe A100 GPU in end-to-end inference latency."}
{"id": "2509.13997", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13997", "abs": "https://arxiv.org/abs/2509.13997", "authors": ["Yu Zhu", "Aditya Dhakal", "Pedro Bruel", "Gourav Rattihalli", "Yunming Xiao", "Johann Lombardi", "Dejan Milojicic"], "title": "An RDMA-First Object Storage System with SmartNIC Offload", "comment": null, "summary": "AI training and inference impose sustained, fine-grain I/O that stresses\nhost-mediated, TCP-based storage paths. Motivated by kernel-bypass networking\nand user-space storage stacks, we revisit POSIX-compatible object storage for\nGPU-centric pipelines. We present ROS2, an RDMA-first object storage system\ndesign that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while\nleaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a\nlightweight control plane (gRPC for namespace and capability exchange) from a\nhigh-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host\nmediation from the data path.\n  Using FIO/DFS across local and remote configurations, we find that on\nserver-grade CPUs RDMA consistently outperforms TCP for both large sequential\nand small random I/O. When the RDMA-driven DAOS client is offloaded to\nBlueField-3, end-to-end performance is comparable to the host, demonstrating\nthat SmartNIC offload preserves RDMA efficiency while enabling DPU-resident\nfeatures such as multi-tenant isolation and inline services (e.g.,\nencryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags\nhost performance, underscoring the importance of RDMA for offloaded\ndeployments.\n  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded\nobject-storage stack is a practical foundation for scaling data delivery in\nmodern LLM training environments; integrating optional GPU-direct placement for\nLLM tasks is left for future work."}
{"id": "2509.14041", "categories": ["cs.AR", "cs.CL", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.14041", "abs": "https://arxiv.org/abs/2509.14041", "authors": ["Henry Kao", "Nikhil Sreekumar", "Prabhdeep Singh Soni", "Ali Sedaghati", "Fang Su", "Bryan Chan", "Maziar Goudarzi", "Reza Azimi"], "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching", "comment": null, "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."}
