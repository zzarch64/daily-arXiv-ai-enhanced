<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [smallNet: Implementation of a convolutional layer in tiny FPGAs](https://arxiv.org/abs/2509.25391)
*Fernanda Zapata Bascuñán,Alan Ezequiel Fuster*

Main category: cs.AR

TL;DR: 开发了一个完全用Verilog实现的卷积层，无需Python库和IP核，可在低成本FPGA、SoM、SoC和ASIC上部署，相比计算机版本实现5.1倍加速，81%分类准确率，功耗仅1.5W。


<details>
  <summary>Details</summary>
Motivation: 当前Xilinx和VLSI中的神经网络开发系统需要与Python库协同开发，限制了在嵌入式设备上的部署。

Method: 基于滤波器多项式结构，完全用Verilog手写设计卷积层，避免使用IP核。

Result: 在单核Cora Z7上验证，实现5.1倍加速，超过81%分类准确率，总功耗1.5W。

Conclusion: 该架构适用于实时、资源受限的嵌入式应用，展示了在FPGA上高效部署神经网络的可行性。

Abstract: Since current neural network development systems in Xilinx and VLSI require
codevelopment with Python libraries, the first stage of a convolutional network
has been implemented by developing a convolutional layer entirely in Verilog.
This handcoded design, free of IP cores and based on a filter polynomial like
structure, enables straightforward deployment not only on low cost FPGAs but
also on SoMs, SoCs, and ASICs. We analyze the limitations of numerical
representations and compare our implemented architecture, smallNet, with its
computer based counterpart, demonstrating a 5.1x speedup, over 81%
classification accuracy, and a total power consumption of just 1.5 W. The
algorithm is validated on a single-core Cora Z7, demonstrating its feasibility
for real time, resource-constrained embedded applications.

</details>


### [2] [LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels](https://arxiv.org/abs/2509.25626)
*Yi Hu,Huiyang Zhou*

Main category: cs.AR

TL;DR: 使用大型语言模型（LLMs）优化3D高斯泼溅（3DGS）GPU内核，在MipNeRF360数据集上实现了显著性能提升（最高42%），展示了LLMs在专业代码优化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 3DGS在实时渲染中很重要，但GPU架构复杂且调优参数空间大，手动优化需要专业知识且耗时易错。

Method: 利用LLMs分析和优化高斯泼溅内核，结合性能分析器提供额外信息，并与LLMs协作优化。

Result: 原始3DGS代码上，Deepseek和GPT-5分别实现19%和24%加速；结合性能分析器后提升至最高42%；在Seele框架上仍能进一步优化6%。

Conclusion: LLMs在GPU内核优化中表现出色，但仍有手动优化无法替代的部分，专家与LLMs协作具有巨大潜力。

Abstract: 3D Gaussian splatting (3DGS) is a transformative technique with profound
implications on novel view synthesis and real-time rendering. Given its
importance, there have been many attempts to improve its performance. However,
with the increasing complexity of GPU architectures and the vast search space
of performance-tuning parameters, it is a challenging task. Although manual
optimizations have achieved remarkable speedups, they require domain expertise
and the optimization process can be highly time consuming and error prone. In
this paper, we propose to exploit large language models (LLMs) to analyze and
optimize Gaussian splatting kernels. To our knowledge, this is the first work
to use LLMs to optimize highly specialized real-world GPU kernels. We reveal
the intricacies of using LLMs for code optimization and analyze the code
optimization techniques from the LLMs. We also propose ways to collaborate with
LLMs to further leverage their capabilities. For the original 3DGS code on the
MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and
24% with GPT-5, demonstrating the different capabilities of different LLMs. By
feeding additional information from performance profilers, the performance
improvement from LLM-optimized code is enhanced to up to 42% and 38% on
average. In comparison, our best-effort manually optimized version can achieve
a performance improvement up to 48% and 39% on average, showing that there are
still optimizations beyond the capabilities of current LLMs. On the other hand,
even upon a newly proposed 3DGS framework with algorithmic optimizations,
Seele, LLMs can still further enhance its performance by 6%, showing that there
are optimization opportunities missed by domain experts. This highlights the
potential of collaboration between domain experts and LLMs.

</details>


### [3] [SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV](https://arxiv.org/abs/2509.25853)
*Jingyao Zhang,Jaewoo Park,Jongeun Lee,Elaheh Sadredini*

Main category: cs.AR

TL;DR: SAIL是一种基于CPU的LLM推理解决方案，通过SRAM加速和查找表技术，支持任意比特精度，在CPU上实现高效的大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: CPU推理对AI民主化至关重要，但面临两个挑战：CPU架构难以处理量化模型的低精度算术，以及token生成阶段的内存瓶颈问题。

Method: 提出SAIL架构，包含三个关键技术：批处理LUT-GEMV与SRAM内存计算、模式感知LUT优化、内存内类型转换算法，仅需2%硬件开销和一个新指令。

Result: 实验显示SAIL相比ARM Neoverse-N1 CPU基线实现10.7倍加速和19.9倍每美元token数提升，比NVIDIA V100 GPU成本效率高7.04倍。

Conclusion: SAIL为基于CPU的高效LLM推理建立了实用路径，解决了CPU推理的关键瓶颈问题。

Abstract: Large Language Model (LLM) inference requires substantial computational
resources, yet CPU-based inference remains essential for democratizing AI due
to the widespread availability of CPUs compared to specialized accelerators.
However, efficient LLM inference on CPUs faces two fundamental challenges: (1)
existing CPU architectures struggle with low-precision arithmetic required by
quantized models, where optimal bit precision varies across models and layers;
and (2) the memory-bound nature of the token generation phase creates severe
performance bottlenecks. To address these challenges, we propose SAIL
(SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that
efficiently supports arbitrary bit precisions with minimal overhead. SAIL
integrates three key innovations: First, we introduce Batched LUT-based General
Matrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory,
enabling high data reuse through lookup tables and reducing memory movement.
Second, our Pattern-Aware LUT optimization identifies and exploits redundancy
in input activation patterns, reducing computation cycles by 13.8\%. Third, we
develop an in-memory type conversion algorithm that leverages PIM's parallelism
for efficient de-/quantization operations, alleviating pressure on CPU's vector
units. Our architecture requires only 2\% hardware overhead and a single new
instruction, while maintaining dual functionality as both compute and storage
units. Experimental evaluations using a modified gem5 simulator demonstrate
that SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar
compared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost
efficiency than NVIDIA V100 GPUs, establishing a practical path for efficient
CPU-based LLM inference.

</details>


### [4] [Runtime Energy Monitoring for RISC-V Soft-Cores](https://arxiv.org/abs/2509.26065)
*Alberto Scionti,Paolo Savio,Francesco Lubrano,Olivier Terzo,Marco Ferretti,Florin Apopei,Juri Bellucci,Ennio Spano,Luca Carriere*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的运行时能耗监控方法，无需复杂架构模型即可实时测量计算系统的能耗


<details>
  <summary>Details</summary>
Motivation: 传统能耗监控依赖复杂的软件工具和架构模型，在设计空间探索时需要不断调整模型参数，存在效率低下和准确性不足的问题

Method: 采用测量板与FPGA系统模块结合的方法，通过测量板捕获电流和电压（最多数十个测量点），并通过特定内存区域暴露这些值，运行服务读取并计算能耗统计数据

Result: 该方法能够在运行时监控能耗，不消耗FPGA设备的额外资源，并可扩展到多节点基础设施（集群）的监控

Conclusion: 该框架为航空设计应用中的实验提供了有效工具，特别关注在RISC-V软核上优化浅层人工神经网络的性能和能耗

Abstract: Energy efficiency is one of the major concern in designing advanced computing
infrastructures. From single nodes to large-scale systems (data centers),
monitoring the energy consumption of the computing system when applications run
is a critical task. Designers and application developers often rely on software
tools and detailed architectural models to extract meaningful information and
determine the system energy consumption. However, when a design space
exploration is required, designers may incur in continuous tuning of the models
to match with the system under evaluation. To overcome such limitations, we
propose a holistic approach to monitor energy consumption at runtime without
the need of running complex (micro-)architectural models. Our approach is based
on a measurement board coupled with a FPGA-based System-on-Module. The
measuring board captures currents and voltages (up to tens measuring points)
driving the FPGA and exposes such values through a specific memory region. A
running service reads and computes energy consumption statistics without
consuming extra resources on the FPGA device. Our approach is also scalable to
monitoring of multi-nodes infrastructures (clusters). We aim to leverage this
framework to perform experiments in the context of an aeronautical design
application; specifically, we will look at optimizing performance and energy
consumption of a shallow artificial neural network on RISC-V based soft-cores.

</details>
