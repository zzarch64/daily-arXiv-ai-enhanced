<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI](https://arxiv.org/abs/2511.21232)
*Muhammed Yildirim,Ozcan Ozturk*

Main category: cs.AR

TL;DR: 提出了一种用于边缘AI加速器的融合像素数据流架构，通过消除中间缓冲区，显著减少深度可分离卷积中的数据移动和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI中深度可分离卷积因逐层执行导致的中间特征图传输瓶颈，克服内存墙问题。

Method: 设计基于RISC-V的定制功能单元，采用融合像素数据流，在深度可分离卷积的所有阶段（扩展、深度卷积、投影）中流式处理数据，无需中间缓冲。

Result: 相比基线软件执行实现59.3倍加速，数据移动减少87%；ASIC合成显示在28nm工艺下面积0.284mm²，功耗910mW@2GHz。

Conclusion: 证实了在TinyML资源约束下实现零缓冲数据流的可行性，为边缘AI加速器克服内存墙提供了有效策略。

Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.

</details>


### [2] [Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration](https://arxiv.org/abs/2511.21346)
*Mohamed Shahawy,Julien de Castelnau,Paolo Ienne*

Main category: cs.AR

TL;DR: Bombyx是一个编译器工具链，将OpenCilk程序转换为Cilk-1风格的中间表示，使CPU导向的任务级并行应用能高效映射到FPGA空间架构上。


<details>
  <summary>Details</summary>
Motivation: 现有系统在FPGA上支持任务级并行时，OpenCilk的隐式任务模型需要昂贵的硬件上下文切换，而Cilk-1的显式延续传递模型更适合FPGA的流式特性。

Method: 开发Bombyx编译器工具链，支持多个编译目标：OpenCilk兼容运行时和可综合处理单元生成器，并引入解耦访问执行优化。

Result: 能够自动生成高性能处理单元，改善内存计算重叠和整体吞吐量。

Conclusion: Bombyx成功将CPU导向的TLP应用映射到FPGA空间架构，通过Cilk-1模型和优化技术提升了性能。

Abstract: Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.

</details>


### [3] [A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm](https://arxiv.org/abs/2511.21451)
*Flurin Arquint,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 首个ASIC实现的多天线抗干扰时间同步系统，支持单天线发射器与16天线接收器之间的同步，能抵御最多2个发射天线的智能干扰器。


<details>
  <summary>Details</summary>
Motivation: 解决无线通信中时间同步信号容易受到智能干扰攻击的问题，确保在干扰环境下仍能实现可靠的时间同步。

Method: 采用多天线处理算法，通过ASIC硬件实现抗干扰同步技术，支持1.28 MS/s的采样率。

Result: 在65nm工艺下实现，核心面积2.87 mm²，功耗310 mW，成功实现了抗干扰时间同步功能。

Conclusion: 该ASIC设计证明了多天线抗干扰时间同步技术的硬件可行性，为安全无线通信系统提供了重要支撑。

Abstract: We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).

</details>


### [4] [A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection](https://arxiv.org/abs/2511.21461)
*Jonas Elmiger,Fabian Stuber,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 提出首个单输入多输出接收器ASIC，联合执行干扰抑制、信道估计和数据检测，采用MAED算法在22nm FD-SOI工艺下实现，面积0.32mm²，吞吐量100Mb/s，功耗223mW。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以有效对抗智能干扰器，需要开发能够同时处理干扰抑制、信道估计和数据检测的集成解决方案。

Method: 采用MAED算法，通过非线性优化问题实现空间滤波，统一干扰估计与消除、信道估计和数据检测。

Result: 支持8个接收天线，可抑制智能干扰器和阻塞干扰器，相比现有抗干扰检测器，用户吞吐量提高3倍，面积效率提高4.5倍。

Conclusion: 该ASIC实现了高效的联合干扰抑制、信道估计和数据检测，为抗干扰通信系统提供了高性能解决方案。

Abstract: We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\times$ higher per-user throughput and 4.5$\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.

</details>


### [5] [Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators](https://arxiv.org/abs/2511.21549)
*Jason Yik,Walter Gallego Gomez,Andrew Cheng,Benedetto Leto,Alessandro Pierro,Noah Pacik-Nelson,Korneel Van den Berghe,Vittorio Fra,Andreea Danielescu,Gianvito Urgese,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: 本文首次对神经形态加速器进行了全面的性能边界和瓶颈分析，揭示了传统指标的不足，提出了floorline性能模型和优化方法，在保持准确性的同时实现了3.86倍运行时间改进和3.38倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 神经形态加速器具有独特的架构特性，其性能动态与传统加速器存在根本差异。现有的工作负载优化方法依赖于聚合的网络级稀疏性和操作计数，但这些指标对实际部署性能的改善程度尚不明确。

Method: 采用理论分析建模和实证表征相结合的方法，分析了三个真实的神经形态加速器（Brainchip AKD1000、Synsense Speck和Intel Loihi 2），建立了floorline性能模型，并开发了结合稀疏感知训练和floorline指导分区的优化方法。

Result: 识别了三种不同的加速器瓶颈状态（内存受限、计算受限和流量受限），并确定了可能表现出这些瓶颈状态的工作负载配置特征。优化方法在保持准确性的同时实现了最高3.86倍的运行时间改进和3.38倍的能耗降低。

Conclusion: 本文提供了对神经形态加速器性能边界的深入理解，提出的floorline性能模型能够识别性能边界并指导工作负载优化，为神经形态加速器的有效利用提供了理论基础和实践方法。

Abstract: Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.

</details>
