<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI Reconstruction and Analysis](https://arxiv.org/abs/2510.01730)
*Ashiyana Abdul Majeed,Mahmoud Meribout,Safa Mohammed Sali*

Main category: cs.AR

TL;DR: 提出了一种硬件加速的多模型系统，用于从CT图像同时进行MRI重建和诊断，在NVIDIA边缘GPU上实现了近150帧/秒的实时性能。


<details>
  <summary>Details</summary>
Motivation: 随着专用边缘设备的出现，高效利用其加速器变得至关重要，但目前很少有研究关注硬件加速的多模型系统优化。

Method: 利用现代NVIDIA边缘GPU的硬件引擎和调度技术，对多个AI模型的不同层进行硬件分配以减少硬件引擎间的理想时间，并对GAN模型进行微调以避免回退到GPU引擎执行。

Result: 在Jetson AGX Xavier和Orin设备上实现了近150帧/秒的吞吐量，微调后的边缘GPU感知AI模型准确率提升了5%，两个微调模型的并行分配使性能比原始模型翻倍。

Conclusion: 结果证明了在医疗图像分析和诊断中采用硬件感知并行模型的有效性。

Abstract: Advancements in AI have greatly enhanced the medical imaging process, making
it quicker to diagnose patients. However, very few have investigated the
optimization of a multi-model system with hardware acceleration. As specialized
edge devices emerge, the efficient use of their accelerators is becoming
increasingly crucial. This paper proposes a hardware-accelerated method for
simultaneous reconstruction and diagnosis of \ac{MRI} from \ac{CT} images.
Real-time performance of achieving a throughput of nearly 150 frames per second
was achieved by leveraging hardware engines available in modern NVIDIA edge
GPU, along with scheduling techniques. This includes the GPU and the \ac{DLA}
available in both Jetson AGX Xavier and Jetson AGX Orin, which were considered
in this paper. The hardware allocation of different layers of the multiple AI
models was done in such a way that the ideal time between the hardware engines
is reduced. In addition, the AI models corresponding to the \ac{GAN} model were
fine-tuned in such a way that no fallback execution into the GPU engine is
required without compromising accuracy. Indeed, the accuracy corresponding to
the fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of
5\%. A further hardware allocation of two fine-tuned GPU-aware GAN models
proves they can double the performance over the original model, leveraging
adequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The
results prove the effectiveness of employing hardware-aware models in parallel
for medical image analysis and diagnosis.

</details>


### [2] [Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic](https://arxiv.org/abs/2510.02099)
*Felix Zeller,John Reuben,Dietmar Fey*

Main category: cs.AR

TL;DR: 本文提出了一种基于分布式算法(DA)的向量矩阵乘法(VMM)方法，通过在ReRAM内存中存储权重和，使用移位加电路实现VMM，完全消除了功耗大的ADC需求。


<details>
  <summary>Details</summary>
Motivation: 神经网络推理中的向量矩阵乘法需要大量数据传输，内存计算可以显著改善性能，但传统方法中的ADC/DAC消耗大量功耗和面积。

Method: 扩展分布式算法(DA)技术，在ReRAM内存中存储权重和，使用移位加外围电路实现向量矩阵乘法，避免了硬件乘法器的使用。

Result: 通过晶体管级仿真验证，相比传统位切片方法，延迟减少4.5倍，能耗减少12倍，完全消除了功耗大的ADC需求。

Conclusion: 基于DA的VMM方法在延迟、能耗和面积方面都优于传统内存计算方法，特别适合神经网络推理应用。

Abstract: Vector-Matrix Multiplication (VMM) is the fundamental and frequently required
computation in inference of Neural Networks (NN). Due to the large data
movement required during inference, VMM can benefit greatly from in-memory
computing. However, ADC/DACs required for in-memory VMM consume significant
power and area. `Distributed Arithmetic (DA)', a technique in computer
architecture prevalent in 1980s was used to achieve inner product or dot
product of two vectors without using a hard-wired multiplier when one of the
vectors is a constant. In this work, we extend the DA technique to multiply an
input vector with a constant matrix. By storing the sum of the weights in
memory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM
memory. We verify functional and also estimate non-functional properties
(latency, energy, area) by performing transistor-level simulations. Using
energy-efficient sensing and fine grained pipelining, our approach achieves 4.5
x less latency and 12 x less energy than VMM performed in memory conventionally
by bit slicing. Furthermore, DA completely eliminated the need for power-hungry
ADCs which are the main source of area and energy consumption in the current
VMM implementations in memory.

</details>
