{"id": "2510.08873", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.08873", "abs": "https://arxiv.org/abs/2510.08873", "authors": ["Haoran Jin", "Jirong Yang", "Yunpeng Liu", "Barry Lyu", "Kangqi Zhang", "Nathaniel Bleier"], "title": "Mozart: A Chiplet Ecosystem-Accelerator Codesign Framework for Composable Bespoke Application Specific Integrated Circuits", "comment": null, "summary": "Modern AI acceleration faces a fundamental challenge: conventional\nassumptions about memory requirements, batching effectiveness, and\nlatency-throughput tradeoffs are systemwide generalizations that ignore the\nheterogeneous computational patterns of individual neural network operators.\nHowever, going towards network-level customization and operator-level\nheterogeneity incur substantial Non-Recurring Engineering (NRE) costs. While\nchiplet-based approaches have been proposed to amortize NRE costs, reuse\nopportunities remain limited without carefully identifying which chiplets are\ntruly necessary. This paper introduces Mozart, a chiplet ecosystem and\naccelerator codesign framework that systematically constructs low cost bespoke\napplication-specific integrated circuits (BASICs). BASICs leverage\noperator-level disaggregation to explore chiplet and memory heterogeneity,\ntensor fusion, and tensor parallelism, with place-and-route validation ensuring\nphysical implementability. The framework also enables constraint-aware\nsystem-level optimization across deployment contexts ranging from datacenter\ninference serving to edge computing in autonomous vehicles. The evaluation\nconfirms that with just 8 strategically selected chiplets, Mozart-generated\ncomposite BASICs achieve 43.5%, 25.4%, 67.7%, and 78.8% reductions in energy,\nenergy-cost product, energy-delay product (EDP), and energy-delay-cost product\ncompared to traditional homogeneous accelerators. For datacenter LLM serving,\nMozart achieves 15-19% energy reduction and 35-39% energy-cost improvement. In\nspeculative decoding, Mozart delivers throughput improvements of 24.6-58.6%\nwhile reducing energy consumption by 38.6-45.6%. For autonomous vehicle\nperception, Mozart reduces energy-cost by 25.54% and energy by 10.53% under\nreal-time constraints.", "AI": {"tldr": "Mozart\u662f\u4e00\u4e2a\u82af\u7247\u751f\u6001\u7cfb\u7edf\u548c\u52a0\u901f\u5668\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7b97\u5b50\u7ea7\u89e3\u6784\u3001\u82af\u7247\u5f02\u6784\u6027\u548c\u7cfb\u7edf\u7ea7\u4f18\u5316\uff0c\u6784\u5efa\u4f4e\u6210\u672c\u5b9a\u5236ASIC\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfAI\u52a0\u901f\u5668\u5047\u8bbe\u5b58\u5728\u5185\u5b58\u9700\u6c42\u3001\u6279\u5904\u7406\u6548\u679c\u548c\u5ef6\u8fdf\u541e\u5410\u91cf\u6743\u8861\u7684\u7cfb\u7edf\u7ea7\u6cdb\u5316\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u7684\u5f02\u6784\u8ba1\u7b97\u6a21\u5f0f\u3002\u82af\u7247\u7ea7\u5b9a\u5236\u548c\u7b97\u5b50\u7ea7\u5f02\u6784\u6027\u4f1a\u5e26\u6765\u9ad8\u6602\u7684\u975e\u91cd\u590d\u6027\u5de5\u7a0b\u6210\u672c\u3002", "method": "\u91c7\u7528\u7b97\u5b50\u7ea7\u89e3\u6784\u63a2\u7d22\u82af\u7247\u548c\u5185\u5b58\u5f02\u6784\u6027\u3001\u5f20\u91cf\u878d\u5408\u548c\u5f20\u91cf\u5e76\u884c\uff0c\u901a\u8fc7\u5e03\u5c40\u5e03\u7ebf\u9a8c\u8bc1\u786e\u4fdd\u7269\u7406\u53ef\u5b9e\u73b0\u6027\uff0c\u652f\u6301\u4ece\u6570\u636e\u4e2d\u5fc3\u5230\u8fb9\u7f18\u8ba1\u7b97\u7684\u591a\u573a\u666f\u7ea6\u675f\u611f\u77e5\u7cfb\u7edf\u7ea7\u4f18\u5316\u3002", "result": "\u4ec5\u4f7f\u75288\u4e2a\u6218\u7565\u9009\u62e9\u7684\u82af\u7247\uff0cMozart\u751f\u6210\u7684\u590d\u5408BASIC\u76f8\u6bd4\u4f20\u7edf\u540c\u6784\u52a0\u901f\u5668\u5728\u80fd\u91cf\u3001\u80fd\u91cf\u6210\u672c\u4e58\u79ef\u3001\u80fd\u91cf\u5ef6\u8fdf\u4e58\u79ef\u548c\u80fd\u91cf\u5ef6\u8fdf\u6210\u672c\u4e58\u79ef\u4e0a\u5206\u522b\u51cf\u5c1143.5%\u300125.4%\u300167.7%\u548c78.8%\u3002\u5728\u6570\u636e\u4e2d\u5fc3LLM\u670d\u52a1\u4e2d\u5b9e\u73b015-19%\u80fd\u91cf\u51cf\u5c11\u548c35-39%\u80fd\u91cf\u6210\u672c\u6539\u8fdb\u3002", "conclusion": "Mozart\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u82af\u7247\u751f\u6001\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u52a0\u901f\u4e2d\u7684\u5f02\u6784\u8ba1\u7b97\u6311\u6218\uff0c\u5728\u591a\u79cd\u90e8\u7f72\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.08940", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.08940", "abs": "https://arxiv.org/abs/2510.08940", "authors": ["Abel Beyene", "Zhongpan Wu", "Yunus Dawji", "Karim Hammad", "Ebrahim Ghafar-Zadeh", "Sebastian Magierowski"], "title": "A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing", "comment": null, "summary": "Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing\nimportance in several life sciences fields as their small footprints enable a\nbroader range of use cases than their larger, stationary counterparts. However,\nas currently designed, they lack sufficient embedded computing to process the\nlarge volume of measurements generated by their internal sensory system. As a\nconsequence, they rely on external devices for additional processing\ncapability. This dependence on external processing places a significant\ncommunication burden on the sequencer's embedded electronics. Moreover, it also\nprevents a truly mobile solution for sequencing in real-time. Anticipating\nnext-generation machines that include suitably advanced processing, we present\na System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide\nsemiconductor (CMOS). Our design, based on a general-purpose reduced\ninstruction set computing (RISC-V) core, also includes accelerators for DNA\ndetection that allow our system to demonstrate a 13X performance improvement\nover commercial embedded multicore processors combined with a near 3000X boost\nin energy efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRISC-V\u7684SoC\u8bbe\u8ba1\uff0c\u7528\u4e8e\u624b\u6301DNA\u6d4b\u5e8f\u673a\uff0c\u901a\u8fc7\u4e13\u7528\u52a0\u901f\u5668\u5b9e\u73b0\u4e8613\u500d\u6027\u80fd\u63d0\u5347\u548c\u8fd13000\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u624b\u6301DNA\u6d4b\u5e8f\u673a\u7f3a\u4e4f\u8db3\u591f\u7684\u5d4c\u5165\u5f0f\u8ba1\u7b97\u80fd\u529b\uff0c\u4f9d\u8d56\u5916\u90e8\u8bbe\u5907\u5904\u7406\u5927\u91cf\u6d4b\u91cf\u6570\u636e\uff0c\u5bfc\u81f4\u901a\u4fe1\u8d1f\u62c5\u91cd\u4e14\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u79fb\u52a8\u5b9e\u65f6\u6d4b\u5e8f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a22nm CMOS\u5de5\u827a\u7684SoC\uff0c\u57fa\u4e8e\u901a\u7528RISC-V\u6838\u5fc3\uff0c\u5e76\u96c6\u6210\u4e86DNA\u68c0\u6d4b\u4e13\u7528\u52a0\u901f\u5668\u3002", "result": "\u76f8\u6bd4\u5546\u7528\u5d4c\u5165\u5f0f\u591a\u6838\u5904\u7406\u5668\uff0c\u7cfb\u7edf\u6027\u80fd\u63d0\u534713\u500d\uff0c\u80fd\u6548\u63d0\u5347\u8fd13000\u500d\u3002", "conclusion": "\u8be5SoC\u8bbe\u8ba1\u4e3a\u4e0b\u4e00\u4ee3\u624b\u6301DNA\u6d4b\u5e8f\u673a\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u9ad8\u80fd\u6548\u7684\u5d4c\u5165\u5f0f\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5b9e\u73b0\u771f\u6b63\u7684\u79fb\u52a8\u5b9e\u65f6\u6d4b\u5e8f\u3002"}}
{"id": "2510.09010", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.09010", "abs": "https://arxiv.org/abs/2510.09010", "authors": ["Yipu Zhang", "Chaofang Ma", "Jinming Ge", "Lin Jiang", "Jiang Xu", "Wei Zhang"], "title": "HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization", "comment": "Accepted by ASPDAC 2026", "summary": "Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction\nmethod, delivering high-quality results for AR/VR applications. While\nquantization methods and hardware accelerators have been proposed to enhance\nNeRF's computational efficiency, existing approaches face crucial limitations.\nCurrent quantization methods operate without considering hardware architecture,\nresulting in sub-optimal solutions within the vast design space encompassing\naccuracy, latency, and model size. Additionally, existing NeRF accelerators\nheavily rely on human experts to explore this design space, making the\noptimization process time-consuming, inefficient, and unlikely to discover\noptimal solutions. To address these challenges, we introduce HERO, a\nreinforcement learning framework performing hardware-aware quantization for\nNeRF. Our framework integrates a NeRF accelerator simulator to generate\nreal-time hardware feedback, enabling fully automated adaptation to hardware\nconstraints. Experimental results demonstrate that HERO achieves 1.31-1.33\n$\\times$ better latency, 1.29-1.33 $\\times$ improved cost efficiency, and a\nmore compact model size compared to CAQ, a previous state-of-the-art NeRF\nquantization framework. These results validate our framework's capability to\neffectively navigate the complex design space between hardware and algorithm\nrequirements, discovering superior quantization policies for NeRF\nimplementation. Code is available at https://github.com/ypzhng/HERO.", "AI": {"tldr": "HERO\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u786c\u4ef6\u611f\u77e5\u91cf\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9NeRF\uff08\u795e\u7ecf\u8f90\u5c04\u573a\uff09\u76843D\u91cd\u5efa\u5e94\u7528\uff0c\u901a\u8fc7\u96c6\u6210NeRF\u52a0\u901f\u5668\u6a21\u62df\u5668\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u786c\u4ef6\u7ea6\u675f\u9002\u5e94\u3002", "motivation": "\u73b0\u6709NeRF\u91cf\u5316\u65b9\u6cd5\u672a\u8003\u8651\u786c\u4ef6\u67b6\u6784\uff0c\u5bfc\u81f4\u5728\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u548c\u6a21\u578b\u5927\u5c0f\u7684\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u96be\u4ee5\u627e\u5230\u6700\u4f18\u89e3\uff1b\u540c\u65f6\u73b0\u6709\u52a0\u901f\u5668\u4f9d\u8d56\u4eba\u5de5\u4e13\u5bb6\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u8fc7\u7a0b\u8017\u65f6\u4f4e\u6548\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210NeRF\u52a0\u901f\u5668\u6a21\u62df\u5668\u751f\u6210\u5b9e\u65f6\u786c\u4ef6\u53cd\u9988\uff0c\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u786c\u4ef6\u7ea6\u675f\u9002\u5e94\u3002", "result": "\u76f8\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684CAQ\u6846\u67b6\uff0cHERO\u5b9e\u73b0\u4e861.31-1.33\u500d\u7684\u5ef6\u8fdf\u6539\u5584\u30011.29-1.33\u500d\u7684\u6210\u672c\u6548\u7387\u63d0\u5347\uff0c\u4ee5\u53ca\u66f4\u7d27\u51d1\u7684\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "HERO\u80fd\u591f\u6709\u6548\u5bfc\u822a\u786c\u4ef6\u4e0e\u7b97\u6cd5\u9700\u6c42\u4e4b\u95f4\u7684\u590d\u6742\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u4e3aNeRF\u5b9e\u73b0\u53d1\u73b0\u66f4\u4f18\u7684\u91cf\u5316\u7b56\u7565\u3002"}}
{"id": "2510.09339", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.09339", "abs": "https://arxiv.org/abs/2510.09339", "authors": ["Sebastian Magierowski", "Zhongpan Wu", "Abel Beyene", "Karim Hammad"], "title": "Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge", "comment": null, "summary": "Miniature DNA sequencing hardware has begun to succeed in mobile contexts,\ndriving demand for efficient machine learning at the edge. This domain\nleverages deep learning techniques familiar from speech and time-series\nanalysis for both low-level signal processing and high-level genomic\ninterpretation. Unlike audio, however, nanopore sequencing presents raw data\nrates over 100X higher, requiring more aggressive compute and memory handling.\nIn this paper, we present a CMOS system-on-chip (SoC) designed for mobile\ngenetic analysis. Our approach combines a multi-core RISC-V processor with\ntightly coupled accelerators for deep learning and bioinformatics. A\nhardware/software co-design strategy enables energy-efficient operation across\na heterogeneous compute fabric, targeting real-time, on-device genome analysis.\nThis work exemplifies the integration of deep learning, edge computing, and\ndomain-specific hardware to advance next-generation mobile genomics.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u57fa\u56e0\u5206\u6790\u7684CMOS\u7247\u4e0a\u7cfb\u7edf\uff0c\u7ed3\u5408\u591a\u6838RISC-V\u5904\u7406\u5668\u548c\u6df1\u5ea6\u5b66\u4e60/\u751f\u7269\u4fe1\u606f\u5b66\u52a0\u901f\u5668\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8bbe\u5907\u7aef\u57fa\u56e0\u7ec4\u5206\u6790", "motivation": "\u968f\u7740\u5fae\u578bDNA\u6d4b\u5e8f\u786c\u4ef6\u5728\u79fb\u52a8\u573a\u666f\u4e2d\u7684\u6210\u529f\u5e94\u7528\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8fb9\u7f18\u673a\u5668\u5b66\u4e60\u6765\u5904\u7406\u6bd4\u97f3\u9891\u6570\u636e\u7387\u9ad8100\u500d\u4ee5\u4e0a\u7684\u7eb3\u7c73\u5b54\u6d4b\u5e8f\u539f\u59cb\u6570\u636e", "method": "\u91c7\u7528\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7b56\u7565\uff0c\u5728\u591a\u6838RISC-V\u5904\u7406\u5668\u4e0a\u7d27\u5bc6\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668\u548c\u751f\u7269\u4fe1\u606f\u5b66\u52a0\u901f\u5668\uff0c\u6784\u5efa\u5f02\u6784\u8ba1\u7b97\u67b6\u6784", "result": "\u5b9e\u73b0\u4e86\u80fd\u91cf\u9ad8\u6548\u7684\u64cd\u4f5c\uff0c\u80fd\u591f\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u65f6\u57fa\u56e0\u7ec4\u5206\u6790", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u3001\u8fb9\u7f18\u8ba1\u7b97\u548c\u9886\u57df\u4e13\u7528\u786c\u4ef6\u7684\u96c6\u6210\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3\u79fb\u52a8\u57fa\u56e0\u7ec4\u5b66\u7684\u53d1\u5c55"}}
