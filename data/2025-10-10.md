<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [How long can you sleep? Idle Time System Inefficiencies and Opportunities](https://arxiv.org/abs/2510.07449)
*Georgia Antoniou,Haris Volos,Jawad Haj Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 提出基于排队论的模型框架，揭示运行延迟关键应用的现代服务器存在大量空闲机会未被利用。


<details>
  <summary>Details</summary>
Motivation: 现代服务器在运行延迟关键应用时存在显著的空闲时间浪费，需要量化分析这种低效现象。

Method: 使用M/M/1、cxM/M/1和M/M/c三种排队模型来估计CPU核心和系统级别的理论空闲时间分布。

Result: 实际服务器空闲时间与理论模型对比显示，存在大量进入深度空闲状态的机会被错过，主要原因是空闲管理器的准确性和传统深度空闲状态转换延迟问题。

Conclusion: 该框架为早期设计探索提供了方法，能够洞察不同服务器配置和负载下的空闲时间行为和优化机会。

Abstract: This work introduces a model-based framework that reveals the idle
opportunity of modern servers running latency-critical applications.
Specifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to
estimate the theoretical idle time distribution at the CPU core and system
(package) level. A comparison of the actual idleness of a real server and that
from the theoretical models reveals significant missed opportunities to enter
deep idle states. This inefficiency is attributed to the idle-governor
inaccuracy and the high latency to transition to/from legacy deep-idle states.
The proposed methodology offers the means for an early-stage design exploration
and insights into idle time behavior and opportunities for varying server
system configurations and load.

</details>


### [2] [DL-PIM: Improving Data Locality in Processing-in-Memory Systems](https://arxiv.org/abs/2510.07719)
*Parker Hao Tian,Zahra Yousefijamarani,Alaa Alameldeen*

Main category: cs.AR

TL;DR: DL-PIM是一种新型PIM架构，通过动态检测数据移动开销并主动将数据移动到本地内存的保留区域，使用分布式地址间接硬件查找表重定向流量，从而减少远程内存访问的数据传输和排队延迟。


<details>
  <summary>Details</summary>
Motivation: 现有PIM架构虽然能提高能效和性能，但其优势依赖于数据与处理单元的邻近性。数据移动开销会降低PIM的性能和能效，因为需要在处理单元和远程内存位置之间移动数据。

Method: 提出DL-PIM架构，动态检测数据移动开销，主动将数据移动到请求处理单元的本地内存保留区域。使用分布式地址间接硬件查找表重定向流量到当前数据位置。在HMC和HBM两种3D堆叠内存上实现，并采用自适应机制评估间接访问的成本和收益，动态启用或禁用间接访问。

Result: DL-PIM将HMC中每个请求的平均内存延迟降低54%，HBM中降低50%。对于具有大量数据重用的工作负载，HMC性能提升15%，HBM提升5%。所有代表性工作负载中，HMC实现6%加速，HBM实现3%加速。

Conclusion: DL-PIM通过增强数据局部性，有效减少了数据移动开销，提高了PIM架构的整体系统性能。

Abstract: PIM architectures aim to reduce data transfer costs between processors and
memory by integrating processing units within memory layers. Prior PIM
architectures have shown potential to improve energy efficiency and
performance. However, such advantages rely on data proximity to the processing
units performing computations. Data movement overheads can degrade PIM's
performance and energy efficiency due to the need to move data between a
processing unit and a distant memory location. %they face challenges due to the
overhead of transferring data from remote memory locations to processing units
inside memory for computation. In this paper, we demonstrate that a large
fraction of PIM's latency per memory request is attributed to data transfers
and queuing delays from remote memory accesses. To improve PIM's data locality,
we propose DL-PIM, a novel architecture that dynamically detects the overhead
of data movement, and proactively moves data to a reserved area in the local
memory of the requesting processing unit. DL-PIM uses a distributed
address-indirection hardware lookup table to redirect traffic to the current
data location. We propose DL-PIM implementations on two 3D stacked memories:
HMC and HBM. While some workloads benefit from DL-PIM, others are negatively
impacted by the additional latency due to indirection accesses. Therefore, we
propose an adaptive mechanism that assesses the cost and benefit of indirection
and dynamically enables or disables it to prevent degrading workloads that
suffer from indirection. Overall, DL-PIM reduces the average memory latency per
request by 54% in HMC and 50% in HBM which resulted in performance improvement
of 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all
representative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup
in HBM, showing that DL-PIM enhances data locality and overall system
performance.

</details>


### [3] [A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations](https://arxiv.org/abs/2510.08137)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的可动态配置DNN推理加速器架构，采用脉动阵列、高带宽内存和UltraRAM，支持多种处理单元配置和权重传输调度，并能模拟模拟内存计算设备。


<details>
  <summary>Details</summary>
Motivation: 随着DNN推理对专用硬件计算效率需求的增长，需要开发灵活可配置的加速器架构来适应不同模型和未来FPGA设计。

Method: 使用FPGA构建动态可配置加速器，包含脉动阵列、高带宽内存和UltraRAM；设计了两种不同计算能力的处理单元配置；采用启发式权重传输调度；支持模拟模拟内存计算设备。

Result: 该架构相比先前工作实现了显著的吞吐效率提升，并能模拟AIMC设备以辅助下一代异构AIMC芯片设计。

Conclusion: 该工作提出了一种适用于各种模型和未来FPGA设计的通用DNN推理加速架构，具有高度适应性和可扩展性。

Abstract: Deep neural network (DNN) inference relies increasingly on specialized
hardware for high computational efficiency. This work introduces a
field-programmable gate array (FPGA)-based dynamically configurable accelerator
featuring systolic arrays, high-bandwidth memory, and UltraRAMs. We present two
processing unit (PU) configurations with different computing capabilities using
the same interfaces and peripheral blocks. By instantiating multiple PUs and
employing a heuristic weight transfer schedule, the architecture achieves
notable throughput efficiency over prior works. Moreover, we outline how the
architecture can be extended to emulate analog in-memory computing (AIMC)
devices to aid next-generation heterogeneous AIMC chip designs and investigate
device-level noise behavior. Overall, this brief presents a versatile DNN
inference acceleration architecture adaptable to various models and future FPGA
designs.

</details>


### [4] [FMCache: File-System Metadata Caching in Programmable Switches](https://arxiv.org/abs/2510.08351)
*Qingxiu Liu,Jiazhen Cai,Siyuan Sheng,Yuhui Chen,Lu Tang,Zhirong Shen,Patrick P. C. Lee*

Main category: cs.AR

TL;DR: FMCache是一个利用可编程交换机在数据平面直接服务文件系统元数据请求的框架，解决了多客户端场景下元数据缓存一致性的问题，相比传统HDFS实现了高达181.6%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 分布式文件系统中，跨多个元数据服务器的快速可扩展元数据管理至关重要。客户端缓存虽然能减轻服务器负载，但在客户端数量增加时会产生显著的缓存一致性维护开销和复杂性。

Method: 提出FMCache框架，利用可编程交换机在交换机数据平面直接服务来自多个客户端的文件系统元数据请求，解决了文件系统特定的路径依赖问题，并在严格的交换机资源约束下实现高效缓存。

Result: 在Tofino交换机测试平台上使用真实文件系统元数据工作负载进行评估，FMCache相比原生HDFS实现了高达181.6%的吞吐量提升，与客户端缓存结合时还能带来额外139.6%的吞吐量增益，同时保持低延迟和有限的交换机资源使用。

Conclusion: FMCache通过利用可编程交换机实现了高效的文件系统元数据缓存，显著提升了分布式文件系统的性能和可扩展性，同时解决了多客户端缓存一致性的挑战。

Abstract: Fast and scalable metadata management across multiple metadata servers is
crucial for distributed file systems to handle numerous files and directories.
Client-side caching of frequently accessed metadata can mitigate server loads,
but incurs significant overhead and complexity in maintaining cache consistency
when the number of clients increases. We propose FMCache, an in-switch
file-system metadata caching framework that leverages programmable switches to
serve file-system metadata requests from multiple clients directly in the
switch data plane. Unlike prior in-switch key-value caching approaches, FMCache
addresses file-system-specific path dependencies under stringent switch
resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on
a Tofino-switch testbed using real-world file-system metadata workloads.
FMCache achieves up to 181.6% higher throughput than vanilla HDFS and
complements client-side caching with additional throughput gains of up to
139.6%. It also incurs low latencies and limited switch resource usage.

</details>


### [5] [SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference](https://arxiv.org/abs/2510.08544)
*Hengrui Zhang,Pratyush Patel,August Ning,David Wentzlaff*

Main category: cs.AR

TL;DR: SPAD提出专门针对LLM推理的预填充和解码阶段设计专用芯片，通过硬件解耦和专业化设计降低成本和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心GPU/TPU采用"越多越好"的设计理念，导致预填充阶段内存带宽利用不足和解码阶段计算资源利用不足，增加了服务成本。

Method: 设计专门的预填充芯片（更大脉动阵列+GDDR内存）和解码芯片（保持高内存带宽但减少计算能力），采用硬件解耦架构。

Result: 相比H100，预填充芯片性能提升8%，硬件成本降低52%；解码芯片达到97%性能，TDP降低28%。端到端模拟显示硬件成本降低19%-41%，TDP降低2%-17%。

Conclusion: SPAD通过专业化硬件设计显著降低LLM推理成本，且具有良好适应性，即使模型和工作负载变化也能保持成本优势。

Abstract: Large Language Models (LLMs) have gained popularity in recent years, driving
up the demand for inference. LLM inference is composed of two phases with
distinct characteristics: a compute-bound prefill phase followed by a
memory-bound decode phase. To efficiently serve LLMs, prior work proposes
prefill-decode disaggregation to run each phase on separate hardware. However,
existing hardware poorly matches the different requirements of each phase.
Current datacenter GPUs and TPUs follow a more-is-better design philosophy that
maximizes compute and memory resources, causing memory bandwidth
underutilization in the prefill phase and compute underutilization in the
decode phase. Such underutilization directly translates into increased serving
costs.
  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting
a less-is-more methodology to design specialized chips tailored to the distinct
characteristics of prefill and decode phases. The proposed Prefill Chips have
larger systolic arrays and use cost-effective GDDR memory, whereas the proposed
Decode Chips retain high memory bandwidth but reduce compute capacity. Compared
to modeled H100s, simulations show that the proposed Prefill Chips deliver 8%
higher prefill performance on average at 52% lower hardware cost, while the
proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.
  End-to-end simulations on production traces show that SPAD reduces hardware
cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while
offering the same performance. Even when models and workloads change, SPAD can
reallocate either type of chip to run either phase and still achieve 11%-43%
lower hardware costs, demonstrating the longevity of the SPAD design.

</details>
