<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing](https://arxiv.org/abs/2510.18525)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: SPEQ是一种算法-硬件协同设计的推测解码方法，使用全模型部分权重位形成量化草稿模型，无需额外训练或存储开销，通过可重构处理单元阵列高效执行草稿和验证过程，在15个LLM和任务上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能优异但推理延迟高，量化会降低性能，推测解码无损但通常有额外开销，需要一种既能加速推理又保持性能的方法。

Method: 使用全模型部分权重位形成量化草稿模型，无需额外训练或存储，通过可重构处理单元阵列高效执行草稿和验证过程。

Result: 在15个LLM和任务上，SPEQ相比FP16、Olive和Tender分别实现了2.07倍、1.53倍和1.45倍的加速。

Conclusion: SPEQ通过算法-硬件协同设计，在保持性能的同时显著降低了LLM推理延迟，是一种高效的推测解码方法。

Abstract: Large language models achieve impressive performance across diverse tasks but
exhibit high inference latency due to their large parameter sizes. While
quantization reduces model size, it often leads to performance degradation
compared to the full model. Speculative decoding remains lossless but typically
incurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed
speculative decoding method that uses part of the full-model weight bits to
form a quantized draft model, thereby eliminating additional training or
storage overhead. A reconfigurable processing element array enables efficient
execution of both the draft and verification passes. Experimental results
across 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,
1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.

</details>
