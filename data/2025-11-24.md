<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Vorion: A RISC-V GPU with Hardware-Accelerated 3D Gaussian Rendering and Training](https://arxiv.org/abs/2511.16831)
*Yipeng Wang,Mengtian Yang,Chieh-pu Lo,Jaydeep P. Kulkarni*

Main category: cs.AR

TL;DR: Vorion是首个支持硬件加速3D高斯泼溅渲染和训练的GPGPU原型，通过可扩展架构、最小硬件改动和混合数据流实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然前景广阔，但其渲染和训练计算量巨大，在边缘设备上无法实现实时渲染，在工作站上无法实现实时4D重建，需要专用硬件支持。

Method: 采用可扩展架构，对传统光栅化器进行最小硬件改动，引入z-tiling增加并行性，使用高斯/像素中心混合数据流，在TSMC 16nm工艺上实现原型系统。

Result: 最小系统（8个SIMT核心，2个高斯光栅化器）实现19 FPS渲染性能；扩展设计（16个光栅化器）达到38.6次迭代/秒的训练速度。

Conclusion: Vorion证明了3D高斯泼溅技术在专用硬件上的可行性，为下一代GPU图形管线提供了硬件加速解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a foundational technique for real-time neural rendering, 3D scene generation, volumetric video (4D) capture. However, its rendering and training impose massive computation, making real-time rendering on edge devices and real-time 4D reconstruction on workstations currently infeasible. Given its fixed-function nature and similarity with traditional rasterization, 3DGS presents a strong case for dedicated hardware in the graphics pipeline of next-generation GPUs. This work, Vorion, presents the first GPGPU prototype with hardware-accelerated 3DGS rendering and training. Vorion features scalable architecture, minimal hardware change to traditional rasterizers, z-tiling to increase parallelism, and Gaussian/pixel-centric hybrid dataflow. We prototype the minimal system (8 SIMT cores, 2 Gaussian rasterizer) using TSMC 16nm FinFET technology, which achieves 19 FPS for rendering. The scaled design with 16 rasterizers achieves 38.6 iterations/s for training.

</details>


### [2] [Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration](https://arxiv.org/abs/2511.17123)
*Jiaxun Fang,Li Zhang,Shaoyi Huang*

Main category: cs.AR

TL;DR: 提出了一种能量感知的逐层压缩框架，通过层感知MAC能量模型和能量-精度协同优化的权重选择算法，在CNN模型上实现最高58.6%的能量减少，精度仅下降2-3%。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用全局激活模型、粗粒度能量代理或层无关策略，限制了在真实硬件上的有效性。需要更精确的层级能量特性和MAC能量模型。

Method: 构建层感知MAC能量模型，结合每层激活统计和22位部分和转换的MSB-Hamming距离分组；在量化感知训练中引入能量-精度协同优化的权重选择算法；采用能量优先的逐层调度策略。

Result: 在不同CNN模型上实验表明，最高可实现58.6%的能量减少，精度仅下降2-3%，优于现有最先进的功率感知基准方法。

Conclusion: 所提出的能量感知逐层压缩框架能有效减少CNN在脉动阵列加速器上的能量消耗，同时保持可接受的精度损失。

Abstract: Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\% energy reduction with 2-3\% accuracy drop, outperforming a state-of-the-art power-aware baseline.

</details>


### [3] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: NX-CGRA是一种可编程硬件加速器，采用粗粒度可重构阵列架构，支持多种transformer推理算法，在边缘计算场景下实现性能、能效和架构灵活性的平衡。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中transformer工作负载的多样性和复杂性日益增加，需要在性能、能效和架构灵活性之间取得平衡，而固定功能加速器无法满足这种需求。

Method: 采用粗粒度可重构阵列(CGRA)架构，具有软件驱动的可编程性，能够高效执行各种内核模式，包括线性和非线性函数。

Result: 使用真实transformer模型的代表性基准测试进行评估，展示了高整体效率和良好的能耗-面积权衡，适用于不同类别的操作。

Conclusion: NX-CGRA作为可扩展和适应性强的硬件解决方案，在受限的功耗和硅面积预算下具有在边缘部署transformer的潜力。

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


### [4] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 提出了一种新的数字内存随机计算架构DISCA，采用压缩的准随机Bent-Pyramid数据格式，在保持数字系统可扩展性和可靠性的同时，实现了类似模拟计算的简单性，显著提升了矩阵乘法任务的能效。


<details>
  <summary>Details</summary>
Motivation: AI应用向边缘迁移面临硬件预算约束，传统冯·诺依曼架构受限于内存墙和摩尔定律终结，现有内存计算架构因设计限制导致性能下降。

Method: 采用压缩的准随机Bent-Pyramid数据格式的数字内存随机计算架构DISCA，结合了模拟计算的简单性和数字系统的优势。

Result: 后布局建模显示，在180nm CMOS技术下，DISCA在500MHz频率下实现每比特3.59 TOPS/W的能效。

Conclusion: DISCA相比同类架构将矩阵乘法工作负载的能效提升了数个数量级，为边缘AI应用提供了高效解决方案。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>


### [5] [MemIntelli: A Generic End-to-End Simulation Framework for Memristive Intelligent Computing](https://arxiv.org/abs/2511.17418)
*Houji Zhou,Ling Yang,Zhiwei Zhou,Yi Li,Xiangshui Miao*

Main category: cs.AR

TL;DR: 提出了MemIntelli端到端仿真框架，支持灵活可变精度计算，实现忆阻器上多样化智能应用的预验证


<details>
  <summary>Details</summary>
Motivation: 忆阻内存计算中电路与算法的耦合使得计算可靠性易受器件和外围电路非理想效应影响，需要高效的软硬件协同仿真工具

Method: 在器件和电路层面使用数学函数进行等效电路建模，在架构层面实现支持整数和浮点数据表示的灵活可变精度内存计算，兼容NumPy和PyTorch

Result: 展示了方程求解、数据聚类、小波变换、神经网络训练和推理等多种智能算法的强大处理能力

Conclusion: 提供了一个从器件到应用的全面仿真工具，促进内存计算系统的协同设计

Abstract: Memristive in-memory computing (IMC) has emerged as a promising solution for addressing the bottleneck in the Von Neumann architecture. However, the couplingbetweenthecircuitandalgorithm in IMC makes computing reliability susceptible to non-ideal effects in devices and peripheral circuits. In this respect, efficient softwarehardwareco-simulationtoolsarehighlydesiredtoembedthedevice and circuit models into the algorithms. In this paper, for the first time, we proposed an end-to-end simulation framework supporting flexible variable-precision computing, named MemIntelli, to realize the pre-verification of diverse intelligent applications on memristive devices. At the device and circuit level, mathematical functions are employed to abstract the devices and circuits through meticulous equivalent circuit modeling. On the architecture level, MemIntelli achieves flexible variable-precision IMC supporting integer and floating data representation with bit-slicing. Moreover, MemIntelli is compatible with NumPy and PyTorch for seamless integration with applications. To demonstrate its capabilities, diverse intelligent algorithms, such as equation solving, data clustering, wavelet transformation, and neural network training and inference, were employed to showcase the robust processing ability of MemIntelli. This research presents a comprehensive simulation tool that facilitates the co-design of the IMC system, spanning from device to application.

</details>
