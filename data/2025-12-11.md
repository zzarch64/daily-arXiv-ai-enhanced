<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference](https://arxiv.org/abs/2512.09304)
*Siyuan Ma,Jiajun Hu,Jeeho Ryoo,Aman Arora,Lizy Kurian John*

Main category: cs.AR

TL;DR: RACAM：首个支持数据重用和减少冗余传输的DRAM内位串行处理架构，通过专用局部性缓冲区和映射机制，在LLM推理中相比GPU和现有PIM系统实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有DRAM处理内存（DRAM-PIM）架构存在数据重用不足、冗余数据传输量大、工作负载映射支持不充分等主要限制，这影响了内存密集型工作负载（如大语言模型推理）的效率。

Method: 提出RACAM架构，包含专用局部性缓冲区、位串行处理单元、popcount归约单元和广播单元，以实现数据重用并减少冗余数据传输。同时提出工作负载映射机制，充分利用DRAM架构的大规模并行性。

Result: 在端到端LLM推理评估中，RACAM相比GPU实现9-102倍加速，相比最先进的DRAM-PIM系统Proteus，在GPT3案例中实现每平方毫米233倍的性能提升。

Conclusion: RACAM通过创新的架构设计和映射机制，有效解决了现有DRAM-PIM系统的关键限制，为内存密集型工作负载（特别是大语言模型推理）提供了高效的加速解决方案。

Abstract: In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.

</details>


### [2] [ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators](https://arxiv.org/abs/2512.09427)
*Guoqiang Zou,Wanyu Wang,Hao Zheng,Longxiang Yin,Yinhe Han*

Main category: cs.AR

TL;DR: ODMA是一个针对随机访问受限内存（RACM）加速器的按需内存分配框架，通过轻量级长度预测器和动态桶分区，显著提升LLM服务的内存利用率和性能。


<details>
  <summary>Details</summary>
Motivation: 在随机访问带宽较差的加速器（如基于LPDDR5的Cambricon MLU370）上服务大语言模型时，现有内存管理方案存在局限性：静态预分配浪费内存，而细粒度分页（如PagedAttention）由于高随机访问成本而不适用。现有HBM中心解决方案未能充分利用RACM加速器的特性。

Method: ODMA框架通过结合轻量级长度预测器、动态桶分区和大桶保护机制来解决分布漂移和重尾请求问题。边界根据实时跟踪定期更新以最大化内存利用率。该方法专门针对RACM加速器的硬件特性进行优化。

Result: 在Alpaca和Google-NQ数据集上，ODMA将预测准确率从82.68%显著提升至93.36%。在Cambricon MLU370-X4上服务DeepSeek-R1-Distill-Qwen-7B模型时，内存利用率从55.05%提升到72.45%，RPS和TPS分别比静态基线提高了29%和27%。

Conclusion: 硬件感知的内存分配能够解锁RACM平台上高效的大语言模型服务。ODMA框架通过针对随机访问受限内存特性的优化，显著提升了内存利用率和系统性能，为在类似硬件上部署LLM提供了有效的解决方案。

Abstract: Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.

</details>
