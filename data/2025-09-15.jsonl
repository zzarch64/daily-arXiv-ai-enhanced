{"id": "2509.09774", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.09774", "abs": "https://arxiv.org/abs/2509.09774", "authors": ["Doru Thom Popovici", "Mario Vega", "Angelos Ioannou", "Fabien Chaix", "Dania Mosuli", "Blair Reasoner", "Tan Nguyen", "Xiaokun Yang", "John Shalf"], "title": "Towards An Approach to Identify Divergences in Hardware Designs for HPC Workloads", "comment": "9 pages, 8 figures", "summary": "Developing efficient hardware accelerators for mathematical kernels used in\nscientific applications and machine learning has traditionally been a\nlabor-intensive task. These accelerators typically require low-level\nprogramming in Verilog or other hardware description languages, along with\nsignificant manual optimization effort. Recently, to alleviate this challenge,\nhigh-level hardware design tools like Chisel and High-Level Synthesis have\nemerged. However, as with any compiler, some of the generated hardware may be\nsuboptimal compared to expert-crafted designs. Understanding where these\ninefficiencies arise is crucial, as it provides valuable insights for both\nusers and tool developers. In this paper, we propose a methodology to\nhierarchically decompose mathematical kernels - such as Fourier transforms,\nmatrix multiplication, and QR factorization - into a set of common building\nblocks or primitives. Then the primitives are implemented in the different\nprogramming environments, and the larger algorithms get assembled. Furthermore,\nwe employ an automatic approach to investigate the achievable frequency and\nrequired resources. Performing this experimentation at each level will provide\nfairer comparisons between designs and offer guidance for both tool developers\nand hardware designers to adopt better practices."}
{"id": "2509.10051", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.10051", "abs": "https://arxiv.org/abs/2509.10051", "authors": ["Tianwei Pan", "Tianao Dai", "Jianlei Yang", "Hongbin Jing", "Yang Su", "Zeyu Hao", "Xiaotao Jia", "Chunming Hu", "Weisheng Zhao"], "title": "Finesse: An Agile Design Framework for Pairing-based Cryptography via Software/Hardware Co-Design", "comment": "Published on 52nd Annual International Symposium on Computer\n  Architecture (ISCA'25)", "summary": "Pairing-based cryptography (PBC) is crucial in modern cryptographic\napplications. With the rapid advancement of adversarial research and the\ngrowing diversity of application requirements, PBC accelerators need regular\nupdates in algorithms, parameter configurations, and hardware design. However,\ntraditional design methodologies face significant challenges, including\nprolonged design cycles, difficulties in balancing performance and flexibility,\nand insufficient support for potential architectural exploration.\n  To address these challenges, we introduce Finesse, an agile design framework\nbased on co-design methodology. Finesse leverages a co-optimization cycle\ndriven by a specialized compiler and a multi-granularity hardware simulator,\nenabling both optimized performance metrics and effective design space\nexploration. Furthermore, Finesse adopts a modular design flow to significantly\nshorten design cycles, while its versatile abstraction ensures flexibility\nacross various curve families and hardware architectures.\n  Finesse offers flexibility, efficiency, and rapid prototyping, comparing with\nprevious frameworks. With compilation times reduced to minutes, Finesse enables\nfaster iteration cycles and streamlined hardware-software co-design.\nExperiments on popular curves demonstrate its effectiveness, achieving\n$34\\times$ improvement in throughput and $6.2\\times$ increase in area\nefficiency compared to previous flexible frameworks, while outperforming\nstate-of-the-art non-flexible ASIC designs with a $3\\times$ gain in throughput\nand $3.2\\times$ improvement in area efficiency."}
{"id": "2509.10372", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.10372", "abs": "https://arxiv.org/abs/2509.10372", "authors": ["Huizheng Wang", "Zichuan Wang", "Zhiheng Yue", "Yousheng Long", "Taiquan Wei", "Jianxun Yang", "Yang Wang", "Chao Li", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging Bit-Slice-enabled Sparsity and Repetitiveness", "comment": null, "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."}
{"id": "2509.10400", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.10400", "abs": "https://arxiv.org/abs/2509.10400", "authors": ["Yang Zhong", "Haoran Wu", "Xueqi Li", "Sa Wang", "David Boland", "Yungang Bao", "Kan Shi"], "title": "TurboFuzz: FPGA Accelerated Hardware Fuzzing for Processor Agile Verification", "comment": null, "summary": "Verification is a critical process for ensuring the correctness of modern\nprocessors. The increasing complexity of processor designs and the emergence of\nnew instruction set architectures (ISAs) like RISC-V have created demands for\nmore agile and efficient verification methodologies, particularly regarding\nverification efficiency and faster coverage convergence. While simulation-based\napproaches now attempt to incorporate advanced software testing techniques such\nas fuzzing to improve coverage, they face significant limitations when applied\nto processor verification, notably poor performance and inadequate test case\nquality. Hardware-accelerated solutions using FPGA or ASIC platforms have tried\nto address these issues, yet they struggle with challenges including host-FPGA\ncommunication overhead, inefficient test pattern generation, and suboptimal\nimplementation of the entire multi-step verification process.\n  In this paper, we present TurboFuzz, an end-to-end hardware-accelerated\nverification framework that implements the entire Test\nGeneration-Simulation-Coverage Feedback loop on a single FPGA for modern\nprocessor verification. TurboFuzz enhances test quality through optimized test\ncase (seed) control flow, efficient inter-seed scheduling, and hybrid fuzzer\nintegration, thereby improving coverage and execution efficiency. Additionally,\nit employs a feedback-driven generation mechanism to accelerate coverage\nconvergence. Experimental results show that TurboFuzz achieves up to 2.23x more\ncoverage collection than software-based fuzzers within the same time budget,\nand up to 571x performance speedup when detecting real-world issues, while\nmaintaining full visibility and debugging capabilities with moderate area\noverhead."}
