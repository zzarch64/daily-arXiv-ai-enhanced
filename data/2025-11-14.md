<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [History-Aware Trajectory k-Anonymization Using an FPGA-Based Hardware Accelerator for Real-Time Location Services](https://arxiv.org/abs/2511.09688)
*Hiroshi Nakano,Hiroaki Nishi*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的实时轨迹k-匿名化方法，结合历史轨迹搜索和最短路径计算，相比之前仅使用最短路径的方法，能更好地保护隐私并保持行为准确性。


<details>
  <summary>Details</summary>
Motivation: 之前的FPGA轨迹匿名化方法仅依赖最短路径计算，无法捕捉真实旅行行为，降低了匿名化数据的实用性。

Method: 开发了新的FPGA硬件架构，集成并行历史轨迹搜索和传统最短路径查找，使用自定义定点计数模块准确加权历史数据贡献。

Result: FPGA实现达到6000+记录/秒的实时吞吐量，数据保留率比之前设计提高1.2%，更有效地保留主要干道。

Conclusion: 该方法实现了在LBS严格延迟约束下的高保真、历史感知匿名化，同时保护隐私和行为准确性。

Abstract: Our previous work established the feasibility of FPGA-based real-time trajectory anonymization, a critical task for protecting user privacy in modern location-based services (LBS). However, that pioneering approach relied exclusively on shortest-path computations, which can fail to capture re- alistic travel behavior and thus reduce the utility of the anonymized data. To address this limitation, this paper introduces a novel, history-aware trajectory k-anonymization methodology and presents an advanced FPGA-based hardware architecture to implement it. Our proposed architecture uniquely integrates par- allel history-based trajectory searches with conventional shortest- path finding, using a custom fixed-point counting module to ac- curately weigh contributions from historical data. This approach enables the system to prioritize behaviorally common routes over geometrically shorter but less-traveled paths. The FPGA implementation demonstrates that our new architecture achieves a real-time throughput of over 6,000 records/s, improves data retention by up to 1.2% compared to our previous shortest-path- only design, and preserves major arterial roads more effectively. These results signify a key advancement, enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under the strict latency constraints of LBS.

</details>


### [2] [AssertMiner: Module-Level Spec Generation and Assertion Mining using Static Analysis Guided LLMs](https://arxiv.org/abs/2511.10007)
*Hongqin Lyu,Yonghao Wang,Jiaxin Zhou,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertMiner是一个模块级断言生成框架，利用AST静态信息辅助LLMs挖掘断言，解决了现有方法只生成顶层断言而忽略微架构级模块验证需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于设计规范的断言生成方法通常只产生顶层断言，忽视了微架构级模块实现细节的验证需求，而这些地方设计错误更频繁发生。

Method: 通过AST结构提取获得模块调用图、I/O表和数据流图，指导LLM生成模块级规范并挖掘模块级断言。

Result: 评估显示AssertMiner在生成高质量模块断言方面优于AssertLLM和Spec2Assertion，与这些方法集成时能显著提高结构覆盖率和错误检测能力。

Conclusion: AssertMiner能够实现更全面和高效的验证过程，通过模块级断言生成解决了微架构级验证的关键挑战。

Abstract: Assertion-based verification (ABV) is a key approach to checking whether a logic design complies with its architectural specifications. Existing assertion generation methods based on design specifications typically produce only top-level assertions, overlooking verification needs on the implementation details in the modules at the micro-architectural level, where design errors occur more frequently. To address this limitation, we present AssertMiner, a module-level assertion generation framework that leverages static information generated from abstract syntax tree (AST) to assist LLMs in mining assertions. Specifically, it performs AST-based structural extraction to derive the module call graph, I/O table, and dataflow graph, guiding the LLM to generate module-level specifications and mine module-level assertions. Our evaluation demonstrates that AssertMiner outperforms existing methods such as AssertLLM and Spec2Assertion in generating high-quality assertions for modules. When integrated with these methods, AssertMiner can enhance the structural coverage and significantly improve the error detection capability, enabling a more comprehensive and efficient verification process.

</details>


### [3] [The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads](https://arxiv.org/abs/2511.10010)
*Shahid Amin,Syed Pervez Hussnain Shah*

Main category: cs.AR

TL;DR: 本文综述了AI与计算机架构的协同演进，分析了GPU、ASIC、FPGA等主流AI加速器架构的设计理念和性能权衡，探讨了数据流优化、内存层次结构等核心原则，并展望了存内计算和神经形态计算等新兴技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型复杂度增加，传统计算架构已无法满足其巨大的计算需求，需要专门优化的硬件架构来加速AI工作负载。

Method: 通过结构化回顾和定量性能数据分析，比较不同架构范式（GPU、ASIC、FPGA）的设计哲学、关键特性和性能权衡，并分析数据流优化、内存层次结构、稀疏性和量化等核心原则。

Result: 研究表明，硬件-软件协同设计对于实现高性能和能效至关重要，不同架构在特定AI工作负载下表现出不同的性能特征。

Conclusion: AI与计算机架构处于共生关系中，硬件-软件协同设计不再是优化选项，而是未来计算进步的必要条件。

Abstract: The remarkable progress in Artificial Intelligence (AI) is foundation-ally linked to a concurrent revolution in computer architecture. As AI models, particularly Deep Neural Networks (DNNs), have grown in complexity, their massive computational demands have pushed traditional architectures to their limits. This paper provides a structured review of this co-evolution, analyzing the architectural landscape designed to accelerate modern AI workloads. We explore the dominant architectural paradigms Graphics Processing Units (GPUs), Appli-cation-Specific Integrated Circuits (ASICs), and Field-Programmable Gate Ar-rays (FPGAs) by breaking down their design philosophies, key features, and per-formance trade-offs. The core principles essential for performance and energy efficiency, including dataflow optimization, advanced memory hierarchies, spar-sity, and quantization, are analyzed. Furthermore, this paper looks ahead to emerging technologies such as Processing-in-Memory (PIM) and neuromorphic computing, which may redefine future computation. By synthesizing architec-tural principles with quantitative performance data from industry-standard benchmarks, this survey presents a comprehensive picture of the AI accelerator landscape. We conclude that AI and computer architecture are in a symbiotic relationship, where hardware-software co-design is no longer an optimization but a necessity for future progress in computing.

</details>


### [4] [Combined power management and congestion control in High-Speed Ethernet-based Networks for Supercomputers and Data Centers](https://arxiv.org/abs/2511.10159)
*Miguel Sánchez de la Rosa,Francisco J. andújar,Jesus Escudero-Sahuquillo,José L. Sánchez,Francisco J. Alfaro-Cortés*

Main category: cs.AR

TL;DR: 本文探讨数据中心和超级计算机网络的两个关键方面：防止重载时性能下降的拥塞控制，以及空闲时节能的电源管理，并研究两者之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心和超级计算机的普及，网络互连成为系统性能的关键瓶颈。需要先进技术来确保系统在重载时保持性能，同时在空闲时节约能源。

Method: 研究网络的两个关键方面：拥塞控制（防止性能下降）和电源管理（节能），并分析它们之间的相互作用关系。

Result: 探索了网络在重载和空闲状态下的优化策略，识别了拥塞控制与电源管理之间的潜在冲突和协同效应。

Conclusion: 网络性能优化需要综合考虑拥塞控制和电源管理两个方面，它们之间的相互作用对系统整体效率和性能至关重要。

Abstract: The demand for computer in our daily lives has led to the proliferation of Datacenters that power indispensable many services. On the other hand, computing has become essential for some research for various scientific fields, that require Supercomputers with vast computing capabilities to produce results in reasonable time. The scale and complexity of these systems, compared to our day-to-day devices, are like comparing a cell to a living organism. To make them work properly, we need state-of-the-art technology and engineering, not just raw resources. Interconnecting the different computer nodes that make up a whole is a delicate task, as it can become the bottleneck for the whole infrastructure. In this work, we explore two aspects of the network: how to prevent degradation under heavy use with congestion control, and how to save energy when idle with power management; and how the two may interact.

</details>


### [5] [Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations](https://arxiv.org/abs/2511.10563)
*Seyed Hadi Mirfarshbafan,Christoph Studer*

Main category: cs.AR

TL;DR: 该论文提出了一种基于波束空间的复杂稀疏自适应均衡器(CSPADE)及其VLSI架构，用于毫米波大规模MIMO系统，相比天线域均衡器可节省高达54-66%的功耗，并实现了最高的吞吐量和更好的能效。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO和毫米波通信在部署时会带来过高的基带处理硬件成本和功耗，需要利用毫米波频段的信道稀疏性来降低基带处理复杂度。

Method: 提出了复杂稀疏自适应均衡器(CSPADE)算法及其对应的VLSI架构，包括全并行实现和基于顺序乘累加(MAC)的架构。

Result: 在22nm FDSOI工艺中实现，全并行CSPADE相比天线域均衡节省54%功耗，MAC架构节省66%功耗，同时实现了现有大规模MIMO数据检测器中最高的吞吐量和更好的能效。

Conclusion: 波束空间处理能有效降低毫米波大规模MIMO系统的基带处理功耗，提出的CSPADE架构在功耗、吞吐量和能效方面均表现出色。

Abstract: Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer.

</details>
