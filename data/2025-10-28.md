<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Simopt-Power: Leveraging Simulation Metadata for Low-Power Design Synthesis](https://arxiv.org/abs/2510.21745)
*Eashan Wadhwa,Shanker Shreejith*

Main category: cs.AR

TL;DR: Simopt-power是一个基于仿真分析的FPGA低功耗优化框架，通过识别高切换路径并选择性重配置，实现约9%的动态功耗降低，仅增加约9%的LUT资源开销。


<details>
  <summary>Details</summary>
Motivation: 现代FPGA中过度切换活动是动态功耗的主要来源，传统低功耗技术需要侵入性设计修改或随着器件密度增加而收益递减。

Method: 利用仿真分析识别高切换路径，通过Shannon分解原理插入重复真值表逻辑并重新定位关键网络，在不改变功能行为的情况下减少不必要的转换。

Result: 在开源RTLLM基准测试中，平均实现约9%的切换诱导功耗降低，算术设计仅增加约9%的LUT等效资源。

Conclusion: 将仿真洞察与针对性优化相结合可以有效降低动态功耗，为在FPGA-CAD流程中使用仿真元数据提供了实用路径。

Abstract: Excessive switching activity is a primary contributor to dynamic power
dissipation in modern FPGAs, where fine-grained configurability amplifies
signal toggling and associated capacitance. Conventional low-power techniques
-- gating, clock-domain partitioning, and placement-aware netlist rewrites -
either require intrusive design changes or offer diminishing returns as device
densities grow. In this work, we present Simopt-power, a simulator-driven
optimisation framework that leverages simulation analysis to identify and
selectively reconfigure high-toggle paths. By feeding activity profiles back
into a lightweight transformation pass, Simopt-power judiciously inserts
duplicate truth table logic using Shannon Decomposition principle and relocates
critical nets, thereby attenuating unnecessary transitions without perturbing
functional behaviour. We evaluated this framework on open-source RTLLM
benchmark, with Simopt-power achieves an average switching-induced power
reduction of ~9\% while incurring only ~9\% additional LUT-equivalent resources
for arithmetic designs. These results demonstrate that coupling simulation
insights with targeted optimisations can yield a reduced dynamic power,
offering a practical path toward using simulation metadata in the FPGA-CAD
flow.

</details>


### [2] [QuArch: A Benchmark for Evaluating LLM Reasoning in Computer Architecture](https://arxiv.org/abs/2510.22087)
*Shvetank Prakash,Andrew Cheng,Arya Tschand,Mark Mazumder,Varun Gohil,Jeffrey Ma,Jason Yik,Zishen Wan,Jessica Quaye,Elisavet Lydia Alvanaki,Avinash Kumar,Chandrashis Mazumdar,Tuhin Khare,Alexander Ingare,Ikechukwu Uchendu,Radhika Ghosal,Abhishek Tyagi,Chenyu Wang,Andrea Mattia Garavagno,Sarah Gu,Alice Guo,Grace Hur,Luca Carloni,Tushar Krishna,Ankita Nayak,Amir Yazdanbakhsh,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: QuArch是首个专门评估大语言模型在计算机架构领域知识和推理能力的基准测试，包含2,671个专家验证的问题-答案对，覆盖处理器设计、内存系统等核心主题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估缺乏计算机架构领域的专门测试，该领域连接高级软件抽象和低级硬件实现，对计算系统创新至关重要。

Method: 开发包含2,671个专家验证QA对的综合基准测试，覆盖计算机架构的多个方面，包括处理器设计、内存系统和互连网络。

Result: 前沿模型在领域知识方面表现良好，但在需要高阶思维的问题上表现不佳，准确率从34%到72%不等，显示出在架构推理方面的持续差距。

Conclusion: QuArch为构建和衡量LLM在计算机架构领域的能力提供了基础，有助于加速计算系统创新，代表了社区在设定LLM架构推理评估标准方面的努力。

Abstract: The field of computer architecture, which bridges high-level software
abstractions and low-level hardware implementations, remains absent from
current large language model (LLM) evaluations. To this end, we present QuArch
(pronounced 'quark'), the first benchmark designed to facilitate the
development and evaluation of LLM knowledge and reasoning capabilities
specifically in computer architecture. QuArch provides a comprehensive
collection of 2,671 expert-validated question-answer (QA) pairs covering
various aspects of computer architecture, including processor design, memory
systems, and interconnection networks. Our evaluation reveals that while
frontier models possess domain-specific knowledge, they struggle with skills
that require higher-order thinking in computer architecture. Frontier model
accuracies vary widely (from 34% to 72%) on these advanced questions,
highlighting persistent gaps in architectural reasoning across analysis,
design, and implementation QAs. By holistically assessing fundamental skills,
QuArch provides a foundation for building and measuring LLM capabilities that
can accelerate innovation in computing systems. With over 140 contributors from
40 institutions, this benchmark represents a community effort to set the
standard for architectural reasoning in LLM evaluation.

</details>


### [3] [RAMAN: Resource-efficient ApproxiMate Posit Processing for Algorithm-Hardware Co-desigN](https://arxiv.org/abs/2510.22627)
*Mohd Faisal Khan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: RAMAN提出了一种基于近似posit(8,2)的MAC架构，通过硬件近似和算法-硬件协同设计，在边缘AI应用中实现资源效率与学习性能的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI应用在资源受限环境下的计算效率挑战，特别是在带宽限制下提升硬件效率。

Method: 提出REAP近似MAC引擎，在posit乘法器中采用近似计算；构建可扩展的向量执行单元(VEU)支持AI工作负载；采用近似感知训练的算法-硬件协同设计框架。

Result: 在FPGA和ASIC平台上验证，REAP MAC相比基准设计节省46% LUT、35.66%面积和31.28%功耗，手写数字识别准确率达98.45%。

Conclusion: RAMAN在硬件效率和学习性能之间实现了有前景的权衡，适用于下一代边缘智能应用。

Abstract: Edge-AI applications still face considerable challenges in enhancing
computational efficiency in resource-constrained environments. This work
presents RAMAN, a resource-efficient and approximate posit(8,2)-based
Multiply-Accumulate (MAC) architecture designed to improve hardware efficiency
within bandwidth limitations. The proposed REAP (Resource-Efficient Approximate
Posit) MAC engine, which is at the core of RAMAN, uses approximation in the
posit multiplier to achieve significant area and power reductions with an
impact on accuracy. To support diverse AI workloads, this MAC unit is
incorporated in a scalable Vector Execution Unit (VEU), which permits hardware
reuse and parallelism among deep neural network layers. Furthermore, we propose
an algorithm-hardware co-design framework incorporating approximation-aware
training to evaluate the impact of hardware-level approximation on
application-level performance. Empirical validation on FPGA and ASIC platforms
shows that the proposed REAP MAC achieves up to 46% in LUT savings and 35.66%
area, 31.28% power reduction, respectively, over the baseline Posit Dot-Product
Unit (PDPU) design, while maintaining high accuracy (98.45%) for handwritten
digit recognition. RAMAN demonstrates a promising trade-off between hardware
efficiency and learning performance, making it suitable for next-generation
edge intelligence.

</details>


### [4] [Approximate Signed Multiplier with Sign-Focused Compressor for Edge Detection Applications](https://arxiv.org/abs/2510.22674)
*L. Hemanth Krishna,Srinivasu Bodapati,Sreehari Veeramachaneni,BhaskaraRao Jammu,Noor Mahammad Sk*

Main category: cs.AR

TL;DR: 提出了一种用于边缘检测的近似有符号乘法器架构，采用符号聚焦压缩器设计，在保持性能的同时显著降低功耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习和信号处理中的边缘检测应用，需要高效处理有符号乘法运算，特别是处理常数"1"和负部分积的频繁出现。

Method: 使用两种符号聚焦压缩器(A+B+C+1和A+B+C+D+1)，结合精确和近似压缩器设计，截断部分积矩阵的下N-1列并采用误差补偿机制。

Result: 8位近似乘法器相比现有最佳乘法器，功耗延迟积(PDP)降低29.21%，功耗降低14.39%。

Conclusion: 该乘法器成功集成到卷积层中实现边缘检测，证明了其在现实应用中的实用价值。

Abstract: This paper presents an approximate signed multiplier architecture that
incorporates a sign-focused compressor, specifically designed for edge
detection applications in machine learning and signal processing. The
multiplier incorporates two types of sign-focused compressors: A + B + C + 1
and A + B + C + D + 1. Both exact and approximate compressor designs are
utilized, with a focus on efficiently handling constant value "1" and negative
partial products, which frequently appear in the partial product matrices of
signed multipliers. To further enhance efficiency, the lower N - 1 columns of
the partial product matrix are truncated, followed by an error compensation
mechanism. Experimental results show that the proposed 8-bit approximate
multiplier achieves a 29.21% reduction in power delay product (PDP) and a
14.39% reduction in power compared to the best of existing multipliers. The
proposed multiplier is integrated into a custom convolution layer and performs
edge detection, demonstrating its practical utility in real-world applications.

</details>
